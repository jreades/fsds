---
title: "Practical 8bis: Working with Text (Part 2)"
subtitle: "The basics of Text Mining and NLP"
embed-resources: True
filters:
  - qna
  - quarto
---

Part 2 of Practical 8 is *optional* and should only be attempted if Part 1 made sense to you.

1. The first few tasks are about finding important vocabulary (think 'keywords' and 'significant terms') in documents so that you can start to think about what is *distinctive* about documents and groups of documents. **This is quite useful and relatively easier to understand than what comes next!**
2. The second part is about fully-fledged NLP using Latent Direclecht Allocation (topic modelling) and Word2Vec (words embeddings for use in clustering or similarity work).

The later parts are largely complete and ready to run; however, that *doesn't* mean you should just skip over them and think you've grasped what's happening and it will be easy to apply in your own analyses. I would *not* pay as much attention to LDA topic mining since I don't think it's results are that good, but I've included it here as it's still commonly-used in the Digital Humanities and by Marketing folks. Word2Vec is much more powerful and forms the basis of the kinds of advances seen in ChatGPT and other LLMs.

::: {.callout-note}

#### &#128279; Connections

Working with text is unquestionably _hard_. In fact, _conceptually_ this is probaly the most challenging practical of the term! But data scientists are _always_ dealing with text because so much of the data that we collect (even more so thanks to the web) is not only text-based (URLs are text!) but, increasingly, unstructured (social media posts, tags, etc.). So while getting to grips with text is a challenge, it also uniquely positions you with respect to the skills and knowledge that other graduates are offering to employers.
:::

# Preamble

Parts of this practical have been written using `nltk` and `gensim` --- both of which I have **removed** from the {{< var docker.name >}} image to keep the size down and because relatively few students used any of this --- but it would be *relatively* easy to rework it to use `spacy`. From what I can see, most programmers tend to use one *or* the other, and the switch wouldn't be hard, other than having to first load the requisite language models. You can [read about the models](https://spacy.io/models/en), and note that they are also [available in other languages](https://spacy.io/usage/models) besides English.

# Setup

::: {.callout-tip collapse="true"}

#### Difficulty Level: Low

But this is only because this has been worked out for you. Starting from sctach in NLP is _hard_ so people try to avoid it as much as possible.

:::

## Required Modules

::: {.callout-note}

Notice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: `sklearn` and `nltk`.

:::

Standard libraries we've seen before.

```{python}
import pandas as pd
import geopandas as gpd
import re
import matplotlib.pyplot as plt

# New, but fairly straightforward (for simple things)
from bs4 import BeautifulSoup
```

Vectorisers we will use from the 'big beast' of Python machine learning: Sci-Kit Learn.

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# We don't use this but I point out where you *could*
from sklearn.preprocessing import OneHotEncoder 
```

NLP-specific libraries that we will use for tokenisation, lemmatisation, and frequency analysis.

```{python}
import nltk

try:
    from nltk.corpus import wordnet as wn
    from nltk.stem.wordnet import WordNetLemmatizer
    from nltk.corpus import stopwords
    stopwords.words('english')
    lemmatizer = WordNetLemmatizer()
    tokenizer = ToktokTokenizer()
except:
    nltk.download("stopwords")
    nltk.download("punkt_tab")
    nltk.download("averaged_perceptron_tagger_eng")
    from nltk.corpus import stopwords
    from nltk.corpus import wordnet as wn
    from nltk.stem.wordnet import WordNetLemmatizer
    stopwords.words('english')

from nltk.corpus import stopwords
stopword_list = set(stopwords.words('english'))

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tokenize.toktok import ToktokTokenizer

from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer

from nltk import ngrams, FreqDist

lemmatizer = WordNetLemmatizer()
tokenizer = ToktokTokenizer()
```

Remaining libraries that we'll use for processing and display text data. Most of this relates to dealing with the various ways that text data cleaning is *hard* because of the myriad formats it comes in.

```{python}
try: 
    from wordcloud import WordCloud
except:
    ! pip install wordcloud
import string
import unicodedata
from wordcloud import WordCloud, STOPWORDS
```

This next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:

```{python}
from IPython.display import display_markdown

def as_markdown(head='', body='Some body text'):
    if head != '':
        display_markdown(f"##### {head}\n\n>{body}\n", raw=True)
    else:
        display_markdown(f">{body}\n", raw=True)

as_markdown('Result!', "Here's my output...")
```

## Loading Data

::: {.callout-note}

#### &#128279; Connections

Because I generally want each practical to stand on its own (unless I'm trying to make a _point_), I've not moved this to a separate Python file (e.g. `utils.py`, but in line with what we covered back in the lectures on [Functions and Packages](https://jreades.github.io/fsds/sessions/week3.html#lectures), this sort of thing is a good candidate for being split out to a separate file to simplify re-use.

:::

Remember this function from last week? We use it to save downloading files that we already have stored locally. But notice I've made some small changes... what do these do to help the user?

{{< include _cache_function.qmd >}}

::: {.callout-tip}

For very large _non_-geographic data sets, remember that you can `use_cols` (or `columns` depending on the file type) to specify a subset of columns to load.

:::

Load the main data set:

```{python}
# Load the data sets created in the previous practical
lux    = gpd.read_parquet(Path('data/clean/luxury.geoparquet'))
aff    = gpd.read_parquet(Path('data/clean/affordable.geoparquet'))
bluesp = gpd.read_parquet(Path('data/clean/bluespace.geoparquet'))
```

# Illustrative Text Cleaning

Now we're going to step through the _parts_ of the process that we apply to clean and transform text. We'll do this individually before using a function to apply them _all at once_. To keep things moving along, we're going to take a small sample from the data set.

```{python}
pd.set_option("display.max_colwidth", 250)
sample = bluesp.description.sample(5, random_state=44)
sample
```

## Removing HTML

::: {.callout-warning collapse="true"}

#### Difficulty level: Moderate

You'll have seen that tehre are some things that are *not* text in the sample, most notably this stuff: `<br>` and `<br />`. That is raw HTML, the markup language for web pages. To go any further we will need to strip this out. Depending on what you needed to extract from a web page you *can* select different elements by name, class, or id, and do a whole bunch of other things, but here we're just going to crudely strip the tags out.

You're also going to see a powerful new method: `apply`. Apply simply runs some function against every row or column in the data set depending on how it's called. In this case, we use a `lambda` function (a function without a formal `def`inition) to create a 'Beautiful Soup' HTML parser and then strip the tags from each row.

:::

*Hint*: you need to need to **get the text** out of the each returned `<p>` and `<div>` element! I'd suggest also commenting this up since there is a *lot* going on on some of these lines of code!

:::: {.qna}

#### Question

```python
cleaned = sample.apply(lambda x: BeautifulSoup(??, 'html.parser').??(" "))
cleaned
```

#### Answer

```{python}
cleaned = sample.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text(" "))
```

::::

```{python}
#| echo: False
cleaned
```

## Lower Case

::: {.callout-tip collapse="true"}

#### Difficulty Level: Low.

:::

:::: {.qna}

#### Question

```python
lower = cleaned.??
lower
```

#### Answer

```{python}
lower = cleaned.str.lower()
```

::::

You should get something like the following:

```{python}
#| echo: False
lower
```

## Stripping 'Punctuation'

::: {.callout-caution collapse="true"}

#### Difficulty level: Hard

This is because you need to understand: 1) why we're _compiling_ the regular expression and how to use character classes; and 2) how the NLTK tokenizer differs in its approach to the regex.

:::

### Regular Expression Approach

We want to clear out punctuation using a regex that takes advantage of the `[...]` (character class) syntax. The really tricky part is remembering how to specify the 'punctuation' when some of that punctuation has 'special' meanings in a regular expression context. For instance, `.` means 'any character', while `[` and `]` mean 'character class'. So this is another *escaping* problem and it works the *same* way it did when we were dealing with the Terminal... 

*Hints*: some other factors... 

1. You will want to match more than one piece of punctuation at a time, so I'd suggest add a `+` to your pattern.
2. You will need to look into *metacharacters* for creating a kind of 'any of the characters *in this class*' bag of possible matches.

:::: {.qna}

#### Question

```python
pat = re.compile(r'[???]+')
subbed = lower.apply(lambda x: pat.sub('',x))
```

#### Answer

```{python}
pat = re.compile(r'[,\.!\-><=\(\)\[\]\/&\'\"’;\+\–\—]+')
subbed = lower.apply(lambda x: pat.sub('',x))
subbed
```

::::

Depending on how thorough you are, you should get something like this:

```{python}
#| echo: False
subbed
```


### Tokenizer

The other way to do this, which is probably *easier*, is to draw on the tokenizers [already provided by NLTK](https://www.nltk.org/api/nltk.tokenize.html). For our purposes `word_tokenize` is probably fine, but depending on your needs there are other options and you can also write your own.

```{python}
nltk.download('punkt')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
tokens = lower.apply(word_tokenize)
tokens
```

::: {.callout-warning}
#### Not Equivalent!

Notice that these aren't giving you the same result!

For our purposes it is easier in the subsequent stages to work with the `list` output from tokenize rather than the string output from `re.sub`, but if you needed to convert tokenise back to a string you could do it this way:

```python
tokens = lower.apply(word_tokenize).apply(lambda x: ' '.join(x))
```

The other way around would be:

```python
lower.apply(lambda x: pat.sub('',x)).str.split(' ')
```

:::

## Stopword Removal

::: {.callout-warning collapse="true"}

#### Difficulty Level: Moderate 

You need to remember how list comprehensions work to use the `stopword_list`.

:::

```{python}
stopword_list = set(stopwords.words('english'))
print(stopword_list)
```

:::: {.qna}

#### Question

```python
passthru = []
for t in tokens[2:4]:
    passthru.append([x for x in t if ?? and len(x) > 1])

for p in passthru:
    as_markdown("Stopwords removed:", p)
```

#### Answer

```{python}
passthru = tokens.apply(lambda x: [t for t in x if t not in stopword_list and len(t) > 1])

passthru
```

::::

## Lemmatisation vs Stemming

::: {.callout-tip collapse="true"}

#### Difficulty level: Low.

:::

```{python}
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer 
```

```{python}
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize('monkeys'))
print(lemmatizer.lemmatize('cities'))
print(lemmatizer.lemmatize('complexity'))
print(lemmatizer.lemmatize('Reades'))
```

```{python}
stemmer = SnowballStemmer(language='english')
print(stemmer.stem('monkeys'))
print(stemmer.stem('cities'))
print(stemmer.stem('complexity'))
print(stemmer.stem('Reades'))
```

```{python}
lemmatizer = WordNetLemmatizer()
stemmer    = SnowballStemmer(language='english')

# This would be better if we passed in a PoS (Part of Speech) tag as well,
# but processing text for parts of speech is *expensive* and for the purposes
# of this tutorial, not necessary.
lemmas  = passthru.apply(lambda x: [lemmatizer.lemmatize(t) for t in x])
stemmed = passthru.apply(lambda x: [stemmer.stem(t) for t in x])

as_markdown("Lemmatised", '\n\n>'.join(lemmas.str.join(' ').to_list()))
print("\n\n")
as_markdown("Stemmed", '\n\n>'.join(stemmed.str.join(' ').to_list()))
```

# Applying Normalisation

The above approach is fairly hard going since you need to loop through every list element applying these changes one at a time. Instead, we could convert the column to a corpus (or use pandas `apply`) together with a function imported from a library to do the work.

## Downloading the Custom Module

In a Jupyter notebook, this code allows us to edit and reload the library dynamically:

```python
%load_ext autoreload
%autoreload 2
```

::: {.callout-tip collapse="true"}

#### Difficulty level: Low.

:::

This custom module is not perfect, but it gets the job done... mostly and has some additional features that you could play around with for a final project (e.g. `detect_entities` and `detect_acronyms`).

```{python}
try: 
    from textual import *
except:
    try:
        from unidecode import unidecode
    except:
        ! pip install unidecode
    import urllib.request
    host  = 'https://orca.casa.ucl.ac.uk'
    turl  = f'{host}/~jreades/__textual__.py'
    tdirs = Path('textual')
    tpath = Path(tdirs,'__init__.py')

    if not tpath.exists():
        tdirs.mkdirs(parents=True, exist_ok=True)
        urllib.request.urlretrieve(turl, tpath)
    from textual import *
```

## Importing the Custom Module

::: {.callout-tip collapse="true"}

#### Difficulty Level: Low. 

But only because you didn't have to write the module! However, the questions could be hard...

:::

```{python}
normalised = sample.apply(normalise_document, remove_digits=True)
as_markdown('Normalised', '\n\n>'.join(normalised.to_list()))
```

```python
help(normalise_document)
```

::: {.callout-caution}

#### Stop!

Beyond this point, we are moving into Natural Language Processing. If you are already struggling with regular expressions, I would recommend _stopping here_. You can come back to revisit the NLP components and creation of word clouds later.

:::

# Process the Selected Listings

::: {.callout-tip collapse="true"}

#### Difficulty level: Low, but you'll need to be patient!

:::

Notice the use of `%%time` here -- this will tell you how long each block of code takes to complete. It's a really useful technique for reminding *yourself* and others of how long something might take to run. I find that with NLP this is particularly important since you have to do a *lot* of processing on each document in order to normalise it.

::: {.callout-tip}

Notice how we can change the default parameters for `normalise_document` even when using `apply`, but that the syntax is different. So whereas we'd use `normalise_document(doc, remove_digits=True)` if calling the function directly, here it's `.apply(normalise_document, remove_digits=True)`!

:::

:::: {.qna}

#### Question

```python
%%time  
# I get about 2 seconds on a M2 Mac
bluesp['description_norm'] = bluesp.???.apply(???, remove_digits=True)
```

#### Answer

```{python}
%%time
# I get about 2 seconds on a M2 Mac
bluesp['description_norm'] = bluesp.description.apply(normalise_document, remove_digits=True)
```

::::

## Select and Tokenise

::: {.callout-tip collapse="true"}

#### Difficulty level: Low, except for the double list-comprehension.

:::

### Select and Extract Corpus

See useful tutorial [here](https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275). Although we shouldn't have any empty descriptions, by the time we've finished normalising the textual data we may have _created_ some empty values and we need to ensure that we don't accidentally pass a NaN to the vectorisers and frequency distribution functions.

```{python}
# This makes it easy to go back and re-run
# the analysis with other data subsets.
srcdf = bluesp 
```

::: {.callout-tip}

#### Coding Tip

Notice how you only need to change the value of the variable here to try any of the different selections we did above? This is a simple kind of parameterisation somewhere between a function and hard-coding everything.

:::

```{python}
corpus = srcdf.description_norm.fillna(' ').values
print(corpus[0:3])
```

### Tokenise

There are different forms of tokenisation and different algorithms will expect differing inputs. Here are two:

```{python}
sentences = [nltk.sent_tokenize(text) for text in corpus]
words     = [[nltk.tokenize.word_tokenize(sentence) 
                  for sentence in nltk.sent_tokenize(text)] 
                  for text in corpus]
```

Notice how this has turned every sentence into an array and each document into an array of arrays:

```{python}
print(f"Sentences 0: {sentences[0]}")
print()
print(f"Words 0: {words[0]}")
```

# Frequencies and Ngrams

::: {.callout-warning collapse="true"}

#### Difficulty level: Moderate.

:::

One new thing you'll see here is the `ngram`: ngrams are 'simply' pairs, or triplets, or quadruplets of words. You may come across the terms unigram (`ngram(1,1)`), bigram (`ngram(2,2)`), trigram (`ngram(3,3)`)... typically, you will rarely find anything beyond trigrams, and these present real issues for text2vec algorithms because the embedding for `geographical`, `information`, and `systems` is _not_ the same as for `geographical information systetms`.

### Build Frequency Distribution

Build counts for ngram range 1..3:

```{python}
fcounts = dict()

# Here we replace all full-stops... can you think why we might do this?
data = nltk.tokenize.word_tokenize(' '.join([text.replace('.','') for text in corpus]))

for size in 1, 2, 3:
    fdist = FreqDist(ngrams(data, size))
    print(fdist)
    # If you only need one note this: https://stackoverflow.com/a/52193485/4041902
    fcounts[size] = pd.DataFrame.from_dict({f'Ngram Size {size}': fdist})
```

### Output Top-n Ngrams

And output the most common ones for each ngram range:

```{python}
for dfs in fcounts.values():
    print(dfs.sort_values(by=dfs.columns.values[0], ascending=False).head(10))
    print()
```

:::: {.qna}

#### Questions

1. Can you think why we don't care about punctuation for frequency distributions and n-grams?
2. Do you understand what n-grams *are*?

#### Answers

1. We are moving away from thinking about text in the way that a human would and are simply looking for statistical patterns in documents. So punctuation no longer matters if, statistically, there is a significant repetition of one **token** preceding another (even across a sentence boundary)! Indeed, for the 'bag of words' approach we will no longer even care about word order in the document!
2. Yes, they are just repeated singles/pairs/triplets of **tokens**. Notice that we're no longer even talking about words. Just sequences of characters at some scale. N-grams may well map on to common terms like "Transport for London" or "Geographic Information Science" but they could also be 'random' in the sense of "minute walk" or "cuddly cat".

::::

# Count Vectoriser

::: {.callout-tip collapse="true"}

#### Difficulty level: Low, but the output needs some thought!

:::

This is a big foray into sklearn (sci-kit learn) which is the main machine learning and clustering module for Python. For processing text we use a *vectorisers* to convert terms to a vector representation. We're doing this on the smallest of the derived data sets because these processes can take a while to run and generate *huge* matrices (remember: one row and one column for each term!).

## Fit the Vectoriser

```{python}
cvectorizer = CountVectorizer(ngram_range=(1,3))
cvectorizer.fit(corpus)
```

## Brief Demonstration

Once we've fit the vectoriser, we can find the integer associated with any word in the corpus and can also ask how many times it occurs:

```{python}
pd.options.display.max_colwidth=750

# Note that the term can be a unigram, a bigram, or a trigram!
term = 'minute walk'
# Find the vocabulary mapping for the term
print(f"Vocabulary mapping for '{term}' is {cvectorizer.vocabulary_[term]}")

# How many times is it in the data
print(f"Found {srcdf.description_norm.str.contains(term).sum():,} rows containing '{term}'")

# Print the descriptions containing the term
for i, x in enumerate(srcdf[srcdf.description_norm.str.contains(term)].description_norm.to_list()):
    as_markdown(f"{term}: {i}",x)
    if i > 4: break
```

## Transform the Corpus 

You can only *tranform* the entire corpus *after* the vectoriser has been fitted. There is an option to `fit_transform` in one go, but I wanted to demonstrate a few things here and some vectorisers are don't support the one-shot fit-and-transform approach. **Note the type of the transformed corpus**:

```{python}
cvtcorpus = cvectorizer.transform(corpus)
cvtcorpus # cvtcorpus for count-vectorised transformed corpus
```

### Single Document

Here is the **first** document from the corpus:

```{python}
doc_df = pd.DataFrame(cvtcorpus[0].T.todense(), 
                      index=cvectorizer.get_feature_names_out(), columns=["Counts"]
                     ).sort_values('Counts', ascending=False)
doc_df.head(10)
```

### Transformed Corpus

```{python}
cvdf = pd.DataFrame(data=cvtcorpus.toarray(),
                        columns=cvectorizer.get_feature_names_out())
print(f"Raw count vectorised data frame has {cvdf.shape[0]:,} rows and {cvdf.shape[1]:,} columns.")
cvdf.iloc[0:3,0:7]
```

```{python}
cvdf.iloc[-3:,-7:]
```

:::: {.qna}

#### Questions

1. Why is the [single document](#single-document) view a list of terms and counts?
2. Why is the [corpus view](#transformed-corpus) a table of terms (columns) and integers (rows)?

#### Answers

1. The [single document](#single-document) view is a 'bag of words' because the count vectoriser has thrown out all other information about the documents. So we just have a frequency count for each word *in* each document.
2. The [corpus view](#transformed-corpus) can be thought of as a kind of massive dummy variable table: each unigram, bigram, and trigram has an entry in the column lists. Each document has an entry in the rows.

::::

### Filter Low-Frequency Words

These are likely to be artefacts of text-cleaning or human input error. As well, if we're trying to look across an entire corpus then we might not want to retain words that only appear in a couple of documents.

Let's start by getting the *column* sums:

```{python}
sums = cvdf.sum(axis=0)
print(f"There are {len(sums):,} terms in the data set.")
sums.head()
```

Remove columns (i.e. terms) appearing in less than 1% of documents. You can do this by thinking about what the shape of the data frame means (rows and/or columns) and how you'd get 1% of that!

:::: {.qna}

#### Question

```python
filter_terms = sums >= cvdf.shape[0] * ???
```

#### Answer

```{python}
filter_terms = sums >= cvdf.shape[0] * 0.01
```

::::

Now see how we can use this to strip out the columns corresponding to low-frequency terms:

```{python}
fcvdf = cvdf.drop(columns=cvdf.columns[~filter_terms].values)
print(f"Filtered count vectorised data frame has {fcvdf.shape[0]:,} rows and {fcvdf.shape[1]:,} columns.")
fcvdf.iloc[0:3,0:7]
```

```{python}
fcvdf.sum(axis=0)
```

We're going to pick this up again in Task 7.

#### Questions

- Can you explain what `doc_df` contains?
- What does `cvdf` contain? Explain the rows and columns.
- What is the function of `filter_terms`?

# TF/IDF Vectoriser

::: {.callout-warning collapse="true"}

#### Difficulty level: Moderate

But only if you want to understand how `max_df` and `min_df` work!

:::

## Fit and Transform

```{python}
tfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), 
                               max_df=0.75, min_df=0.01) # <-- these matter!
tftcorpus    = tfvectorizer.fit_transform(corpus) # TF-transformed corpus
```

## Single Document

```{python}
doc_df = pd.DataFrame(tftcorpus[0].T.todense(), index=tfvectorizer.get_feature_names_out(), columns=["Weights"])
doc_df.sort_values('Weights', ascending=False).head(10)
```

## Transformed Corpus

```{python}
tfidf = pd.DataFrame(data=tftcorpus.toarray(),
                        columns=tfvectorizer.get_feature_names_out())
print(f"TF/IDF data frame has {tfidf.shape[0]:,} rows and {tfidf.shape[1]:,} columns.")
tfidf.head()
```

#### Questions

- What does the TF/IDF score *represent*?
- What is the role of `max_df` and `min_df`?

# Word Clouds

## For Counts

::: {.callout-tip collapse="true"}

#### Difficulty level: Easy!

:::

```{python}
fcvdf.sum().sort_values(ascending=False)
```

```{python}
ff = Path('RobotoSlab-Thin.ttf')
dp = Path('/home/jovyan/.fonts/')
tp = Path(Path.home(),'Library','Fonts')
if tp.exists():
    fp = tp / ff
else:
    fp = dp / ff
```

```{python}
f,ax = plt.subplots(1,1,figsize=(5,5))
plt.gcf().set_dpi(300)
Cloud = WordCloud(
    background_color="white", 
    max_words=75,
    font_path=fp
).generate_from_frequencies(fcvdf.sum())
ax.imshow(Cloud) 
ax.axis("off");
#plt.savefig("Wordcloud 1.png")
```

## For TF/IDF Weighting

::: {.callout-tip collapse="true"}

#### Difficulty level: Low, but you'll need to be patient!

:::

```{python}
tfidf.sum().sort_values(ascending=False)
```

```{python}
f,ax = plt.subplots(1,1,figsize=(5,5))
plt.gcf().set_dpi(300)
Cloud = WordCloud(
    background_color="white", 
    max_words=100,
    font_path=fp
).generate_from_frequencies(tfidf.sum())
ax.imshow(Cloud) 
ax.axis("off");
#plt.savefig("Wordcloud 2.png")
```

#### Questions

- What does the `sum` represent for the count vectoriser?
- What does the `sum` represent for the TF/IDF vectoriser?

# Latent Dirchlet Allocation

::: {.callout-tip}

I would give this a _low_ priority. It's a commonly-used method, but on small data sets it really isn't much use and I've found its answers to be... unclear... even on large data sets.

:::

Adapted from [this post](https://stackabuse.com/python-for-nlp-topic-modeling/) on doing LDA using sklearn. Most other examples use the `gensim` library.

```{python}
# Notice change to ngram range 
# (try 1,1 and 1,2 for other options)
vectorizer = CountVectorizer(ngram_range=(1,2))
```

## Calculate Topics

```{python}
vectorizer.fit(corpus) 
tcorpus = vectorizer.transform(corpus) # tcorpus for transformed corpus

LDA = LatentDirichletAllocation(n_components=3, random_state=42) # Might want to experiment with n_components too
LDA.fit(tcorpus)
```

```{python}
first_topic = LDA.components_[0]
top_words = first_topic.argsort()[-25:]

for i in top_words:
    print(vectorizer.get_feature_names_out()[i])
```

```{python}
for i,topic in enumerate(LDA.components_):
    as_markdown(f'Top 10 words for topic #{i}', ', '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]]))
```

## Maximum Likelihood Topic

```{python}
topic_values = LDA.transform(tcorpus)
topic_values.shape
```

```{python}
pd.options.display.max_colwidth=20
srcdf['Topic'] = topic_values.argmax(axis=1)
srcdf.head()
```

```{python}
pd.options.display.max_colwidth=75
srcdf[srcdf.Topic==1].description_norm.head(10)
```

```{python}
vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', analyzer='word', max_df=0.7, min_df=0.05)
topic_corpus = vectorizer.fit_transform(srcdf[srcdf.Topic==1].description.values) # tcorpus for transformed corpus
```

```{python}
topicdf = pd.DataFrame(data=topic_corpus.toarray(),
                        columns=vectorizer.get_feature_names_out())
```

```{python}
f,ax = plt.subplots(1,1,figsize=(5,5))
plt.gcf().set_dpi(300)
Cloud = WordCloud(
            background_color="white", 
            max_words=75).generate_from_frequencies(topicdf.sum())
ax.imshow(Cloud) 
ax.axis("off");
# plt.savefig('Wordcloud 3.png')
```

# Word2Vec

::: {.callout-tip}

This algorithm works almost like magic. You should play with the configuration parameters and see how it changes your results. **However**, at this time I've not preinstalled `gensim` as it bloated the image substantially.

:::

## Configure

```python
from gensim.models.word2vec import Word2Vec
```

```python
dims = 100
print(f"You've chosen {dims} dimensions.")

window = 4
print(f"You've chosen a window of size {window}.")

min_v_freq  = 0.01 # Don't keep words appearing less than 1% frequency
min_v_count = math.ceil(min_v_freq * srcdf.shape[0])
print(f"With a minimum frequency of {min_v_freq} and {srcdf.shape[0]:,} documents, minimum vocab frequency is {min_v_count:,}.")
```

## Train

```python
%%time 

corpus      = srcdf.description_norm.fillna(' ').values
#corpus_sent = [nltk.sent_tokenize(text) for text in corpus] # <-- with more formal writing this would work well
corpus_sent = [d.replace('.',' ').split(' ') for d in corpus] # <-- deals better with many short sentences though context may end up... weird
model       = Word2Vec(sentences=corpus_sent, vector_size=dims, window=window, epochs=200, 
                 min_count=min_v_count, seed=42, workers=1)

#model.save(f"word2vec-d{dims}-w{window}.model") # <-- You can then Word2Vec.load(...) which is useful with large corpora
```

## Explore Similarities

This next bit of code only runs if you have calculated the frequencies above in the [Frequencies and Ngrams](#frequencies-and-ngrams) section.

```python
pd.set_option('display.max_colwidth',150)

df = fcounts[1] # <-- copy out only the unigrams as we haven't trained anything else

n     = 14 # number of words
topn  = 7  # number of most similar words

selected_words = df[df['Ngram Size 1'] > 5].reset_index().level_0.sample(n, random_state=42).tolist()

words = []
v1    = []
v2    = []
v3    = []
sims  = []

for w in selected_words:
    try: 
        vector = model.wv[w]  # get numpy vector of a word
        #print(f"Word vector for '{w}' starts: {vector[:5]}...")
    
        sim = model.wv.most_similar(w, topn=topn)
        #print(f"Similar words to '{w}' include: {sim}.")
    
        words.append(w)
        v1.append(vector[0])
        v2.append(vector[1])
        v3.append(vector[2])
        sims.append(", ".join([x[0] for x in sim]))
    except KeyError:
        print(f"Didn't find {w} in model. Can happen with low-frequency terms.")
    
vecs = pd.DataFrame({
    'Term':words,
    'V1':v1, 
    'V2':v2, 
    'V3':v3,
    f'Top {topn} Similar':sims
})

vecs
```

```python
#print(model.wv.index_to_key) # <-- the full vocabulary that has been trained
```

## Apply

We're going to make *use* of this further next week...

### Questions

- What happens when *dims* is very small (e.g. 25) or very large (e.g. 300)?
- What happens when *window* is very small (e.g. 2) or very large (e.g. 8)?

# Processing the Full File

::: {.callout-caution}

This code can take *some time* (**&gt; 5 minutes on a M2 Mac**) to run, so **don't run this** until you've understood what we did before!

:::

You will get a warning about `"." looks like a filename, not markup` — this looks a little scary, but is basically suggesting that we have a description that consists only of a '.' or that looks like some kind of URL (which the parser thinks means you're trying to pass it something to download). 

```python
%%time 
# This can take up to 8 minutes on a M2 Mac
gdf['description_norm'] = ''
gdf['description_norm'] = gdf.description.apply(normalise_document, remove_digits=True, special_char_removal=True)
```

```python
gdf.to_parquet(os.path.join('data','geo',f'{fn.replace(".","-with-nlp.")}'))
```

::: {.callout-tip}

Saving an intermediate file at this point is useful because you've done quite a bit of _expensive_ computation. You _could_ restart-and-run-all and then go out for the day, but probably easier to just save this output and then, if you need to restart your analysis at some point in the future, just remember to deserialise amenities back into a list format.

:::

## Applications

The above is _still_ only the results for the one of the subsets of apartments _alone_. At this point, you would probably want to think about how your results might change if you changed any of the following:

1. Using one of the other data sets that we created, or even the entire data set!
2. Applying the CountVectorizer or TfidfVectorizer _before_ selecting out any of our 'sub' data sets.
3. Using the visualisation of information to improve our regex selection process.
4. Reducing, increasing, or constraining (i.e. `ngrams=(2,2)`) the size of the ngrams while bearing in mind the impact on processing time and interpretability.
5. Filtering by type of listing or host instead of keywords found in the description (for instance, what if you applied TF/IDF to the entire data set and then selected out 'Whole Properties' before splitting into those advertised by hosts with only one listing vs. those with multiple listings?).
6. Linking this back to the geography.

Over the next few weeks we'll also consider alternative means of visualising the data!

## Resources

There is a lot more information out there, including a [whole book](https://www.nltk.org/book/) and your standard [O'Reilly text](http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf).

And some more useful links:

- [Pandas String Contains Method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html)
- [Using Regular Expressions with Pandas](https://kanoki.org/2019/11/12/how-to-use-regex-in-pandas/)
- [Summarising Chapters from Frankenstein using TF/IDF](https://towardsdatascience.com/using-tf-idf-to-form-descriptive-chapter-summaries-via-keyword-extraction-4e6fd857d190)
