{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preamble\n",
        "\n",
        "    <h1 style=\"width:450px\">Practical 10: Grouping Data</h1>\n",
        "    <h2 style=\"width:450px\">Classification &amp; Clusters</h2>\n",
        "\n",
        ":::\n",
        "\n",
        "<img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" />\n",
        "\n",
        ":::\n",
        "\n",
        "A common challenge in data analysis is how to group observations in a\n",
        "data set together in a way that allows for generalisation: *this* group\n",
        "of observations are similar to one another, *that* group is dissimilar\n",
        "to this group. But what defines similarity and difference? There is no\n",
        "*one* answer to that question and so there are many different ways to\n",
        "cluster data, each of which has strengths and weaknesses that make them\n",
        "more, or less, appropriate in different contexts.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        ">     **&#128279; Connections**: \n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib as mpl\n",
        "import re\n",
        "import os\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# All of these are potentially useful, though\n",
        "# not all have been used in this practical --\n",
        "# I'd suggest exploring the use of different \n",
        "# Scalers/Transformers as well as clustering \n",
        "# algorithms...\n",
        "import sklearn\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
        "from esda.adbscan import ADBSCAN\n",
        "\n",
        "import random\n",
        "random.seed(42)    # For reproducibility\n",
        "np.random.seed(42) # For reproducibility\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
        "```\n",
        "\n",
        "##### 0.0.1 Initialise the Scalers and Transformers\n",
        "\n",
        "``` python\n",
        "rbs = RobustScaler(quantile_range=[0.025,0.975])\n",
        "mms = MinMaxScaler(feature_range=(-1,1))\n",
        "pts = PowerTransformer()\n",
        "```\n",
        "\n",
        "##### 0.0.2 Set Up Functions for Plotting\n",
        "\n",
        "``` python\n",
        "def plt_ldn(w, b):\n",
        "    fig, ax = plt.subplots(1, figsize=(14, 12))\n",
        "    w.plot(ax=ax, color='#79aef5', zorder=2)\n",
        "    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n",
        "    ax.set_xlim([502000,563000])\n",
        "    ax.set_ylim([155000,201500])\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    return fig, ax\n",
        "\n",
        "########################\n",
        "# These may no longer be relevant because of changes to geopandas API\n",
        "\n",
        "def default_cmap(n, outliers=False):\n",
        "    cmap = mpl.cm.get_cmap('viridis_r', n)\n",
        "    colors = cmap(np.linspace(0,1,n))\n",
        "    if outliers:\n",
        "        gray = np.array([225/256, 225/256, 225/256, 1])\n",
        "        colors = np.insert(colors, 0, gray, axis=0)\n",
        "    return ListedColormap(colors)\n",
        "\n",
        "# mappable = ax.collections[-1] if you add the geopandas\n",
        "# plot last.\n",
        "def add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n",
        "    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n",
        "                    boundaries=breaks,\n",
        "                    extend=('min' if outliers else 'neither'), \n",
        "                    spacing='uniform',\n",
        "                    orientation='horizontal',\n",
        "                    fraction=0.05, shrink=0.5, pad=0.05)\n",
        "    cb.set_label(\"Cluster Number\")\n",
        "```\n",
        "\n",
        "### 1 Load Data\n",
        "\n",
        "##### 1.0.1 Load Water and Boroughs\n",
        "\n",
        "``` python\n",
        "# Load Water GeoPackage\n",
        "w_path = os.path.join('data','geo','Water.gpkg')\n",
        "if not os.path.exists(w_path):\n",
        "    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n",
        "    water.to_file(w_path)\n",
        "    print(\"Downloaded Water.gpkg file.\")\n",
        "else:\n",
        "    water = gpd.read_file(w_path)\n",
        "\n",
        "# Boroughs GeoPackage\n",
        "b_path = os.path.join('data','geo','Boroughs.gpkg')\n",
        "if not os.path.exists(b_path):\n",
        "    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n",
        "    boroughs.to_file(b_path)\n",
        "    print(\"Downloaded Boroughs.gpkg file.\")\n",
        "else:\n",
        "    boroughs = gpd.read_file(b_path)\n",
        "```\n",
        "\n",
        "##### 1.0.2 Load the MSOA GeoData\n",
        "\n",
        "You should have this locally…\n",
        "\n",
        "``` python\n",
        "msoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\n",
        "msoas = msoas.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm'])\n",
        "```\n",
        "\n",
        "##### 1.0.3 Load the Listings Data\n",
        "\n",
        "Feel free to load your local copy rather than loading via the URL:\n",
        "\n",
        "``` python\n",
        "url = 'https://github.com/jreades/i2p/blob/master/data/clean/2020-08-24-listings.csv.gz?raw=true'\n",
        "df = pd.read_csv(url, compression='gzip', low_memory=False, \n",
        "                 usecols=['room_type','longitude','latitude','price'])\n",
        "print(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n",
        "```\n",
        "\n",
        "You should have: `Data frame is 74,120 x 4`.\n",
        "\n",
        "``` python\n",
        "gdf = gpd.GeoDataFrame(df, \n",
        "      geometry=gpd.points_from_xy(?, ?, crs=?))\n",
        "gdf = gdf.to_crs(?)\n",
        "```\n",
        "\n",
        "``` python\n",
        "gdf.plot(column='price', cmap='plasma', scheme='quantiles', k=10, markersize=1, figsize=(8,6));\n",
        "```\n",
        "\n",
        "##### 1.0.4 Aggregate Listings by MSOA\n",
        "\n",
        "Next, use a spatial join to link the listings to MSOAs.\n",
        "\n",
        "``` python\n",
        "# ml == MSOA Listings\n",
        "ml = gpd.sjoin(??).drop(\n",
        "                    columns=['latitude','longitude','index_right']\n",
        ")\n",
        "```\n",
        "\n",
        "Now we want a count of room types by MSOA:\n",
        "\n",
        "``` python\n",
        "mlgc = ml.groupby([??])['price'].agg(Count='??').reset_index() # msoa listings grouped counts\n",
        "mlgc.head()\n",
        "```\n",
        "\n",
        "You should get:\n",
        "\n",
        "|     | MSOA11CD  | room_type       | Count |\n",
        "|----:|:----------|:----------------|------:|\n",
        "|   0 | E02000001 | Entire home/apt |   398 |\n",
        "|   1 | E02000001 | Hotel room      |     4 |\n",
        "|   2 | E02000001 | Private room    |    48 |\n",
        "|   3 | E02000001 | Shared room     |     2 |\n",
        "|   4 | E02000002 | Private room    |    10 |\n",
        "\n",
        "And let’s also get the median price by MSOA…\n",
        "\n",
        "``` python\n",
        "mlgp = ml.groupby('MSOA11CD')['price'].agg(??).reset_index() # msoa listings grouped price\n",
        "mlgp.head()\n",
        "```\n",
        "\n",
        "For comparison purposes:\n",
        "\n",
        "|     | MSOA11CD  |  price |\n",
        "|----:|:----------|-------:|\n",
        "|   0 | E02000001 | 125.00 |\n",
        "|   1 | E02000002 |  50.00 |\n",
        "|   2 | E02000003 |  40.00 |\n",
        "|   3 | E02000004 |  29.00 |\n",
        "|   4 | E02000005 |  69.00 |\n",
        "\n",
        "The highest median price is \\$1,880, which implies not just heavy skew\n",
        "but something a bit out-of-this-world.\n",
        "\n",
        "*Warning*: I have a strong suspicion that price is *so* skewed that\n",
        "using it in most clustering contexts is almost impossible. This is why I\n",
        "use the PowerTransformer below.\n",
        "\n",
        ":::\n",
        "\n",
        "Now we can make use of the pivot table function to generate counts by\n",
        "MSOA in ‘wide’ format. I’d suggest using the empty cell below to work\n",
        "out the pivot table function first before assigning it back to the\n",
        "`mlgc` variable and overwriting the existing data.\n",
        "\n",
        "``` python\n",
        "#mlgc.pivot(index=??, columns=??, values=??).reset_index()\n",
        "```\n",
        "\n",
        "|  | MSOA11CD |  |  |  | Count |\n",
        "|-----:|:------------------|-----------:|-----------:|-----------:|-----------:|\n",
        "| **room_type** |  | **Entire home/apt** | **Hotel room** | **Private room** | **Shared room** |\n",
        "| 0 | E02000001 | 398.00 | 4.00 | 48.00 | 2.00 |\n",
        "| 1 | E02000002 | nan | nan | 10.00 | nan |\n",
        "| 2 | E02000003 | 5.00 | nan | 19.00 | nan |\n",
        "\n",
        "``` python\n",
        "mlgc = mlgc.pivot().reset_index()\n",
        "mlgc.columns = ['MSOA11CD','Entire home/apt','Hotel room','Private room','Shared room']\n",
        "mlgc.drop(columns=['Hotel room','Shared room'], inplace=True)\n",
        "mlgc.head()\n",
        "```\n",
        "\n",
        "Merge the grouped counts and price back to a `mlg` (MSOA Listings\n",
        "Grouped) data frame:\n",
        "\n",
        "``` python\n",
        "mlg = pd.merge(mlgc, mlgp, on=??)\n",
        "mlg = mlg.fillna(0).set_index('MSOA11CD')\n",
        "mlg.head()\n",
        "```\n",
        "\n",
        "Apply a PowerTransform to every column using a loop and `fit_transform`:\n",
        "\n",
        "``` python\n",
        "for c in mlg.columns.values:\n",
        "    mlg[c] = ??\n",
        "mlg.head(3)\n",
        "```\n",
        "\n",
        "For comparison:\n",
        "\n",
        "|              | Entire home/apt | Private room | price |\n",
        "|:-------------|----------------:|-------------:|------:|\n",
        "| **MSOA11CD** |                 |              |       |\n",
        "| E02000001    |            2.12 |         0.81 |  1.81 |\n",
        "| E02000002    |           -1.99 |        -0.76 | -0.21 |\n",
        "| E02000003    |           -0.66 |        -0.16 | -0.83 |\n",
        "\n",
        "``` python\n",
        "sns.set(style=\"white\")\n",
        "sns.pairplot(mlg, markers=\".\", height=3, diag_kind='kde');\n",
        "```\n",
        "\n",
        "##### 1.0.5 Load the Reduced Dimension Data from Week 9\n",
        "\n",
        "``` python\n",
        "rddf = gpd.read_file(os.path.join('data','clean','Reduced_Dimension_Data.gpkg'), driver='GPKG').set_index('MSOA11CD')\n",
        "rddf.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm','Subregion',\n",
        "                   'geometry','Dimension 1','Dimension 2'], inplace=True)\n",
        "print(f\"Reduced Dimensionality Data Frame is {rddf.shape[0]:,} x {rddf.shape[1]:,}\")\n",
        "```\n",
        "\n",
        "You should get `Reduced Dimensionality Data Frame is 983 x 10`.\n",
        "\n",
        "Merge the `rddf` and `mlg` files using the MSOA11CD and rescale all\n",
        "columns using MinMaxScaler:\n",
        "\n",
        "``` python\n",
        "# Merge the reducded dimensionality data frame with the msoa listings grouped data frame\n",
        "cldf = pd.merge(??)\n",
        "\n",
        "# Rescale the columns using the MinMaxScaler set up in the Preamble\n",
        "for c in cldf.columns:\n",
        "    cldf[c] = ??\n",
        "\n",
        "# We also create a 'result set' (rs) data frame to hold the \n",
        "# results of the clustering operation\n",
        "rs   = pd.merge(msoas, cldf, left_on='MSOA11CD', right_index=True).set_index('MSOA11CD')\n",
        "\n",
        "# Check output\n",
        "cldf.head(2)[['Component 1','Component 2','price']]\n",
        "```\n",
        "\n",
        "A partial sample:\n",
        "\n",
        "| MSOA11CD  | Component 1 | Component 2 | price |\n",
        "|:----------|------------:|------------:|------:|\n",
        "| E02000001 |       -0.08 |        0.12 |  0.25 |\n",
        "| E02000002 |       -0.88 |       -0.33 | -0.42 |\n",
        "\n",
        "##### 1.0.6 Select columns to plot\n",
        "\n",
        "Plotting all columns is computationally costly and for little utility…\n",
        "it’s better to just select a few at random.\n",
        "\n",
        "``` python\n",
        "cols_to_plot = np.random.choice(cldf.columns.values, 4, replace=False)\n",
        "print(cols_to_plot)\n",
        "```\n",
        "\n",
        "### 2 First K-Means Clustering\n",
        "\n",
        "``` python\n",
        "c_nm   = 'KMeans' # Clustering name\n",
        "k_pref = 3 # Number of clusters\n",
        "\n",
        "kmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n",
        "\n",
        "print(kmeans.labels_) # The results\n",
        "```\n",
        "\n",
        "Now capture the labels (i.e. clusters) and write them to a data series\n",
        "that we store on the result set df (`rs`):\n",
        "\n",
        "``` python\n",
        "# Add it to the data frame\n",
        "rs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n",
        "```\n",
        "\n",
        "``` python\n",
        "# How are the clusters distributed?\n",
        "ax = sns.histplot(data=rs, x=c_nm, bins=k_pref);\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Going to be a bit hard to read if \n",
        "# we plot every variable against every\n",
        "# other variables, so we'll just pick a few\n",
        "sns.set(style=\"white\")\n",
        "sns.pairplot(rs, \n",
        "             vars=cols_to_plot, \n",
        "             hue=c_nm, markers=\".\", height=3, diag_kind='kde');\n",
        "```\n",
        "\n",
        "And here’s a map!\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn(water, boroughs)\n",
        "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
        "rs.plot(column=??, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n",
        "```\n",
        "\n",
        "*Stop*: What critical assumption did we make when running this analysis?\n",
        "\n",
        ":::\n",
        "\n",
        "### 3 Second K-Means Clustering\n",
        "\n",
        "##### 3.0.1 The ‘Right’ Number of Clusters\n",
        "\n",
        "There’s more than one way to find the ‘right’ number of clusters. In\n",
        "Singleton’s *Geocomputation* chapter they use WCSS to pick the ‘optimal’\n",
        "number of clusters. The idea is that you plot the average WCSS for each\n",
        "number of possible clusters in the range of interest (2…n) and then look\n",
        "for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is\n",
        "that you look for the point where there is declining benefit from adding\n",
        "more clusters. The problem is that there is always some benefit to\n",
        "adding more clusters (the perfect clustering is k==n), so you don’t\n",
        "always see a knee.\n",
        "\n",
        "Another way to try to make the process of selecting the number of\n",
        "clusters a little less arbitrary is called the silhouette plot and (like\n",
        "WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome\n",
        "by examining the distance between each observation and the rest of the\n",
        "cluster. In this case it’s based on Partitioning Around the Medoid\n",
        "(PAM).\n",
        "\n",
        "Either way, to evaluate this in a systematic way, we want to do multiple\n",
        "k-means clusterings for multiple values of k and then we can look at\n",
        "which gives the best results…\n",
        "\n",
        "Let’s try clustering across a wider range.\n",
        "\n",
        "``` python\n",
        "# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for k in range(??,??):\n",
        "    \n",
        "    #############\n",
        "    # Do the clustering using the main columns\n",
        "    kmeans = KMeans(??).fit(cldf)\n",
        "    \n",
        "    # Calculate the overall silhouette score\n",
        "    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n",
        "    \n",
        "    y.append(k)\n",
        "    x.append(silhouette_avg)\n",
        "    \n",
        "    print('.', end='')\n",
        "\n",
        "print()\n",
        "print(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n",
        "\n",
        "plt.plot(y, x)\n",
        "plt.gcf().suptitle(\"Average Silhouette Scores\");\n",
        "```\n",
        "\n",
        "We can use the largest average silhouette score to determine the\n",
        "‘natural’ number of clusters in the data, but that that’s only if we\n",
        "don’t have any kind of underlying theory, other empirical evidence, or\n",
        "even just a reason for choosing a different value… Again, we’re now\n",
        "getting in areas where your judgement and your ability to communicate\n",
        "your rationale to readers is the key thing.\n",
        "\n",
        "##### 3.0.2 Final Clustering\n",
        "\n",
        "So although we should probably pick the largest silhouette scores,\n",
        "that’s `k=2` which kind of defeats the purpose of clustering in the\n",
        "first place. In the absence of a *compelling* reason to pick 2 or 3\n",
        "clusters, let’s have a closer look at the *next* maximum silhouetted\n",
        "score:\n",
        "\n",
        "``` python\n",
        "k_pref=??\n",
        "    \n",
        "#############\n",
        "# Do the clustering using the main columns\n",
        "kmeans = KMeans(??).fit(cldf)\n",
        "\n",
        "# Convert to a series\n",
        "s = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n",
        "rs[c_nm] = s\n",
        "    \n",
        "# Calculate the overall silhouette score\n",
        "silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n",
        "\n",
        "# Calculate the silhouette values\n",
        "sample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n",
        "    \n",
        "#############\n",
        "# Create a subplot with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.set_size_inches(9, 5)\n",
        "\n",
        "# The 1st subplot is the silhouette plot\n",
        "# The silhouette coefficient can range from -1, 1\n",
        "ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
        "    \n",
        "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "# plots of individual clusters, to demarcate them clearly.\n",
        "ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n",
        "    \n",
        "y_lower = 10\n",
        "    \n",
        "# For each of the clusters...\n",
        "for i in range(k_pref):\n",
        "    # Aggregate the silhouette scores for samples belonging to\n",
        "    # cluster i, and sort them\n",
        "    ith_cluster_silhouette_values = \\\n",
        "        sample_silhouette_values[kmeans.labels_ == i]\n",
        "\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "        \n",
        "    # Set the color ramp\n",
        "    color = plt.cm.Spectral(i/k)\n",
        "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                        0, ith_cluster_silhouette_values,\n",
        "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "    # Label the silhouette plots with their cluster numbers at the middle\n",
        "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "    # Compute the new y_lower for next plot\n",
        "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed --\n",
        "    # we can only do this for the first two dimensions\n",
        "    # so we may not see fully what is causing the \n",
        "    # resulting assignment\n",
        "    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n",
        "    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n",
        "                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = kmeans.cluster_centers_\n",
        "    \n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
        "                marker='o', c=\"white\", alpha=1, s=200)\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
        "\n",
        "    ax2.set_title(\"Visualization of the clustered data\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "plt.suptitle((\"Silhouette results for KMeans clustering \"\n",
        "                \"with %d clusters\" % k_pref),\n",
        "                fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "*Stop*: Make sure that you understand how the silhouette plot and value\n",
        "work, and why your results may diverge from mine..\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn(water, boroughs)\n",
        "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n",
        "```\n",
        "\n",
        "##### 3.0.3 ‘Representative’ Centroids\n",
        "\n",
        "To get a sense of how these clusters differ we can try to extract\n",
        "‘representative’ centroids (mid-points of the multi-dimensional cloud\n",
        "that constitutes a cluster). In the case of k-means this will work quite\n",
        "will since the clusters are explicitly built around mean centroids.\n",
        "There’s also a k-medoids clustering approach built around the median\n",
        "centroid.\n",
        "\n",
        "``` python\n",
        "centroids = None\n",
        "for k in sorted(rs[c_nm].unique()):\n",
        "    print(f\"Processing cluster {k}\")\n",
        "\n",
        "    c = rs[rs[c_nm]==k]\n",
        "    if centroids is None:\n",
        "        centroids = pd.DataFrame(columns=c.columns.values)\n",
        "    centroids = centroids.append(c.mean(), ignore_index=True)\n",
        "    \n",
        "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
        "for i in range(0,len(centroids.index)):\n",
        "    row = centroids.iloc[i,:]\n",
        "    c_index = list(centroids.columns.values).index(c_nm)\n",
        "    for c in range(0,c_index):\n",
        "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
        "        odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "odf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\n",
        "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
        "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
        "```\n",
        "\n",
        "### 4 DBSCAN\n",
        "\n",
        "For what it’s worth, I’ve had *enormous* trouble with DBSCAN and this\n",
        "kind of data. I don’t think it deals very well with more than three\n",
        "dimensions, so the flexbility to not have to specify the number of\n",
        "clusters is balanced with a density-based approach that is severely\n",
        "hampered by high-dimensional distance-inflation.\n",
        "\n",
        "``` python\n",
        "cldf2 = cldf.loc[:,['Component 1','Component 2']]\n",
        "cldf2.head()\n",
        "```\n",
        "\n",
        "##### 4.0.1 Work out the Neighbour Distance\n",
        "\n",
        "We normally look for some kind of ‘knee’ to set the distance.\n",
        "\n",
        "``` python\n",
        "nbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\n",
        "distances, indices = nbrs.kneighbors(cldf2)\n",
        "\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances)\n",
        "plt.gcf().suptitle(\"Nearest Neighbour Distances\");\n",
        "```\n",
        "\n",
        "##### 4.0.2 Exploration\n",
        "\n",
        "There are two values that need to be specified: `eps` and `min_samples`.\n",
        "Both seem to be set largely by trial and error. It’s easiest to set\n",
        "`min_samples` first since that sets a floor for your cluster size and\n",
        "then `eps` is basically a distance metric that governs how far away\n",
        "something can be from a cluster and still be considered part of that\n",
        "cluster.\n",
        "\n",
        "WARNING. This next step may take quite a lot of time since we are\n",
        "iterating through many, many values of Epsilon to explore how the\n",
        "clustering result changes and how well this matches up with (or doesn’t)\n",
        "the graph above.\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "c_nm = 'DBSCAN'\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "el  = []\n",
        "\n",
        "max_clusters  = 10\n",
        "cluster_count = 1\n",
        "\n",
        "iters = 0\n",
        "\n",
        "for e in np.arange(0.025, 0.25, 0.005):\n",
        "    \n",
        "    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n",
        "    \n",
        "    # Run the clustering\n",
        "    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n",
        "    \n",
        "    # See how we did\n",
        "    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n",
        "    \n",
        "    row = [e]\n",
        "    data = s.value_counts()\n",
        "    \n",
        "    for c in range(-1, max_clusters+1):\n",
        "        try:\n",
        "            if np.isnan(data[c]):\n",
        "                row.append(None)\n",
        "            else: \n",
        "                row.append(data[c])\n",
        "        except KeyError:\n",
        "            row.append(None)\n",
        "    \n",
        "    el.append(row)\n",
        "    iters+=1\n",
        "\n",
        "edf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
        "\n",
        "print(\"Done.\")\n",
        "```\n",
        "\n",
        "``` python\n",
        "odf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n",
        "\n",
        "for i in range(0,len(edf.index)):\n",
        "    row = edf.iloc[i,:]\n",
        "    for c in range(1,len(edf.columns.values)):\n",
        "        if row[c] != None and not np.isnan(row[c]):\n",
        "            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n",
        "            odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "odf['Count'] = odf.Count.astype(float)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "sns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\n",
        "ax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')\n",
        "```\n",
        "\n",
        "``` python\n",
        "e = 0.06\n",
        "dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\n",
        "s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n",
        "rs[c_nm] = s\n",
        "print(s.value_counts())\n",
        "```\n",
        "\n",
        "##### 4.0.3 Mapping Clustering Results\n",
        "\n",
        "WARNING. My sense is that these results are a bit rubbish: the majority\n",
        "of items are assigned to *one cluster*??? I’ve tried PCA on the\n",
        "standardised data and that made little difference. This should also have\n",
        "worked *better* but it seems that a small number of LSOAs are so utterly\n",
        "different that the more sophisticated clustering algorithm effectively\n",
        "‘chokes’ on them.\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn(water, boroughs)\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n",
        "```\n",
        "\n",
        "``` python\n",
        "centroids = None\n",
        "for k in sorted(rs[c_nm].unique()):\n",
        "    print(f\"Processing cluster {k}\")\n",
        "\n",
        "    clsoas = rs[rs[c_nm]==k]\n",
        "    if centroids is None:\n",
        "        centroids = pd.DataFrame(columns=clsoas.columns.values)\n",
        "    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n",
        "\n",
        "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
        "for i in range(0,len(centroids.index)):\n",
        "    row = centroids.iloc[i,:]\n",
        "    c_index = list(centroids.columns.values).index(c_nm)\n",
        "    for c in range(0,c_index):\n",
        "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
        "        odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "# Drop outliers\n",
        "odf = odf[odf.Cluster >= 0]\n",
        "odf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n",
        "\n",
        "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
        "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
        "\n",
        "del(odf, centroids)\n",
        "```\n",
        "\n",
        "##### 4.0.4 Bonus!\n",
        "\n",
        "See if you can work out why we have this issue by plotting the first 2\n",
        "components… We *could* actually try clusteirng on secondary components\n",
        "instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so\n",
        "dominant.\n",
        "\n",
        "### 5 OPTICS\n",
        "\n",
        "This one invites you to experiment wih different values of $\\epsilon$ so\n",
        "it makes even fewer assumptions than DBSCAN but is even more\n",
        "computationally intensive. I’ve deliberately left this untested so I\n",
        "can’t be sure it will work and you’ll need to debug…\n",
        "\n",
        "WARNING. This next step may take quite a lot of time since the algorithm\n",
        "is making far fewer assumptions about the structure of the data. On a\n",
        "2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "c_nm = 'Optics'\n",
        "\n",
        "# Can try to set this from DBSCAN results\n",
        "e = 0.06\n",
        "\n",
        "import math\n",
        "\n",
        "# Run the clustering\n",
        "opt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n",
        "\n",
        "# See how we did\n",
        "s = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\n",
        "rs[c_nm] = s\n",
        "\n",
        "# Distribution\n",
        "print(s.value_counts())\n",
        "```\n",
        "\n",
        "### 6 Self-Organising Maps\n",
        "\n",
        "SOMs offer a *fourth* type of clustering algorithm. They are a\n",
        "relatively ‘simple’ type of neural network in which the ‘map’ (of the\n",
        "SOM) adjusts to the data: we’re going to see how this works over the\n",
        "next few code blocks, but the main thing is that, unlike the above\n",
        "approaches, SOMs build a 2D map of a higher-dimensional space and use\n",
        "this as a mechanism for subsequently clustering the raw data. In this\n",
        "sense there is a conceptual link between SOMs and PCA or tSNE (another\n",
        "form of dimensionality reduction).\n",
        "\n",
        "##### 6.0.1 Training the SOM\n",
        "\n",
        "We are going to actually train the SOM using the input data. This is\n",
        "where you specify the input parameters that have the main effect on the\n",
        "clustering results.\n",
        "\n",
        "``` python\n",
        "from sompy.sompy import SOMFactory\n",
        "```\n",
        "\n",
        "``` python\n",
        "c_nm = 'SOM'\n",
        "\n",
        "sm = SOMFactory().build(\n",
        "    cldf.values, mapsize=(10,15),\n",
        "    normalization='var', initialization='random', component_names=cldf.columns.values)\n",
        "sm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\n",
        "```\n",
        "\n",
        "How good is the fit?\n",
        "\n",
        "``` python\n",
        "topographic_error  = sm.calculate_topographic_error()\n",
        "quantization_error = np.mean(sm._bmu[1])\n",
        "print(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\n",
        "```\n",
        "\n",
        "How do the results look?\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.mapview import View2D\n",
        "view2D = View2D(10, 10, \"rand data\", text_size=10)\n",
        "view2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\n",
        "plt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n",
        "```\n",
        "\n",
        "##### 6.0.2 Here’s What I Got\n",
        "\n",
        "*Note*: Your results may differ.\n",
        "\n",
        ":::\n",
        "\n",
        "<img src=\"https://github.com/jreades/i2p/blob/master/practicals/img/SOM-Map.png?raw=true\" alt=\"SOM Clustering Results\" width=\"800\" />\n",
        "\n",
        "How many data points were assigned to each BMU?\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.bmuhits import BmuHitsView\n",
        "vhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\n",
        "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\n",
        "plt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\n",
        "```\n",
        "\n",
        "How many clusters do we want and where are they on the map?\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.hitmap import HitMapView\n",
        "\n",
        "k_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\n",
        "sm.cluster(k_val)\n",
        "hits  = HitMapView(15, 15, \"Clustering\", text_size=14)\n",
        "a     = hits.show(sm)\n",
        "plt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\n",
        "```\n",
        "\n",
        "Finally, let’s get the cluster results and map them back on to the data\n",
        "points:\n",
        "\n",
        "``` python\n",
        "# Get the labels for each BMU\n",
        "# in the SOM (15 * 10 neurons)\n",
        "clabs = sm.cluster_labels\n",
        "\n",
        "# Project the data on to the SOM\n",
        "# so that we get the BMU for each\n",
        "# of the original data points \n",
        "# (This is similar to the transform \n",
        "# function is sklearn)\n",
        "bmus  = sm.project_data(cldf.values)\n",
        "\n",
        "# Turn the BMUs into cluster labels\n",
        "# and append to the data frame\n",
        "s = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n",
        "\n",
        "rs[c_nm] = s\n",
        "```\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn(water, boroughs)\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n",
        "```\n",
        "\n",
        "##### 6.0.3 Result!\n",
        "\n",
        "WARNING. These are the results from the approach that is closest to the\n",
        "one outlined in *Geocomputation*.\n",
        "\n",
        ":::\n",
        "\n",
        "<img src=\"https://github.com/jreades/i2p/blob/master/practicals/img/SOM-Cluster-Map.png?raw=true\" alt=\"SOM Clustering Results Mapped\" width=\"800\" />\n",
        "\n",
        "### 7 ADBSCAN\n",
        "\n",
        "https://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n",
        "\n",
        "### 8 Wrap-Up\n",
        "\n",
        "-   Find the appropriate eps value: [Nearest Neighbour Distance\n",
        "    Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Nearest-Neighbor-Distance-Functions)\n",
        "    or [Interevent Distance\n",
        "    Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Interevent-Distance-Functions)\n",
        "-   [Clustering\n",
        "    Points](https://darribas.org/gds_course/content/bH/lab_H.html#clusters-of-points)\n",
        "-   [Regionalisation algorithms with Aglomerative\n",
        "    Clustering](https://darribas.org/gds_course/content/bG/lab_G.html#regionalization-algorithms)\n",
        "\n",
        "You’ve reached the end, you’re done…\n",
        "\n",
        "Er, no. This is barely scratching the surface! I’d suggest that you go\n",
        "back through the above code and do three things: 1. Add a lot more\n",
        "comments to the code to ensure that really have understood what is going\n",
        "on. 2. Try playing with some of the parameters (e.g. my thresholds for\n",
        "skew, or non-normality) and seeing how your results change. 3. Try\n",
        "outputting additional plots that will help you to understand the\n",
        "*quality* of your clustering results (e.g. what *is* the makeup of\n",
        "cluster 1? Or 6? What has it picked up? What names would I give these\n",
        "clsuters?).\n",
        "\n",
        "If all of that seems like a lot of work then why not learn a bit more\n",
        "about machine learning before calling it a day?\n",
        "\n",
        "See: [Introduction to Machine Learning with\n",
        "Scikit-Learn](http://www.slideshare.net/BenjaminBengfort/introduction-to-machine-learning-with-scikitlearn)."
      ],
      "id": "b9a0ec47-7072-45f3-9626-d2f4cd778c16"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/jreades/anaconda3/envs/sds/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  }
}