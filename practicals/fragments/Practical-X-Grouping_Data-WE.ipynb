{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preamble\n",
        "\n",
        "A common challenge in data analysis is how to group observations in a\n",
        "data set together in a way that allows for generalisation: *this* group\n",
        "of observations are similar to one another, *that* group is dissimilar\n",
        "to this group. But what defines similarity and difference? There is no\n",
        "*one* answer to that question and so there are many different ways to\n",
        "cluster data, each of which has strengths and weaknesses that make them\n",
        "more, or less, appropriate in different contexts.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> **🔗 Connections**:\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import requests\n",
        "import zipfile\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import pickle as pk\n",
        "\n",
        "from io import BytesIO, StringIO\n",
        "from os.path import join as pj\n",
        "from pathlib import Path\n",
        "import matplotlib as mpl\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import sklearn\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
        "from esda.adbscan import ADBSCAN\n",
        "\n",
        "import random\n",
        "random.seed(42)    # For reproducibility\n",
        "np.random.seed(42) # For reproducibility\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
        "```\n",
        "\n",
        "### 1 Load GeoData for Display\n",
        "\n",
        "``` python\n",
        "# Load Water GeoPackage\n",
        "w_path = os.path.join('data','geo','Water.gpkg')\n",
        "if not os.path.exists(w_path):\n",
        "    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n",
        "    water.to_file(w_path)\n",
        "    print(\"Downloaded Water.gpkg file.\")\n",
        "else:\n",
        "    water = gpd.read_file(w_path)\n",
        "\n",
        "# Boroughs GeoPackage\n",
        "b_path = os.path.join('data','geo','Boroughs.gpkg')\n",
        "if not os.path.exists(b_path):\n",
        "    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n",
        "    boroughs.to_file(b_path)\n",
        "    print(\"Downloaded Boroughs.gpkg file.\")\n",
        "else:\n",
        "    boroughs = gpd.read_file(b_path)\n",
        "```\n",
        "\n",
        "##### 1.0.1 Useful Functions for Plotting\n",
        "\n",
        "``` python\n",
        "def plt_ldn(w=water, b=boroughs):\n",
        "    fig, ax = plt.subplots(1, figsize=(14, 12))\n",
        "    w.plot(ax=ax, color='#79aef5', zorder=2)\n",
        "    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n",
        "    ax.set_xlim([502000,563000])\n",
        "    ax.set_ylim([155000,201500])\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    return fig, ax\n",
        "\n",
        "def default_cmap(n, outliers=False):\n",
        "    cmap = mpl.cm.get_cmap('viridis_r', n)\n",
        "    colors = cmap(np.linspace(0,1,n))\n",
        "    if outliers:\n",
        "        gray = np.array([225/256, 225/256, 225/256, 1])\n",
        "        colors = np.insert(colors, 0, gray, axis=0)\n",
        "    return ListedColormap(colors)\n",
        "\n",
        "# mappable = ax.collections[-1] if you add the geopandas\n",
        "# plot last.\n",
        "def add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n",
        "    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n",
        "                    boundaries=breaks,\n",
        "                    extend=('min' if outliers else 'neither'), \n",
        "                    spacing='uniform',\n",
        "                    orientation='horizontal',\n",
        "                    fraction=0.05, shrink=0.5, pad=0.05)\n",
        "    cb.set_label(\"Cluster Number\")\n",
        "```\n",
        "\n",
        "### 2 Loading Data\n",
        "\n",
        "##### 2.0.1 Load the Listings Data\n",
        "\n",
        "Feel free to download this manually and load it locally rather that\n",
        "loading via the URL:\n",
        "\n",
        "``` python\n",
        "vsize=100\n",
        "```\n",
        "\n",
        "``` python\n",
        "we = pd.read_csv(os.path.join('data','clean',f'2020-08-24-listing-description-w2v-{vsize}.csv.gz'), compression='gzip', low_memory=False)\n",
        "```\n",
        "\n",
        "``` python\n",
        "print(f\"Data frame is {we.shape[0]:,} x {we.shape[1]}\")\n",
        "we.head(3)\n",
        "```\n",
        "\n",
        "You should have: `Data frame is 74,184 x 101`.\n",
        "\n",
        "##### 2.0.2 Link Back to Listings\n",
        "\n",
        "``` python\n",
        "df = pd.read_csv(os.path.join('data','raw','2020-08-24-listings.csv.gz'), compression='gzip',\n",
        "                low_memory=False, usecols=['id','longitude','latitude','room_type','price'])\n",
        "df['price'] = df.price.str.replace('$','').str.replace(',','').astype(float)\n",
        "```\n",
        "\n",
        "``` python\n",
        "df.drop(df[((df.latitude.isna())|(df.longitude.isna()))].index, axis=0, inplace=True)\n",
        "df.drop(df[((df.latitude < 40)|(df.longitude > 1))].index, axis=0, inplace=True)\n",
        "df['id'] = df.id.astype(int)\n",
        "df = df[df.room_type=='Entire home/apt']\n",
        "print(df.shape)\n",
        "```\n",
        "\n",
        "``` python\n",
        "df.set_index('id')\n",
        "wedf = pd.merge(df, we, on='id')\n",
        "gdf = gpd.GeoDataFrame(wedf, \n",
        "      geometry=gpd.points_from_xy(wedf['longitude'], wedf['latitude'], crs='epsg:4326'))\n",
        "gdf = gdf.to_crs('epsg:27700').drop(columns=['longitude','latitude','room_type']).set_index('id')\n",
        "print(gdf.shape)\n",
        "```\n",
        "\n",
        "``` python\n",
        "gdf.plot(column='WordVec 1', scheme='quantiles', k=5, \n",
        "         cmap='plasma', figsize=(10,8), markersize=1, alpha=0.5);\n",
        "```\n",
        "\n",
        "##### 2.0.3 Aggregating Listings by MSOA\n",
        "\n",
        "Next, let’s link all this using the MSOA Geography that we created last\n",
        "week and a mix or merge and sjoin!\n",
        "\n",
        "``` python\n",
        "msoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\n",
        "```\n",
        "\n",
        "``` python\n",
        "# ml == MSOA Listings\n",
        "ml = gpd.sjoin(gdf, msoas[['MSOA11CD','geometry']], op='within').drop(columns=\n",
        "    ['index_right','geometry']\n",
        ")\n",
        "ml.head()\n",
        "```\n",
        "\n",
        "``` python\n",
        "pcts = [0.15, 0.85]\n",
        "price_range = ml[['MSOA11CD','price']].groupby('MSOA11CD').describe(percentiles=pcts).reset_index().set_index('MSOA11CD')\n",
        "price_range.columns = price_range.columns.get_level_values(1)\n",
        "price_range = price_range[[f\"{x*100:0.0f}%\" for x in pcts]]\n",
        "print(price_range.shape)\n",
        "price_range.head()\n",
        "```\n",
        "\n",
        "``` python\n",
        "mlp = pd.merge(ml, price_range, left_on='MSOA11CD', right_index=True)\n",
        "mlp.head()[['price']+[f\"{x*100:0.0f}%\" for x in pcts]]\n",
        "```\n",
        "\n",
        "``` python\n",
        "mlp['keep'] = (mlp.price >= mlp[f\"{pcts[0]*100:0.0f}%\"]) & (mlp.price <= mlp[f\"{pcts[1]*100:0.0f}%\"])\n",
        "print(f\"Keeping {mlp.keep.sum()} of {mlp.shape[0]} ({mlp.keep.sum()/mlp.shape[0]:0.3f})\")\n",
        "```\n",
        "\n",
        "``` python\n",
        "mlg = mlp[mlp.keep==True].groupby('MSOA11CD').mean().reset_index().set_index('MSOA11CD')\n",
        "print(mlg.shape)\n",
        "mlg.head()\n",
        "```\n",
        "\n",
        "``` python\n",
        "#rs = RobustScaler(quantile_range=[2.5,97.5])\n",
        "```\n",
        "\n",
        "You should see wide ranges of counts by roomt type in the first MSOA\n",
        "alone: the largest number of listings is for Entire home/apt but there\n",
        "are 242 multi-host listings compared to ‘just’ 156 non-multis. I believe\n",
        "this is the City of London though, which is quite unusual for a MSOA and\n",
        "should probably be treated as an outlier in most cases.\n",
        "\n",
        "``` python\n",
        "tsne = TSNE(n_components=3, perplexity=40, early_exaggeration=3.0,\n",
        "            learning_rate=9.0, n_iter=500, n_iter_without_progress=300,\n",
        "            min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0,\n",
        "            random_state=42, method='barnes_hut', angle=0.5)\n",
        "X = tsne.fit_transform(mlg)\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "dtsne = pd.DataFrame({'MSOA11CD':mlg.index,'D1':X[:,0], 'D2':X[:,1], 'D3':X[:,2]})\n",
        "f = plt.figure(figsize=(10,10))\n",
        "ax = Axes3D(f)\n",
        "ax.scatter(dtsne.D1, dtsne.D2, dtsne.D3, marker='.')\n",
        "```\n",
        "\n",
        "### 3 Clustering Data Frame\n",
        "\n",
        "``` python\n",
        "dtsne.head()\n",
        "```\n",
        "\n",
        "``` python\n",
        "cldf = dtsne.set_index('MSOA11CD').copy()\n",
        "```\n",
        "\n",
        "``` python\n",
        "rs = pd.merge(msoas[['MSOA11CD','geometry']], cldf, on='MSOA11CD', how='inner').set_index('MSOA11CD')\n",
        "rs.head()\n",
        "```\n",
        "\n",
        "``` python\n",
        "cols_to_plot = np.random.choice(cldf.columns.values, 2, replace=False)\n",
        "print(\"Plotting cols: \" + \", \".join(cols_to_plot))\n",
        "```\n",
        "\n",
        "``` python\n",
        "c_nm   = 'KMeans' # Clustering name\n",
        "k_pref = 4 # Number of clusters\n",
        "\n",
        "kmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n",
        "\n",
        "print(kmeans.labels_) # The results\n",
        "\n",
        "# Add it to the data frame\n",
        "s = pd.Series(kmeans.labels_, index=cldf.index) \n",
        "rs[c_nm] = s\n",
        "\n",
        "# How are the clusters distributed?\n",
        "ax = sns.histplot(data=rs, x=c_nm)\n",
        "plt.xticks(range(rs[c_nm].min(), math.ceil(rs[c_nm].max())+1));\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Going to be a bit hard to read if \n",
        "# we plot every variable against every\n",
        "# other variables, so we'll just pick a few\n",
        "sns.set(style=\"white\")\n",
        "sns.pairplot(rs, \n",
        "             vars=cols_to_plot, \n",
        "             hue=c_nm, markers=\".\", height=3, diag_kind='kde');\n",
        "```\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True)\n",
        "```\n",
        "\n",
        "One More Thing…\n",
        "\n",
        "There’s just one little problem: what assumption did I make when I\n",
        "started this k-means cluster analysis? It’s a huge one, and it’s one of\n",
        "the reasons that k-means clustering can be problematic when used\n",
        "naively…\n",
        "\n",
        "STOP. What critical assumption did we make when running this analysis?\n",
        "The ‘Right’ Number of Clusters Again, there’s more than one way to skin\n",
        "this cat. In Geocomputation they use WCSS to pick the ‘optimal’ number\n",
        "of clusters. The idea is that you plot the average WCSS for each number\n",
        "of possible clusters in the range of interest (2…n) and then look for a\n",
        "‘knee’ (i.e. kink) in the curve. The principle of this approach is that\n",
        "you look for the point where there is declining benefit from adding more\n",
        "clusters. The problem is that there is always some benefit to adding\n",
        "more clusters (the perfect clustering is k==n), so you don’t always see\n",
        "a knee.\n",
        "\n",
        "Another way to try to make the process of selecting the number of\n",
        "clusters a little less arbitrary is called the silhouette plot and (like\n",
        "WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome\n",
        "by examining the distance between each observation and the rest of the\n",
        "cluster. In this case it’s based on Partitioning Around the Medoid\n",
        "(PAM).\n",
        "\n",
        "Either way, to evaluate this in a systematic way, we want to do multiple\n",
        "k-means clusterings for multiple values of k and then we can look at\n",
        "which gives the best results…\n",
        "\n",
        "Let’s try it for a range of values…\n",
        "\n",
        "``` python\n",
        "# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for k in range(2,41):\n",
        "    \n",
        "    #############\n",
        "    # Do the clustering using the main columns\n",
        "    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(cldf)\n",
        "    \n",
        "    # Calculate the overall silhouette score\n",
        "    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n",
        "    \n",
        "    y.append(k)\n",
        "    x.append(silhouette_avg)\n",
        "    \n",
        "    print('.', end='')\n",
        "\n",
        "print()\n",
        "print(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n",
        "\n",
        "plt.plot(y, x)\n",
        "plt.gcf().suptitle(\"Average Silhouette Scores\");\n",
        "```\n",
        "\n",
        "``` python\n",
        "k_pref=7\n",
        "    \n",
        "#############\n",
        "# Do the clustering using the main columns\n",
        "kmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf)\n",
        "\n",
        "# Convert to a series\n",
        "s = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n",
        "\n",
        "# We do this for plotting\n",
        "rs[c_nm] = s\n",
        "    \n",
        "# Calculate the overall silhouette score\n",
        "silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n",
        "\n",
        "# Calculate the silhouette values\n",
        "sample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n",
        "    \n",
        "#############\n",
        "# Create a subplot with 1 row and 2 columns\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "fig.set_size_inches(9, 5)\n",
        "\n",
        "# The 1st subplot is the silhouette plot\n",
        "# The silhouette coefficient can range from -1, 1\n",
        "ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
        "    \n",
        "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "# plots of individual clusters, to demarcate them clearly.\n",
        "ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n",
        "    \n",
        "y_lower = 10\n",
        "    \n",
        "# For each of the clusters...\n",
        "for i in range(k_pref):\n",
        "    # Aggregate the silhouette scores for samples belonging to\n",
        "    # cluster i, and sort them\n",
        "    ith_cluster_silhouette_values = \\\n",
        "        sample_silhouette_values[kmeans.labels_ == i]\n",
        "\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "        \n",
        "    # Set the color ramp\n",
        "    color = plt.cm.Spectral(i/k)\n",
        "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                        0, ith_cluster_silhouette_values,\n",
        "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "    # Label the silhouette plots with their cluster numbers at the middle\n",
        "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "    # Compute the new y_lower for next plot\n",
        "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed --\n",
        "    # we can only do this for the first two dimensions\n",
        "    # so we may not see fully what is causing the \n",
        "    # resulting assignment\n",
        "    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n",
        "    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n",
        "                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = kmeans.cluster_centers_\n",
        "    \n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
        "                marker='o', c=\"white\", alpha=1, s=200)\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
        "\n",
        "    ax2.set_title(\"Visualization of the clustered data\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "plt.suptitle((\"Silhouette results for KMeans clustering \"\n",
        "                \"with %d clusters\" % k_pref),\n",
        "                fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Interpreting the Results\n",
        "\n",
        "STOP. Make sure that you understand how the silhouette plot and value\n",
        "work, and why your results may diverge from mine.\n",
        "\n",
        "We can use the largest average silhouette score to determine the\n",
        "‘natural’ number of clusters in the data, but that that’s only if we\n",
        "don’t have any kind of underlying theory, other empirical evidence, or\n",
        "even just a reason for choosing a different value… Again, we’re now\n",
        "getting in areas where your judgement and your ability to communicate\n",
        "your rationale to readers is the key thing.\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True)\n",
        "```\n",
        "\n",
        "‘Representative’ Centroids To get a sense of how these clusters differ\n",
        "we can try to extract ‘representative’ centroids (mid-points of the\n",
        "multi-dimensional cloud that constitutes a cluster). In the case of\n",
        "k-means this will work quite will since the clusters are explicitly\n",
        "built around mean centroids. There’s also a k-medoids clustering\n",
        "approach built around the median centroid.\n",
        "\n",
        "DBScan\n",
        "\n",
        "Of course, as we’ve said above k-means is just one way of clustering,\n",
        "DBScan is another. Unlike k-means, we don’t need to specify the number\n",
        "of clusters in advance. Which sounds great, but we still need to specify\n",
        "other parameters (typically, these are known as hyperparameters because\n",
        "they are about specifying parameters that help the aglorithm to find the\n",
        "right solution… or final set of parameters!) and these can have a huge\n",
        "impact on our results!\n",
        "\n",
        "Find a Reasonable Value for Epsilon\n",
        "\n",
        "Before we an use DBSCAN it’s useful to find a good value for Epsilon. We\n",
        "can look for the point of maximum ‘curvature’ in a nearest neigbhours\n",
        "plot. Which seems to be in the vicinity of 0.55. Tips on selecting\n",
        "min_pts can be found here.\n",
        "\n",
        "``` python\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(cldf)\n",
        "distances, indices = nbrs.kneighbors(cldf)\n",
        "\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances);\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.0,2.5])\n",
        "```\n",
        "\n",
        "``` python\n",
        "c_nm = 'DBSCAN'\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "el  = []\n",
        "\n",
        "max_clusters  = 10\n",
        "cluster_count = 1\n",
        "\n",
        "iters = 0\n",
        "\n",
        "for e in np.arange(0.5, 1.5, 0.01):\n",
        "    \n",
        "    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n",
        "    \n",
        "    # Run the clustering\n",
        "    dbs = DBSCAN(eps=e, min_samples=cldf.shape[1]+1).fit(cldf.values)\n",
        "    \n",
        "    # See how we did\n",
        "    s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n",
        "    \n",
        "    row = [e]\n",
        "    data = s.value_counts()\n",
        "    \n",
        "    for c in range(-1, max_clusters+1):\n",
        "        try:\n",
        "            if np.isnan(data[c]):\n",
        "                row.append(None)\n",
        "            else: \n",
        "                row.append(data[c])\n",
        "        except KeyError:\n",
        "            row.append(None)\n",
        "    \n",
        "    el.append(row)\n",
        "    iters+=1\n",
        "\n",
        "edf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
        "\n",
        "print(\"Done.\")\n",
        "```\n",
        "\n",
        "``` python\n",
        "odf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n",
        "\n",
        "for i in range(0,len(edf.index)):\n",
        "    row = edf.iloc[i,:]\n",
        "    for c in range(1,len(edf.columns.values)):\n",
        "        if row[c] != None and not np.isnan(row[c]):\n",
        "            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n",
        "            odf = odf.append(d, ignore_index=True)\n",
        "```\n",
        "\n",
        "``` python\n",
        "odf['Count'] = odf.Count.astype(float)\n",
        "```\n",
        "\n",
        "``` python\n",
        "xmin = odf[odf.Cluster=='Cluster 0'].Epsilon.min()\n",
        "xmax = odf[(odf.Cluster=='Cluster -1') & (odf.Count < cldf.shape[0]/5)].Epsilon.min()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "sns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster', ax=ax);\n",
        "ax.vlines(0.97, 0, ax.get_ylim()[1], colors='r', linestyles='dashed', label='Epsilon');\n",
        "```\n",
        "\n",
        "``` python\n",
        "e = 0.97\n",
        "dbs = DBSCAN(eps=e, min_samples=cldf.shape[1]+1).fit(cldf.values)\n",
        "s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n",
        "rs[c_nm] = s\n",
        "print(s.value_counts())\n",
        "```\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, cmap='plasma', categorical=True, legend=True);\n",
        "```\n",
        "\n",
        "``` python\n",
        "from sompy.sompy import SOMFactory\n",
        "```\n",
        "\n",
        "``` python\n",
        "c_nm = 'SOM'\n",
        "\n",
        "sm = SOMFactory().build(\n",
        "    cldf.values, mapsize=(10,15),\n",
        "    normalization='var', initialization='random', component_names=cldf.columns.values)\n",
        "sm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\n",
        "```\n",
        "\n",
        "``` python\n",
        "topographic_error  = sm.calculate_topographic_error()\n",
        "quantization_error = np.mean(sm._bmu[1])\n",
        "print(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\n",
        "```\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.mapview import View2D\n",
        "view2D = View2D(10, 10, \"rand data\", text_size=10)\n",
        "view2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\n",
        "```\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.bmuhits import BmuHitsView\n",
        "vhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\n",
        "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\n",
        "```\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.hitmap import HitMapView\n",
        "\n",
        "k_val = 4\n",
        "sm.cluster(k_val)\n",
        "hits  = HitMapView(15, 15, \"Clustering\", text_size=14)\n",
        "a     = hits.show(sm)\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Get the labels for each BMU\n",
        "# in the SOM (15 * 10 neurons)\n",
        "clabs = sm.cluster_labels\n",
        "\n",
        "# Project the data on to the SOM\n",
        "# so that we get the BMU for each\n",
        "# of the original data points\n",
        "bmus  = sm.project_data(cldf.values)\n",
        "\n",
        "# Turn the BMUs into cluster labels\n",
        "# and append to the data frame\n",
        "s = pd.Series(clabs[bmus], index=rs.index, name=c_nm)\n",
        "\n",
        "rs[c_nm] = s\n",
        "```\n",
        "\n",
        "``` python\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "rs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True)\n",
        "```\n",
        "\n",
        "#### 3.1 Which Clustering Approach is Right?\n",
        "\n",
        "The reason that there is no ‘right’ approach to clustering is that it\n",
        "all depends on what you’re trying to accomplish and how you’re\n",
        "*reasoning* about your problem. The image below highlights the extent to\n",
        "which the different clustering approaches in sklearn can produce\n",
        "different results – and this is only for the *non-geographic*\n",
        "algorithms!\n",
        "\n",
        "<a href=\"http://scikit-learn.org/stable/modules/clustering.html#clustering\"><img alt=\"Clustering algorithm comparison\" src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png\" width=\"700px\" /></a>\n",
        "\n",
        "*Note:* for geographically-aware clustering you need to look at PySAL.\n",
        "\n",
        "To think about this in a little more detail:\n",
        "\n",
        "-   If I run an online company and I want to classify my customers on\n",
        "    the basis of their product purchases, then I probably don’t care\n",
        "    much about where they are, only about what they buy, and so my\n",
        "    clustering approach doesn’t need to take geography into account. I\n",
        "    might well *discover* that many of my most valuable customers live\n",
        "    in a few areas, but that is a finding, not a factor, in my research.\n",
        "-   Conversely, if I am looking for cancer clusters then I might well\n",
        "    care a *lot* about geography because I want to make sure that I\n",
        "    don’t overlook an important cluster of cases because it’s ‘hidden’\n",
        "    inside an area with lots of people who don’t have cancer. In that\n",
        "    case, I want my clusters to take geography into account. That\n",
        "    approach might classify an area with a smaller proportion of cancer\n",
        "    patients as part of a ‘cancer cluster’ but that’s because it is\n",
        "    still significant *because* of the geography.\n",
        "\n",
        "So you can undertake a spatial analysis using *either* approach, it just\n",
        "depends on the role that you think geography should play in producing\n",
        "the clusters in the first place. We’ll see this in action today!\n",
        "\n",
        "##### 3.1.1 Ensure Plotting Output\n",
        "\n",
        "``` python\n",
        "import matplotlib as mpl\n",
        "mpl.use('TkAgg')\n",
        "%matplotlib inline\n",
        "```\n",
        "\n",
        "##### 3.1.2 Importing the Libraries\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "import os\n",
        "import pickle as pk\n",
        "\n",
        "from io import BytesIO, StringIO\n",
        "from os.path import join as pj\n",
        "from pathlib import Path\n",
        "import matplotlib as mpl\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import sklearn\n",
        "sklv = int(sklearn.__version__.replace(\".\",\"\"))\n",
        "if sklv < 210:\n",
        "    print(\"SciKit-Learn verion is: \" + sklearn.__version__)\n",
        "    print(\"The OPTICS part of this notebook relies on a version >= 0.21.0\")\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn import preprocessing\n",
        "from sklearn import cluster\n",
        "\n",
        "import random\n",
        "random.seed(42)    # For reproducibility\n",
        "np.random.seed(42) # For reproducibility\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
        "```\n",
        "\n",
        "``` python\n",
        "src = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Census.zip?raw=true'\n",
        "dst = os.path.join('analysis','Census.zip')\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "    if not os.path.exists(os.path.dirname(dst)):\n",
        "        os.makedirs(os.path.dirname(dst))\n",
        "    \n",
        "    print(\"Downloading...\")\n",
        "    r = requests.get(src, stream=True)\n",
        "    \n",
        "    with open(dst, 'wb') as fd:\n",
        "        for chunk in r.iter_content(chunk_size=128):\n",
        "            fd.write(chunk)\n",
        "else:\n",
        "    print(\"File already downloaded.\")\n",
        "    \n",
        "print(\"Done.\")\n",
        "```\n",
        "\n",
        "#### 3.2 Loading the NomisWeb Data\n",
        "\n",
        "You may need to make a few adjustments to the path to get the data\n",
        "loaded on your own computer. But notice what we’re now able to do here:\n",
        "using the `zipfile` library we can extract a data file (or any other\n",
        "file) from the Zip archive without even having to open it. Saves even\n",
        "more time *and* disk space!\n",
        "\n",
        "``` python\n",
        "z = zipfile.ZipFile(os.path.join('analysis','Census.zip'))\n",
        "z.namelist()\n",
        "```\n",
        "\n",
        "We’re going to save each data set to a separate data frame to make it\n",
        "easier to work with during cleaning. But note that this code is fairly\n",
        "flexible since we stick each new dataframe in a dictionary (`d`) where\n",
        "we can retrieve them via an iterator.\n",
        "\n",
        "``` python\n",
        "raw   = {}\n",
        "clean = {}\n",
        "total_cols = 0\n",
        "\n",
        "for r in range(0, len(z.namelist())):\n",
        "    \n",
        "    m  = re.search(\"(?:-)([^\\.]+)\", z.namelist()[r])\n",
        "    nm = m.group(1)\n",
        "    \n",
        "    print(\"Processing {0} file: \".format(nm))\n",
        "    \n",
        "    with z.open(z.namelist()[r]) as f:\n",
        "                \n",
        "        if z.namelist()[r] == '99521530-Activity.csv': \n",
        "            raw[nm] = pd.read_csv(BytesIO(f.read()), header=7, skip_blank_lines=True, skipfooter=7, engine='python')\n",
        "        else:\n",
        "            raw[nm] = pd.read_csv(BytesIO(f.read()), header=6, skip_blank_lines=True, skipfooter=7, engine='python')\n",
        "    \n",
        "    print(\"\\tShape of dataframe is {0} rows by {1} columns\".format(raw[nm].shape[0], raw[nm].shape[1]))\n",
        "    total_cols += raw[nm].shape[1]\n",
        "\n",
        "print(\"There are {0} columns in all.\".format(total_cols))\n",
        "```\n",
        "\n",
        "``` python\n",
        "nm  = 'Occupation'\n",
        "url = 'https://github.com/jreades/urb-studies-predicting-gentrification/raw/master/data/src/2011/ks610ew.csv.gz'\n",
        "\n",
        "print(f\"Processing {nm}\")\n",
        "df = pd.read_csv(url, header=7, skip_blank_lines=True, compression='gzip', low_memory=False)\n",
        "mapping = {\n",
        "    '1. Managers, directors and senior officials':'Managers',\n",
        "    '2. Professional occupations':'Professionals',\n",
        "    '3. Associate professional and technical occupations':'Associates',\n",
        "    '4. Administrative and secretarial occupations':'Administrative',\n",
        "    '5. Skilled trades occupations':'Skilled trades',\n",
        "    '6. Caring, leisure and other service occupations':'Caring and Leisure',\n",
        "    '7. Sales and customer service occupations':'Customer Service',\n",
        "    '8. Process plant and machine operatives':'Operatives',\n",
        "    '9. Elementary occupations':'Elementary'\n",
        "}\n",
        "df.rename(columns=mapping, inplace=True)\n",
        "df.drop(['2011 super output area - lower layer','All categories: Occupation'], axis=1, inplace=True)\n",
        "\n",
        "raw[nm] = df\n",
        "df.sample(3, random_state=42)\n",
        "```\n",
        "\n",
        "``` python\n",
        "nm  = 'Income'\n",
        "url  = 'https://data.london.gov.uk/download/household-income-estimates-small-areas/7c1099d9-327b-4f20-abb8-8c24a3c10c47/modelled-household-income-estimates-lsoa.csv'\n",
        "\n",
        "print(f\"Processing {nm}\")\n",
        "df = pd.read_csv(url, encoding='latin-1')[['Code','Median 2011/12']]\n",
        "df['Median Income'] = df['Median 2011/12'].str.replace('£','').str.replace(',','').astype('float')\n",
        "df.drop('Median 2011/12', axis=1, inplace=True)\n",
        "\n",
        "raw[nm] = df\n",
        "df.sample(3, random_state=42)\n",
        "```\n",
        "\n",
        "``` python\n",
        "nm  = 'Housing'\n",
        "url = 'https://data.london.gov.uk/download/average-house-prices/9a92fbaf-c04e-498a-9f8c-6c85f280817e/land-registry-house-prices-LSOA.csv'\n",
        "\n",
        "print(f\"Processing {nm}\")\n",
        "df = pd.read_csv(url, encoding='latin-1', low_memory=False)\n",
        "\n",
        "df['Borough'] = df.Area.str.replace(' [0-9A-Z]{4}$','')\n",
        "df.drop('Area', axis=1, inplace=True)\n",
        "\n",
        "df = df[ (df.Year=='Year ending Dec 2011') & (df.Measure=='Median') ][['Code','Value','Borough']]\n",
        "\n",
        "# Note: not all have a value for this year!\n",
        "df['Median House Price'] = df.Value.str.replace(':','-1').astype(float)\n",
        "\n",
        "la = df.groupby('Borough')\n",
        "la_prices = pd.DataFrame(la['Median House Price'].median())\n",
        "\n",
        "df = df.join(la_prices, how='inner', on='Borough', rsuffix='_la')\n",
        "\n",
        "df.loc[df['Median House Price'] < 50000,'Median House Price'] =  df[df['Median House Price'] < 50000]['Median House Price_la']\n",
        "df.drop(['Value','Borough','Median House Price_la'], inplace=True, axis=1)\n",
        "\n",
        "raw[nm] = df\n",
        "df.sample(3, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.3 ONS Boundary Data\n",
        "\n",
        "We also need to download the LSOA boundary data. A quick Google search\n",
        "on “2011 LSOA boundaries” will lead you to the [Data.gov.uk\n",
        "portal](https://data.gov.uk/dataset/lower_layer_super_output_area_lsoa_boundaries).\n",
        "The rest is fairly straightforward: \\* We want ‘generalised’ because\n",
        "that means that they’ve removed some of the detail from the boundaries\n",
        "so the file will load (and render) more quickly. \\* We want ‘clipped’\n",
        "because that means that the boundaries have been clipped to the edges of\n",
        "the land (e.g. the Thames; the ‘Full’ data set splits the Thames down\n",
        "the middle between adjacent LSOAs).\n",
        "\n",
        "##### 3.3.1 Saving Time\n",
        "\n",
        "Again, in order to get you started more quickly I’ve already created a\n",
        "‘pack’ for you. However, note that the format of this is a GeoPackage,\n",
        "this is a fairly new file format designed to replace ESRI’s antique\n",
        "Shapefile format, and it allows us to include all kinds of useful\n",
        "information as part of the download as well as doing away with the need\n",
        "to unzip a download first! So here we load the data directly into a\n",
        "geopandas dataframe:\n",
        "\n",
        "``` python\n",
        "src = 'https://github.com/kingsgeocomp/applied_gsa/raw/master/data/London%20LSOAs.gpkg'\n",
        "\n",
        "gdf = gpd.read_file(src)\n",
        "print(\"Shape of LSOA file: {0} rows by {1} columns\".format(gdf.shape[0], gdf.shape[1]))\n",
        "gdf.columns = [x.lower() for x in gdf.columns.values]\n",
        "gdf.set_index('lsoa11cd', drop=True, inplace=True)\n",
        "gdf.sample(4)\n",
        "```\n",
        "\n",
        "##### 3.3.2 <span style=\"color:red\">Error!</span>\n",
        "\n",
        "Depending on your version of GDAL/Fiona, you may not be able to read the\n",
        "GeoPackage file directly. In this case you will need to replace the code\n",
        "above with the code below for downloading and extracting a Shapefile\n",
        "from a Zip archive:\n",
        "\n",
        "``` python\n",
        "src = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.zip?raw=true'\n",
        "dst = os.path.join('analysis','LSOAs.zip')\n",
        "zpd = 'analysis'\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "    if not os.path.exists(os.path.dirname(dst)):\n",
        "        os.makedirs(os.path.dirname(dst))\n",
        "\n",
        "    r = requests.get(src, stream=True)\n",
        "\n",
        "    with open(dst, 'wb') as fd:\n",
        "        for chunk in r.iter_content(chunk_size=128):\n",
        "            fd.write(chunk)\n",
        "\n",
        "if not os.path.exists(zpd):\n",
        "    os.makedirs(os.path.dirname(zpd))\n",
        "    \n",
        "zp = zipfile.ZipFile(dst, 'r')\n",
        "zp.extractall(zpd)\n",
        "zp.close()\n",
        "\n",
        "gdf = gpd.read_file(os.path.join('analysis','lsoas','Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.shp'))\n",
        "gdf.crs = {'init' :'epsg:27700'}\n",
        "print(\\\"Shape of LSOA file: {0} rows by {1} columns\\\".format(gdf.shape[0], gdf.shape[1]))\n",
        "gdf.set_index('lsoa11cd', drop=True, inplace=True)\n",
        "gdf.sample(4)\n",
        "```\n",
        "\n",
        "You can probably see why I’m a big fan of GeoPackages when they’re\n",
        "available!\n",
        "\n",
        "#### 3.4 Other Sources of Data\n",
        "\n",
        "If you’re more interested in US Census data then there’s a nice-looking\n",
        "(though I haven’t used it) [wrapper to the Census\n",
        "API](https://pypi.python.org/pypi/census). And [Spielman and\n",
        "Singleton](https://www.tandfonline.com/doi/full/10.1080/00045608.2015.1052335)\n",
        "have done some work on large-scale geodemographic clustering of U.S.\n",
        "Census geographies.\n",
        "\n",
        "#### 3.5 Direct Downloads\n",
        "\n",
        "These are already clean, so we can just copy them over.\n",
        "\n",
        "``` python\n",
        "for t in ['Occupation','Housing','Income']:\n",
        "    raw[t].rename(columns={'Code':'mnemonic'}, inplace=True)\n",
        "    print(raw[t].columns)\n",
        "    clean[t] = raw[t]\n",
        "```\n",
        "\n",
        "#### 3.6 Dwellings\n",
        "\n",
        "From dwellings we’re mainly interested in the housing type since we\n",
        "would expect that housing typologies will be a determinant of the types\n",
        "of people who live in an area. We *could* look at places with no usual\n",
        "residents as well, or explore the distribution of shared dwellings, but\n",
        "this is a pretty good start.\n",
        "\n",
        "``` python\n",
        "t = 'Dwellings'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'Whole house or bungalow: Detached', \n",
        "    'Whole house or bungalow: Semi-detached',\n",
        "    'Whole house or bungalow: Terraced (including end-terrace)',\n",
        "    'Flat, maisonette or apartment: Purpose-built block of flats or tenement',\n",
        "    'Flat, maisonette or apartment: Part of a converted or shared house (including bed-sits)',\n",
        "    'Flat, maisonette or apartment: In a commercial building'\n",
        "]\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "mapping = {}\n",
        "for c in selection[1:]:\n",
        "    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n",
        "    nm = m.group(1).strip()\n",
        "    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n",
        "    mapping[c] = nm\n",
        "\n",
        "clean[t].rename(columns=mapping, inplace=True)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.7 Age\n",
        "\n",
        "Clearly, some areas have more young people, some have older people, and\n",
        "some will be composed of families. A lot of these are going to be tied\n",
        "to ‘lifestage’ and so will help us to understand something about the\n",
        "types of areas in which they live.\n",
        "\n",
        "``` python\n",
        "t = 'Age'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'Age 0 to 14',\n",
        "    'Age 15 to 24',\n",
        "    'Age 25 to 44',\n",
        "    'Age 45 to 64',\n",
        "    'Age 65+'\n",
        "]\n",
        "\n",
        "# Derived columns\n",
        "raw[t]['Age 0 to 14']  = raw[t]['Age 0 to 4'] + raw[t]['Age 5 to 7'] + raw[t]['Age 8 to 9'] + raw[t]['Age 10 to 14'] \n",
        "raw[t]['Age 15 to 24'] = raw[t]['Age 15'] + raw[t]['Age 16 to 17'] + raw[t]['Age 18 to 19'] + raw[t]['Age 20 to 24']\n",
        "raw[t]['Age 25 to 44'] = raw[t]['Age 25 to 29'] + raw[t]['Age 30 to 44']\n",
        "raw[t]['Age 45 to 64'] = raw[t]['Age 45 to 59'] + raw[t]['Age 60 to 64']\n",
        "raw[t]['Age 65+']      = raw[t]['Age 65 to 74'] + raw[t]['Age 75 to 84'] + raw[t]['Age 85 to 89'] + raw[t]['Age 90 and over']\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.8 Ethnicity\n",
        "\n",
        "We might also think that the balance of ethnic groups might impact a\n",
        "categorisation of LSOAs in London.\n",
        "\n",
        "``` python\n",
        "t = 'Ethnicity'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'White', \n",
        "    'Mixed/multiple ethnic groups', \n",
        "    'Asian/Asian British', \n",
        "    'Black/African/Caribbean/Black British', \n",
        "    'Other ethnic group'\n",
        "]\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.9 Rooms\n",
        "\n",
        "Let’s next incorporate the amount of space available to each household.\n",
        "\n",
        "``` python\n",
        "t = 'Rooms'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'Occupancy rating (bedrooms) of -1 or less',\n",
        "    'Occupancy rating (rooms) of -1 or less',\n",
        "    'Average household size', \n",
        "#    'Average number of bedrooms per household',\n",
        "#    'Average number of rooms per household',\n",
        "]\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.10 Vehicles\n",
        "\n",
        "Car ownership and use is also known to be a good predictor of social and\n",
        "economic ‘status’: Guy Lansley’s article on the DLVA’s registration\n",
        "database offers a useful perpective on the usefulness of this approach.\n",
        "\n",
        "``` python\n",
        "t = 'Vehicles'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'No cars or vans in household', \n",
        "    '1 car or van in household',\n",
        "    '2 cars or vans in household', \n",
        "    '3 or more cars or vans in household'\n",
        "]\n",
        "\n",
        "# Calculate a new column\n",
        "raw[t]['3 or more cars or vans in household'] = raw[t]['3 cars or vans in household'] + raw[t]['4 or more cars or vans in household']\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.11 Tenure\n",
        "\n",
        "Ownership structure is another categorisation predictor.\n",
        "\n",
        "``` python\n",
        "t = 'Tenure'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'Owned', \n",
        "    'Social rented', \n",
        "    'Private rented',\n",
        "    'Shared ownership (part owned and part rented)'\n",
        "]\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "clean[t].rename(columns={'Shared ownership (part owned and part rented)':'Shared ownership'}, inplace=True)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.12 Qualifications\n",
        "\n",
        "You can find out a bit more about qualifications\n",
        "[here](https://www.surreyi.gov.uk/2011-census/highest-level-of-qualification/#header-qualifications-categories).\n",
        "\n",
        "``` python\n",
        "t = 'Qualifications'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'Highest level of qualification: Below Level 3 qualifications',\n",
        "    'Highest level of qualification: Level 3 and above qualifications',\n",
        "    'Highest level of qualification: Other qualifications'\n",
        "]\n",
        "\n",
        "# Derive a new aggregate field for 'didn't complete HS'\n",
        "raw[t]['Highest level of qualification: Below Level 3 qualifications'] = \\\n",
        "    raw[t]['No qualifications'] + \\\n",
        "    raw[t]['Highest level of qualification: Level 1 qualifications'] + \\\n",
        "    raw[t]['Highest level of qualification: Level 2 qualifications'] + \\\n",
        "    raw[t]['Highest level of qualification: Apprenticeship'] \n",
        "\n",
        "raw[t]['Highest level of qualification: Level 3 and above qualifications'] = \\\n",
        "    raw[t]['Highest level of qualification: Level 3 qualifications'] + \\\n",
        "    raw[t]['Highest level of qualification: Level 4 qualifications and above']\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "mapping = {}\n",
        "for c in selection[1:]:\n",
        "    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n",
        "    nm = m.group(1).strip()\n",
        "    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n",
        "    mapping[c] = nm\n",
        "\n",
        "clean[t].rename(columns=mapping, inplace=True)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.13 Activity\n",
        "\n",
        "``` python\n",
        "t = 'Activity'\n",
        "raw[t].columns\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Select the columns we're interested in analysing\n",
        "selection = ['mnemonic',\n",
        "    'Economically active: In employment',\n",
        "    'Economically active: Unemployed',\n",
        "    'Economically active: Full-time student',\n",
        "#    'Economically inactive: Retired',\n",
        "    'Economically inactive: Looking after home or family',\n",
        "    'Economically inactive: Long-term sick or disabled',\n",
        "#    'Economically inactive: Other'\n",
        "]\n",
        "\n",
        "# Drop everything *not* in the selection\n",
        "clean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n",
        "\n",
        "mapping = {}\n",
        "for c in selection[1:]:\n",
        "    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n",
        "    nm = m.group(1).strip()\n",
        "    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n",
        "    mapping[c] = nm\n",
        "\n",
        "clean[t].rename(columns=mapping, inplace=True)\n",
        "\n",
        "clean[t].sample(5, random_state=42)\n",
        "```\n",
        "\n",
        "#### 3.14 Standardisation with SKLearn\n",
        "\n",
        "Let’s try standardising the data now:\n",
        "\n",
        "``` python\n",
        "# Here's how we can rescale and transform data easily\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "```\n",
        "\n",
        "``` python\n",
        "random.seed(42)\n",
        "t   = random.sample(population=list(clean.keys()), k=1)[0]\n",
        "col = random.sample(population=list(clean[t].columns.values[1:]), k=1)[0]\n",
        "print(f\"Looking at {col} column from {t}.\")\n",
        "```\n",
        "\n",
        "Here’s the ‘original’ distribution:\n",
        "\n",
        "``` python\n",
        "plt.rcParams['figure.figsize']=(7,3)\n",
        "sns.distplot(clean[t][col], kde=False)\n",
        "```\n",
        "\n",
        "Here’s the version that has been re-scaled (standardised) using Min/Max\n",
        "rescaling:\n",
        "\n",
        "``` python\n",
        "plt.rcParams['figure.figsize']=(7,3)\n",
        "sns.distplot(preprocessing.minmax_scale(clean[t][col].values.reshape(-1,1)), kde=False)\n",
        "```\n",
        "\n",
        "Here’s a version that has been robustly rescaled:\n",
        "\n",
        "``` python\n",
        "plt.rcParams['figure.figsize']=(7,3)\n",
        "sns.distplot(preprocessing.robust_scale(clean[t][col].values.reshape(-1,1), quantile_range=[5.0, 95.0]), kde=False)\n",
        "```\n",
        "\n",
        "And here’s a version that has been Power Transformed… spot the\n",
        "difference!\n",
        "\n",
        "``` python\n",
        "sns.distplot(\n",
        "    preprocessing.power_transform(clean[t][col].values.reshape(-1,1), method='yeo-johnson'), kde=False)\n",
        "```\n",
        "\n",
        "Combining transformation *and* rescaling:\n",
        "\n",
        "``` python\n",
        "sns.distplot(\n",
        "    preprocessing.robust_scale(\n",
        "        preprocessing.power_transform(\n",
        "            clean[t][col].values.reshape(-1,1), method='yeo-johnson'\n",
        "        ), quantile_range=[5.0, 95.0] \n",
        "    ), \n",
        "    kde=False\n",
        ")\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Set up a new dictionary for the transforms\n",
        "transformed = {}\n",
        "\n",
        "transformer = preprocessing.PowerTransformer()\n",
        "scaler      = preprocessing.RobustScaler(quantile_range=[5.0, 95.0])\n",
        "#scaler      = preprocessing.MinMaxScaler()\n",
        "\n",
        "# Simple way to drop groups of data we don't want...\n",
        "suppress       = set(['Rooms','Vehicles'])\n",
        "\n",
        "for k in set(clean.keys()).difference(suppress):\n",
        "    print(f\"Transforming {k}\")\n",
        "    df = clean[k].copy(deep=True)\n",
        "    df.set_index('mnemonic', inplace=True)\n",
        "    \n",
        "    # For rescale and transforming everything when the operations\n",
        "    # apply to each Series separately you can do it as a 1-liner like this:\n",
        "    #df[df.columns] = scaler.fit_transform(transformer.fit_transform(df[df.columns]))\n",
        "\n",
        "    # To calculate within-*column* proportions it's like this:\n",
        "    #for c in df.columns.values: \n",
        "    #    df[c] = scaler.fit_transform( (df[c]/df[c].max() ).values.reshape(-1, 1) )\n",
        "\n",
        "    # To calculate within-*group* proportions it's like this:\n",
        "    if k in ['Housing','Income','Rooms']:\n",
        "        df[df.columns] = scaler.fit_transform( df[df.columns] )\n",
        "    else: \n",
        "        df['sum'] = df[list(df.columns)].sum(axis=1)\n",
        "        for c in df.columns.values:\n",
        "            if c == 'sum':\n",
        "                df.drop(['sum'], axis=1, inplace=True)\n",
        "            else:\n",
        "                df[c] = scaler.fit_transform( (df[c]/df['sum']).values.reshape(-1, 1) )\n",
        "            \n",
        "    #print(df.sample(5, random_state=42))\n",
        "    transformed[k] = df\n",
        "```\n",
        "\n",
        "#### 3.15 Creating the Single Data Set\n",
        "\n",
        "Now that we’ve converted everything to percentages, it’s time to bring\n",
        "the data together! We’ll initialise the data frame using the first\n",
        "matching data set, and then iterate over the rest, merging the data\n",
        "frames as we go.\n",
        "\n",
        "``` python\n",
        "matching = list(transformed.keys())\n",
        "print(\"Found the following data sets:\\n\\t\" + \", \".join(matching))\n",
        "\n",
        "# Initialise the data frame simply by grabbing the\n",
        "# very first existing data frame and copying it \n",
        "# directly (SCaled Data Frame == scdf)\n",
        "scdf  = transformed[matching[0]].copy() \n",
        "lsoac = clean[matching[0]].copy() \n",
        "\n",
        "for m in range(1, len(matching)):\n",
        "    scdf  = scdf.merge(transformed[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')\n",
        "    lsoac = lsoac.merge(clean[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')\n",
        "\n",
        "scdf.to_csv(os.path.join('data','Scaled_and_Transformed.csv.gz'), compression='gzip')\n",
        "lsoac.to_csv(os.path.join('data','Cleaned.csv.gz'), compression='gzip')\n",
        "```\n",
        "\n",
        "``` python\n",
        "print(\"Shape of full data frame is {0} by {1}\".format(scdf.shape[0], scdf.shape[1]))\n",
        "```\n",
        "\n",
        "With luck you still have 4,835 rows, but now you have rather fewer than\n",
        "88 columns.\n",
        "\n",
        "``` python\n",
        "random.seed(42)\n",
        "cols_to_plot = random.sample(population=list(scdf.columns.values), k=3)\n",
        "print(\"Columns to plot: \" + \", \".join(cols_to_plot))\n",
        "```\n",
        "\n",
        "``` python\n",
        "# The data as it is now...\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.pairplot(lsoac, \n",
        "             vars=cols_to_plot, \n",
        "             markers=\".\", height=3, diag_kind='kde')\n",
        "```\n",
        "\n",
        "``` python\n",
        "# The data as it is now...\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.pairplot(scdf, \n",
        "             vars=cols_to_plot, \n",
        "             markers=\".\", height=3, diag_kind='kde')\n",
        "```\n",
        "\n",
        "STOP. Making sure that you understand how and why this results differns\n",
        "from the *same* plot above.\n",
        "\n",
        ":::\n",
        "\n",
        "Right, so you can see that rescaling the dimension hasn’t *actually*\n",
        "changed the relationships within each dimension, or even between\n",
        "dimensions, but it has changed the overall range so that the the data is\n",
        "broadly re-centered on 0 but we *still* have the original outliers from\n",
        "the raw data. You could *also* do IQR standardisation (0.25 and 0.75)\n",
        "with the percentages, but in those cases you would have *more* outliers\n",
        "and then *more* extreme values skewing the results of the clustering\n",
        "algorithm.\n",
        "\n",
        "##### 3.15.1 Freeing Up Memory\n",
        "\n",
        "We now have quite a few variables/datasets in memory, so it’s a good\n",
        "idea to free up some RAM by getting rid of anything we no longer need…\n",
        "\n",
        "``` python\n",
        "in_scope  = set([x for x in dir() if not x.startswith('_')])\n",
        "to_delete = set(['raw','clean','transformed','col','k','c','lsoac','scdf'])\n",
        "z = list(in_scope.intersection(to_delete))\n",
        "del(z)\n",
        "```\n",
        "\n",
        "##### 3.15.2 Clustering Your Data\n",
        "\n",
        "OK, we’re finally here! It’s time to cluster the cleaned, normalised,\n",
        "and standardised data set! We’re going to start with the best-known\n",
        "clustering technique (k-means) and work from there… Don’t take my word\n",
        "for it, here are the [5 Clustering Techniques Every Data Scientist\n",
        "Should\n",
        "Know](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68).\n",
        "This is also a good point to refer back to some of what we’ve been doing\n",
        "(and it’s a good point to potentially disagree with me!) since\n",
        "[clustering in high dimensions can be\n",
        "problematic](https://towardsdatascience.com/how-to-cluster-in-high-dimensions-4ef693bacc6)\n",
        "(*i.e.* the more dimensions the worse the Euclidean distance gets as a\n",
        "cluster metric).\n",
        "\n",
        "The effectiveness of clustering algorithms is usually demonstrated using\n",
        "the ‘iris data’ – it’s available by default with both Seaborn and\n",
        "SciKit-Learn. This data doesn’t usually need normalisation but it’s a\n",
        "good way to start looking at the data across four dimensions and seeing\n",
        "how it varies and why some dimensions are ‘good’ for clustering, while\n",
        "others are ‘not useful’…\n",
        "\n",
        "Unfortunately, our data is a *lot* messier and has many more dimensions\n",
        "(\\>25) than this.\n",
        "\n",
        ":::\n",
        "\n",
        "-   Find the appropriate eps value: [Nearest Neighbour Distance\n",
        "    Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Nearest-Neighbor-Distance-Functions)\n",
        "\n",
        "##### 3.15.3 Brief Discussion\n",
        "\n",
        "In the practical I’ve followed the *Geocomputation* approach of\n",
        "basically converting everything to a share (percentage) and then\n",
        "clustering on that. This is *one* way to approach this problem, but\n",
        "there are *many* others. For instance, many people might skip the\n",
        "percentages part and apply robust rescaling\n",
        "([`sklearn.preprocessing.RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html))\n",
        "using centering and quantile standardisation (the 5th and 95th, for\n",
        "example) instead. And possibly using a normalising transformation (such\n",
        "as a [Power\n",
        "Transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html))\n",
        "as well.\n",
        "\n",
        "I would also consider using PCA on groups of related variables (*e.g.*\n",
        "the housing features as a group, the ethnicity features as a group,\n",
        "etc.) and then take the first few eigenvalues from each group and\n",
        "cluster on all of those together. This would remove quite a bit of the\n",
        "correlation between variables and still allow us to perform hierarchical\n",
        "and other types of clustering on the result. It might also do a better\n",
        "job of preserving outliers.\n",
        "\n",
        "##### 3.15.4 Create an Output Directory and Load the Data\n",
        "\n",
        "``` python\n",
        "o_dir = os.path.join('outputs','clusters')\n",
        "if os.path.isdir(o_dir) is not True:\n",
        "    print(\"Creating '{0}' directory.\".format(o_dir))\n",
        "    os.makedirs(o_dir)\n",
        "```\n",
        "\n",
        "``` python\n",
        "df = pd.read_csv(os.path.join('data','Scaled_and_Transformed.csv.gz'))\n",
        "df.rename(columns={'mnemonic':'lsoacd'}, inplace=True)\n",
        "df.set_index('lsoacd', inplace=True)\n",
        "df.describe()\n",
        "```\n",
        "\n",
        "``` python\n",
        "df.sample(3, random_state=42)\n",
        "```\n",
        "\n",
        "##### 3.15.5 Grab Borough Boundaries and Water Courses\n",
        "\n",
        "*Note:* if reading these GeoPackages gives you errors then you will need\n",
        "to comment out the following two lines from the `plt_ldn` function\n",
        "immediately below:\n",
        "\n",
        "``` python\n",
        "    w.plot(ax=ax, color='#79aef5', zorder=2)\n",
        "    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n",
        "```\n",
        "\n",
        "``` python\n",
        "# Load Water GeoPackage\n",
        "w_path = os.path.join('data','Water.gpkg')\n",
        "if not os.path.exists(w_path):\n",
        "    water = gpd.read_file('https://github.com/kingsgeocomp/applied_gsa/raw/master/data/Water.gpkg')\n",
        "    water.to_file(w_path)\n",
        "    print(\"Downloaded Water.gpkg file.\")\n",
        "else:\n",
        "    water = gpd.read_file(w_path)\n",
        "\n",
        "# Boroughs GeoPackage\n",
        "b_path = os.path.join('data','Boroughs.gpkg')\n",
        "if not os.path.exists(b_path):\n",
        "    boroughs = gpd.read_file('https://github.com/kingsgeocomp/applied_gsa/raw/master/data/Boroughs.gpkg')\n",
        "    boroughs.to_file(b_path)\n",
        "    print(\"Downloaded Boroughs.gpkg file.\")\n",
        "else:\n",
        "    boroughs = gpd.read_file(b_path)\n",
        "```\n",
        "\n",
        "##### 3.15.6 Useful Functions for Plotting\n",
        "\n",
        "``` python\n",
        "def plt_ldn(w=water, b=boroughs):\n",
        "    fig, ax = plt.subplots(1, figsize=(14, 12))\n",
        "    w.plot(ax=ax, color='#79aef5', zorder=2)\n",
        "    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n",
        "    ax.set_xlim([502000,563000])\n",
        "    ax.set_ylim([155000,201500])\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    return fig, ax\n",
        "\n",
        "def default_cmap(n, outliers=False):\n",
        "    cmap = mpl.cm.get_cmap('viridis_r', n)\n",
        "    colors = cmap(np.linspace(0,1,n))\n",
        "    if outliers:\n",
        "        gray = np.array([225/256, 225/256, 225/256, 1])\n",
        "        colors = np.insert(colors, 0, gray, axis=0)\n",
        "    return ListedColormap(colors)\n",
        "\n",
        "# mappable = ax.collections[-1] if you add the geopandas\n",
        "# plot last.\n",
        "def add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n",
        "    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n",
        "                    boundaries=breaks,\n",
        "                    extend=('min' if outliers else 'neither'), \n",
        "                    spacing='uniform',\n",
        "                    orientation='horizontal',\n",
        "                    fraction=0.05, shrink=0.5, pad=0.05)\n",
        "    cb.set_label(\"Cluster Number\")\n",
        "```\n",
        "\n",
        "##### 3.15.7 Select 4 Columns to Plot\n",
        "\n",
        "``` python\n",
        "random.seed(42)\n",
        "cols_to_plot = random.sample(population=list(df.columns.values), k=4)\n",
        "print(\"Columns to plot: \" + \", \".join(cols_to_plot))\n",
        "```\n",
        "\n",
        "##### 3.15.8 Storing Results\n",
        "\n",
        "``` python\n",
        "result_set = None\n",
        "\n",
        "def add_2_rs(s, rs=result_set):\n",
        "    if rs is None:\n",
        "        # Initialise\n",
        "        rs = pd.DataFrame()\n",
        "    rs[s.name] = s\n",
        "    return rs\n",
        "```\n",
        "\n",
        "#### 3.16 K-Means\n",
        "\n",
        "##### 3.16.1 Importing the Library\n",
        "\n",
        "``` python\n",
        "from sklearn.cluster import KMeans\n",
        "#help(KMeans)\n",
        "```\n",
        "\n",
        "The next few code blocks may take a while to complete, largely because\n",
        "of the `pairplot` at the end where we ask Seaborn to plot every\n",
        "dimension against every other dimension *while* colouring the points\n",
        "according to their cluster. I’ve reduced the plotting to just three\n",
        "dimensions, if you want to plot all of them, then just replace the array\n",
        "attached to `vars` with `main_cols`, but you have to bear in mind that\n",
        "that is plotting 4,300 points *each* time it draws a plot… and there are\n",
        "81 of them! It’ll take a while, but it *will* do it, and try doing that\n",
        "in Excel or SPSS?\n",
        "\n",
        "##### 3.16.2 A First Cluster Analysis\n",
        "\n",
        "``` python\n",
        "c_nm   = 'KMeans' # Clustering name\n",
        "k_pref = 6 # Number of clusters\n",
        "\n",
        "# Quick sanity check in case something hasn't\n",
        "# run successfully -- these muck up k-means\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "kmeans = KMeans(n_clusters=k_pref, n_init=20, random_state=42, n_jobs=-1).fit(cldf) # The process\n",
        "\n",
        "print(kmeans.labels_) # The results\n",
        "\n",
        "# Add it to the data frame\n",
        "cldf[c_nm] = pd.Series(kmeans.labels_, index=df.index) \n",
        "\n",
        "# How are the clusters distributed?\n",
        "cldf[c_nm].hist(bins=k_pref)\n",
        "\n",
        "# Going to be a bit hard to read if \n",
        "# we plot every variable against every\n",
        "# other variables, so we'll just pick a few\n",
        "sns.set(style=\"white\")\n",
        "sns.pairplot(cldf, \n",
        "             vars=cols_to_plot, \n",
        "             hue=c_nm, markers=\".\", height=3, diag_kind='kde')\n",
        "```\n",
        "\n",
        "``` python\n",
        "cgdf = gdf.join(cldf, how='inner')\n",
        "\n",
        "breaks = np.arange(0,cldf[c_nm].max()+2,1)\n",
        "cmap   = default_cmap(len(breaks))\n",
        "\n",
        "norm    = mpl.colors.BoundaryNorm(breaks, cmap.N)\n",
        "\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
        "cgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0)\n",
        "\n",
        "add_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n",
        "\n",
        "del(cgdf)\n",
        "```\n",
        "\n",
        "##### 3.16.3 One More Thing…\n",
        "\n",
        "There’s just *one* little problem: what assumption did I make when I\n",
        "started this *k*-means cluster analysis? It’s a huge one, and it’s one\n",
        "of the reasons that *k*-means clustering *can* be problematic when used\n",
        "naively…\n",
        "\n",
        "STOP. What critical assumption did we make when running this analysis?\n",
        "\n",
        ":::\n",
        "\n",
        "##### 3.16.4 The ‘Right’ Number of Clusters\n",
        "\n",
        "Again, there’s more than one way to skin this cat. In *Geocomputation*\n",
        "they use WCSS to pick the ‘optimal’ number of clusters. The idea is that\n",
        "you plot the average WCSS for each number of possible clusters in the\n",
        "range of interest (`2...n`) and then look for a ‘knee’ (i.e. kink) in\n",
        "the curve. The principle of this approach is that you look for the point\n",
        "where there is declining benefit from adding more clusters. The problem\n",
        "is that there is always *some* benefit to adding more clusters (the\n",
        "perfect clustering is *k==n*), so you don’t always see a knee.\n",
        "\n",
        "Another way to try to make the process of selecting the number of\n",
        "clusters a little less arbitrary is called the silhouette plot and (like\n",
        "WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome\n",
        "by examining the distance between each observation and the rest of the\n",
        "cluster. In this case it’s based on Partitioning Around the Medoid\n",
        "(PAM).\n",
        "\n",
        "Either way, to evaluate this in a systematic way, we want to do\n",
        "*multiple* *k*-means clusterings for *multiple* values of *k* and then\n",
        "we can look at which gives the best results…\n",
        "\n",
        "Let’s try it for the range 3-9.\n",
        "\n",
        "``` python\n",
        "# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "text = []\n",
        "\n",
        "for k in range(3,10):\n",
        "    # Debugging\n",
        "    print(\"Cluster count: \" + str(k))\n",
        "    \n",
        "    #############\n",
        "    # Do the clustering using the main columns\n",
        "    clusterer = KMeans(n_clusters=k, n_init=15, random_state=42, n_jobs=-1)\n",
        "    cluster_labels = clusterer.fit_predict(cldf)\n",
        "    \n",
        "    # Calculate the overall silhouette score\n",
        "    silhouette_avg = silhouette_score(cldf, cluster_labels)\n",
        "    text = text + [f\"For k={k} the average silhouette_score is: {silhouette_avg:6.4f}\"]\n",
        "    \n",
        "    # Calculate the silhouette values\n",
        "    sample_silhouette_values = silhouette_samples(cldf, cluster_labels)\n",
        "    \n",
        "    #############\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(9, 5)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1\n",
        "    ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n",
        "    \n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n",
        "    \n",
        "    y_lower = 10\n",
        "    \n",
        "    # For each of the clusters...\n",
        "    for i in range(k):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        \n",
        "        # Set the color ramp\n",
        "        #cmap  = cm.get_cmap(\"Spectral\")\n",
        "        color = plt.cm.Spectral(i/k)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed --\n",
        "    # we can only do this for the first two dimensions\n",
        "    # so we may not see fully what is causing the \n",
        "    # resulting assignment\n",
        "    colors = plt.cm.Spectral(cluster_labels.astype(float) / k)\n",
        "    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors)\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    \n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
        "                marker='o', c=\"white\", alpha=1, s=200)\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
        "\n",
        "    ax2.set_title(\"Visualization of the clustered data\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % k),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\".join(text))    \n",
        "\n",
        "del(cldf)\n",
        "```\n",
        "\n",
        "##### 3.16.5 Interpreting the Results\n",
        "\n",
        "STOP. Make sure that you understand how the silhouette plot and value\n",
        "work, and why your results may diverge from mine.\n",
        "\n",
        ":::\n",
        "\n",
        "We can use the largest average silhouette score to determine the\n",
        "‘natural’ number of clusters in the data, but that that’s only if we\n",
        "don’t have any kind of underlying *theory*, other *empirical evidence*,\n",
        "or even just a *reason* for choosing a different value… Again, we’re now\n",
        "getting in areas where *your judgement* and your ability to *communicate\n",
        "your rationale* to readers is the key thing.\n",
        "\n",
        "##### 3.16.6 Final Clustering\n",
        "\n",
        "Let’s repeat the clustering process *one more time* using the silhouette\n",
        "score as a guide and then map it.\n",
        "\n",
        "``` python\n",
        "#| scrolled: true\n",
        "c_nm = 'KMeans'\n",
        "# Quick sanity check in case something hasn't\n",
        "# run successfully -- these muck up k-means\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "k_pref = 4\n",
        "kmeans = KMeans(n_clusters=k_pref, n_init=75, random_state=42).fit(cldf)\n",
        "\n",
        "# Convert to a series\n",
        "s = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n",
        "\n",
        "# We do this for plotting\n",
        "cldf[c_nm] = s\n",
        "\n",
        "# We do this to keep track of the results\n",
        "result_set=add_2_rs(s)\n",
        "```\n",
        "\n",
        "##### 3.16.7 Mapping Results\n",
        "\n",
        "``` python\n",
        "cgdf = gdf.join(cldf, how='inner')\n",
        "\n",
        "breaks = np.arange(0,cldf[c_nm].max()+2,1)\n",
        "cmap   = default_cmap(len(breaks))\n",
        "\n",
        "norm    = mpl.colors.BoundaryNorm(breaks, cmap.N)\n",
        "\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\n",
        "cgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0)\n",
        "\n",
        "add_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n",
        "\n",
        "plt.savefig(os.path.join(o_dir,f\"{c_nm}-{k_pref}.png\"), dpi=200)\n",
        "del(cgdf)\n",
        "```\n",
        "\n",
        "To make sense of whether this is a ‘good’ result, you might want to\n",
        "visit\n",
        "[datashine](http://datashine.org.uk/#table=QS607EW&col=QS607EW0050&ramp=RdYlGn&layers=BTTT&zoom=10&lon=-0.1751&lat=51.4863)\n",
        "or think back to last year when we examined the NS-SeC data.\n",
        "\n",
        "You could also think of ways of plotting how these groups differ. For\n",
        "instance…\n",
        "\n",
        "##### 3.16.8 ‘Representative’ Centroids\n",
        "\n",
        "To get a sense of how these clusters differ we can try to extract\n",
        "‘representative’ centroids (mid-points of the multi-dimensional cloud\n",
        "that constitutes a cluster). In the case of *k*-means this will work\n",
        "quite will since the clusters are explicitly built around mean\n",
        "centroids. There’s also a *k*-medoids clustering approach built around\n",
        "the median centroid.\n",
        "\n",
        "``` python\n",
        "centroids = None\n",
        "for k in sorted(cldf[c_nm].unique()):\n",
        "    print(f\"Processing cluster {k}\")\n",
        "\n",
        "    clsoas = cldf[cldf[c_nm]==k]\n",
        "    if centroids is None:\n",
        "        centroids = pd.DataFrame(columns=clsoas.columns.values)\n",
        "    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n",
        "\n",
        "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
        "for i in range(0,len(centroids.index)):\n",
        "    row = centroids.iloc[i,:]\n",
        "    c_index = list(centroids.columns.values).index(c_nm)\n",
        "    for c in range(0,c_index):\n",
        "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
        "        odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
        "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
        "\n",
        "del(odf, centroids, cldf)\n",
        "```\n",
        "\n",
        "#### 3.17 DBScan\n",
        "\n",
        "Of course, as we’ve said above *k*-means is just one way of clustering,\n",
        "DBScan is another. Unlike *k*-means, we don’t need to specify the number\n",
        "of clusters in advance. Which sounds great, but we still need to specify\n",
        "*other* parameters (typically, these are known as *hyperparameters*\n",
        "because they are about specifying parameters that help the aglorithm to\n",
        "find the right solution… or final set of parameters!) and these can have\n",
        "a huge impact on our results!\n",
        "\n",
        "##### 3.17.1 Importing the Library\n",
        "\n",
        "``` python\n",
        "from sklearn.cluster import DBSCAN\n",
        "#?DSCAN\n",
        "```\n",
        "\n",
        "##### 3.17.2 Find a Reasonable Value for Epsilon\n",
        "\n",
        "Before we an use DBSCAN it’s useful to find a good value for Epsilon. We\n",
        "can [look for the point of maximum\n",
        "‘curvature’](https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc)\n",
        "in a nearest neigbhours plot. Which seems to be in the vicinity of 0.55.\n",
        "Tips on selecting `min_pts` can be [found\n",
        "here](https://towardsdatascience.com/how-dbscan-works-and-why-should-i-use-it-443b4a191c80).\n",
        "\n",
        "``` python\n",
        "# Quick sanity check in case something hasn't\n",
        "# run successfully -- these muck up k-means\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(cldf)\n",
        "distances, indices = nbrs.kneighbors(cldf)\n",
        "\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances)\n",
        "```\n",
        "\n",
        "##### 3.17.3 Exploration\n",
        "\n",
        "There are two values that need to be specified: `eps` and `min_samples`.\n",
        "Both seem to be set largely by trial and error. It’s easiest to set\n",
        "`min_samples` first since that sets a floor for your cluster size and\n",
        "then `eps` is basically a distance metric that governs how far away\n",
        "something can be from a cluster and still be considered part of that\n",
        "cluster.\n",
        "\n",
        "WARNING. This next step may take quite a lot of time since we are\n",
        "iterating through many, many values of Epsilon to explore how the\n",
        "clustering result changes and how well this matches up with (or doesn’t)\n",
        "the graph above.\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "c_nm = 'DBSCAN'\n",
        "\n",
        "# Quick sanity check in case something hasn't\n",
        "# run successfully -- these muck up k-means\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n",
        "\n",
        "# There's an argument for making min_samples = len(df.columns)+1\n",
        "\n",
        "el  = []\n",
        "\n",
        "max_clusters  = 10\n",
        "cluster_count = 1\n",
        "\n",
        "iters = 0\n",
        "\n",
        "for e in np.arange(0.15, 1.55, 0.01):\n",
        "    \n",
        "    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n",
        "    \n",
        "    # Run the clustering\n",
        "    dbs = DBSCAN(eps=e, min_samples=12, n_jobs=-1).fit(cldf.values)\n",
        "    \n",
        "    # See how we did\n",
        "    s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n",
        "    \n",
        "    row = [e]\n",
        "    data = s.value_counts()\n",
        "    for c in range(-1, max_clusters+1):\n",
        "        try:\n",
        "            row.append(data[c])\n",
        "        except KeyError:\n",
        "            row.append(None)\n",
        "    \n",
        "    el.append(row)\n",
        "    iters+=1\n",
        "\n",
        "edf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n",
        "edf.sample(random_state=42)\n",
        "\n",
        "# Make numeric display a bit neater\n",
        "pd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n",
        "\n",
        "print(\"Done.\")\n",
        "```\n",
        "\n",
        "``` python\n",
        "odf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n",
        "\n",
        "for i in range(0,len(edf.index)):\n",
        "    row = edf.iloc[i,:]\n",
        "    for c in range(1,len(edf.columns.values)):\n",
        "        if not np.isnan(row[c]):\n",
        "            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n",
        "            odf = odf.append(d, ignore_index=True)       \n",
        "```\n",
        "\n",
        "``` python\n",
        "xmin = odf[odf.Cluster=='Cluster 0'].Epsilon.min()\n",
        "xmax = odf[(odf.Cluster=='Cluster -1') & (odf.Count < cldf.shape[0]/5)].Epsilon.min()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.set_xlim([xmin,xmax])\n",
        "sns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster')\n",
        "```\n",
        "\n",
        "##### 3.17.4 Final Clustering\n",
        "\n",
        "``` python\n",
        "e = 0.835\n",
        "dbs = DBSCAN(eps=e, min_samples=12, n_jobs=-1).fit(cldf.values)\n",
        "s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n",
        "cldf[c_nm] = s\n",
        "result_set=add_2_rs(s)\n",
        "print(s.value_counts())\n",
        "```\n",
        "\n",
        "``` python\n",
        "cgdf = gdf.join(cldf, how='inner')\n",
        "\n",
        "breaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\n",
        "cmap   = default_cmap(len(breaks), outliers=True)\n",
        "norm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n",
        "\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "\n",
        "cgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n",
        "\n",
        "add_colorbar(ax.collections[-1], ax, cmap, norm, breaks, outliers=True)\n",
        "\n",
        "plt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\n",
        "del(cgdf)\n",
        "```\n",
        "\n",
        "##### 3.17.5 ‘Representative’ Centroids\n",
        "\n",
        "To get a sense of how these clusters differ we can try to extract\n",
        "‘representative’ centroids (mid-points of the multi-dimensional cloud\n",
        "that constitutes a cluster). For algorithms other than *k*-means it may\n",
        "be better to use medians than means.\n",
        "\n",
        "``` python\n",
        "centroids = None\n",
        "for k in sorted(cldf[c_nm].unique()):\n",
        "    print(f\"Processing cluster {k}\")\n",
        "\n",
        "    clsoas = cldf[cldf[c_nm]==k]\n",
        "    if centroids is None:\n",
        "        centroids = pd.DataFrame(columns=clsoas.columns.values)\n",
        "    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n",
        "\n",
        "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
        "for i in range(0,len(centroids.index)):\n",
        "    row = centroids.iloc[i,:]\n",
        "    c_index = list(centroids.columns.values).index(c_nm)\n",
        "    for c in range(0,c_index):\n",
        "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
        "        odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "# Drop outliers\n",
        "odf = odf[odf.Cluster >= 0]\n",
        "\n",
        "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
        "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
        "\n",
        "del(odf, centroids)\n",
        "```\n",
        "\n",
        "#### 3.18 OPTICS Clustering\n",
        "\n",
        "This is a fairly new addition to `sklearn` and is similar to DBSCAN in\n",
        "that there are very few (if any) parameters to specify. This means that\n",
        "we’re making fewer assumptions about the nature of any clustering in the\n",
        "data. It also allows us to have outliers that don’t get assigned to\n",
        "*any* cluster. The focus is mainly on local density, so in some sense\n",
        "it’s more like a geographically aware clustering approach, but applied\n",
        "in the data space, not geographical space.\n",
        "\n",
        "##### 3.18.1 Importing the Library\n",
        "\n",
        "``` python\n",
        "from sklearn.cluster import OPTICS\n",
        "```\n",
        "\n",
        "##### 3.18.2 Final Clustering\n",
        "\n",
        "WARNING. This next step may take quite a lot of time since the algorithm\n",
        "is making far fewer assumptions about the structure of the data. On a\n",
        "2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "c_nm = 'Optics'\n",
        "\n",
        "# Can try to set this from DBSCAN results\n",
        "e = 0.9850\n",
        "\n",
        "# Quick sanity check in case something hasn't\n",
        "# run successfully -- these muck up k-means\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "import math\n",
        "\n",
        "# Run the clustering\n",
        "opt = OPTICS(min_samples=len(df.columns)+1, max_eps=math.ceil(e * 100)/100, n_jobs=-1).fit(cldf.values)\n",
        "\n",
        "# See how we did\n",
        "s = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\n",
        "cldf[c_nm] = s\n",
        "result_set=add_2_rs(s)\n",
        "\n",
        "# Distribution\n",
        "print(s.value_counts())\n",
        "```\n",
        "\n",
        "##### 3.18.3 Mapping Clustering Results\n",
        "\n",
        "WARNING. My sense is that these results are a bit rubbish: the majority\n",
        "of items are assigned to *one cluster*??? I’ve tried PCA on the\n",
        "standardised data and that made little difference. This should also have\n",
        "worked *better* but it seems that a small number of LSOAs are so utterly\n",
        "different that the more sophisticated clustering algorithm effectively\n",
        "‘chokes’ on them.\n",
        "\n",
        ":::\n",
        "\n",
        "``` python\n",
        "cgdf = gdf.join(cldf, how='inner')\n",
        "\n",
        "breaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\n",
        "cmap   = default_cmap(len(breaks), outliers=True)\n",
        "norm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n",
        "\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "\n",
        "cgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n",
        "\n",
        "add_colorbar(ax.collections[-1], ax, cmap, norm, breaks, outliers=True)\n",
        "\n",
        "plt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\n",
        "del(cgdf)\n",
        "```\n",
        "\n",
        "##### 3.18.4 ‘Representative’ Centroids\n",
        "\n",
        "To get a sense of how these clusters differ we can try to extract\n",
        "‘representative’ centroids (mid-points of the multi-dimensional cloud\n",
        "that constitutes a cluster). For algorithms other than *k*-Means it may\n",
        "be better to use medians, not means.\n",
        "\n",
        "``` python\n",
        "centroids = None\n",
        "for k in sorted(cldf[c_nm].unique()):\n",
        "    print(f\"Processing cluster {k}\")\n",
        "\n",
        "    clsoas = cldf[cldf[c_nm]==k]\n",
        "    if centroids is None:\n",
        "        centroids = pd.DataFrame(columns=clsoas.columns.values)\n",
        "    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n",
        "\n",
        "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
        "for i in range(0,len(centroids.index)):\n",
        "    row = centroids.iloc[i,:]\n",
        "    c_index = list(centroids.columns.values).index(c_nm)\n",
        "    for c in range(0,c_index):\n",
        "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
        "        odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
        "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
        "\n",
        "del(odf, centroids)\n",
        "```\n",
        "\n",
        "STOP. Aside from the fact that we should probably reduce the number of\n",
        "dimensions on which we’re clustering, what about the process of\n",
        "selecting variables (a.k.a. feature selection) might have led to the\n",
        "result that our results are a bit crap? *Hint: how did we decide what to\n",
        "keep and what to drop, and is this a robust approach?*\n",
        "\n",
        ":::\n",
        "\n",
        "#### 3.19 HDBSCAN\n",
        "\n",
        "Not implemented, but you could give it a try after installing the\n",
        "package:\n",
        "\n",
        "``` bash\n",
        "conda activate <your environment name here>\n",
        "conda install -c conda-forge sklearn-contrib-hdbscan\n",
        "```\n",
        "\n",
        "Then it should be something like:\n",
        "\n",
        "``` python\n",
        "import hdbscan\n",
        "clusterer = hdbscan.HDBSCAN()\n",
        "# HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n",
        "#    gen_min_span_tree=False, leaf_size=40, memory=Memory(cachedir=None),\n",
        "#    metric='euclidean', min_cluster_size=5, min_samples=None, p=None)\n",
        "clusterer.fit(<data>)\n",
        "clusterer.labels_\n",
        "```\n",
        "\n",
        "#### 3.20 Hierarchical Clustering\n",
        "\n",
        "Probably not appropriate as it tends to be confused by noise.\n",
        "\n",
        "#### 3.21 Self-Organising Maps\n",
        "\n",
        "SOMs offer a third type of clustering algorithm. They are a relatively\n",
        "‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts\n",
        "to the data: we’re going to see how this works over the next few code\n",
        "blocks, but the main thing is that, unlike the above approaches, SOMs\n",
        "build a 2D map of a higher-dimensional space and use this as a mechanism\n",
        "for subsequently clustering the raw data. In this sense there is a\n",
        "conceptual link between SOMs and PCA or tSNE (another form of\n",
        "dimensionality reduction).\n",
        "\n",
        "##### 3.21.1 (Re)Installing SOMPY\n",
        "\n",
        "WARNING. The maintainers of the main SOMPY library are fairly inactive,\n",
        "so we’ve had to write our own version that fixes a few Python3 bugs, but\n",
        "this means that it can’t be installed the ‘usual’ way without also\n",
        "having Git installed. Consequently, I have left the output from SOMPY in\n",
        "place so that you can see what it will produce *even if you cannot\n",
        "successfully install SOMPY during this practical*\n",
        "\n",
        "To work out if there is an issue, check to see if the `import` statement\n",
        "below gives you errors:\n",
        "\n",
        "``` python\n",
        "from sompy.sompy import SOMFactory\n",
        "```\n",
        "\n",
        "If this import has failed with a warning about being unable to find SOM\n",
        "or something similar, then you will need to *re*-install SOMPY using a\n",
        "fork that I created on our Kings GSA GitHub account. For *that* to work,\n",
        "you will need to ensure that you have `git` installed.\n",
        "\n",
        "If the following Terminal command (which should also work in the Windows\n",
        "Terminal) does not give you an error then `git` is already installed:\n",
        "\n",
        "``` shell\n",
        "git --version\n",
        "```\n",
        "\n",
        "To install `git` on a Mac is fairly simple. Again, from the Terminal\n",
        "issue the following command:\n",
        "\n",
        "``` shell\n",
        "xcode-select --install\n",
        "```\n",
        "\n",
        "This installation may take some time over eduroam since there is a lot\n",
        "to download.\n",
        "\n",
        "Once that’s complete, you can move on to installing SOMPY from our fork.\n",
        "On a Mac this is done on the Terminal with:\n",
        "\n",
        "``` shell\n",
        "conda activate <your kernel name here>\n",
        "pip install -e git+git://github.com/kingsgeocomp/SOMPY.git#egg=SOMPY\n",
        "conda deactivate\n",
        "```\n",
        "\n",
        "On Windows you probably drop the `conda` part of the command.\n",
        "\n",
        "##### 3.21.2 Training the SOM\n",
        "\n",
        "We are going to actually train the SOM using the input data. This is\n",
        "where you specify the input parameters that have the main effect on the\n",
        "clustering results.\n",
        "\n",
        "``` python\n",
        "from sompy.sompy import SOMFactory\n",
        "```\n",
        "\n",
        "``` python\n",
        "c_nm = 'SOM'\n",
        "\n",
        "# Quick sanity check in case something hasn't\n",
        "# run successfully -- these muck up k-means\n",
        "cldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n",
        "\n",
        "sm = SOMFactory().build(\n",
        "    cldf.values, mapsize=(10,15),\n",
        "    normalization='var', initialization='random', component_names=cldf.columns.values)\n",
        "sm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\n",
        "```\n",
        "\n",
        "How good is the fit?\n",
        "\n",
        "``` python\n",
        "topographic_error  = sm.calculate_topographic_error()\n",
        "quantization_error = np.mean(sm._bmu[1])\n",
        "print(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\n",
        "```\n",
        "\n",
        "How do the results look?\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.mapview import View2D\n",
        "view2D = View2D(10, 10, \"rand data\", text_size=10)\n",
        "view2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\n",
        "plt.savefig(os.path.join(o_dir, f\"{c_nm}-Map.png\"), dpi=200)\n",
        "```\n",
        "\n",
        "##### 3.21.3 Here’s What I Got\n",
        "\n",
        "WARNING. These are the results from the approach that is closest to the\n",
        "one outlined in *Geocomputation*.\n",
        "\n",
        ":::\n",
        "\n",
        "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/SOM-Map.png\" alt=\"SOM Clustering Results\" width=\"800\" />\n",
        "\n",
        "How many data points were assigned to each BMU?\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.bmuhits import BmuHitsView\n",
        "vhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\n",
        "vhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\n",
        "plt.savefig(os.path.join(o_dir,f\"{c_nm}-BMU Hit View.png\"), dpi=200)\n",
        "```\n",
        "\n",
        "##### 3.21.4 BMU Hit Map\n",
        "\n",
        "WARNING. These are the results from the approach that is closest to the\n",
        "one outlined in *Geocomputation*.\n",
        "\n",
        ":::\n",
        "\n",
        "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/SOM-BMU Hit View.png\" alt=\"SOM Heat Map Results\" width=\"800\" />\n",
        "\n",
        "How many clusters do we want and where are they on the map?\n",
        "\n",
        "``` python\n",
        "from sompy.visualization.hitmap import HitMapView\n",
        "\n",
        "k_val = 5\n",
        "sm.cluster(k_val)\n",
        "hits  = HitMapView(15, 15, \"Clustering\", text_size=14)\n",
        "a     = hits.show(sm)\n",
        "plt.savefig(os.path.join(o_dir,f\"{c_nm}-Hit Map View.png\"), dpi=200)\n",
        "```\n",
        "\n",
        "##### 3.21.5 Clustering the BMUs\n",
        "\n",
        "WARNING. These are the results from the approach that is closest to the\n",
        "one outlined in *Geocomputation*.\n",
        "\n",
        ":::\n",
        "\n",
        "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/SOM-Hit Map View.png\" alt=\"SOM Clustering Results\" width=\"800\" />\n",
        "\n",
        "Finally, let’s get the cluster results and map them back on to the data\n",
        "points:\n",
        "\n",
        "``` python\n",
        "# Get the labels for each BMU\n",
        "# in the SOM (15 * 10 neurons)\n",
        "clabs = sm.cluster_labels\n",
        "\n",
        "try:\n",
        "    cldf.drop(c_nm,inplace=True,axis=1)\n",
        "except KeyError:\n",
        "    pass\n",
        "\n",
        "# Project the data on to the SOM\n",
        "# so that we get the BMU for each\n",
        "# of the original data points\n",
        "bmus  = sm.project_data(cldf.values)\n",
        "\n",
        "# Turn the BMUs into cluster labels\n",
        "# and append to the data frame\n",
        "s = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n",
        "\n",
        "cldf[c_nm] = s\n",
        "result_set = add_2_rs(s)\n",
        "```\n",
        "\n",
        "``` python\n",
        "cgdf = gdf.join(cldf, how='inner')\n",
        "\n",
        "breaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\n",
        "cmap   = default_cmap(len(breaks))\n",
        "norm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n",
        "\n",
        "fig, ax = plt_ldn()\n",
        "fig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n",
        "\n",
        "cgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n",
        "\n",
        "add_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n",
        "\n",
        "plt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\n",
        "del(cgdf)\n",
        "```\n",
        "\n",
        "##### 3.21.6 Result!\n",
        "\n",
        "WARNING. These are the results from the approach that is closest to the\n",
        "one outlined in *Geocomputation*.\n",
        "\n",
        ":::\n",
        "\n",
        "<img src=\"https://github.com/kingsgeocomp/applied_gsa/raw/master/img/SOM.png\" alt=\"SOM Clustering Results Mapped\" width=\"800\" />\n",
        "\n",
        "##### 3.21.7 Representative Centroids\n",
        "\n",
        "``` python\n",
        "centroids = None\n",
        "for k in sorted(cldf[c_nm].unique()):\n",
        "    print(f\"Processing cluster {k}\")\n",
        "\n",
        "    clsoas = cldf[cldf[c_nm]==k]\n",
        "    if centroids is None:\n",
        "        centroids = pd.DataFrame(columns=clsoas.columns.values)\n",
        "    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n",
        "\n",
        "odf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\n",
        "for i in range(0,len(centroids.index)):\n",
        "    row = centroids.iloc[i,:]\n",
        "    c_index = list(centroids.columns.values).index(c_nm)\n",
        "    for c in range(0,c_index):\n",
        "        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n",
        "        odf = odf.append(d, ignore_index=True)\n",
        "\n",
        "g = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\n",
        "g = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n",
        "\n",
        "del(odf, centroids)\n",
        "```\n",
        "\n",
        "### 4 Wrap-Up\n",
        "\n",
        "-   Find the appropriate eps value: [Nearest Neighbour Distance\n",
        "    Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Nearest-Neighbor-Distance-Functions)\n",
        "    or [Interevent Distance\n",
        "    Functions](https://nbviewer.jupyter.org/github/pysal/pointpats/blob/master/notebooks/distance_statistics.ipynb#Interevent-Distance-Functions)\n",
        "-   [Clustering\n",
        "    Points](https://darribas.org/gds_course/content/bH/lab_H.html#clusters-of-points)\n",
        "-   [Regionalisation algorithms with Aglomerative\n",
        "    Clustering](https://darribas.org/gds_course/content/bG/lab_G.html#regionalization-algorithms)\n",
        "\n",
        "You’ve reached the end, you’re done…\n",
        "\n",
        "Er, no. This is barely scratching the surface! I’d suggest that you go\n",
        "back through the above code and do three things: 1. Add a lot more\n",
        "comments to the code to ensure that really have understood what is going\n",
        "on. 2. Try playing with some of the parameters (e.g. my thresholds for\n",
        "skew, or non-normality) and seeing how your results change. 3. Try\n",
        "outputting additional plots that will help you to understand the\n",
        "*quality* of your clustering results (e.g. what *is* the makeup of\n",
        "cluster 1? Or 6? What has it picked up? What names would I give these\n",
        "clsuters?).\n",
        "\n",
        "If all of that seems like a lot of work then why not learn a bit more\n",
        "about machine learning before calling it a day?\n",
        "\n",
        "See: [Introduction to Machine Learning with\n",
        "Scikit-Learn](http://www.slideshare.net/BenjaminBengfort/introduction-to-machine-learning-with-scikitlearn)."
      ],
      "id": "949155d9-5712-4193-9c8f-b76968556d5f"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/jreades/anaconda3/envs/sds/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  }
}