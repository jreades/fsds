---
author: "Jon Reades"
email: "j.reades@ucl.ac.uk"
title: "Running Multiple Containers"
subtitle: "Using Pods to link it all together"
other: ""
date-as-string: "1st October 2025"
format: revealjs
user: postgres 
password: test
db: test
pod: myapp
---

## Find a PostGIS Image

Search [Quay.io](https://quay.io/) for an image:

- On a modern M-chip Mac you need to use `arm64` images, 
- On Windows it's usually `amd64`

::: {.callout-tip} 
However, in most cases you don't need to search for these explicitly because 'builds' for most images are completed for both.
::: 

## Create a 'Pod'

By default, containers can't talk to each other for security reasons. We need to join them together in a 'pod' to allow this to happen. 

This command creates a pod that exposes two 'ports' (`8888` and `5432`) to the wider world. 

```bash
podman pod create -p 8888:8888 -p 5432:5432 {{< meta pod >}}
```

::: {.callout-tip}

This 'maps' port `8888` inside the pod to port `8888` outside the pod, but we could change it to `-p 7777:8888` so that requests for `7777` from the outside world are 'forwarded' to `8888` inside the pod. That would allow copies of containers to run at the same time in different pods.

:::

## Attach PostGIS to the Pod

There's a lot going on here that took quite some to figure out^[Part of an ESRC research project I was supporting.], but the key thing turned out to be the `pg_hba.conf` file which tells Postgres on which ports it can use. 

```bash
podman run --rm -d --name postgres --pod {{< meta pod >}} \
-e POSTGRES_USER={{< meta user >}} \
-e POSTGRES_PASSWORD={{< meta password >}} \
-e POSTGRES_DB={{< meta db >}} \
-e PGDATA=/var/lib/postgresql/data/pgdata \
-v "${PWD}"/data/postgres:/var/lib/postgresql/data \
-v /tmp:/tmp \
-v "${PWD}"/data/postgres/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf \
quay.io/taolu/postgis:14-3.5-alpine
```

::: {.notes}

This command is telling Podman to start a postgis image (`14-3.5-alpine`) it downloads from Quay with a test of startup options set at launch. The first three are specifying the databasei, user, and password. The remainder are connecting various 'mount points' on the container to locations on the host computer. So the data that is added to the database will be stored under the *current working directory* (`PWD` == Print Working Directory). We allow Postgis to the use the computer's `/tmp` folder for working data. And the final bit is taking a local copy that we've set up of `pg_hba.conf` and putting that in the place that a regular Postgres server would expect to find it.

:::

##  Attach SDS to the Pod

Only containers *inside* the Pod can talk to other containers in the pod. So for the SDS container to talk to PostGIS, they both need to be attached to the pod using `{{< meta pod >}}`.

```bash
podman run --rm -d --name sds --pod={{< meta pod >}} \
-v "$(pwd):/home/jovyan/work" \
docker.io/{{< var docker.all >}} \
start.sh \
jupyter lab --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''
```

You can now connect using your browser: [http://localhost:8888/](http://localhost:8888/)

::: {.callout-tip}

The rest of this short tutorial is all run on the SDS container using your browser as the interface. This is true even for bits about the command line interface: in Jupyter you pick `File` > `New` > `Terminal`.

:::


## Install psycopg2

If I haven't had time to update the SDS container then you can do this on the *SDS Terminal* in your browser using the folllowing command:

```bash
pip install psycopg2`
```

This is because the `sqlalchemy` framework is already there but the `psycopg2` driver for Postgres isn't.

## Load Data

::: {.callout-tip}

### Run Python 

Now you will start a new Notebook (`File` > `New` > `Notebook`) and create code cells for each of the following sections of code.

:::

```python
# Connect to the database
from sqlalchemy import create_engine
engine = create_engine('postgresql://postgres:{{< meta password >}}@localhost:5432/{{< meta db >}}')

# Insert data into a new table
import geopandas as gpd
gdf = gpd.read_file('work/data/src/TM_WORLD_BORDERS-0.3.gpkg')
gdf.to_postgis('world', engine)
```

## Querying Data

```python
# Get information about the database
insp = inspect(engine) 
insp.get_table_names()
```

```python
# Get data out of the database
import geopandas as gpd

# Simple query
gdf = gpd.read_postgis('SELECT * FROM msoa', geom_col='geometry', con=engine)

# What's in the table
gdf.head(2)
```

## Plotting the Data

Since PostGIS is geospatial and Python can 'talk' to PostGIS:

```python
# Plotting the table
gdf.plot()
```

## More Complex Queries

```python
# Query the database
gdf = gpd.read_postgis("""
    SELECT * 
    FROM msoa 
    WHERE "MSOA21NM" 
    LIKE 'Waltham%%'
""", geom_col='geometry', con=con)
gdf.plot()
```

## Querying the Data without Python

One final thing: if you run a Terminal on your computer (so *not* in the SDS terminal any more) you can directly query the data.

```bash
psql -h localhost -p 5432 -U {{< meta user >}} -d {{< meta db >}}
```

::: {.callout-tip}

The 'host' computer is the only other machine that can access the pod.

:::

```sql
SELECT * FROM world LIMIT 0;
SELECT "NAME", "ISO3", "POP2005", "REGION" FROM world LIMIT 5;
SELECT "NAME", "POP2005" FROM world WHERE AREA > 900000;
```
