---
author: "Jon Reades"
email: "j.reades@ucl.ac.uk"
title: "Diverse Data"
other: ""
date-as-string: "1st October 2025"
format: revealjs
---

> One of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few...

::: {.notes}

I like easy measures.

:::

## Location Quotient {.smaller}

The LQ for industry *i* in zone *z* is the share of employment for *i* in *z* divided by the share of employment of *i* in the entire region *R*.
$$
LQ_{zi} = \dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}
$$

|  &nbsp; | High Local Share | Low Local Share |
| ------- | ---------------- | --------------- |
| High Regional Share | $$\approx 1$$ | $$< 1$$ |
| Low Regional Share  | $$> 1$$ | $$\approx 1$$ |

::: {.notes}

In other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.

Note that this is influenced by small sample sizes (e.g. the number of Fijians in Britain).

:::

## Herfindahl-Hirschman index {.smaller}

The HHI for an industry *i* is the sum of squared market shares for each company in that industry:
$$
H = \sum_{i=1}^{N} s_{i}^{2}
$$

| Concentration Level | HHI | 
| ------------------- | --: |
| Monopolistic: one firm accounts for 100% of the market | $$1.0$$ |
| Oligopolistic: top five firms account for 60% of the market | $$\approx 0.8$$  |
| Competitive: anything else? | $$< 0.5$$? |

::: {.notes}

If $s_{i} = 1$ then $s_{i}^{2} = 1$, while if $s_{i} = 0.5$ then $s_{i}^{2} = 0.25$ and $s_{i} = 0.1$ then $s_{i}^{2} = 0.01$.

This can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse *in aggregate* but highly segregated *at the local level*.

Note that this is influenced by the number of 'firms' (or ethnicities or...).

:::

## Shannon Entropy

Shannon Entropy is an information-theoretic measure:
$$
H(X) = - \sum_{i=1}^{n} P(x_{i}) log P(x_{i})
$$


::: {.notes}

I often think of this as 'surprise': a high entropy measure means that it's hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has *low* entropy (even if the result is surprising on the level of intuition: I wasn't expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s.

:::

## Additional Resources {.smaller}

- [Stop aggregating away the signal in your data](https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/)

# Thank You {background-image="/img/web/title-slide.png" background-color="#0F0C11" background-opacity="0.2"}

<h3>References</h3>