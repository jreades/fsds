---
author: "Jon Reades"
email: "j.reades@ucl.ac.uk"
title: "Scaling Data"
other: ""
date-as-string: "1st October 2025"
format: revealjs
---

# The Challenge

<h3>
As the data get bigger<br />
Pandas gets slower.
</h3>

::: {.notes}

I don't consider myself to be someone who works with 'big data' any more, but the data sets still range into the tens of millions and there are points where pandas just doesn't work. It gets slower or even breaks down entirely. So now I'm going to talk about some technologies that have changed my life. Sad as that sounds, it has reduced the time need to load and query large data sets from minutes to milliseconds. And it has solved many of the errors that I just listed above!

:::

## The Options

1. Make pandas *faster*.
2. Make the storage *better*.
3. Make using more cores *easier*.

::: {.notes}

Within each of these options there are several types of solutions:

1. To make pandas faster we can look both at how it runs code and how it manages data internally.
2. We could also store data more intelligently so that pandas can access it more efficiently.
3. Or we could move away from pandas entirely as a tool for managing and analysing data.

There are pros and cons to each of these.

:::

## Make Pandas Faster: Code

::: {style="margin-top: 200px;"}
<h3>Introducing:<br /> lazy evaluation and filtergraphs.</h3>
:::

::: {.notes}

This is the approach taken by Polars and Dask. So while something like strong data typing (which all of the tools I'm talking about today do) is an 'easy' win because it allows you to plan make a better plan for handling the data, there are bigger gains to be made from 'lazy evaluation'.

:::

## Make Pandas Faster: Storage

::: {style="margin-top: 200px;"}
<h3>Introducing:<br /> Arrow and Parquet.</h3>
:::

::: {.notes}

- **Arrow** is an *in-memory* columnar format for data. Data is stored in a structured way *in RAM* making it blazingly fast for operations.
- **Parquet** is a highly-compressed columnar *file* format for data. Data is stored in a structured way *on your hard drive*.
- **Feather** is a raw storage format for Arrow.

TL;DR: for most applications Parquet will give nice, small files on disk and the benefits of columnar file storage; for computationally intensive applications where disk space and interoperability with other systems isn't an issue then Feather might work.

:::

## Make Pandas Faster: Scaling

::: {style="margin-top: 200px;"}
<h3>Introducing:<br /> Parallelisation.</h3>
:::

::: {.notes}

We've already talked about this a bit, but let's be explicit: pandas is basically bound to one processor, and if we can find ways to lift that restriction then things are going to run a lot faster. If you build in lazy evaluation, query planning, and better storage, then removing this restriction is a lot easier.

:::


# The Tools

::: {style="margin-top: 50px;"}
<h3>Introducing:<br /> Polars, Dask, DuckDB, and Postgres.</h3>
:::

## What About Polars?

:::: {.columns}
::: {.column width="30%"}

[![](./img/Polars_software_logo.svg)](https://pola.rs/)

:::
::: {.column width="69%"}

Pros: 

- Looks like pandas code^[This can be a 'gotcha', see [migration guide](https://docs.pola.rs/user-guide/migration/pandas/)], doesn't run like it.
- Multi-threaded execution.
- Arrow behind the scenes.
- Streaming data processing possible.

Cons:

- Easy to miss out on the benefits.
- Designed for single machines^[Paid for cloud services gets around this, sort of.].

:::
::::

## What About the Duck?

:::: {.columns}
::: {.column width="30%"}

[![](./img/DuckDB.png)](https://duckdb.org/)

:::
::: {.column width="69%"}

Pros:

- Serverless SQL queries against Parquet files.
- Queries can be returned as Pandas data frames.
- Select and filter *before* loading using Arrow.
- Plugins for geospatial and remote parquet files.

Cons:

- Need to learn SQL.
- Still constrained by available memory.
- Really still optimised for a single machine.

:::
::::

## What About Dask?

:::: {.columns}
::: {.column width="30%"}

[![](./img/dask_horizontal.svg){width="125%"}](https://www.dask.org/)

:::
::: {.column width="69%"}

Pros:

- Distributed and parallel data processing
- Queries returned as Pandas data frames
- Lazy evaluation of code
- Built-in scaling

Cons:

- Lots of overhead for small queries.
- Moving a lot of data around.
- Not all problems readily parallelisable.

:::
::::

## Postgres?

:::: {.columns}
::: {.column width="30%"}

[![](./img/postgres.svg)](https://www.postgresql.org/)

:::
::: {.column width="69%"}

Pros: 

- Fully-fledged database.
- Industry-leading handling of geospatial data.

Cons:

- Need to learn SQL.
- Need to learn how to design databases to maximise gains.
- Moving lots of data in/out of the database is *slow*.

:::
::::


# Is it Worth It?

## A Simple Comparison

```{python}
import pandas as pd
import polars as pl
import duckdb as duck
import time
import seaborn as sns
sns.load_dataset('titanic').to_parquet('titanic.pq')
```

```{python}
#| echo: False
pd.read_parquet('titanic.pq').head()
```

## Measuring Performance

We can compare performance using profiling^[See especially the `timeit` module discussed [here](https://data-ai.theodo.com/en/technical-blog/python-profiling)]:

```{python}
import timeit

def load_pandas():
    pd.read_parquet('titanic.pq', columns=['age']).age.mean()

def load_polars():
    pl.read_parquet('titanic.pq', columns=['age'])['age'].mean()

def load_duck():
    duck.sql("SELECT MEAN(age) FROM read_parquet('titanic.pq')")

num_reps = 1000
```

## The Results

#### Pandas

```{python}
pd_execution_time = timeit.timeit(load_pandas, number=num_reps)
print(f"Pandas execution time: {pd_execution_time:.6f} seconds")
```

#### Polars

```{python}
pl_execution_time = timeit.timeit(load_polars, number=num_reps)
print(f"Polars execution time: {pl_execution_time:.6f} seconds")
```

```{python}
#| echo: False
#| output: asis
print(f'[{abs((pl_execution_time - pd_execution_time)/pd_execution_time*100):0.2f}% faster than pandas.]{{style="font-size: 0.8em;"}}')
```

#### DuckDB

```{python}
db_execution_time = timeit.timeit(load_duck, number=num_reps)
print(f"DuckDB execution time: {db_execution_time:.6f} seconds")
```

```{python}
#| echo: False
#| output: asis
print(f'[{abs((db_execution_time - pd_execution_time)/pd_execution_time*100):0.2f}% faster than pandas and', end=" ")
print(f'{abs((db_execution_time - pl_execution_time)/pl_execution_time*100):0.2f}% faster than polars.]{{style="font-size: 0.8em;"}}')
```

# Summarising Data

## Useful, But Limited?

| Method           | Achieves                        |
| ---------------- | ------------------------------- |
| `count()`          | Total number of items           |
| `first()`, `last()`  | First and last item             |
| `mean()`, `median()` | Mean and median                 |
| `min()`, `max()`     | Minimum and maximum             |
| `std()`, `var()`     | Standard deviation and variance |
| `mad()`            | Mean absolute deviation         |
| `prod()`           | Product of all items            |
| `sum()`            | Sum of all items                |

::: {.notes}

Here are a bunch of pandas functions that have to do with aggregating data in some way. Some of these you'll have seen before, some you may not. However, up to this point if you wanted to to know the median price of each type of Airbnb listing, or the sum of each type of vehicle sold, you'd have had to select out one listing type or vehicle type, call median or sum, and then remember the result. Let's change that.

:::

## Grouping Operations {.smaller}

In Pandas these follow a split / apply / combine approach:

::: {layout="[0.24, -0.02, 0.24, -0.02, 0.23, -0.02, 0.23]"}

:::: {#first-column}

|    | class |   sex |   age |
| :- | :---- | :---- | ----: |
| 0  | Third |    m  | 22.0 |
| 1  | First |    f  | 38.0 |
| 2  | Third |    f  | 26.0 |
| 3  | First |    f  | 35.0 |
| 4  | Third |    m  | 35.0 |
| 5  | Third |    m  |  NaN |
| 6  | First |    m  | 54.0 |
| 7  | Third |    m  |  2.0 |

: 1. Raw data {.hover .striped}

::::
:::: {#second-column}

|    | class |   sex |   age |
| :- | :---- | :---- | ----: |
| 1  | First |     f |  38.0 |
| 3  | First |     f |  35.0 |
| 6  | First |     m |  54.0 |

: 2a. Grouped First Class {.hover .striped}

|    | class |   sex |   age |
| :- | :---- | :---- | ----: |
| 0  | Third |     m |  22.0 |
| 2  | Third |     f |  26.0 |
| 4  | Third |     m |  35.0 |
| 5  | Third |     m |   NaN |
| 7  | Third |     m |   2.0 |

: 2b. Grouped Third Class {.hover .striped}

::::
:::: {#third-column}

|    | class |   age |
| :- | :---- | ----: |
|    | First | 42.33 |

: 3a. Summary First Class {.hover .striped}

|    | class |   age |
| :- | :---- | ----: |
|    | Third | 21.25 |

: 3b. Summary Third Class {.hover .striped}
::::
:::: {#fourth-column}

|    | class |   age |
| :- | :---- | ----: |
|    | First | 42.33 |
|    | Third | 21.25 |

: 4. Result {.hover .striped}
::::

:::


## In Practice

```{python}
#| echo: false
#| output: asis
df = pd.read_parquet('titanic.pq')
```

```python
grouped_df = df.groupby(<fields>).agg(['<function1>', ..., '<functionn>'])
```

For instance, to grouped statistics by Local Authority:

```{python}
# This will throw an error when there are non-numeric columns
# grouped_df = df.groupby('class').agg(['sum','mean'])
# This will only aggregate one column
grouped_df = df.groupby('class').age.agg(['sum','mean'])
grouped_df.head()
```

## Grouping by Arbitrary Mappings

In cases where we want to group by a derived value that doesn't exist in the data we can apply a mapping as part of the process:

```{python}
mapping = {'First':'Fancy', 'Second':'Not-Fancy', 'Third':'Not-Fancy'}
grouped_df = df.set_index('class').groupby(mapping).age.agg(['sum','mean'])
grouped_df.head()
```

::: {.aside}

*Note* you need set to the index for this to work!

:::

## Apply

You can take this one step further by 'applying' an arbitrary function in a row- or column-wise fashion.

```{python}
def norm(x): # x is a column from the grouped df
	return x['age'] * x['fare'] 

df.groupby('class').apply(norm)
```

::: {.aside}
Not saying this is a *useful* calculation.
:::

## Pivot Tables

A 'special case' of Group By features:

- Commonly-used in business to summarise data for reporting.
- Grouping (summarisation) happens along both axes (Group By operates only on one).
- `pandas.cut(<series>, <bins>)` can be a useful feature here since it chops a continuous feature into bins suitable for grouping.

## In Pandas

```{python}
df = pd.read_parquet('titanic.pq')
age = pd.cut(df['age'], [0, 18, 80])
pd.set_option('display.float_format', '{:,.3f}'.format)
df.pivot_table('survived', ['sex', age], 'class', observed=False)
```

## In SQL^[Note the critical difference with Pandas output!]

```{python}
sql = """PIVOT (
    SELECT sex, survived, class, CASE WHEN age < 18 then '(0, 18]' else '(18, 80]' end as age 
    FROM read_parquet('titanic.pq'))
  ON class USING SUM(survived) AS survived GROUP BY sex, age"""
  
pd.set_option('display.float_format', '{:,.0f}'.format)
df = duck.sql(sql).to_df().set_index('sex')
df.columns = ['Age','First','Second','Third']
df
```

## Counts

Other ways to summarise:

```{python}
duck.sql("""SELECT class, sex, case WHEN survived=1 THEN 'Y' ELSE 'N' end AS survived, COUNT(*) AS n
  FROM read_parquet('titanic.pq') WHERE sex='female' GROUP BY sex, class, survived
  ORDER BY sex ASC, class ASC, survived DESC""").to_df().set_index(['class','sex'])
```

## Additional Resources

- [Life Altering Postgres Patterns](https://mccue.dev/pages/3-11-25-life-altering-postgresql-patterns)
- [pandas 2.0 and the Arrow revolution (Part 1)](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)
- [What parquet files are my preferred API for bulk open data](https://www.robinlinacre.com/parquet_api/)
- [DuckDB Documentation](https://duckdb.org/docs/)
- [Migrating from Pandas to Polars](https://docs.pola.rs/user-guide/migration/pandas/)
- [Read remote Parquet files with DuckDB](https://rfsaldanha.github.io/posts/query_remot_parquet_file.html)

# Thank You {background-image="/img/web/title-slide.png" background-color="#f7eff5" background-opacity="0.2"}

<h3>References</h3>

