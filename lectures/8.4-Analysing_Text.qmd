---
author: "Jon Reades"
email: "j.reades@ucl.ac.uk"
title: "Analysing Text"
other: ""
date-as-string: "1st October 2025"
format: revealjs
---

## Dummy Variables

The concept of 'dummy variables' in economics/regression is a useful point to start thinking about text:

| Topic         | Dummy | 
| ------------- | ----- | 
| News          | 0     | 
| Culture       | 1     | 
| Politics      | 2     | 
| Entertainment | 3     | 

::: {.notes}
What's the problem with this approach when you're thinking about the topics in a document? You either have to assign each document to one, and only one, topic, or you need a _lot_ of dummies.
:::

## One-Hot Encoders

| Document           | UK   | Top  | Pop  | Coronavirus |
| ------------------ | ---- | ---- | ---- | ----------- |
| News item          | 1    | 1    | 0    | 1           |
| Culture item       | 0    | 1    | 1    | 0           |
| Politics item      | 1    | 0    | 0    | 1           |
| Entertainment item | 1    | 1    | 1    | 1           |

::: {.notes}

One-Hot encoders are not _often_ used this way, but for keyword detection or keyword-based classification this might be appropriate: i.e. this keyword was used in this document! 

So the big difference is One Hot == $n$ variables, Dummy == $n-1$.

Definitely some 'gotchas' in deployment: one-hot models shouldn't have an intercept unless you apply a 'ridge shrinkage penalty'. Standardisation affects whether or not an intercept is needed.

:::

## The 'Bag of Words'

Just like a one-hot (binarised approach) on preceding slide but now we count occurences:

| Document           | UK   | Top  | Pop  | Coronavirus |
| ------------------ | ---- | ---- | ---- | ----------- |
| News item          | 4    | 2    | 0    | 6           |
| Culture item       | 0    | 4    | 7    | 0           |
| Politics item      | 3    | 0    | 0    | 3           |
| Entertainment item | 3    | 4    | 8    | 1           |


## BoW in Practice

Enter, stage left, [scikit-learn](https://scikit-learn.org/stable/):

```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()

# Non-reusable transformer
vectors = vectorizer.fit_transform(texts)

# Reusable transformer
vectorizer.fit(texts)
vectors1 = vectorizer.transform(texts1)
vectors2 = vectorizer.transform(texts2)

print(f'Vocabulary: {vectorizer.vocabulary_}')
print(f'All vectors: {vectors.toarray()}')
```


## TF/IDF

Builds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:

$$
W(t,d) = TF(t,d) / log(N/DF_{t})
$$

For example: 

- If the term 'cat' appears 3 times in a document of 100 words then Term Frequency given by: $TF(t,d)=3/100$, and
- If there are 10,000 documents and cat appears in 1,000 documents then Normalised Document Frequency given by: $N/DF_{t}=10000/1000$ so the Inverse Document Frequency is $log(10)=1$,
- So IDF=1 and TF/IDF=0.03.


## TF/IDF in Practice


```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

# Non-reusable form:
vectors=vectorizer.fit_transform(texts)

# Reusable form:
vectorizer.fit(texts)
vectors = vectorizer.transform(texts)

print(f'Vocabulary: {vectorizer.vocabulary_}')
print(f'Full vector: {vectors.toarray()}')
```

::: {.notes}

What do you notice about how this code differs from the CountVectorizer?

:::

## Term Co-Occurence Matrix (TCM) {.smaller}

Three input texts with a distance weighting ($d/2$, where $d<3$):

- the cat sat on the mat
- the cat sat on the fluffy mat
- the fluffy ginger cat sat on the mat

|        | fluffy |  mat | ginger |  sat |   on |  cat |  the |
| :----- | -----: | ---: | -----: | ---: | ---: | ---: | ---: |
| fluffy |        |    1 |      1 |      |  0.5 |  0.5 |  2.0 |
| mat    |        |      |        |      |  0.5 |      |  1.5 |
| ginger |        |      |        |  0.5 |  0.5 |  1.0 |  1.5 |
| sat    |        |      |        |      |  3.0 |  3.0 |  2.5 |
| on     |        |      |        |      |      |  1.5 |  3.0 |
| cat    |        |      |        |      |      |      |  2.0 |
| the    |        |      |        |      |      |      |      |


## How Big is a TCM?

The problem:

- A corpus with 10,000 words has a TCM of size $10,000^{2}$ (100,000,000)
- A corpus with 50,000 words has a TCM of size $50,000^{2}$ (2,500,000,000)

Cleaning is _necessary_, but it's not _sufficient_ to create a tractable TCM on a large corpus.

::: {.notes}
Although usually used in the context of clustering, there's also a curse of dimensionality here!
:::

## Enter Document Embeddings

Typically, some kind of 2 or 3-layer neural network that 'learns' how to _embed_ the TCM into a lower-dimension representation: from $m \times m$ to $m \times n, n << m$. 

Similar to PCA in terms of what we're trying to achieve, but the *process* is utterly different.

::: {.notes}
Many different approaches, but [GloVe](https://nlp.stanford.edu/projects/glove/) (Stanford), [word2vec](https://code.google.com/archive/p/word2vec/) (Google), [fastText](https://fasttext.cc/docs/en/english-vectors.html) (Facebook), and [ELMo](https://allennlp.org/elmo) (Allen) or [BERT](https://github.com/google-research/bert) (Google) are probably the best-known.
:::

## Sentiment Analysis 

Requires us to deal in great detail with bi- and tri-grams because *negation* and *sarcasm* are hard. Also tends to require training/labelled data.

![](./img/Sentiment_Analysis.png){width="450"}

[Source](https://medium.com/@tomyuz/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42).


## Clustering {.smaller}

| Cluster | Geography | Earth Science | History | Computer Science |  Total |
| :------ | --------: | ------------: | ------: | ---------------: | -----: |
| 1       |       126 |           310 |     104 |           11,018 | 11,558 |
| 2       |       252 |        10,673 |     528 |              126 | 11,579 |
| 3       |       803 |           485 |   6,730 |              135 |  8,153 |
| 4       |       100 |           109 |   6,389 |               28 |  6,626 |
| Total   |     1,281 |        11,577 |  13,751 |           11,307 | 37,916 |


## Topic Modelling

Learning associations of words (or images or many other things) to hidden 'topics' that generate them:

![](img/LDA.png)


## Word Clouds


![](img/Cluster_Clouds.png)


## Additional Resources {.smaller}

- [One-Hot vs Dummy Encoding](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn)
- [Categorical encoding using Label-Encoding and One-Hot-Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)
- [Count Vectorization with scikit-learn](https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e)
- [Corpus Analysis with Spacy](https://programminghistorian.org/en/lessons/corpus-analysis-with-spacy)
- [The TF*IDF Algorithm Explained](https://www.onely.com/blog/what-is-tf-idf/)
- [How to Use TfidfTransformer and TfidfVectorizer](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.X7gXhhP7Tlw)
- [SciKit Learn Feature Extraction](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction)
- [Your Guide to LDA](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
- [Machine Learning — Latent Dirichlet Allocation LDA](https://jonathan-hui.medium.com/machine-learning-latent-dirichlet-allocation-lda-1d9d148f13a4)
- [A Beginner’s Guide to Latent Dirichlet Allocation(LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)
- [Analyzing Documents with TF-IDF](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf)

Basically any of the lessons on [The Programming Historian](https://programminghistorian.org/en/lessons/).


## Additional Resources {.smaller}

:::: {.columns}

::: {.column width="50%"}

- [Introduction to Word Embeddings](https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc)
- [The Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
- [Using GloVe Embeddings](http://text2vec.org/glove.html)
- [Working with Facebook's FastText Library](https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/)
- [Word2Vec and FastText Word Embedding with Gensim](https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c)
- [Sentence Embeddings. Fast, please!](https://towardsdatascience.com/fse-2b1ffa791cf9)
:::

::: {.column width="50%"}

- [PlasticityAI Embedding Models](https://github.com/plasticityai/magnitude)
- [Clustering text documents using *k*-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)
- [Topic extraction with Non-negative Matrix Factorization and LDA](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)
- [Topic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison](https://towardsdatascience.com/topic-modeling-with-lsa-plsa-lda-nmf-bertopic-top2vec-a-comparison-5e6ce4b1e4a5)
- 
:::

::::

# Thank You {background-image="/img/web/title-slide.png" background-color="#f7eff5" background-opacity="0.2"}

<h3>References</h3>