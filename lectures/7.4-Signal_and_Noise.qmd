---
author: "Jon Reades"
email: "j.reades@ucl.ac.uk"
title: "Signal & Noise"
other: ""
date-as-string: "1st October 2025"
format: revealjs
---

## What is it?

<div align="center">
<img src="https://github.com/jreades/i2p/raw/master/lectures/img/Doggy.png" width="90%" />
</div>


## What is it?

<div align="center">
<img src="https://github.com/jreades/i2p/raw/master/lectures/img/Cow.png" width="90%" />
</div>


## What is it?

<div align="center">
<img src="https://github.com/jreades/i2p/raw/master/lectures/img/Kanizsa.png" width="65%" />
</div>


## Start with a Chart

The problem of relying on statistics alone was amply illustrated by Anscombe’s Quartet (1973)...

- We *are not* very good at looking at spreadsheets. 
- We *are* very good at spotting patterns visually.

Sometimes, we are *too* good; that’s where the stats comes in. Think of it as the ‘tiger in the jungle’ problem..


## Anscombe's Quartet {.smaller}

|   X1 |    Y1 |   X2 |   Y2 |   X3 |    Y3 |   X4 |   Y4 |
| ---: | ----: | ---: | ---: | ---: | ----: | ---: | ---: |
| 10.0 |  8.04 | 10.0 | 9.14 | 10.0 |  7.46 | 10.0 | 6.58 |
|  8.0 |  6.95 |  8.0 | 8.14 |  8.0 |  6.77 |  8.0 | 5.76 |
| 13.0 |  7.58 | 13.0 | 8.74 | 13.0 | 12.74 | 13.0 | 7.71 |
|  9.0 |  8.81 |  9.0 | 8.77 |  9.0 |  7.11 |  9.0 | 8.84 |
| 11.0 |  8.33 | 11.0 | 9.26 | 11.0 |  7.81 | 11.0 | 8.47 |
| 14.0 |  9.96 | 14.0 | 8.10 | 14.0 |  8.84 | 14.0 | 7.04 |
|  6.0 |  7.24 |  6.0 | 6.13 |  6.0 |  6.08 |  6.0 | 5.25 |
|  4.0 |  4.26 |  4.0 | 3.10 |  4.0 |  5.39 |  4.0 | 12.5 |
| 12.0 | 10.84 | 12.0 | 9.13 | 12.0 |  8.15 | 12.0 | 5.56 |
|  7.0 |  4.82 |  7.0 | 7.26 |  7.0 |  6.42 |  7.0 | 7.91 |
|  5.0 |  5.68 |  5.0 | 4.74 |  5.0 |  5.73 |  5.0 | 6.89 |


## Summary Statistics for the Quartet {.smaller}

| Property                        |          Value |
| ------------------------------- | -------------: |
| Mean of `x`                     |            9.0 |
| Variance of `x`                 |           11.0 |
| Mean of `y`                     |            7.5 |
| Variance of `y`                 |           4.12 |
| Correlation between `x` and `y` |          0.816 |
| Linear Model                    | `y = 3 + 0.5x` |


## But What do They _Look_ Like?

![](img/Quartet.png)


# The Tiger that Isn't

> I would argue that the basic purpose of charts and of statistics as a whole is to help us untangle signal from noise. We are ‘programmed’ to see signals, so we need to set the standard for ‘it’s a tiger!’ quite high in research & in policy-making.



::: {.notes}

Or, as Albert Einstein reportedly said: "If I can't picture it, I can't understand it."

:::

## Think it Through

You can make a lot of progress in your research without *any* advanced statistics!

- A 'picture' isn’t *just* worth 1,000 words, it could be a whole dissertation!
- The right chart makes your case eloquently and succinctly. 

Always ask yourself:

- What am I trying to say?
- How can I say it most effectively?
- Is there anything I’m overlooking in the data?

A good chart is a good way to start!


## What Makes a Good Plot?

A good chart or table:

1. Serves a **purpose** — it is clear how it advances the argument in a way that could not be done in the text _alone_.
2. Contains only what is **relevant** — zeroes in on what the reader _needs_ and is not needlessly cluttered.
3. Uses precision that is **meaningful** — doesn't clutter the chart with needless numbers.

::: {.notes}

Far too many charts or tables could be easily written up in a single sentence.

Far too many charts or tables contain redundancy, clutter, and 'flair'.

Don't report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale.

:::

## For Example... {.smaller}

How much precision is necessary in measuring [degrees at the equator](https://en.wikipedia.org/wiki/Decimal_degrees#Precision)?

| Decimal Places | Degrees    | Distance |
| -------------- | ---------- | -------: |
| 0              | 1          |    111km |
| 1              | 0.1        |   11.1km |
| 2              | 0.01       |   1.11km |
| 3              | 0.001      |     111m |
| 4              | 0.0001     |    11.1m |
| 5              | 0.00001    |    1.11m |
| 6              | 0.000001   |   11.1cm |
| 7              | 0.0000001  |   1.11cm |
| 8              | 0.00000001 |   1.11mm |


## Goals by World Cup Final

![](img/Goals_1.png){align="center"}

## Goals by World Cup Final

![](img/Goals_2.png){align="center"}


## Average Goals by World Cup Final

![](img/Goals_3.png){align="center"}

::: {.notes}

In 1982 the number of teams went from 16 to 24, and in 1998 it went from 24 to 32!

:::

## How far from Equality?

![](img/Brazil_Senate_1.png){align="center"}


## How far from Equality?

![](img/Brazil_Senate_2.png){align="center"}


## The Purpose of a Chart

The purpose of a graph is to show that there are *relationships* within the data set that are _not_ trivial/expected.

Choose the chart to highlight relationships, or the lack thereof:

- Think of a chart or table as part of your ‘argument’ – if you can’t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably don’t need it.
- Identify & prioritise the relationships in the data.
- Choose a chart type/chart symbology that gives emphasis to the most important relationships.

# If a picture is worth 1,000 words, make sure those words aren't "blah, blah, blah..."


# Beyond the Chart

## Not Everyone Likes Tables

> Getting information from a table is like extracting sunlight from a cucumber.
> Arthur & Henry Fahrquhar 1891)


## Real Numbers

Consider the difference in emphasis between:

- 11316149
- 11,316,149
- 11.3 million
- 11 x 10$^{6}$
- 22%
- 22.2559%

Always keep in mind the *purpose* of the number.


## There's _Still_ a Role for Tables

Why a table is sometimes better than a chart:

- You need to present data values with greater detail
- You need to enable readers to draw comparisons between data values
- You need to present the same data in multiple ways (*e.g.* raw number *and* percentage)
- You want to show many dimensions for a small number of observations

::: {.notes}

*e.g.* percentage of people falling into each ethnic or income category for a small number of wards or boroughs.

:::

## Undergraduate Tables (Failing Grade)

![](img/ugtables-1.png)


## Undergraduate Tables (Passing Grade)

![](img/ugtables-2.png)


## Postgraduate Tables (Failing Grade)

![](img/pgtables-1.png)


## Postgraduate Tables (Passing Grade)

![](img/pgtables-2.png)


## Design for Tables

Principles:

- Reduce the number of lines to a minimum (and you should almost never need vertical lines).
- Use ‘white-space’ to create visual space between groups of unrelated (or less related) elements.
- Remove redundancy (if you find yourself typing ‘millions’ or ‘GBP’ or ‘Male’ repeatedly then you’ve got redundancy).
- Ensure that meta-data is clearly separate from, but attached to, the graph (*i.e.* source, title, etc.).


# In Practice

## Getting Started {.smaller}

You can follow along by loading the Inside Airbnb sample:

```python
import pandas as pd
import geopandas as gpd
url='https://bit.ly/3I0XDrq'
df = pd.read_csv(url)
df.set_index('id', inplace=True)
df['price'] = df.price.str.replace('$','',regex=False).astype('float')
gdf = gpd.GeoDataFrame(df, 
            geometry=gpd.points_from_xy(
                        df['longitude'], 
                        df['latitude'], 
                        crs='epsg:4326'
            )
      )
gdf.to_file('Airbnb_Sample.gpkg', driver='GPKG')
```

::: {.notes}
Note that this  (re)loads the sampled Airbnb data from GitHub every time you run it. For a large data set on someone else'e server you might want to save and (re)load it locally. A simple helper function would then just check if the file already existed locally *before* trying to download the file again: that would allow you to work while offline and speed up your code substantially too!
:::

## What Can We Do? (Series) {.smaller}

This is by no means all that we can do...

+---------------------------------------------------------+--------------------------+
| Command                                                 | Returns                  |
+=========================================================+==========================+
| ```python                                               | ```                      |
| print(f"Host count is {gdf.host_name.count()}")         | Count of non-nulls       |
| print(f"Mean is {gdf.price.mean():.0f}")                | Mean                     |
| print(f"Max price is {gdf.price.max()}")                | Highest value            |
| print(f"Min price is {gdf.price.min()}")                | Lowest value             |
| print(f"Median price is {gdf.price.median()}")          | Median                   |
| print(f"Standard dev is {gdf.price.std():.2f}")         | Standard deviation       |
| print(f"25th quantile is {gdf.price.quantile(q=0.25)}") | 25th quantile            |
| ```                                                     | ```                      |
+---------------------------------------------------------+--------------------------+

: Series-level Methods.

## What Can We Do? (Data Frame) {.smaller}

+----------------------+------------------------------------------+
| Command              | Returns                                  |
+======================+==========================================+
| ```python            | ```                                      |
| print(df.mean())     | Mean of each column                      |
| print(df.count())    | Number of non-null values in each column |
| print(df.max())      | Highest value in each column             |
| # ...                | $\vdots$                                 |
| print(df.corr())     | Correlation between columns              |
| print(df.describe()) | Summarise                                |
| ```                  | ```                                      |
+----------------------+------------------------------------------+

::: {.notes}

Notice how we have the *same* functionality, but it operates at the level of the data set itself now. We gain a few *new* functions as well that relate to interactions between columns (a.k.a. data series).

:::

## Measures

So pandas provides functions for commonly-used measures:

```python
print(f"{df.price.mean():.2f}")
print(f"{df.price.median():.2f}")
print(f"{df.price.quantile(0.25):.2f}")
```

Output:
```
118.4542
80.50
40.75
```

## More Complex Measures {.smaller}

But Pandas *also* makes it easy to derive new variables... Here's the *z*-score:

$$ z = \frac{x - \bar{x}}{s}$$

```python
df['zscore'] = (df.price - df.price.mean())/df.price.std()
df.plot.box(column='zscore')
```

![](./img/Price_BoxPlot.png)

## And Even More Complex {.smaller}

And here's the Interquartile Range Standardised score:

$$ x_{iqrs} = \frac{x - \widetilde{x}}{Q_{75} - Q_{25}} $$

```python
df['iqr_std'] = (df.price - df.price.median())/ \
      (df.price.quantile(q=0.75)-df.price.quantile(q=0.25))
df.plot.box(column='iqr_std')
```

![](./img/Price_IQR_Standardised.png)


## The Plot Thickens

We'll get to more complex plotting over the course of the term, but here's a good start for *exploring* the data! All plotting depends on `matplotlib` which is the ogre in the attic to R's `ggplot`.

```python
import matplotlib.pyplot as plt
```

Get used to this import as it will allow you to save and manipulate the figures created in Python. It is *not* the most intuitive approach (unless you've used MATLAB before) but it *does* work. 

# **Confession Time:** I *do* like `ggplot` and sometimes even finish off graphics for articles in R *just* so that I can use `ggplot`; however, it *is* possible to generate great-looking figures in `matplotlib` but it is often more work because it's a _lot_ less intuitive.


## Boxplot

```python
df.price.plot.box()
plt.savefig('pboxplot.png', dpi=150, transparent=True)
```

![](img/pboxplot.png)


## Frequency

```python
df.room_type.value_counts().plot.bar()
plt.savefig('phistplot.png', dpi=150, transparent=True)
```

![](img/phistplot.png)


## A Correlation Heatmap

We'll get to these in more detail in a couple of weeks, but here's some output... 

![](img/correlation.png)


## A 'Map'

```python
df.plot.scatter(x='longitude',y='latitude')
plt.savefig('pscatterplot.png', dpi=150, transparent=True)
```

![](img/pscatterplot.png)


## A Fancy 'Map'

```python
df.plot.scatter(x='longitude',y='latitude',
                c='price',colormap='viridis',
                figsize=(10,5),title='London',
                grid=True,s=24,marker='x')
plt.savefig('pscatterplot.png', dpi=150, transparent=True)
```

![](img/pscatterplot-fancy.png)


## An Actual 'Map'

```python
gdf.plot(column='price', cmap='viridis', 
         scheme='quantiles', markersize=8, legend=True)
```

![](img/scattermap.png)


## Additional Resources {.smaller}

There's [so much more](https://www.google.com/search?q=eda+with+pandas) to find, but:

:::: {.columns}
::: {.column width="50%"}
- [Pandas Reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)
- [A Guide to EDA in Python](https://levelup.gitconnected.com/cozy-up-with-your-data-6aedfb651172) (Looks very promising)
- [EDA with Pandas on Kaggle](https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas)
- [EDA Visualisation using Pandas](https://towardsdatascience.com/exploratory-data-analysis-eda-visualization-using-pandas-ca5a04271607)
- [Python EDA Analysis Tutorial](https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python)
- [Better EDA with Pandas Profiling](https://towardsdatascience.com/a-better-eda-with-pandas-profiling-e842a00e1136) **[Requires module installation]**
- [EDA: DataPrep.eda vs Pandas-Profiling](https://towardsdatascience.com/exploratory-data-analysis-dataprep-eda-vs-pandas-profiling-7137683fe47f) **[Requires module installation]**
:::
::: {.column width="50%"}
- [A Data Science Project for Beginners (EDA)](https://medium.com/analytics-vidhya/a-data-science-project-for-beginners-exploratory-data-analysis-eda-d334f58f94ee)
- [EDA: A Pracitcal Guide and Template for Structured Data](https://towardsdatascience.com/exploratory-data-analysis-eda-a-practical-guide-and-template-for-structured-data-abfbf3ee3bd9)
- [EDA—Don't ask how, ask what](https://medium.com/towards-artificial-intelligence/exploratory-data-analysis-eda-dont-ask-how-ask-what-2e29703fb24a) (Part 1)
- [Preparing your Dataset for Modeling – Quickly and Easily](https://medium.com/towards-artificial-intelligence/preparing-your-dataset-for-modeling-quickly-and-easily-c8c1b89fdb2e) (Part 2)
- [Handling Missing Data](https://towardsdatascience.com/handling-missing-data-for-a-beginner-6d6f5ea53436)
- [Introduction to Exploratory Data Analysis (EDA)](https://medium.com/code-heroku/introduction-to-exploratory-data-analysis-eda-c0257f888676)
:::
:::: 

# Thank You {background-image="/img/web/title-slide.png" background-color="#0F0C11" background-opacity="0.2"}

<h3>References</h3>