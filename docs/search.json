[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Jon Reades1\nThe Foundations of Spatial Data Science (FSDS) module is an optional element of CASA’s MSc programmes and is intended provide an introduction to doing data science in Python for students who are new to programming or whose previous exposure to coding is fairly limited. The module seeks to enable students to access, understand, and communicate data in a spatial context. FSDS is not about pushing buttons, but about using logic, programming, and your growing analytical skills to tackle real-world problems in a creative, reproducible, and open manner.\nFSDS is not easy: in order to make the most of the module—and the foundation that it provides both for Term 2 modules on the MSc and for post-Masters employment—you will need to work hard. This does not mean cramming before each practical, it means practicing between practicals, doing the readings, and really watching the videos. UCL expectations for a Masters-level module is 150 hours of study time: there are 4 hours of timetabled activity per week, and about an hour of videos to watch before each workshop, leaving up to 100 hours of ‘self-study’. By implication, you should expect to spend about 1.25 hours/day studying for this module: reading, coding, and (above all) practicing.\nIn exchange for your hard work, there is a pressing need for analysts, planners, and geographers able to think computationally using programming, analytics, and data manipulation skills that are anchored in the needs of policy-makers, businesses, and non-profits. There is a severe skills shortage in this domain across all sectors and, consequently, significant opportunity for those who can ‘make sense’ of data+code."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWhile this module in indebted to both feedback from students and colleagues over the years, several people played a particularly outsize role in my thinking and deserve special acknowledgement:\n\nDani for help with Docker, geopandas, and any number of other new tools with which I’ve had to familiarise myself.\nAndy for somehow knowing about all kinds of new web apps that I could use to support the module.\nThe Geocomp team at King’s College London, who supported my hare-brained scheme to teach Geography undergrads to code and offered all manner of useful feedback on what we could/could not feasibly cover."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Bartlett Centre for Advanced Spatial Analysis↩︎"
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Foundations (Pt. 1)",
    "section": "",
    "text": "This week we will be quickly covering the fundamentals of Python programming, while developing a critical appreciation of data science as an ongoing ‘process’ that calls for iterative improvement and deeper reflection. We will be contextualising computers within a wider landscape of geographical/spatial research. We will also be looking to the Unix Shell/Terminal as a ‘power user feature’ that is often overlooked by novice data scientists. And we will be (briefly) reviewing the basics of Python with a focus on simple data structures. We’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nA review of basic Python syntax and operators.\nAn introduction to making use of Git+GitHub.\nAn introdution to making use of the Shell/Terminal.\nAn understanding of how none of this all that new.\n\n\n\nSo we will be contextualising all of this within the longer history of the study of geography (or planning!) through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!"
  },
  {
    "objectID": "sessions/week2.html#overview",
    "href": "sessions/week2.html#overview",
    "title": "Foundations (Pt. 1)",
    "section": "",
    "text": "This week we will be quickly covering the fundamentals of Python programming, while developing a critical appreciation of data science as an ongoing ‘process’ that calls for iterative improvement and deeper reflection. We will be contextualising computers within a wider landscape of geographical/spatial research. We will also be looking to the Unix Shell/Terminal as a ‘power user feature’ that is often overlooked by novice data scientists. And we will be (briefly) reviewing the basics of Python with a focus on simple data structures. We’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nA review of basic Python syntax and operators.\nAn introduction to making use of Git+GitHub.\nAn introdution to making use of the Shell/Terminal.\nAn understanding of how none of this all that new.\n\n\n\nSo we will be contextualising all of this within the longer history of the study of geography (or planning!) through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!"
  },
  {
    "objectID": "sessions/week2.html#lectures",
    "href": "sessions/week2.html#lectures",
    "title": "Foundations (Pt. 1)",
    "section": "Lectures",
    "text": "Lectures\nThis week is very busy because we need to cover off the basics for those of you who were unable to engage with Code Camp, while recapping only the crucial bits for those of you who were able to do so.\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nComputers in Urban Studies\nIn Class\nSlides\n\n\nPrinciples of Programming\nIn Class\nSlides\n\n\nPython: the Basics\nVideo\nSlides\n\n\nLists\nVideo\nSlides\n\n\nIteration\nVideo\nSlides\n\n\nThe Command Line\nVideo\nSlides\n\n\nGetting Stuck into Git\nVideo\nSlides\n\n\n\n\nOther Preparatory Activities\n\nCome to class prepared to present/discuss:\n\nBurton (1963) &lt;URL&gt;\nArribas-Bel and Reades (2018) &lt;URL&gt;\n\nComplete the short Moodle quiz associated with this week’s activities."
  },
  {
    "objectID": "sessions/week2.html#installing-the-programming-environment",
    "href": "sessions/week2.html#installing-the-programming-environment",
    "title": "Foundations (Pt. 1)",
    "section": "Installing the Programming Environment",
    "text": "Installing the Programming Environment\n\n\n\n\n\n\nWarning\n\n\n\nThis week’s practical requires you to have completed installation of the programming environment. Make sure you have completed setup of the environment.\n\n\nIn principle, we fully support students who want to do things their own way; however, we are also not able to sit down with each person and develop a custom learning environment. With Docker, we can give you full access to the cutting-edge Python libraries and other tools needed to ‘do’ spatial data science, while only needing to install 1 application, download 1 (big) file, and run 1 command. When it works… There are alternatives, but there are more things that can go wrong and they can go wrong in more complex ways. Solving the Anaconda environment can take several hours before it even starts installing.\nSo here’s what we ask: if you already know what to do with an Anaconda YAML file, or can work out how to edit the Dockerfile and build a new image, then by all means knock yourself out! We are not going to tell you that cannot do something, and eventually you will need to learn to stand on your own two feet. But please do not expect us to support you individually if you’ve gone off and done your own thing and ‘it doesn’t work’. OK? We’ll offer advice (if we can) but only if no one else is waiting for help."
  },
  {
    "objectID": "sessions/week2.html#practical",
    "href": "sessions/week2.html#practical",
    "title": "Foundations (Pt. 1)",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\nThis week’s practical will take you through the fundamentals of Python, including the use of simple1 Boolean logic and lists.\nHowever, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling Notebooks 3, 5 and 7 at some point this week. You can either:\n\nFollow the instructions for running these in Google’s Collaboratory; or\nDownload the Raw files from the Code Camp Repository on GitHub.\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nEnsuring that you are set up with Git/GitHub\nReviewing Python basics\nReviewing Python lists and logic\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week2.html#footnotes",
    "href": "sessions/week2.html#footnotes",
    "title": "Foundations (Pt. 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: simple does not mean ‘easy’! Just because we say something is ‘basic’ or ‘simple’ does not mean that we think it is straightforward for someone learning to code for the first time!↩︎"
  },
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "In order to get you started on your data science ‘journey’ you will need to follow the guidance provided on the pages we’ve linked to below from the CASA Computing Environment web site.\n\n\nBefore trying to do anything else please complete the basic health check, which also includes our recommendations if you are considering buying a new computer when you start your studies. Once you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment. We also provide information about Code Camp which is a self-paced introduction to the fundamentals of programming in Python.\n\n\n\nOnce you’ve ticked off the Requirements, you can start installing the tools that you will use to write and run both code and documentation. You will need to:\n\nDid you set up the Base Utilities linked to in Section 1.1 (Requirements) above?\nInstall and configure Git and GitHub in order to manage, share, and version code.\nInstall a Markdown editor and familiarise yourself with Markdown’s syntax in order to write documentation, comments, and the Group Submission.\nInstall the programming environment, including Docker and, potentially, VSCode.\n\n\n\n\n\n\n\nWarning\n\n\n\nIn an emergency, as a backup some part of the term will work with the ‘no install’ option or Anaconda Python, but this will make your life significantly harder if it is your first choice as we are unable to provide support for these.\n\n\n\n\n\nOver the years, based on student experience and feedback we have collected a range of advice that is not purely technical in nature. This section covers managing distractions, an introduction to how to read (in the academic sense of reading journal articles and books for meaning and relevance) as well as how to think (in the sense of why the modules are the way they are and the importance of reflection), and how to ask for help (because that’s what we’re to do, but first you need to help yourself!)."
  },
  {
    "objectID": "setup/index.html#requirements",
    "href": "setup/index.html#requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Before trying to do anything else please complete the basic health check, which also includes our recommendations if you are considering buying a new computer when you start your studies. Once you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment. We also provide information about Code Camp which is a self-paced introduction to the fundamentals of programming in Python."
  },
  {
    "objectID": "setup/index.html#setup",
    "href": "setup/index.html#setup",
    "title": "Getting Started",
    "section": "",
    "text": "Once you’ve ticked off the Requirements, you can start installing the tools that you will use to write and run both code and documentation. You will need to:\n\nDid you set up the Base Utilities linked to in Section 1.1 (Requirements) above?\nInstall and configure Git and GitHub in order to manage, share, and version code.\nInstall a Markdown editor and familiarise yourself with Markdown’s syntax in order to write documentation, comments, and the Group Submission.\nInstall the programming environment, including Docker and, potentially, VSCode.\n\n\n\n\n\n\n\nWarning\n\n\n\nIn an emergency, as a backup some part of the term will work with the ‘no install’ option or Anaconda Python, but this will make your life significantly harder if it is your first choice as we are unable to provide support for these."
  },
  {
    "objectID": "setup/index.html#soft-skills",
    "href": "setup/index.html#soft-skills",
    "title": "Getting Started",
    "section": "",
    "text": "Over the years, based on student experience and feedback we have collected a range of advice that is not purely technical in nature. This section covers managing distractions, an introduction to how to read (in the academic sense of reading journal articles and books for meaning and relevance) as well as how to think (in the sense of why the modules are the way they are and the importance of reflection), and how to ask for help (because that’s what we’re to do, but first you need to help yourself!)."
  },
  {
    "objectID": "sessions/week8.html",
    "href": "sessions/week8.html",
    "title": "Dimensions in Data",
    "section": "",
    "text": "Changes before 22/23\n\n\n\n\nLink to clustering (Week 6 GIS & QM)"
  },
  {
    "objectID": "sessions/week8.html#overview",
    "href": "sessions/week8.html#overview",
    "title": "Dimensions in Data",
    "section": "Overview",
    "text": "Overview\nThis is the most profoundly abstract aspect of data analysis: how to conceive of your data as a multi-dimensional space that can be reshaped and transformed to support your analytical objectives. This foregrounds the importance of judgement since, as the economist Ronald Coase is reputed to have said:\n\n“If you torture the data long enough, it will confess.”\n\nBy which you should understand that transformation is a form of ‘torture’1: it can force the data to reveal relationships that were previously hidden from the data scientist. However, taken too far the data will confess to whatever you want, which isn’t the purpose of critical, reproducible, sound data science!\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nA deeper understanding of the issues surrounding clustering that were covered in Week 6 of CASA0005 (GIS) and CASA0007 (QM).\nAn understanding of how data transformation works and the reasons for choosing one transform over another.\nAn appreciation of the pros and cons of at least two dimensionality reduction techniques.\n\n\n\n\nLectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nThe Data Space\nVideo\nSlides\n\n\nTransformation\nVideo\nSlides\n\n\nDimensionality\nVideo\nSlides\n\n\n\n\n\nOther Preparatory Activities\n\nCome to class prepared to present/discuss (these are short, mostly non-academic pieces!):\n\nBunday (n.d.) &lt;URL&gt;\nHarris (n.d.) &lt;URL&gt;\nCima (n.d.) &lt;URL, PDF with Figures&gt;\nLu and Henning (2013) &lt;URL&gt;\n\nComplete the short Moodle quiz associated with this week’s activities."
  },
  {
    "objectID": "sessions/week8.html#practical",
    "href": "sessions/week8.html#practical",
    "title": "Dimensions in Data",
    "section": "Practical",
    "text": "Practical\nThis practical will show you how data transformation is an essential, but often overlooked, aspect of data analysis: depending on the choices we make here, we can reduce (or increase) the dimensionality of the data and make it more (or less) tractable for subsequent analysis. This approach to the pipeline relies on you being able to see your data as existing in an abstract ‘space’ that can be manipulated in order to foreground, compress, or even mask attributes.\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nWorking with a more complex data structure to create new ‘grouped’ variables (as the simplest form of transformation)\nUsing sklearn to fit and transform data in a flexible manner.\nDoing two types of dimensionality reduction to demonstrate how different linear and non-linear dimensionality reduction are.\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week8.html#footnotes",
    "href": "sessions/week8.html#footnotes",
    "title": "Dimensions in Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this is a metaphor only!↩︎"
  },
  {
    "objectID": "sessions/week6.html",
    "href": "sessions/week6.html",
    "title": "Spatial Data",
    "section": "",
    "text": "This week we will be focussing on the use of the geopandas library for spatial data analysis and management through a focus on spatial data and its distribution(s). Geopandas will help to clarify how Object-Oriented design and inheritance processes work, while also allowing to interrogate and map the assigned data set(s). A critical concept that should be emerging here is that spatial and numerical data analyses are, fundamentally, just two different views of the same data.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou develop better judgement about interpreting and representing data.\nYou understand how GeoPandas extends Pandas with spatial functionality.\nYou build on material covered in Week 1-3, and 5 of CASA0005 to extend your understanding of mapping and spatial data.\nYou develop better practices for (spatial) data exploration.\n\n\n\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMapping\nVideo\nSlides\n\n\nGeoPandas\nVideo\nSlides\n\n\nEDA\nVideo\nSlides\n\n\nESDA\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present:\n\nD’Ignazio and Klein (2020), chap. 6, The Numbers Don’t Speak for Themselves &lt;URL&gt;\nElwood and Wilson (2017) &lt;URL&gt;\n\nComplete the short Moodle quiz associated with this week’s activities.\nStudent Dialogue: Mentimeter poll to be completed by the start of class."
  },
  {
    "objectID": "sessions/week6.html#overview",
    "href": "sessions/week6.html#overview",
    "title": "Spatial Data",
    "section": "",
    "text": "This week we will be focussing on the use of the geopandas library for spatial data analysis and management through a focus on spatial data and its distribution(s). Geopandas will help to clarify how Object-Oriented design and inheritance processes work, while also allowing to interrogate and map the assigned data set(s). A critical concept that should be emerging here is that spatial and numerical data analyses are, fundamentally, just two different views of the same data.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou develop better judgement about interpreting and representing data.\nYou understand how GeoPandas extends Pandas with spatial functionality.\nYou build on material covered in Week 1-3, and 5 of CASA0005 to extend your understanding of mapping and spatial data.\nYou develop better practices for (spatial) data exploration.\n\n\n\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMapping\nVideo\nSlides\n\n\nGeoPandas\nVideo\nSlides\n\n\nEDA\nVideo\nSlides\n\n\nESDA\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present:\n\nD’Ignazio and Klein (2020), chap. 6, The Numbers Don’t Speak for Themselves &lt;URL&gt;\nElwood and Wilson (2017) &lt;URL&gt;\n\nComplete the short Moodle quiz associated with this week’s activities.\nStudent Dialogue: Mentimeter poll to be completed by the start of class."
  },
  {
    "objectID": "sessions/week6.html#practical",
    "href": "sessions/week6.html#practical",
    "title": "Spatial Data",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\nIn the practical we will continue to work with the InsideAirbnb data, here focussing on the second ‘class’ of data in the data set: geography. We will see how to use GoePandas and PySAL for (geo)visualisation and analysis.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nCreating/working with geo-data in Python.\nMaking maps with Python.\nExploring the data visually.\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Objects",
    "section": "",
    "text": "This week we will see how Python actually works by looking beyond simple functions and into methods, classes, and the outlines of Object Oriented Design and Programming (OOD/OOP). We’ll also look at what to do when ‘things go wrong’, because they will, but sometimes we want that to blow up the application while other times we want Python to handle the ‘exception’ gracefully. Learning to read Exceptions is essential to debugging code: the one thing that almost never works when you get an exception is ignoring it.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nDevelop enough of an understanding of classes and inheritance that you can make effective use of Python.\nDevelop an understanding of how to read and write Exceptions so as to be able to create robust code.\nUnder the process of moving code from in-line scripting, to functions, to packages of functions.\nBegin developing an appreciation of the substantive concerns of the module (data, ethics, bias, and the risks of ‘experts’ who ‘know it all’).\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMethods\nVideo\nSlides\n\n\nClasses\nVideo\nSlides\n\n\nDesign\nVideo\nSlides\n\n\nExceptions\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nCox and Slee (2016) &lt;PDF&gt;\nD’Ignazio and Klein (2020), chap. 5, Unicorns Janitors, Ninjas, Wizards, and Rock Stars &lt;URL&gt;\n\nWe’ll aim to do the ‘Student Dialogue’ Mentimeter poll at the start of class."
  },
  {
    "objectID": "sessions/week4.html#overview",
    "href": "sessions/week4.html#overview",
    "title": "Objects",
    "section": "",
    "text": "This week we will see how Python actually works by looking beyond simple functions and into methods, classes, and the outlines of Object Oriented Design and Programming (OOD/OOP). We’ll also look at what to do when ‘things go wrong’, because they will, but sometimes we want that to blow up the application while other times we want Python to handle the ‘exception’ gracefully. Learning to read Exceptions is essential to debugging code: the one thing that almost never works when you get an exception is ignoring it.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nDevelop enough of an understanding of classes and inheritance that you can make effective use of Python.\nDevelop an understanding of how to read and write Exceptions so as to be able to create robust code.\nUnder the process of moving code from in-line scripting, to functions, to packages of functions.\nBegin developing an appreciation of the substantive concerns of the module (data, ethics, bias, and the risks of ‘experts’ who ‘know it all’).\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.\n\n\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMethods\nVideo\nSlides\n\n\nClasses\nVideo\nSlides\n\n\nDesign\nVideo\nSlides\n\n\nExceptions\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nCox and Slee (2016) &lt;PDF&gt;\nD’Ignazio and Klein (2020), chap. 5, Unicorns Janitors, Ninjas, Wizards, and Rock Stars &lt;URL&gt;\n\nWe’ll aim to do the ‘Student Dialogue’ Mentimeter poll at the start of class."
  },
  {
    "objectID": "sessions/week4.html#practical",
    "href": "sessions/week4.html#practical",
    "title": "Objects",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\nThis practical will lead you through the process of converting inline scripts into functions and, ultimately, into a simple package. The last parts of the practical are optional – creating classes in hierarchies is integral to how Python works, but many data scientists will rarely need to write their own classes… they just make use of classes written by others (which is why understanding what they are is important, but being able to write your own is a little less so).\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nBedding in the ‘data thinking’ from last week’s practical.\nCreating functions to perform repetitive tasks.\nPackaging these functions up and accessing via the appropriate namespace.\nImplementing a few simple classes so that you understand the basics of how they work.\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Visualising Data",
    "section": "",
    "text": "This week is focussed on formalising our understanding of two things that we’ve already touched on at multiple points throughout the term: 1) how to link data sets; 2) how to tune matplotlib plots to ‘publication quality’ by tweaking fine-grained elements. So this week begins by looking at how to join data together in both spatial and non-spatial contexts, before facing off with the monster that most Python visualisation libraries ultimately depend on: matplotlib. As a tool it is both incredibly powerful and intensely counter-intuitive if you are used to R’s ggplot. We will also be looking more widely to the future of quantitative methods, the potential of a geographic data science, and the ways in which we can move between spatial and non-spatial paradigms of analysis within the same piece of work.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou have formalised your understanding of how to link data in Python.\nYou have broadened your thinking about the purpose of data visualisation.\nYou are thinking about the final project.\n\n\n\n\n\nYou are strongly advised to watch these videos on linking data and spatial data, as well how to group data within pandas; however, you will not be asked to present any of these because our attention is now on the final assessments. As well, you should by now be familiar with the concept of how to join data from the GIS module (CASA0005), so this focusses on two things: 1) how to do this in Python; and 2) how to approach this process with large or mismatched data sets.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLinking Data\nVideo\nSlides\n\n\nLinking Spatial Data\nVideo\nSlides\n\n\nData Visualisation\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to briefly present/discuss:\n\nWolf et al. (2021) &lt;URL&gt;\nShapiro and Yavuz (2017) &lt;URL&gt;\nSingleton and Arribas-Bel (2021) &lt;DOI&gt;\n\nStudent Dialogue: Mentimeter poll to be completed at the start of class.\nComplete the short Moodle quiz associated with this week’s activities.\n\nYou may also want to look at the following reports / profiles with a view to thinking about employability and how the skills acquired in this module can be applied beyond the end of your MSc:\n\nGeospatial Skills Report &lt;URL&gt;\nAAG Profile of Nicolas Saravia &lt;URL&gt;\nWolf et al. (2021) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week10.html#overview",
    "href": "sessions/week10.html#overview",
    "title": "Visualising Data",
    "section": "",
    "text": "This week is focussed on formalising our understanding of two things that we’ve already touched on at multiple points throughout the term: 1) how to link data sets; 2) how to tune matplotlib plots to ‘publication quality’ by tweaking fine-grained elements. So this week begins by looking at how to join data together in both spatial and non-spatial contexts, before facing off with the monster that most Python visualisation libraries ultimately depend on: matplotlib. As a tool it is both incredibly powerful and intensely counter-intuitive if you are used to R’s ggplot. We will also be looking more widely to the future of quantitative methods, the potential of a geographic data science, and the ways in which we can move between spatial and non-spatial paradigms of analysis within the same piece of work.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou have formalised your understanding of how to link data in Python.\nYou have broadened your thinking about the purpose of data visualisation.\nYou are thinking about the final project.\n\n\n\n\n\nYou are strongly advised to watch these videos on linking data and spatial data, as well how to group data within pandas; however, you will not be asked to present any of these because our attention is now on the final assessments. As well, you should by now be familiar with the concept of how to join data from the GIS module (CASA0005), so this focusses on two things: 1) how to do this in Python; and 2) how to approach this process with large or mismatched data sets.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLinking Data\nVideo\nSlides\n\n\nLinking Spatial Data\nVideo\nSlides\n\n\nData Visualisation\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to briefly present/discuss:\n\nWolf et al. (2021) &lt;URL&gt;\nShapiro and Yavuz (2017) &lt;URL&gt;\nSingleton and Arribas-Bel (2021) &lt;DOI&gt;\n\nStudent Dialogue: Mentimeter poll to be completed at the start of class.\nComplete the short Moodle quiz associated with this week’s activities.\n\nYou may also want to look at the following reports / profiles with a view to thinking about employability and how the skills acquired in this module can be applied beyond the end of your MSc:\n\nGeospatial Skills Report &lt;URL&gt;\nAAG Profile of Nicolas Saravia &lt;URL&gt;\nWolf et al. (2021) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week10.html#practical",
    "href": "sessions/week10.html#practical",
    "title": "Visualising Data",
    "section": "Practical",
    "text": "Practical\nThe practical will lead you through the application of grouping and aggregation functions in pandas, and the fine-tuning of data visualisations in Matplotlib/Seaborn. In many ways, this should be seen (bar the aggregation) as largely a recap of material encountered in previous sessions. However, you should see this as a critical step in the production of outputs and analyses needed for the final project.\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nLinking data as part of a visualisation process.\nAutomating the production of map outputs in Python to create an ‘atlas’.\nThinking about proxies; e.g. why would you find ppsqm more/less useful than price? what are you really measuring?\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/to_dos.html",
    "href": "sessions/to_dos.html",
    "title": "To Dos",
    "section": "",
    "text": "Items collated from previous years’ feedback that I’ve not had time to consider and/or implement yet.\n\n\n\n\n\n\nMid-Term Changes\n\n\n\n\nChange all height=\"\\d+\" to r-stretch on images.\nLook at this example for vertically positioning things.\nShift focus of (new) Week 9 & 10 to review/discussing the final assessment. Content still available to view/practice but not required/delivered.\nRequire more active use of GitHub/Git – a submission in which it’s easy to automate checks that they’ve created a repo and populated it with the completed notebooks?\nRefresh Code Camp and make it more self-test oriented. Use the self-test to set expectations about level of effort/preparation required (and whether or not taking the module will be useful)."
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Overview",
    "section": "",
    "text": "We have ‘flipped’ the classroom for this module so we expect you to come to ‘lecture’ (except in Week 1!) having already watched the assigned videos and completed the assigned readings. You may be called upon to present a short summary of the key points in, and relevance of, an assigned video or reading to the rest of the class.\nThis means that there is a mix of ‘asynchronous’ (work that you do in your own time) and ‘synchronous’ (work that we do during scheduled hours) interaction. Synchronous activities will normally be recorded for review afterwards, but you should bear in the mind the following: 1) we cannot be responsible for equipment failure; 2) we are unable to record practicals and other small-group activities; and 3) a 2-hour video of a group discussion and live coding session will be rather less educational and informative than actually being there.\nIn short, recordings should not be used as a substitute for attendance save in exceptional circumstances."
  },
  {
    "objectID": "sessions/index.html#preparation",
    "href": "sessions/index.html#preparation",
    "title": "Overview",
    "section": "Preparation",
    "text": "Preparation\nThe nature and amount of preparation will vary from week to week, but may include:\n\nReadings from both academic and non-academic sources.\nRecorded lectures from CASA staff.\nRecorded videos from non-CASA staff.\nShort Moodle quizzes to test your completion of readings and videos.\nPreparing contributions to set tasks (e.g. summaries, Q&A, etc.)\n\nTo get the most value from the module you must do the readings even if we are not specifically referencing them in-class because of time-constraints. In previous years students who did not do the readings often failed the first assessment and struggled with the final assessment, leaving a lot of easy marks on the table. More importantly, we believe that the single most important skill that you can acquire from FSDS is not the ability to code, it’s the ability to critically interrogate data and recognise the strengths and limitations that are relevant to the problem at-hand. You will learn the technical aspects of this in the practicals. You will learn the theoretical dimension from doing the readings."
  },
  {
    "objectID": "sessions/index.html#class",
    "href": "sessions/index.html#class",
    "title": "Overview",
    "section": "Class",
    "text": "Class\nThe ‘lecture’ in your timetable will be used for a mix of discussion and ‘live coding’ (eeek!) using the following framework:\n\nWe will review questions and issues arising from the previous week’s practical session and the weekly Padlet. We will use this to prioritise discussion around concepts and readings with which students are struggling or wish to engage further.\nWe will have a ‘live coding’ session following an ‘I do/We do’ format: we will employ concepts covered in the week’s activities, as well as approaches that will be explored further in the practical, to look a real-world data set together using code."
  },
  {
    "objectID": "sessions/index.html#practicals",
    "href": "sessions/index.html#practicals",
    "title": "Overview",
    "section": "Practicals",
    "text": "Practicals\nIn order to make use of these materials you will need to install the Spatial Data Science programming environment.\nPracticals are run in small groups to maximise your ability to ask questions and interact with other students. You will be notified of your group by the Professional Services team; there may be limited opportunities to switch, but the best way would be to swap with another student and then notify us of the arrangment. You may wish to download the week’s Jupyter notebook before the start of class in order to familiarise yourself with the material."
  },
  {
    "objectID": "sessions/index.html#improvements",
    "href": "sessions/index.html#improvements",
    "title": "Overview",
    "section": "Improvements",
    "text": "Improvements\nWe are always making improvements to FSDS and try to keep track of student-feedback here."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#choices-choices",
    "href": "lectures/9.4-Visualising_Data.html#choices-choices",
    "title": "Visualising Data",
    "section": "Choices, Choices…",
    "text": "Choices, Choices…\n\nmatplotlib: the ‘big beast’ of visualisation in Python. Similar to MATLAB. Highly customisable. Very complex.\nseaborn: a layer that sits over top of matplotlib and makes it easier to produce good-quality graphics.\nbokeh: web-based visualisation tool that can integrate with Jupyter or output to static HTML files.\nplotly: another web-based visualisation tool that can integrate with Jupyter.\n\nMore emerging all the time: Vega/Altair, HoloViews, etc.\n\n\nIf you’re really wedded to ggplot, plotnine is a clone of ggplot’s interface (Grammer of Graphics) in Python. A brief overview of visualisation libraries could be helpful."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#seaborn",
    "href": "lectures/9.4-Visualising_Data.html#seaborn",
    "title": "Visualising Data",
    "section": "Seaborn",
    "text": "Seaborn\nDesigned to provide ggplot-like quality output using matplotlib:\n\nImprove on default colourmaps and colour defaults.\nIntegration with pandas data frames (Note: not geopandas!).\nOffers more plot types out of the box.\nStill offers access to matplotlib’s back-end."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#plot-types",
    "href": "lectures/9.4-Visualising_Data.html#plot-types",
    "title": "Visualising Data",
    "section": "Plot Types",
    "text": "Plot Types\n\n\n\nPartial Overview of Seaborn Plots\n\n\n\n\nFor the fuller overview see Overview of seaborn plotting functions and the full API reference."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#in-practice",
    "href": "lectures/9.4-Visualising_Data.html#in-practice",
    "title": "Visualising Data",
    "section": "In Practice",
    "text": "In Practice\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\",\n             hue=\"region\", style=\"event\",\n             data=fmri)"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#in-practice-2",
    "href": "lectures/9.4-Visualising_Data.html#in-practice-2",
    "title": "Visualising Data",
    "section": "In Practice 2",
    "text": "In Practice 2\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\ndf = sns.load_dataset(\"penguins\")\n\nax = sns.swarmplot(data=df, x=\"body_mass_g\", y=\"sex\", hue=\"species\")\nax.set(ylabel=\"\")"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#configuring-seaborn",
    "href": "lectures/9.4-Visualising_Data.html#configuring-seaborn",
    "title": "Visualising Data",
    "section": "Configuring Seaborn",
    "text": "Configuring Seaborn\nSeaborn ‘themes’ act as shortcuts for setting multiple matplotlib parameters:\n\n\n\nSeaborn Command\nAccomplishes\n\n\n\n\nset_theme(...)\nSet multiple theme parameters in one step.\n\n\naxes_style(...)\nReturn a parameter dict for the aesthetic style of the plots.\n\n\nset_style(...)\nSet the aesthetic style of the plots.\n\n\nplotting_context(...)\nReturn a parameter dict to scale elements of the figure.\n\n\nset_context(...)\nSet the plotting context parameters.\n\n\n\nYou can also access:\n\nPalettes: colormaps can be generated using sns.color_palette(...) and set using sns.set_palette(...).\nAxes Styles: includes darkgrid, whitegrid, dark, white, ticks."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#anatomy-of-a-figure",
    "href": "lectures/9.4-Visualising_Data.html#anatomy-of-a-figure",
    "title": "Visualising Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\nSource."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#writing-a-figure",
    "href": "lectures/9.4-Visualising_Data.html#writing-a-figure",
    "title": "Visualising Data",
    "section": "Writing a Figure",
    "text": "Writing a Figure\nThere are multiple ways to access/write elements of a plot:\n\nFigure: high-level features (e.g. title, padding, etc.). Can be accessed via plt.gcf() (get current figure) or upon creation (e.g. f, ax = plt.subplots(1,1) or f = plt.figure()).\nAxes: axis-level features (e.g. labels, tics, spines, limits, etc.). Can be accessed via plt.gca() (get current axes) or upon creation (e.g. f, ax = plt.subplots(1,1) or ax = f.add_subplot(1,1,1)).\n\nAnnotations, artists, and other features are typically written into the axes using the coordinate space of the figure (e.g. decimal degrees for lat/long, metres for BNG, etc.)."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#adding-a-3rd-dimension",
    "href": "lectures/9.4-Visualising_Data.html#adding-a-3rd-dimension",
    "title": "Visualising Data",
    "section": "Adding a 3rd Dimension",
    "text": "Adding a 3rd Dimension\nThis ‘feature’ is less well-developed but does work:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax  = plt.axes(projection='3d')\n# OR\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\n# THEN\nax.contour3D(X, Y, Z, ...)\nax.plot_surface(x, y, z, ...)\nax.plot3D(xline, yline, zline, ...)\nax.scatter3D(x, y, z, ...)\n# ax.plot_surface and ax.plot_wire also give you 3D renderings\nYou can then set the elevation and azimuth using: ax.view_init(&lt;elevation&gt;, &lt;azimuth&gt;)."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#saving-outputs",
    "href": "lectures/9.4-Visualising_Data.html#saving-outputs",
    "title": "Visualising Data",
    "section": "Saving Outputs",
    "text": "Saving Outputs\nStraightforward via save figure function, but lots of options!\nplt.savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n    orientation='portrait', papertype=None, format=None,\n    transparent=False, bbox_inches=None, pad_inches=0.1,\n    frameon=None, metadata=None)\nThe format can be largely determined by the file extension in the fname (file name) and the supported formats depends on what you’ve installed! You can find out what’s available to you using: plt.gcf().canvas.get_supported_filetypes()."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#jupyter",
    "href": "lectures/9.4-Visualising_Data.html#jupyter",
    "title": "Visualising Data",
    "section": "Jupyter",
    "text": "Jupyter\nBy default, Jupyter’s output is static matplotlib, but we can extend this in three ways:\n\nMake the static plot zoomable and pannable using %matplotlib widget (declare this at the top of your notebook).\nMake the plot more directly interactive using ipywidgets (import interact and related libs as needed).\nUse a browser-based visualisation tool such as bokeh, plotly, altair/vega, holoviews, or even d3 (format may be very, very different from what you are ‘used to’ in Python)."
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#widgets",
    "href": "lectures/9.4-Visualising_Data.html#widgets",
    "title": "Visualising Data",
    "section": "Widgets",
    "text": "Widgets\n%matplotlib widget\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = hackney.plot(edgecolor='k', facecolor='none')\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#interact",
    "href": "lectures/9.4-Visualising_Data.html#interact",
    "title": "Visualising Data",
    "section": "Interact()",
    "text": "Interact()\nTaking an example from Dani’s work:\nfrom ipywidgets import interact\n# Alternatives: interactive, fixed, interact_manual\ninteract(\n    &lt;function&gt;, # Function to make interactive\n    &lt;param0&gt;,   # e.g. Data to use\n    &lt;param1&gt;,   # e.g. Range start/end/step\n    &lt;param2&gt;    # e.g. Fixed value\n);"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#bokeh",
    "href": "lectures/9.4-Visualising_Data.html#bokeh",
    "title": "Visualising Data",
    "section": "Bokeh",
    "text": "Bokeh"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#automation",
    "href": "lectures/9.4-Visualising_Data.html#automation",
    "title": "Visualising Data",
    "section": "Automation",
    "text": "Automation\nPlots built on top of matploblib can, to some extent, be automated using functions. For example, to draw circles and place text:\ndef circle(ax, x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, \n                    linewidth=1, edgecolor='black', \n                    facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, \n                                  foreground='w')])\n    ax.add_artist(circle)\n\ndef text(ax, x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n         ha='center', va='top', weight='bold', color='blue')"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#resources",
    "href": "lectures/9.4-Visualising_Data.html#resources",
    "title": "Visualising Data",
    "section": "Resources",
    "text": "Resources\n\n\n\nIntroduction to PyPlot (includes lots of parameter information)\nVisualisation with Seaborn\nSeaborn Tutorial\nElite Data Science Seaborn Tutorial\nDatacamp Seaborn Tutorial\nThree-Dimensional Plotting in Matplotlib\nAn easy introduction to 3D plotting with Matplotlib\n\n\n\nUsing text effectively in data viz\nChoosing fonts for charts and tables\nBokeh Gallery\nBokeh User Guide\nProgramming Historian: Visualizing Data with Bokeh and Pandas\nReal Python: Data Viz with Bokeh\nData Viz with Bokeh (Pt. 1)\nUsing Interact\nText in Data Visualizations"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#section",
    "href": "lectures/9.2-Linking_Spatial_Data.html#section",
    "title": "Linking Spatial Data",
    "section": "",
    "text": "Data Set 1\n\n\n\nSensorID\nLatitude\nLongitude\n\n\n\n\n1\n51.5070\n-0.1347\n\n\n2\n51.5071\n-0.0042\n\n\n3\n51.5074\n-0.1223\n\n\n4\n51.5073\n-0.1122\n\n\n5\n51.5072\n0.1589\n\n\n\ndf = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Latitude': [51.5070, 51.5071, 51.5074, 51.5073, 51.5073],\n  'Longitude': [-0.1347, -0.0042, -0.1223, -0.1122, 0.1589]\n})\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1\nTemperature\n5ºC\n\n\n1\nHumidity\n15%\n\n\n3\nTemperature\n7ºC\n\n\n4\nTemperature\n7ºC\n\n\n6\nHumidity\n18%\n\n\n\ndf1 = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Parameter': ['Temperature','Humidity','Temperature','Temperature','Humidity'], \n  'Value': ['5ºC', '15%', '7ºC', '7ºC', '18%']\n})\n\n\n\n\nObviously, we can use non-spatial operations on spatial data sets.\n\n\nThis wouldn’t be a particularly good design for a data structure… why?"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking Spatial Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking Spatial Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking Spatial Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking Spatial Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking Spatial Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking Spatial Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking Spatial Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking Spatial Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\nA. Here’s how I’d do this:\n\nCity = New York (probably a key)\n\nCuisine = Italian (probably a key)\n\nStyle = Family (probably an enumeration/free text)\n\nLocation = Within Distance of X from Request (probably a buffered spatial query)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking Spatial Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking Spatial Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server.\n\n\n\n\n\nSee also Carto and competitors."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking Spatial Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich, but £££ for commercial use.\nMySQL: free, unless you want dedicated support; was feature-poor (though this looks to have changed with MySQL8); heavyweight.\nPostreSQL: free, unless you want dedicated support; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS"
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#curse-and-blessing",
    "href": "lectures/8.3-Dimensionality.html#curse-and-blessing",
    "title": "Dimensionality",
    "section": "Curse and Blessing…",
    "text": "Curse and Blessing…\n\nMore dimensions means more information.\nMore dimensions makes for easier seperation.\nMore dimensions inflates distance.\nMore dimensions increases the risk of overfitting.\n\n\nOr as Analytics India Magazine puts it:\n\nHigh-dimensional spaces have geometrical properties that are counter-intuitive and far from the properties observed in two- or three-dimensional spaces.\nData analysis tools are often designed with intuitive properties and low-dimensional spaces in mind."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#pca",
    "href": "lectures/8.3-Dimensionality.html#pca",
    "title": "Dimensionality",
    "section": "PCA",
    "text": "PCA\nWorkhorse dimensionality reduction method: simple, fast, and effective. Can be thought of as freely rotating axes to align with directions of maximum variance. I like this summary:\n\nPCA (Principal Components Analysis) gives us our ‘ideal’ set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, the second has higher variance than the third, and so on), uncorrelated (all components are orthogonal), and low in number (we can throw away the lower ranked components as they usually contain little signal).\n\nBut I particularly liked this exposition in Towards Data Science."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#in-practice",
    "href": "lectures/8.3-Dimensionality.html#in-practice",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\npca.transform(data)\nSee also: Kernel PCA for non-linear problems."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#rtfm",
    "href": "lectures/8.3-Dimensionality.html#rtfm",
    "title": "Dimensionality",
    "section": "RT(F)M",
    "text": "RT(F)M\nWhy was I banging on about transformations? Well, what does this assume about the data?\n\nLinear dimensionality reduction using Singular Value Decomposition projects data into to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\n\nI found a nice explanation of PCA using dinner conversation over several bottles of wine as an example on Stats.StackExhcange.com. There are many good illustrations of this process on stats.stackexchange.com."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#other-considerations",
    "href": "lectures/8.3-Dimensionality.html#other-considerations",
    "title": "Dimensionality",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nPCA is a form of unsupervised learning that does not take output labels into account. Other approaches (such as Linear Discriminant Analysis [note: not Latent Dirichlet Allocation]) consider the output as part of the transformation. PCA is also deterministic.\n\nSee this discussion."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#t-sne",
    "href": "lectures/8.3-Dimensionality.html#t-sne",
    "title": "Dimensionality",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbour Embedding is best understood as a visualisation technique, not an analytical one. This is because it is probabilistic and not deterministic."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#in-practice-1",
    "href": "lectures/8.3-Dimensionality.html#in-practice-1",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.manifold import TSNE\nembedded = TSNE(n_components=d).fit_transform(x)\nThe choice of perplexity and n_iter matter, and so does the metric. In practice you will need to experiment with these.\n\nt-SNE is also much harder computationally than PCA and it may be preferrable on very high-D data sets to apply PCA first and then t-SNE to the reduced data set! The output could then be fed to a clustering algorithm to make predictions about where new observations belong, but do not confuse that with meaning."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#umap",
    "href": "lectures/8.3-Dimensionality.html#umap",
    "title": "Dimensionality",
    "section": "UMAP",
    "text": "UMAP\nNon-linear dimensionality reduction that tries to preserve both local and global structure. Puts it between PCA and t-SNE.\n\n\n\nSee examples on umap.scikit-tda.org.\n\n\nNote that the library is called umap-learn."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#in-pracice",
    "href": "lectures/8.3-Dimensionality.html#in-pracice",
    "title": "Dimensionality",
    "section": "In Pracice…",
    "text": "In Pracice…\nimport umap\ntransformer = umap.UMAP(n_components=d)\nembedded = transformer.fit_transform(x)\nThe choice of n_neighbors, min_dist, and metric matter. In practice you may need to experiment with these."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#gotcha",
    "href": "lectures/8.3-Dimensionality.html#gotcha",
    "title": "Dimensionality",
    "section": "Gotcha!",
    "text": "Gotcha!\nt-SNE (less so UMAP) requires very careful handling:\n\nHyperparameters matter a lot\nCluster size means nothing\nCluster distances mean nothing\nClusters may mean nothing (low neighbour count/perplexity)\nOutputs are stochastic (not deterministic)\n\nBoth likely require repeated testing and experimentation."
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#other-approaches",
    "href": "lectures/8.3-Dimensionality.html#other-approaches",
    "title": "Dimensionality",
    "section": "Other Approaches",
    "text": "Other Approaches\n\nFeature selection, including forwards/backwards (sklearn.feature_selection here)\nDecomposition (sklearn.decomposition here, especiall SVD)\nOther types of manifold learning (sklearn.manifold here)\nRandom projection (sklearn.random_projection here)\nSupport Vector Machines (sklearn.svm here)\nEnsemble Methods (such as Random Forests: sklearn.ensemble.ExtraTreesClassifier and sklearn.ensemble.ExtraTreesRegressor here and here)"
  },
  {
    "objectID": "lectures/8.3-Dimensionality.html#resources",
    "href": "lectures/8.3-Dimensionality.html#resources",
    "title": "Dimensionality",
    "section": "Resources",
    "text": "Resources\n\n\n\nRethinking ‘distance’ in New York City Medium URL\nFive Boroughs for the 21\\(^{st}\\) Century Medium URL\nCurse of Dimensionality\nThe Curse of Dimensionality\nUnderstanding Curse of Dimensionality\nCurse of Dimensionality – A ‘Curse’ to Machine Learning\nImportance of Feature Scaling\nUnderstanding PCA\nIntroduction to t-SNE in Python\n\n\n\nHow to Use t-SNE Effectively\nHow to tune the Hyperparameters of t-SNE\nUnderstanding UMAP (Compares to t-SNE)\nHow UMAP Works\n3 New Techniques for Data-Dimensionality Reduction in ML\nUMAP for Dimensionality Reduction (Video)\nA Bluffer’s Guide to Dimensionality Reduction (Video)"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#the-data-generating-process",
    "href": "lectures/8.1-Data_Space.html#the-data-generating-process",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\nSource."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#the-data-generating-process-1",
    "href": "lectures/8.1-Data_Space.html#the-data-generating-process-1",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\nSource."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#cashier-income-as-dgp",
    "href": "lectures/8.1-Data_Space.html#cashier-income-as-dgp",
    "title": "The Data Space",
    "section": "Cashier Income as DGP",
    "text": "Cashier Income as DGP\nQuestion: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#house-prices-as-dgp",
    "href": "lectures/8.1-Data_Space.html#house-prices-as-dgp",
    "title": "The Data Space",
    "section": "House Prices as DGP",
    "text": "House Prices as DGP\n\nSource.\n\nWhat does this suggest about the DGP?"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "href": "lectures/8.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "title": "The Data Space",
    "section": "Is Bill Gates as Rich as He is Tall?",
    "text": "Is Bill Gates as Rich as He is Tall?\nInstinctively, we know that Bill Gates’ wealth is much further from ‘normal’ than is his height. But how?\n\nHow can we compare income and height if they share no common units?\nHow can we compare the biodiversity of sites in the tropics with those of sub-Arctic areas given that there are different numbers of species to begin with?\n\nWe need:\n\nWays to make different dimensions comparable, and\nWays to remove unit effects from distance measures."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#distance-in-1d",
    "href": "lectures/8.1-Data_Space.html#distance-in-1d",
    "title": "The Data Space",
    "section": "Distance in 1D",
    "text": "Distance in 1D\n\\[\nd(i,j) = |(i_{1}-j_{1})|\n\\]"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#distance-in-2d",
    "href": "lectures/8.1-Data_Space.html#distance-in-2d",
    "title": "The Data Space",
    "section": "Distance in 2D",
    "text": "Distance in 2D\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}}\n\\]"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#distance-in-3d-or-more",
    "href": "lectures/8.1-Data_Space.html#distance-in-3d-or-more",
    "title": "The Data Space",
    "section": "Distance in 3D… or More",
    "text": "Distance in 3D… or More\nWe can keep adding dimensions…\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}+(i_{3}-j_{3})^{2}}\n\\]\nYou continue adding dimensions indefinitely, but from here on out you are dealing with hyperspaces!"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#thinking-in-data-space",
    "href": "lectures/8.1-Data_Space.html#thinking-in-data-space",
    "title": "The Data Space",
    "section": "Thinking in Data Space",
    "text": "Thinking in Data Space\nWe can write the coordinates of an observation with 3 attributes (e.g. height, weight, income) as:\n\\[\nx_{i} = { {x_{i1}, x_{i2}, x_{i3} } }\n\\]\nSomething with 8 attributes (e.g. height, weight, income, age, year of birth, …) ‘occupies’ an 8-dimensional space…"
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#two-propositions",
    "href": "lectures/8.1-Data_Space.html#two-propositions",
    "title": "The Data Space",
    "section": "Two Propositions",
    "text": "Two Propositions\n\nThat geographical space is no different from any other dimension in a data set.\nThat geographical space is still special when it comes to thinking about relationships."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#implication",
    "href": "lectures/8.1-Data_Space.html#implication",
    "title": "The Data Space",
    "section": "Implication",
    "text": "Implication\nIf you can shift from thinking in columns of data, to thinking of a data space then you’ll have a much easier time dealing with dimensionality reduction and clustering."
  },
  {
    "objectID": "lectures/8.1-Data_Space.html#resources",
    "href": "lectures/8.1-Data_Space.html#resources",
    "title": "The Data Space",
    "section": "Resources",
    "text": "Resources\n\nAre Statisticians Cold-Blooded Bosses?\nBeyond 3D: Thinking in Higher Dimensions\nVisualizing beyond 3 Dimensions\nThe things you’ll find in higher dimensions (also useful for dimensionality reduction)\nWhat’s a Tensor? (heavy on the Physics relevance, but a lot of useful terminology and abstraction)"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "href": "lectures/7.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "title": "Cleaning Text",
    "section": "When a Loop Is Not Best",
    "text": "When a Loop Is Not Best\nIf you need to apply the same operation to lots of data why do it sequentially?\n\nYour computer has many cores and can run many threads in parallel.\nThe computer divides the work across the threads it sees fit.\nThe computer reassemble the answer at the end from the threads.\n\nIf you have 4 cores then parallelisation cuts analysis time by 75%."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "href": "lectures/7.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "title": "Cleaning Text",
    "section": "So Do More with Each Clock Cycle",
    "text": "So Do More with Each Clock Cycle\n\nMany libraries/packages implement weak forms of vectorisation or parallelisation, but some libraries do more.\nYou must request it because it requires hardware or other support and it is highly optimsed.\nMultiple separate machines acting as one.\nMultiple GPUs acting as one.\n\n\nConceptually, these get challenging if you can’t clearly separate/parallelise tasks."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "href": "lectures/7.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "title": "Cleaning Text",
    "section": "Pandas.apply() vs. Numpy",
    "text": "Pandas.apply() vs. Numpy\nNumpy is fully vectorised and will almost always out-perform Pandas apply, but both are massive improvements on for loops:\n\nExecute row-wise and column-wise operations.\nApply any arbitrary function to individual elements or whole axes (i.e. row or col).\nCan make use of lambda functions too for ‘one off’ operations (ad-ohoc functions)."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#lambda-functions",
    "href": "lectures/7.3-Cleaning_Text.html#lambda-functions",
    "title": "Cleaning Text",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nFunctional equivalent of list comprehensions: 1-line, anonymous functions.\nFor example:\nx = lambda a : a + 10\nprint(x(5)) # 15\nOr:\nfull_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\nprint(full_name('guido', 'van rossum')) # 'Guido Van Rossum'\nThese are very useful with pandas."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#lets-compare",
    "href": "lectures/7.3-Cleaning_Text.html#lets-compare",
    "title": "Cleaning Text",
    "section": "Let’s Compare",
    "text": "Let’s Compare\nimport time\nimport numpy as np\ndef func(a,b):\n  c = 0\n  for i in range(len(a)): c += a[i]*b[i]\n  return c\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nt1 = time.time()\nprint(func(a,b))\nt2 = time.time()\nprint(np.dot(a,b))\nt3 = time.time()\n\nprint(f\"For loop took {(t2-t1)*1000:.0f} milliseconds\")\nprint(f\"Numpy took {(t3-t2)*1000:.0f} milliseconds\")\nGenerally, I get numpy taking 86ms, while the for loop takes 331ms!\n\n\n/ht to The Last Byte for inspiration."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#beautiful-soup-selenium",
    "href": "lectures/7.3-Cleaning_Text.html#beautiful-soup-selenium",
    "title": "Cleaning Text",
    "section": "Beautiful Soup & Selenium",
    "text": "Beautiful Soup & Selenium\nTwo stages to acquiring web-based documents:\n\nAccessing the document: urllib can deal with many issues (even authentication), but not with dynamic web pages (which are increasingly common); for that, you need Selenium (library + driver).\nProcessing the document: simple data can be extracted from web pages with RegularExpressions, but not with complex (esp. dynamic) content; for that, you need BeautifulSoup4.\n\nThese interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#regular-expressions-breaks",
    "href": "lectures/7.3-Cleaning_Text.html#regular-expressions-breaks",
    "title": "Cleaning Text",
    "section": "Regular Expressions / Breaks",
    "text": "Regular Expressions / Breaks\nNeed to look at how the data is organised:\n\nFor very large corpora, you might want one document at a time (batch).\nFor very large files, you might want one line at a time (streaming).\nFor large files in large corpora, you might want more than one ‘machine’.\n\n\nSee the OpenVirus Project."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#starting-points",
    "href": "lectures/7.3-Cleaning_Text.html#starting-points",
    "title": "Cleaning Text",
    "section": "Starting Points",
    "text": "Starting Points\nThese strategies can be used singly or all-together:\n\nStopwords\nCase\nAccent-stripping\nPunctuation\nNumbers\n\nBut these are just a starting point!\n\nWhat’s the semantic difference between 1,000,000 and 999,999?"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#distributional-pruning",
    "href": "lectures/7.3-Cleaning_Text.html#distributional-pruning",
    "title": "Cleaning Text",
    "section": "Distributional Pruning",
    "text": "Distributional Pruning\nWe can prune from both ends of the distribution:\n\nOverly rare words: what does a word used in one document help us to understand about a corpus?\nOverly common ones: what does a word used in every document help us to understand about a corpus?\n\n\nAgain, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#different-approaches",
    "href": "lectures/7.3-Cleaning_Text.html#different-approaches",
    "title": "Cleaning Text",
    "section": "Different Approaches",
    "text": "Different Approaches\nHumans use a lot of words/concepts1:\n\nStemming: rules-based truncation to a stem (can be augmented by language awareness).\nLemmatisation: usually dictionary-based ‘deduplication’ to a lemma (can be augmented by POS-tagging).\n\nA recent digitsation effort by Harvard and Google estimated 1,022,000 unique word-forms in English alone (Source)."
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#different-outcomes",
    "href": "lectures/7.3-Cleaning_Text.html#different-outcomes",
    "title": "Cleaning Text",
    "section": "Different Outcomes",
    "text": "Different Outcomes\n\n\n\nSource\nPorter\nSnowball\nLemmatisation\n\n\n\n\nmonkeys\nmonkey\nmonkey\nmonkey\n\n\ncities\nciti\nciti\ncity\n\n\ncomplexity\ncomplex\ncomplex\ncomplexity\n\n\nReades\nread\nread\nReades\n\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nfor w in ['monkeys','cities','complexity','Reades']:\n    print(f\"Porter: {PorterStemmer().stem(w)}\")\n    print(f\"Snowball: {SnowballStemmer('english').stem(w)}\")\n    print(f\"Lemmatisation: {wnl.lemmatize(w)}\")"
  },
  {
    "objectID": "lectures/7.3-Cleaning_Text.html#resources",
    "href": "lectures/7.3-Cleaning_Text.html#resources",
    "title": "Cleaning Text",
    "section": "Resources",
    "text": "Resources\n\nVectorisation in Python\nLambda Functions\nReal Python Lambda Functions\nStemming words with NLTK\nStemming and Lemmatisation in Python\nKD Nuggets: A Practitioner’s Guide to NLP\nKD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics\nRoadmap to Natural Language Processing (NLP)"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "href": "lectures/7.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "title": "Note(book)s to Documents",
    "section": "Recall: Tangled Workflows",
    "text": "Recall: Tangled Workflows\nIt’s not just about mixing code and comment, we also want:\n\nTo separate content from presentation\nTo define mappings between presentation styles\nTo produce the best-quality output for the format chosen\n\n\n\nExamples of this include CSS for web sites, LaTeX templates, and Markdown styles.\nMVC approach to software design."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#pandoc",
    "href": "lectures/7.1-Notebooks_as_Documents.html#pandoc",
    "title": "Note(book)s to Documents",
    "section": "Pandoc",
    "text": "Pandoc\nTool for converting documents between formats, including:\n\nPlain Text/YAML\nMarkdown\nLaTeX/PDF\nHTML/Reveal.js\nJupyter Notebook\nXML/ODT/DOCX\nEPUB/DocBook\nBibTeX/JSON"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#latex",
    "href": "lectures/7.1-Notebooks_as_Documents.html#latex",
    "title": "Note(book)s to Documents",
    "section": "LaTeX",
    "text": "LaTeX\n\n\n\n\n\n\n\n\nIntended for type-setting of scientific documents, but has been used for slides, posters, CVs, etc. It is not a word processor, it’s more like a compiler.\nThis format is based on Edward Tufte’s VSQD and can be found on GitHub."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#latex-in-practice",
    "href": "lectures/7.1-Notebooks_as_Documents.html#latex-in-practice",
    "title": "Note(book)s to Documents",
    "section": "LaTeX in Practice",
    "text": "LaTeX in Practice\nYou write LaTeX in any text editor, but specialist apps like Texpad or Overleaf make it easier.\n\\documentclass[11pt,article,oneside]{memoir}\n\\newcommand{\\bl}{\\textsc{bl}~\\/}\n\\usepackage{tabularx}\n\n\\begin{document}\n\\maketitle \n\nThis report provides an overview of activities ...\n\n\\section{Applications}\nA primary objective was the submission...\n\nUCL has an institutional license for Overleaf.\nThis document is then compiled (or ‘typeset’) with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.\nCompilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#bibtex",
    "href": "lectures/7.1-Notebooks_as_Documents.html#bibtex",
    "title": "Note(book)s to Documents",
    "section": "BibTeX",
    "text": "BibTeX\nProvides bilbiographic support for LaTeX but widely used by other utilities as is also plain-text.\n@article{Lavin:2019,\n        Author = {Lavin, Matthew J.},\n        Doi = {10.46430/phen0082},\n        Journal = {The Programming Historian},\n        Number = {8},\n        Title = {Analyzing Documents with TF-IDF},\n        Year = {2019},\n        Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}\n\n@incollection{Kitchin:2016,\n        Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},\n        Booktitle = {Smart Urbanism},\n        Chapter = {2},\n        Editor = {Marvin, Luque-Ayala, McFarlane},\n        Title = {Smart Cities and the Politics of Urban Data},\n        Year = {2016}}"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "href": "lectures/7.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "title": "Note(book)s to Documents",
    "section": "BibTeX in Practice",
    "text": "BibTeX in Practice\nTo reference a document we then need to tell LaTeX or Pandoc where to look:\n\\bibliographystyle{apacite} # LaTeX\n\\bibliography{Spatial_Data_Chapter.bib} # LaTeX\nWith citations following formats like:\n\\citep[p.22]{Reades2018} # LaTeX\nOr:\n[@dignazio:2020, chap. 4] # Markdown"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#reveal.js",
    "href": "lectures/7.1-Notebooks_as_Documents.html#reveal.js",
    "title": "Note(book)s to Documents",
    "section": "Reveal.js",
    "text": "Reveal.js\nJavaScript-based presentation framework. Can use Markdown to generate portable interactive slides including references/bibliographies.\nHow this presentation was created.\nCompare:\n\nMarkdown\nHTML\nReveal"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#headings",
    "href": "lectures/7.1-Notebooks_as_Documents.html#headings",
    "title": "Note(book)s to Documents",
    "section": "Headings",
    "text": "Headings\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n# Heading Level 1\n\\section{Heading Level 1}\n\n\n## Heading Level 2\n\\subsection{Heading Level 2}\n\n\n### Heading Level 3\n\\subsubsection{Heading Level 3}"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#inline-elements",
    "href": "lectures/7.1-Notebooks_as_Documents.html#inline-elements",
    "title": "Note(book)s to Documents",
    "section": "Inline Elements",
    "text": "Inline Elements\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n1. Numbered item 1\n\\begin{enumerate} \\n \\item ... \\end{enumerate}\n\n\n- Bulleted list item 1\n\\begin{itemize} \\n \\item ... \\n \\end{itemize}\n\n\n_italics_ or *italics*\n\\emph{italics} or \\textit{italics}\n\n\n**bold**\n\\textbf{bold}\n\n\n&gt; blockquote\n\\begin{quote} \\n blockquote \\end{quote}\n\n\nSome `code` is here\nSome \\texttt{code} is here\n\n\n[Link Text](URL)\n\\href{Link Text}{URL}\n\n\n![Alt Text](Image URL)\n\\begin{figure}\\n \\includegraphics[opts]{...}\\n \\end{figure}"
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#mathematics",
    "href": "lectures/7.1-Notebooks_as_Documents.html#mathematics",
    "title": "Note(book)s to Documents",
    "section": "Mathematics",
    "text": "Mathematics\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\nSame, but either 1 or 2 $’s\n$x=5$\n\n\nSame, but either 1 or 2 $’s\n$\\pi$\n\n\nSame, but either 1 or 2 $’s\n$e = mc^{2}$\n\n\n\nWe can show all this directly in the Notebook! \\(\\pi\\); \\(e = mc^{2}\\); \\(\\int_{0}^{\\inf} x^2 \\,dx\\); \\(\\sum_{n=1}^{\\infty} 2^{-n} = 1\\)\n\n\nOverleaf has good documentation for most (basic) applications."
  },
  {
    "objectID": "lectures/7.1-Notebooks_as_Documents.html#resources",
    "href": "lectures/7.1-Notebooks_as_Documents.html#resources",
    "title": "Note(book)s to Documents",
    "section": "Resources",
    "text": "Resources\n\nJupyter Tips and Tricks\nPandoc Demos\nBeginner’s Guide to Jupyter Notebooks\n7 Essential Tips to Writing With Jupyter Notebooks\nVersion Control with Jupyter\nSustainable Publishing using Pandoc and Markdown\nMaking Pretty PDFs with Quarto\nHOw to use Quarto for Parameterised Reporting"
  },
  {
    "objectID": "lectures/6.3-EDA.html#epicyclic-feedback",
    "href": "lectures/6.3-EDA.html#epicyclic-feedback",
    "title": "Exploratory Data Analysis",
    "section": "Epicyclic Feedback",
    "text": "Epicyclic Feedback\nPeng and Matsui, The Art of Data Science, p.8\n\n\n\n\n\n\n\n\n\n\nSet Expectations\nCollect Information\nRevise Expectations\n\n\n\n\nQuestion\nQuestion is of interest to audience\nLiterature search/experts\nSharpen question\n\n\nEDA\nData are appropriate for question\nMake exploratory plots\nRefine question or collect more data\n\n\nModelling\nPrimary model answers question\nFit secondary models / analysis\nRevise model to include more predictors\n\n\nInterpretation\nInterpretation provides specific and meaningful answer\nInterpret analyses with focus on effect and uncertainty\nRevise EDA and/or models to provide more specific answers\n\n\nCommunication\nProcess & results are complete and meaningful\nSeek feedback\nRevises anlyses or approach to presentation"
  },
  {
    "objectID": "lectures/6.3-EDA.html#approaching-eda",
    "href": "lectures/6.3-EDA.html#approaching-eda",
    "title": "Exploratory Data Analysis",
    "section": "Approaching EDA",
    "text": "Approaching EDA\nThere’s no hard and fast way of doing EDA, but as a general rule you’re looking to:\n\nClean\nCanonicalise\nClean More\nVisualise & Describe\nReview\nClean Some More\n…\n\nThe ‘joke’ is that 80% of Data Science is data cleaning.\n\nCleaning Part 1: testing validity of records (possibly while tracking rejected records for subsequent analysis)\nCanonicalisation: controling for variation (e.g. typos, capitalisation, formatting, leading/trailing whitespace, different types of NULL values, etc.) and in a spatial context deal with projection and geo-data issues.\nCleaning Part 2: further testing of records (e.g. deciding what to do with NaNs, missing values, outside of study area, etc.)\nVisualise & Describe: covered in QM but we’ll take a high-level look at this."
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-related-take",
    "href": "lectures/6.3-EDA.html#a-related-take",
    "title": "Exploratory Data Analysis",
    "section": "A Related Take",
    "text": "A Related Take\nEDA—Don’t ask how, ask what:\n\nDescriptive Statistics: get a high-level understanding of your dataset.\nMissing values: come to terms with how bad your dataset is.\nDistributions and Outliers: and why countries that insist on using different units make our jobs so much harder.\nCorrelations: and why sometimes even the most obvious patterns still require some investigating."
  },
  {
    "objectID": "lectures/6.3-EDA.html#another-take",
    "href": "lectures/6.3-EDA.html#another-take",
    "title": "Exploratory Data Analysis",
    "section": "Another Take",
    "text": "Another Take\nHere’s another view of how to do EDA:\n\nPreview data randomly and substantially\nCheck totals such as number of entries and column types\nCheck nulls such as at row and column levels\nCheck duplicates: do IDs recurr, did the servers fail\nPlot distribution of numeric data (univariate and pairwise joint distribution)\nPlot count distribution of categorical data\nAnalyse time series of numeric data by daily, monthly and yearly frequencies"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-is-it",
    "href": "lectures/6.3-EDA.html#what-is-it",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-is-it-1",
    "href": "lectures/6.3-EDA.html#what-is-it-1",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-is-it-2",
    "href": "lectures/6.3-EDA.html#what-is-it-2",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#start-with-a-chart",
    "href": "lectures/6.3-EDA.html#start-with-a-chart",
    "title": "Exploratory Data Analysis",
    "section": "Start with a Chart",
    "text": "Start with a Chart\nThe problem of relying on statistics alone was amply illustrated by Anscombe’s Quartet (1973)…\n\nWe are not very good at looking at spreadsheets.\nWe are very good at spotting patterns visually.\n\nSometimes, we are too good; that’s where the stats comes in. Think of it as the ‘tiger in the jungle’ problem.."
  },
  {
    "objectID": "lectures/6.3-EDA.html#anscombes-quartet",
    "href": "lectures/6.3-EDA.html#anscombes-quartet",
    "title": "Exploratory Data Analysis",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\n\nX1\nY1\nX2\nY2\nX3\nY3\nX4\nY4\n\n\n\n\n10.0\n8.04\n10.0\n9.14\n10.0\n7.46\n10.0\n6.58\n\n\n8.0\n6.95\n8.0\n8.14\n8.0\n6.77\n8.0\n5.76\n\n\n13.0\n7.58\n13.0\n8.74\n13.0\n12.74\n13.0\n7.71\n\n\n9.0\n8.81\n9.0\n8.77\n9.0\n7.11\n9.0\n8.84\n\n\n11.0\n8.33\n11.0\n9.26\n11.0\n7.81\n11.0\n8.47\n\n\n14.0\n9.96\n14.0\n8.10\n14.0\n8.84\n14.0\n7.04\n\n\n6.0\n7.24\n6.0\n6.13\n6.0\n6.08\n6.0\n5.25\n\n\n4.0\n4.26\n4.0\n3.10\n4.0\n5.39\n4.0\n12.5\n\n\n12.0\n10.84\n12.0\n9.13\n12.0\n8.15\n12.0\n5.56\n\n\n7.0\n4.82\n7.0\n7.26\n7.0\n6.42\n7.0\n7.91\n\n\n5.0\n5.68\n5.0\n4.74\n5.0\n5.73\n5.0\n6.89"
  },
  {
    "objectID": "lectures/6.3-EDA.html#summary-statistics-for-the-quartet",
    "href": "lectures/6.3-EDA.html#summary-statistics-for-the-quartet",
    "title": "Exploratory Data Analysis",
    "section": "Summary Statistics for the Quartet",
    "text": "Summary Statistics for the Quartet\n\n\n\nProperty\nValue\n\n\n\n\nMean of x\n9.0\n\n\nVariance of x\n11.0\n\n\nMean of y\n7.5\n\n\nVariance of y\n4.12\n\n\nCorrelation between x and y\n0.816\n\n\nLinear Model\ny = 3 + 0.5x"
  },
  {
    "objectID": "lectures/6.3-EDA.html#but-what-do-they-look-like",
    "href": "lectures/6.3-EDA.html#but-what-do-they-look-like",
    "title": "Exploratory Data Analysis",
    "section": "But What do They Look Like?",
    "text": "But What do They Look Like?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#the-tiger-that-isnt",
    "href": "lectures/6.3-EDA.html#the-tiger-that-isnt",
    "title": "Exploratory Data Analysis",
    "section": "The Tiger that Isn’t",
    "text": "The Tiger that Isn’t\n\nI would argue that the basic purpose of charts and of statistics as a whole is to help us untangle signal from noise. We are ‘programmed’ to see signals, so we need to set the standard for ‘it’s a tiger!’ quite high in research & in policy-making.\n\n\nOr, as Albert Einstein reportedly said: “If I can’t picture it, I can’t understand it.”"
  },
  {
    "objectID": "lectures/6.3-EDA.html#think-it-through",
    "href": "lectures/6.3-EDA.html#think-it-through",
    "title": "Exploratory Data Analysis",
    "section": "Think it Through",
    "text": "Think it Through\nYou can make a lot of progress in your research without any advanced statistics!\n\nA ‘picture’ isn’t just worth 1,000 words, it could be a whole dissertation!\nThe right chart makes your case eloquently and succinctly.\n\nAlways ask yourself:\n\nWhat am I trying to say?\nHow can I say it most effectively?\nIs there anything I’m overlooking in the data?\n\nA good chart is a good way to start!"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-makes-a-good-plot",
    "href": "lectures/6.3-EDA.html#what-makes-a-good-plot",
    "title": "Exploratory Data Analysis",
    "section": "What Makes a Good Plot?",
    "text": "What Makes a Good Plot?\nA good chart or table:\n\nServes a purpose — it is clear how it advances the argument in a way that could not be done in the text alone.\nContains only what is relevant — zeroes in on what the reader needs and is not needlessly cluttered.\nUses precision that is meaningful — doesn’t clutter the chart with needless numbers.\n\n\nFar too many charts or tables could be easily written up in a single sentence.\nFar too many charts or tables contain redundancy, clutter, and ‘flair’.\nDon’t report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale."
  },
  {
    "objectID": "lectures/6.3-EDA.html#for-example",
    "href": "lectures/6.3-EDA.html#for-example",
    "title": "Exploratory Data Analysis",
    "section": "For Example…",
    "text": "For Example…\nHow much precision is necessary in measuring degrees at the equator?\n\n\n\nDecimal Places\nDegrees\nDistance\n\n\n\n\n0\n1\n111km\n\n\n1\n0.1\n11.1km\n\n\n2\n0.01\n1.11km\n\n\n3\n0.001\n111m\n\n\n4\n0.0001\n11.1m\n\n\n5\n0.00001\n1.11m\n\n\n6\n0.000001\n11.1cm\n\n\n7\n0.0000001\n1.11cm\n\n\n8\n0.00000001\n1.11mm"
  },
  {
    "objectID": "lectures/6.3-EDA.html#goals-by-world-cup-final",
    "href": "lectures/6.3-EDA.html#goals-by-world-cup-final",
    "title": "Exploratory Data Analysis",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/6.3-EDA.html#goals-by-world-cup-final-1",
    "href": "lectures/6.3-EDA.html#goals-by-world-cup-final-1",
    "title": "Exploratory Data Analysis",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/6.3-EDA.html#average-goals-by-world-cup-final",
    "href": "lectures/6.3-EDA.html#average-goals-by-world-cup-final",
    "title": "Exploratory Data Analysis",
    "section": "Average Goals by World Cup Final",
    "text": "Average Goals by World Cup Final\n\n\nIn 1982 the number of teams went from 16 to 24, and in 1998 it went from 24 to 32!"
  },
  {
    "objectID": "lectures/6.3-EDA.html#how-far-from-equality",
    "href": "lectures/6.3-EDA.html#how-far-from-equality",
    "title": "Exploratory Data Analysis",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#how-far-from-equality-1",
    "href": "lectures/6.3-EDA.html#how-far-from-equality-1",
    "title": "Exploratory Data Analysis",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/6.3-EDA.html#the-purpose-of-a-chart",
    "href": "lectures/6.3-EDA.html#the-purpose-of-a-chart",
    "title": "Exploratory Data Analysis",
    "section": "The Purpose of a Chart",
    "text": "The Purpose of a Chart\nThe purpose of a graph is to show that there are relationships within the data set that are not trivial/expected.\nChoose the chart to highlight relationships, or the lack thereof:\n\nThink of a chart or table as part of your ‘argument’ – if you can’t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably don’t need it.\nIdentify & prioritise the relationships in the data.\nChoose a chart type/chart symbology that gives emphasis to the most important relationships."
  },
  {
    "objectID": "lectures/6.3-EDA.html#not-everyone-likes-tables",
    "href": "lectures/6.3-EDA.html#not-everyone-likes-tables",
    "title": "Exploratory Data Analysis",
    "section": "Not Everyone Likes Tables",
    "text": "Not Everyone Likes Tables\n\nGetting information from a table is like extracting sunlight from a cucumber. Arthur & Henry Fahrquhar 1891)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#real-numbers",
    "href": "lectures/6.3-EDA.html#real-numbers",
    "title": "Exploratory Data Analysis",
    "section": "Real Numbers",
    "text": "Real Numbers\nConsider the difference in emphasis between:\n\n11316149\n11,316,149\n11.3 million\n11 x 10\\(^{6}\\)\n22%\n22.2559%\n\nAlways keep in mind the purpose of the number."
  },
  {
    "objectID": "lectures/6.3-EDA.html#theres-still-a-role-for-tables",
    "href": "lectures/6.3-EDA.html#theres-still-a-role-for-tables",
    "title": "Exploratory Data Analysis",
    "section": "There’s Still a Role for Tables",
    "text": "There’s Still a Role for Tables\nWhy a table is sometimes better than a chart:\n\nYou need to present data values with greater detail\nYou need to enable readers to draw comparisons between data values\nYou need to present the same data in multiple ways (e.g. raw number and percentage)\nYou want to show many dimensions for a small number of observations\n\n\ne.g. percentage of people falling into each ethnic or income category for a small number of wards or boroughs."
  },
  {
    "objectID": "lectures/6.3-EDA.html#undergraduate-tables-failing-grade",
    "href": "lectures/6.3-EDA.html#undergraduate-tables-failing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Undergraduate Tables (Failing Grade)",
    "text": "Undergraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#undergraduate-tables-passing-grade",
    "href": "lectures/6.3-EDA.html#undergraduate-tables-passing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Undergraduate Tables (Passing Grade)",
    "text": "Undergraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#postgraduate-tables-failing-grade",
    "href": "lectures/6.3-EDA.html#postgraduate-tables-failing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Postgraduate Tables (Failing Grade)",
    "text": "Postgraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#postgraduate-tables-passing-grade",
    "href": "lectures/6.3-EDA.html#postgraduate-tables-passing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Postgraduate Tables (Passing Grade)",
    "text": "Postgraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#design-for-tables",
    "href": "lectures/6.3-EDA.html#design-for-tables",
    "title": "Exploratory Data Analysis",
    "section": "Design for Tables",
    "text": "Design for Tables\nPrinciples:\n\nReduce the number of lines to a minimum (and you should almost never need vertical lines).\nUse ‘white-space’ to create visual space between groups of unrelated (or less related) elements.\nRemove redundancy (if you find yourself typing ‘millions’ or ‘GBP’ or ‘Male’ repeatedly then you’ve got redundancy).\nEnsure that meta-data is clearly separate from, but attached to, the graph (i.e. source, title, etc.)."
  },
  {
    "objectID": "lectures/6.3-EDA.html#getting-started",
    "href": "lectures/6.3-EDA.html#getting-started",
    "title": "Exploratory Data Analysis",
    "section": "Getting Started",
    "text": "Getting Started\nYou can follow along by loading the Inside Airbnb sample:\nimport pandas as pd\nimport geopandas as gpd\nurl='https://bit.ly/3I0XDrq'\ndf = pd.read_csv(url)\ndf.set_index('id', inplace=True)\ndf['price'] = df.price.str.replace('$','',regex=False).astype('float')\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.to_file('Airbnb_Sample.gpkg', driver='GPKG')\n\nNote that this (re)loads the sampled Airbnb data from GitHub every time you run it. For a large data set on someone else’e server you might want to save and (re)load it locally. A simple helper function would then just check if the file already existed locally before trying to download the file again: that would allow you to work while offline and speed up your code substantially too!"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-can-we-do-series",
    "href": "lectures/6.3-EDA.html#what-can-we-do-series",
    "title": "Exploratory Data Analysis",
    "section": "What Can We Do? (Series)",
    "text": "What Can We Do? (Series)\nThis is by no means all that we can do…\n\nSeries-level Methods.\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(f\"Host count is {gdf.host_name.count()}\")\nprint(f\"Mean is {gdf.price.mean():.0f}\")\nprint(f\"Max price is {gdf.price.max()}\")\nprint(f\"Min price is {gdf.price.min()}\")\nprint(f\"Median price is {gdf.price.median()}\")\nprint(f\"Standard dev is {gdf.price.std():.2f}\")\nprint(f\"25th quantile is {gdf.price.quantile(q=0.25)}\")\nCount of non-nulls\nMean\nHighest value\nLowest value\nMedian\nStandard deviation\n25th quantile"
  },
  {
    "objectID": "lectures/6.3-EDA.html#what-can-we-do-data-frame",
    "href": "lectures/6.3-EDA.html#what-can-we-do-data-frame",
    "title": "Exploratory Data Analysis",
    "section": "What Can We Do? (Data Frame)",
    "text": "What Can We Do? (Data Frame)\n\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(df.mean())\nprint(df.count())\nprint(df.max())\n# ...\nprint(df.corr())\nprint(df.describe())\nMean of each column\nNumber of non-null values in each column\nHighest value in each column\n$\\vdots$\nCorrelation between columns\nSummarise\n\n\n\n\nNotice how we have the same functionality, but it operates at the level of the data set itself now. We gain a few new functions as well that relate to interactions between columns (a.k.a. data series)."
  },
  {
    "objectID": "lectures/6.3-EDA.html#measures",
    "href": "lectures/6.3-EDA.html#measures",
    "title": "Exploratory Data Analysis",
    "section": "Measures",
    "text": "Measures\nSo pandas provides functions for commonly-used measures:\nprint(f\"{df.price.mean():.2f}\")\nprint(f\"{df.price.median():.2f}\")\nprint(f\"{df.price.quantile(0.25):.2f}\")\nOutput:\n118.4542\n80.50\n40.75"
  },
  {
    "objectID": "lectures/6.3-EDA.html#more-complex-measures",
    "href": "lectures/6.3-EDA.html#more-complex-measures",
    "title": "Exploratory Data Analysis",
    "section": "More Complex Measures",
    "text": "More Complex Measures\nBut Pandas also makes it easy to derive new variables… Here’s the z-score:\n\\[ z = \\frac{x - \\bar{x}}{s}\\]\ndf['zscore'] = (df.price - df.price.mean())/df.price.std()\ndf.plot.box(column='zscore')"
  },
  {
    "objectID": "lectures/6.3-EDA.html#and-even-more-complex",
    "href": "lectures/6.3-EDA.html#and-even-more-complex",
    "title": "Exploratory Data Analysis",
    "section": "And Even More Complex",
    "text": "And Even More Complex\nAnd here’s the Interquartile Range Standardised score:\n\\[ x_{iqrs} = \\frac{x - \\widetilde{x}}{Q_{75} - Q_{25}} \\]\ndf['iqr_std'] = (df.price - df.price.median())/ \\\n      (df.price.quantile(q=0.75)-df.price.quantile(q=0.25))\ndf.plot.box(column='iqr_std')"
  },
  {
    "objectID": "lectures/6.3-EDA.html#the-plot-thickens",
    "href": "lectures/6.3-EDA.html#the-plot-thickens",
    "title": "Exploratory Data Analysis",
    "section": "The Plot Thickens",
    "text": "The Plot Thickens\nWe’ll get to more complex plotting over the course of the term, but here’s a good start for exploring the data! All plotting depends on matplotlib which is the ogre in the attic to R’s ggplot.\nimport matplotlib.pyplot as plt\nGet used to this import as it will allow you to save and manipulate the figures created in Python. It is not the most intuitive approach (unless you’ve used MATLAB before) but it does work."
  },
  {
    "objectID": "lectures/6.3-EDA.html#boxplot",
    "href": "lectures/6.3-EDA.html#boxplot",
    "title": "Exploratory Data Analysis",
    "section": "Boxplot",
    "text": "Boxplot\ndf.price.plot.box()\nplt.savefig('pboxplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#frequency",
    "href": "lectures/6.3-EDA.html#frequency",
    "title": "Exploratory Data Analysis",
    "section": "Frequency",
    "text": "Frequency\ndf.room_type.value_counts().plot.bar()\nplt.savefig('phistplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-correlation-heatmap",
    "href": "lectures/6.3-EDA.html#a-correlation-heatmap",
    "title": "Exploratory Data Analysis",
    "section": "A Correlation Heatmap",
    "text": "A Correlation Heatmap\nWe’ll get to these in more detail in a couple of weeks, but here’s some output…"
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-map",
    "href": "lectures/6.3-EDA.html#a-map",
    "title": "Exploratory Data Analysis",
    "section": "A ‘Map’",
    "text": "A ‘Map’\ndf.plot.scatter(x='longitude',y='latitude')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#a-fancy-map",
    "href": "lectures/6.3-EDA.html#a-fancy-map",
    "title": "Exploratory Data Analysis",
    "section": "A Fancy ‘Map’",
    "text": "A Fancy ‘Map’\ndf.plot.scatter(x='longitude',y='latitude',\n                c='price',colormap='viridis',\n                figsize=(10,5),title='London',\n                grid=True,s=24,marker='x')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#an-actual-map",
    "href": "lectures/6.3-EDA.html#an-actual-map",
    "title": "Exploratory Data Analysis",
    "section": "An Actual ‘Map’",
    "text": "An Actual ‘Map’\ngdf.plot(column='price', cmap='viridis', \n         scheme='quantiles', markersize=8, legend=True)"
  },
  {
    "objectID": "lectures/6.3-EDA.html#resources",
    "href": "lectures/6.3-EDA.html#resources",
    "title": "Exploratory Data Analysis",
    "section": "Resources",
    "text": "Resources\nThere’s so much more to find, but:\n\n\n\nPandas Reference\nA Guide to EDA in Python (Looks very promising)\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nEDA: DataPrep.eda vs Pandas-Profiling [Requires module installation]\n\n\n\nA Data Science Project for Beginners (EDA)\nEDA: A Pracitcal Guide and Template for Structured Data\nEDA—Don’t ask how, ask what (Part 1)\nPreparing your Dataset for Modeling – Quickly and Easily (Part 2)\nHandling Missing Data\nIntroduction to Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#two-cultures",
    "href": "lectures/6.1-Mapping.html#two-cultures",
    "title": "Computational Mapping",
    "section": "Two Cultures",
    "text": "Two Cultures\n\n\nPurely Computational\nBoth analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\nMostly Computational\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#the-challenge",
    "href": "lectures/6.1-Mapping.html#the-challenge",
    "title": "Computational Mapping",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#constituency-cards",
    "href": "lectures/6.1-Mapping.html#constituency-cards",
    "title": "Computational Mapping",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nClone and reproduce: github.com/alasdairrae/wpc and explanation: cconstituency cards."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/6.1-Mapping.html#short-term-lets-in-scotland",
    "title": "Computational Mapping",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#every-building-in-america",
    "href": "lectures/6.1-Mapping.html#every-building-in-america",
    "title": "Computational Mapping",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#think-it-through",
    "href": "lectures/6.1-Mapping.html#think-it-through",
    "title": "Computational Mapping",
    "section": "Think it Through!",
    "text": "Think it Through!"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/6.1-Mapping.html#a-deceptively-simple-problem",
    "title": "Computational Mapping",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#classification",
    "href": "lectures/6.1-Mapping.html#classification",
    "title": "Computational Mapping",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#six-views-of-employment",
    "href": "lectures/6.1-Mapping.html#six-views-of-employment",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#six-views-of-employment-1",
    "href": "lectures/6.1-Mapping.html#six-views-of-employment-1",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#six-views-of-employment-2",
    "href": "lectures/6.1-Mapping.html#six-views-of-employment-2",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#consider",
    "href": "lectures/6.1-Mapping.html#consider",
    "title": "Computational Mapping",
    "section": "Consider",
    "text": "Consider\nWe want to:\n\nGroup features with similar values together.\nShow these in a way that doesn’t mislead the viewer.\n\nBut we have the following problems:\n\nToo many classes confuse the viewer.\nToo few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#choices-choices",
    "href": "lectures/6.1-Mapping.html#choices-choices",
    "title": "Computational Mapping",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#look-at-the-data",
    "href": "lectures/6.1-Mapping.html#look-at-the-data",
    "title": "Computational Mapping",
    "section": "Look at the Data!",
    "text": "Look at the Data!\nDifferent colour and break schemes not only give us different views of the data, they give us different understandings of the data! Each scheme changes how the data looks and, consequently, how we perceive the distribution."
  },
  {
    "objectID": "lectures/6.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/6.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Computational Mapping",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/6.1-Mapping.html#resources",
    "href": "lectures/6.1-Mapping.html#resources",
    "title": "Computational Mapping",
    "section": "Resources",
    "text": "Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation\nWorking with Spatial Data in Python\nWeb Mapping Notes"
  },
  {
    "objectID": "lectures/5.3-Files.html#from-files-to-data",
    "href": "lectures/5.3-Files.html#from-files-to-data",
    "title": "File Formats",
    "section": "From Files to Data",
    "text": "From Files to Data\nIn order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data set have row or column names? (a.k.a. headers & metadata)\nIs the metadata in a separate file or embedded in the file?"
  },
  {
    "objectID": "lectures/5.3-Files.html#structure-of-a-tabular-data-file",
    "href": "lectures/5.3-Files.html#structure-of-a-tabular-data-file",
    "title": "File Formats",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names make it a lot easier to find and refer to data (e.g. the ‘East of England row’ or the ‘Total column’) but they are not data and don’t belong in the data set itself.\nUsually, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ’carriage return (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nUsually, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,) which gives rise to CSV (Comma-Separate Values), but it could be tabs (\\t) or anything else too (; or | or EOF).\n\nHow would we choose a good field separator?\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/5.3-Files.html#most-common-formats",
    "href": "lectures/5.3-Files.html#most-common-formats",
    "title": "File Formats",
    "section": "Most Common Formats",
    "text": "Most Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/5.3-Files.html#mapping-data-types",
    "href": "lectures/5.3-Files.html#mapping-data-types",
    "title": "File Formats",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often see the term ‘mapping’ used in connection to data that is not spatial, what do they mean? A map is the term used in some programming languages for a dict! So it’s about key : value pairs again.\nHere’s a mapping\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/5.3-Files.html#testing-a-mapping",
    "href": "lectures/5.3-Files.html#testing-a-mapping",
    "title": "File Formats",
    "section": "Testing a Mapping",
    "text": "Testing a Mapping\nWorking out an appropriate mapping (representation of the data) is hugely time-consuming.\n\nIt’s commonly held that 80% of data science is data cleaning.\n\nThe Unix utilities (grep, awk, tail, head) can be very useful for quickly exploring the data in order to develop a basic understanding of the data and to catch obvious errors.\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/5.3-Files.html#label-these",
    "href": "lectures/5.3-Files.html#label-these",
    "title": "File Formats",
    "section": "Label These",
    "text": "Label These\n\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/5.3-Files.html#things-that-can-go-wrong",
    "href": "lectures/5.3-Files.html#things-that-can-go-wrong",
    "title": "File Formats",
    "section": "Things That Can Go Wrong…",
    "text": "Things That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/5.3-Files.html#resources",
    "href": "lectures/5.3-Files.html#resources",
    "title": "File Formats",
    "section": "Resources",
    "text": "Resources\n\n\n\nUnderstanding Directories and Subdirectories\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\n\n\n\nText vs. binary mode\nText files\npetl\npandas 2.0 and the Arrow revolution (Part 1)\nWhat parquet files are my preferred API for bulk open data\nDuckDB Documentation"
  },
  {
    "objectID": "lectures/5.1-Logic.html#do-you-want",
    "href": "lectures/5.1-Logic.html#do-you-want",
    "title": "Logic",
    "section": "Do You Want?",
    "text": "Do You Want?\n\n\nConsider the following three things I’ve said to my toddler:\n\nDo you want to go to the park and have ice cream?\nDo you want cereal or toast for breakfast?\nDo not touch that again or else!\n\nUnlike my toddler, the computer does listen, but my toddler has more common sense!"
  },
  {
    "objectID": "lectures/5.1-Logic.html#in-code-now",
    "href": "lectures/5.1-Logic.html#in-code-now",
    "title": "Logic",
    "section": "In Code Now",
    "text": "In Code Now\n(x,y) = True,False\nif x:\n  print(\"x\")\n\nif x and y:\n  print(\"x and y\")\n\nif x or y:\n  print(\"x or y\")\n\nif x and not y:\n  print(\"x and not y\")\n\nif not(x and y):\n  print(\"not x and y\")\n\nif x is the simplest: if True then print \"x\". There is no logic, we just do it or we don’t depending on whether x is True.\nif x and y is the ‘park and ice cream question’: there’s no ice cream without going to the park. So both things have to be True for this to work out happily and print(\"x and y\").\nif x or y is the ‘cereal or toast for breakfast’ question: my daughter is having breakfast either way, so if she answers ‘Yes’ (True) to either then it doesn’t matter that the other one is False, she’s still had breakfast, or we’re still printing “x or y”. And note, if she was really hungry and said yes to both then that would also be breakfast. So if they are both True that’s also fine.\nif x and not y: is the negation of one term. So ‘if you behave and don’t touch that again then you will get a treat’ is what we’re looking for here. But you can also negate the entire thing: not(x and y) is also valid logic. So this turns a False on if x and y into a True."
  },
  {
    "objectID": "lectures/5.1-Logic.html#combining-logic-with-operators",
    "href": "lectures/5.1-Logic.html#combining-logic-with-operators",
    "title": "Logic",
    "section": "Combining Logic With Operators",
    "text": "Combining Logic With Operators\nRemember that operators like &lt;= and == also produce True/False answers:\nx = y = 5\nz = 3\nif x==y:\n  print(\"x==y\")\n\nif x==y and x==z:\n  print(\"x==y and x==z\")\n\nif x==y or x==z:\n  print(\"x==y or x==z\")\n\nif x==y and not x==z:\n  print(\"x==y and not x==z\")"
  },
  {
    "objectID": "lectures/5.1-Logic.html#a-special-case",
    "href": "lectures/5.1-Logic.html#a-special-case",
    "title": "Logic",
    "section": "A Special Case",
    "text": "A Special Case\nThere is a second set of logical operators that apply in very specific circumstances. These are called ‘bitwise’ operators and apply to data specified in bits.\n\n\n\nRegular Operator\nBitwise Equivalent\n\n\n\n\nand\n&\n\n\nor\n|\n\n\nnot\n~\n\n\n\nLet’s see (briefly) how these work…"
  },
  {
    "objectID": "lectures/5.1-Logic.html#working-with-bits",
    "href": "lectures/5.1-Logic.html#working-with-bits",
    "title": "Logic",
    "section": "Working With Bits",
    "text": "Working With Bits\nx,y = 38,3\nprint(f\"{x:b}\") # `:b` means byte-format\nprint(f\"{y:b}\")\nThis gives us that x is '100110' and y is '11', so now:\nprint(f\"{x & y:b}\")  # 10\nprint(f\"{x | y:b}\")  # 100111\nprint(f\"{x & ~y:b}\") # 100100"
  },
  {
    "objectID": "lectures/5.1-Logic.html#perhaps-easier-to-see-this-way",
    "href": "lectures/5.1-Logic.html#perhaps-easier-to-see-this-way",
    "title": "Logic",
    "section": "Perhaps Easier to See This Way?",
    "text": "Perhaps Easier to See This Way?\n\n\n\nOperator\n1\n2\n3\n4\n5\n6\n\n\n\n\nx\n1\n0\n0\n1\n1\n0\n\n\ny\n0\n0\n0\n0\n1\n1\n\n\nx & y\n0\n0\n0\n0\n1\n0\n\n\nx | y\n1\n0\n0\n1\n1\n1\n\n\n~y\n1\n1\n1\n1\n0\n0\n\n\nx & ~y\n1\n0\n0\n1\n0\n0\n\n\n\nBitwise operations are very, very fast and so are a good way to, say, find things in large data sets. You’ve been warned.\n\nThey are how pandas and numpy manage indexes and queries against data frames."
  },
  {
    "objectID": "lectures/5.1-Logic.html#nulls-none-vs.-nan",
    "href": "lectures/5.1-Logic.html#nulls-none-vs.-nan",
    "title": "Logic",
    "section": "Nulls: None vs. NaN",
    "text": "Nulls: None vs. NaN\nBeware of using logic with things that are not what they appear:\n\nNone is Python’s way of saying that something has no value at all (not 0 or \"\"… but None). It is a class.\nNaN (Not a Number) is a special numeric data type provided by the numpy package to deal with things like -ve and +ve infinity and similar ‘issues’.\n\nnp.nan should be used whenever you are dealing with data (e.g. see Pandas!)."
  },
  {
    "objectID": "lectures/5.1-Logic.html#none-vs.-nan",
    "href": "lectures/5.1-Logic.html#none-vs.-nan",
    "title": "Logic",
    "section": "None vs. NaN",
    "text": "None vs. NaN\nimport numpy as np\nprint(type(np.nan))     # float\nprint(type(None))       # NoneType\nCritically:\nprint(\"\"==None)         # False\nprint(None==None)       # True\nprint(np.nan==None)     # False\nprint(np.nan==np.nan)   # False!\nprint(np.nan is np.nan) # True\nprint(np.isnan(np.nan)) # True"
  },
  {
    "objectID": "lectures/5.1-Logic.html#in-not-in",
    "href": "lectures/5.1-Logic.html#in-not-in",
    "title": "Logic",
    "section": "In / Not In",
    "text": "In / Not In\nWe’ve touched on these before:\ng = ['Harvey','Rose','Batty','Jefferson']\n\nif 'Batty' in g:\n  print(\"In the group!\")\n\nif 'Marx' not in g:\n  print(\"Not in the group!\")\nThe set data type also supports in, and not in together with all of the set maths (union, intersect, etc.).\n\nThis is a good place for a recap though."
  },
  {
    "objectID": "lectures/5.1-Logic.html#sets",
    "href": "lectures/5.1-Logic.html#sets",
    "title": "Logic",
    "section": "Sets",
    "text": "Sets\nMembership maths:\ns1 = {'cherry','orange','banana','tomato'} # Or s1(...)\ns2 = {'potato','celery','carrot','tomato'} # Or s2(...)\nprint('potato' in s1)      # False\nprint(s1.difference(s2))   # {'banana', ...}\nprint(s1.intersection(s2)) # {'tomato'}\nprint(s1.union(s2))        # {'orange', ...}\n\nThese more advanced functions are only for sets, not lists or dicts."
  },
  {
    "objectID": "lectures/5.1-Logic.html#resources",
    "href": "lectures/5.1-Logic.html#resources",
    "title": "Logic",
    "section": "Resources",
    "text": "Resources\n\n\n\nLogical operators: And, or, not\nComparison operators\nBitwise operators\nComparison operators\n\n\n\nBoolean operators\nOperator precedence\nNaN and None in Python\nHandling Missing Data"
  },
  {
    "objectID": "lectures/4.3-Design.html#tree-of-life",
    "href": "lectures/4.3-Design.html#tree-of-life",
    "title": "Object-Oriented Design",
    "section": "Tree of Life",
    "text": "Tree of Life"
  },
  {
    "objectID": "lectures/4.3-Design.html#tree-of-vehicles",
    "href": "lectures/4.3-Design.html#tree-of-vehicles",
    "title": "Object-Oriented Design",
    "section": "Tree of Vehicles",
    "text": "Tree of Vehicles\nMost people would call this a class hierarchy or diagram.\n\nThere is no natural order here: where do e-bikes, unicycles, and rickshaws go?\n\nIndeed, we could map out vehicles based on the number of axles, their source of power, their driver positioning, etc., etc. The point here is that the designer must make choices that are influenced by (and will influence) the design of the application for which this class hierarchy is buing developed. An automobile manufacturer might make different choices from a government trying to implement a tax policy."
  },
  {
    "objectID": "lectures/4.3-Design.html#classes-vs-packages",
    "href": "lectures/4.3-Design.html#classes-vs-packages",
    "title": "Object-Oriented Design",
    "section": "Classes vs Packages",
    "text": "Classes vs Packages\n\nFunctionally, a class and a package are indistinguishable, but a class produces objects that use methods and instance or class variables, whereas a package is a group of functions and constants that may, or may not, include classes.\n\nUgh, now try to keep this straight in your head."
  },
  {
    "objectID": "lectures/4.3-Design.html#key-takeaways",
    "href": "lectures/4.3-Design.html#key-takeaways",
    "title": "Object-Oriented Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nYou’ve been using Classes and Methods since you started.\nYou can ‘package up’ useful code into functions, and useful functions into packages.\nTogether, packages and classes will turbo-charge your programming skills.\nYou can stand on the shoulders of giants!"
  },
  {
    "objectID": "lectures/4.3-Design.html#resources",
    "href": "lectures/4.3-Design.html#resources",
    "title": "Object-Oriented Design",
    "section": "Resources",
    "text": "Resources\n\nWhat is object-oriented programming?\nPython object-oriented programming\nObject-oriented programming refresher\nUnderstanding inheritance\nAbstract base classes\nPython - Object-Oriented"
  },
  {
    "objectID": "lectures/4.1-Methods.html#todays-question",
    "href": "lectures/4.1-Methods.html#todays-question",
    "title": "Methods",
    "section": "Today’s Question",
    "text": "Today’s Question\nWe know that a function looks like this:\n&lt;function name&gt;( &lt;input&gt; )\nAnd we know that a function in a package looks like this:\n&lt;package name&gt;.&lt;function name&gt;( &lt;input&gt; )\nSo is list a package?\nmy_list.append( &lt;value&gt; )"
  },
  {
    "objectID": "lectures/4.1-Methods.html#well",
    "href": "lectures/4.1-Methods.html#well",
    "title": "Methods",
    "section": "Well…",
    "text": "Well…\nmy_list.append( &lt;value&gt; ) is a function.\nmy_list.append( &lt;value&gt; ) is a special type of function called a method."
  },
  {
    "objectID": "lectures/4.1-Methods.html#whats-a-method-then",
    "href": "lectures/4.1-Methods.html#whats-a-method-then",
    "title": "Methods",
    "section": "What’s a Method Then?",
    "text": "What’s a Method Then?\nPackages group useful constants and functions together in one place.\nMethods group constants and functions together in one place with data.\nSo my_list.append(...) is called a list method:\n\nIt only knows how to append things to lists.\nIt is only available as a function when you have an insantiated list (e.g. [] or [1,'dog',3.5]).\nIt is bound to variables (aka. objects) of class list."
  },
  {
    "objectID": "lectures/4.1-Methods.html#proof",
    "href": "lectures/4.1-Methods.html#proof",
    "title": "Methods",
    "section": "Proof!",
    "text": "Proof!\nmy_list = [] # \nhelp(my_list)\nThis will give you:\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  Built-in mutable sequence.\n |  If no argument is given, the constructor creates a new empty list.\n |  Methods defined here:\n | ...\n |  append(self, object, /)\n |      Append object to the end of the list.\n |\n |  clear(self, /)\n |      Remove all items from list.\n |\n |  copy(self, /)\n |      Return a shallow copy of the list.\n | ...\n\nIt’s not obvious here, but you can also create lists by writing list()."
  },
  {
    "objectID": "lectures/4.1-Methods.html#its-all-methods",
    "href": "lectures/4.1-Methods.html#its-all-methods",
    "title": "Methods",
    "section": "It’s all Methods",
    "text": "It’s all Methods\nmsg = 'Hello World'\ndir(msg)\n['__add__', '__class__', ..., 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', ... 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\nAnd then we can inquire about these methods:\nhelp(msg.capitalize)\nHelp on built-in function capitalize:\n\ncapitalize() method of builtins.str instance\n    Return a capitalized version of the string.\n\n    More specifically, make the first character have upper case and the rest lower\n    case."
  },
  {
    "objectID": "lectures/4.1-Methods.html#final-fundamental-concepts",
    "href": "lectures/4.1-Methods.html#final-fundamental-concepts",
    "title": "Methods",
    "section": "Final Fundamental Concepts",
    "text": "Final Fundamental Concepts\nFrom here on out, nearly all of what you learn will be new applications, not new concepts and terminology.\n\n\n\n\n\n\n\n\nTerm\nMeans\nExample\n\n\n\n\nClass\nThe template for a ‘thing’.\nRecipe for a pizza.\n\n\nObject\nThe instantiated ‘thing’.\nA pizza I can eat!\n\n\nMethod\nFunctions defined for the class and available to the object.\nThings I can do with a pizza (eat, cook, make).\n\n\nConstructor\nThe special method that builds new objects of that class.\nHow to start a new pizza!\n\n\nSelf\nA reference to the current object.\nThe pizza in front of me!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#what-does-a-function-look-like",
    "href": "lectures/3.4-Functions.html#what-does-a-function-look-like",
    "title": "Functions",
    "section": "What Does a Function Look Like?",
    "text": "What Does a Function Look Like?\nlen(&lt;some_list&gt;) is a function\nSo len(...) encapsulates the process of figuring out how long something with ‘countable units’ actually is, whether it’s a string or a list.\nprint(&lt;some_value&gt;) is also a function\nBecause print(...) encapsulates the process of sending output to the command line, a file, or even a database or API!\n\nlen(123) is a Type Error.\nlen(‘123’) is not.\nCan you think why?"
  },
  {
    "objectID": "lectures/3.4-Functions.html#so-what-does-a-function-look-like",
    "href": "lectures/3.4-Functions.html#so-what-does-a-function-look-like",
    "title": "Functions",
    "section": "So What Does a Function Look like?",
    "text": "So What Does a Function Look like?\nAll function ‘calls’ looking something like this:\nfunction_name(...)\nWhere the ‘...’ are the inputs to the function; it could be one variable, 25 variables, a list, even another function!\nAnd if the function ‘returns’ something it will look like this:\nreturn_data = function_name(...)"
  },
  {
    "objectID": "lectures/3.4-Functions.html#in-action",
    "href": "lectures/3.4-Functions.html#in-action",
    "title": "Functions",
    "section": "In Action!",
    "text": "In Action!\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data)) # 8.666666666666666\ndata2 = [200000,2500000,-4,1400000,70,900000]\nprint(calc_mean(data2)) # 833344.3333333334"
  },
  {
    "objectID": "lectures/3.4-Functions.html#but-notice",
    "href": "lectures/3.4-Functions.html#but-notice",
    "title": "Functions",
    "section": "But Notice!",
    "text": "But Notice!\ndata    = [1,25,-4,14,7,9]\ntotal   = 1\nnumbers = []\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\nprint(calc_mean(data))\n\n# Why haven't these changed????\nprint(total)\nprint(numbers)\n\nFunctions encapsulate information: the total and numbers used within the function are different from the variables with the same name that we created outside the function. When we get to libraries and packages you’ll understand why, but the key concept here is ‘namespace’ and that these variables might have the same name but they sit in different namespaces."
  },
  {
    "objectID": "lectures/3.4-Functions.html#simple-function",
    "href": "lectures/3.4-Functions.html#simple-function",
    "title": "Functions",
    "section": "Simple Function",
    "text": "Simple Function\nBy ‘simple’ I don’t mean easy, I mean it does one thing only:\ndef hello():\n  print(\"Hello world!\")\nWe then run it with:\nhello()\nAnd that produces:\nHello world!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#passing-in-information",
    "href": "lectures/3.4-Functions.html#passing-in-information",
    "title": "Functions",
    "section": "Passing in Information",
    "text": "Passing in Information\nWe can pass information to a function if we tell the function what to expect:\ndef hello(name:str):\n  print(f\"Hello {name}!\")\nNow we can do this:\nhello(\"new programmers\")\nAnd that produces:\nHello new programmers!"
  },
  {
    "objectID": "lectures/3.4-Functions.html#getting-information-out",
    "href": "lectures/3.4-Functions.html#getting-information-out",
    "title": "Functions",
    "section": "Getting Information Out",
    "text": "Getting Information Out\nWe can also get information out of them!\ndef hello(name:str) -&gt; str:\n  return f\"Hello {name}!\"\nNow we can do this:\noutput = hello(\"new programmers\")\nprint(output.title())\n# Same as: print(hello(\"new programmers\").title())\nAnd this produces:\n'Hello New Programmers!'"
  },
  {
    "objectID": "lectures/3.4-Functions.html#writing-a-function",
    "href": "lectures/3.4-Functions.html#writing-a-function",
    "title": "Functions",
    "section": "Writing a Function",
    "text": "Writing a Function\ndef &lt;function_name&gt;(&lt;var_name&gt;: &lt;var_type&gt;) -&gt; &lt;var_type&gt;:\n  ...\n  return &lt;var&gt;\nThis can also be written:\ndef &lt;function_name&gt;(&lt;var_name&gt;):\n  ...\n  return &lt;var&gt;\nPython is ‘friendly’ in the sense that all of the &lt;var_type&gt; information is optional, but it will help you (and Python) to know what you were expecting to see happen."
  },
  {
    "objectID": "lectures/3.4-Functions.html#complicating-things",
    "href": "lectures/3.4-Functions.html#complicating-things",
    "title": "Functions",
    "section": "Complicating Things…",
    "text": "Complicating Things…\nds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz': [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ndef get_city_info(city:str, field:str, city_lookup:str='name', data:dict=ds2) -&gt; str:\n  return str(data[field][ data[city_lookup].index(city) ])\n\ncity = 'New York'\nprint(f\"The latitude of {city} is {get_city_info(city,'lat')}\")\n# The latitude of New York is 40.71"
  },
  {
    "objectID": "lectures/3.4-Functions.html#resources",
    "href": "lectures/3.4-Functions.html#resources",
    "title": "Functions",
    "section": "Resources",
    "text": "Resources\n\n\n\nWhat is a function?\nPython functions\nBuilt-in functions\nDefine your own functions\nTypes of functions\nDefining a function\n\n\n\nFunction arguments\nArgument lists\nKeyword arguments\nReturn values\nDecorators\nVariable Scopes\nRobust Python with Type Hints"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-sense-of-this",
    "href": "lectures/3.2-LOLs.html#making-sense-of-this",
    "title": "LoLs",
    "section": "Making Sense of This",
    "text": "Making Sense of This\nWe can ‘unpack’ my_list in stages in order to make sense of it:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nfor i in my_list:\n  print(i)\nWhat do you think this will print?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "href": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "title": "LoLs",
    "section": "Debugging Our Thinking",
    "text": "Debugging Our Thinking\nLet’s make it a little more obvious:\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [7, 8, 9]\n\nmy_list = [a, b, c]\n\nfor i in my_list:\n  print(i) # Prints a, b, c in turn..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#the-next-step",
    "href": "lectures/3.2-LOLs.html#the-next-step",
    "title": "LoLs",
    "section": "The Next Step",
    "text": "The Next Step\nWe could then try this:\nfor i in my_list:\n  print(f\" &gt;&gt; {i}\")\n  for j in i: # Remember that i is a list!\n    print(j)\nThis produces:\n &gt;&gt; [1, 2, 3]\n1\n2\n3\n &gt;&gt; [4, 5, 6]\n4\n..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#putting-it-together",
    "href": "lectures/3.2-LOLs.html#putting-it-together",
    "title": "LoLs",
    "section": "Putting It Together",
    "text": "Putting It Together\nSome observations:\n\nWe can access i in my_list using either for i in my_list (every element in turn) or my_list[i] (one element only).\nWe can access j in list i using for j in i (every element in turn) or i[j] (one element only).\n\nDoes that mean we can also do this:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\ni,j = 0,1\nprint(my_list[i][j])"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "href": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "title": "LoLs",
    "section": "Let’s Talk It Out!",
    "text": "Let’s Talk It Out!\nSo if we write:\ni,j = 0,1\nprint(my_list[i][j])\nThen:\n\nmy_list[i] returns [1,2,3] (because i==0 and the first list is [1,2,3]), and\nmy_list[i][j] returns 2 (because j==1 and the [1,2,3][1]==2).\n\nSimilarly, my_list[2] grabs the third list ([7,8,9]) and then my_list[2][2] tells Python to get the third item in that third list (i.e. 9).\n\nHow you print the number 5 from this list-of-lists?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-this-useful",
    "href": "lectures/3.2-LOLs.html#making-this-useful",
    "title": "LoLs",
    "section": "Making This Useful",
    "text": "Making This Useful\nIf I rewrite the list this way perhaps it looks a little more useful?\nmy_cities = [\n  ['London', 51.5072, 0.1275, +0], \n  ['New York', 40.7127, 74.0059, -5], \n  ['Tokyo', 35.6833, 139.6833, +8]\n]\nNow we have something that is starting to look like data!"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "href": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "title": "LoLs",
    "section": "Down the Rabbit Hole",
    "text": "Down the Rabbit Hole"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lols-of-lols",
    "href": "lectures/3.2-LOLs.html#lols-of-lols",
    "title": "LoLs",
    "section": "LOLs of LOLs",
    "text": "LOLs of LOLs\nThis is also a legitimate list in Python.\nmy_cities = [\n  ['London', [51.5072, 0.1275], +0], \n  ['New York', [40.7127, 74.0059], -5], \n  ['Tokyo', [35.6833, 139.6833], +8]\n]\nprint(my_cities[0][0])\n&gt; London\nprint(my_cities[0][1][0])\n&gt; 51.5072\n\nWhy might it be a better choice of data structure than the earlier version?"
  },
  {
    "objectID": "lectures/2.7-Git.html#how-it-works",
    "href": "lectures/2.7-Git.html#how-it-works",
    "title": "Getting to Grips with Git",
    "section": "How It Works",
    "text": "How It Works\nThe natural way normal people think about managing versions of a document is to save a copy with a new name that somehow shows which version is most recent.\nThe natural way developers used to think about managing versions of a document is to have a master copy somewhere. Everyone asks the server for the master copy, makes some changes, and then checks those changes back in.\nThis is not how Git works.\n\nThe way normal people approach this problem assumes that, usually, only one or two people are making changes. But how do you coordinate with 20 other people to find out who has the most recent copy then collect all 21 people’s changes?\nThe way developers used to approach this problem assumes that someone is in final charge. That a company or organisation runs a server which will decide whose changes are allowed, and whose are not."
  },
  {
    "objectID": "lectures/2.7-Git.html#how-git-works",
    "href": "lectures/2.7-Git.html#how-git-works",
    "title": "Getting to Grips with Git",
    "section": "How Git Works",
    "text": "How Git Works\nGit is distributed, meaning that every computer where git is installed has its own master copy.\nSo every computer has a full history of any git project (aka. repository or ‘repo’). Indeed, you don’t have to synchronise your repo with any other computer or server at all! 1\n\nIn order to make this useful, you need ways to synchronise changes between computers that all think they’re right.\n\nI’d suggest that this is leaving the benefit of free backups on the table for no good reason!"
  },
  {
    "objectID": "lectures/2.7-Git.html#github",
    "href": "lectures/2.7-Git.html#github",
    "title": "Getting to Grips with Git",
    "section": "GitHub",
    "text": "GitHub\nGitHub is nothing special to Git, just another Git server with which to negotiate changes. Do not think of GitHub as the ‘master’ copy. There isn’t one.\nThere are, however, upstream and remote repositories.\n\nAn ‘upstream’ repository is where there’s a ‘gatekeeper’: e.g. the people who run PySAL have a repo that is considered the ‘gatekeeper’ for PySAL.\nA remote repository is any repository with which your copy synchronises. So the remote repository can be ‘upstream’ or it can just be another computer you run, or you GitHub account."
  },
  {
    "objectID": "lectures/2.7-Git.html#getting-started",
    "href": "lectures/2.7-Git.html#getting-started",
    "title": "Getting to Grips with Git",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\nTerm\nMeans\n\n\n\n\nRepository (Repo)\nA project or achive stored in Git.\n\n\ninit\nTo create a new repo on your computer.\n\n\nclone\nTo make a full copy of a repo somewhere else.\n\n\n\nThis creates a local repo that is unsynchronised with anything else:\nmkdir test\ncd test\ngit init\nWhereas this creates a local clone that is fully synchronised with GitHub:\ncd .. # To move out of 'test'\ngit clone https://github.com/jreades/fsds.git"
  },
  {
    "objectID": "lectures/2.7-Git.html#working-on-a-file",
    "href": "lectures/2.7-Git.html#working-on-a-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a File",
    "text": "Working on a File\n\n\n\nTerm\nMeans\n\n\n\n\nadd\nAdd a file to a repo.\n\n\nmv\nMove/Rename a file in a repo.\n\n\nrm\nRemove a file from a repo.\n\n\n\nFor example:\ncd test # Back into the new Repo\ntouch README.md # Create empty file called README.md\ngit add README.md # Add it to the repository\ngit mv README.md fileA.md # Rename it (move it)\ngit rm fileA.md # Remove it... which is an Error!\nThis produces:\nerror: the following file has changes staged in the index:\n    fileA.md\n(use --cached to keep the file, or -f to force removal)\n\nThis is telling you that you can force remove (git rm -f fileA.md) if you really want, but you’d probably be better off commiting the changes that have been ‘staged’… more on this in a second!\nAlso: no one else knows about these changes yet!"
  },
  {
    "objectID": "lectures/2.7-Git.html#looking-at-the-history",
    "href": "lectures/2.7-Git.html#looking-at-the-history",
    "title": "Getting to Grips with Git",
    "section": "Looking at the History",
    "text": "Looking at the History\n\n\n\nTerm\nMeans\n\n\n\n\ndiff\nShow changes between commits.\n\n\nstatus\nShow status of files in repo.\n\n\nlog\nShow history of commits.\n\n\n\nFor example:\ncd ../test/ # In case you weren't already there\ngit status  # What's the status\nThis produces:\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   fileA.md\n\nSo again, git is giving us hints as to the options: ‘changes to be committed’ vs. ‘unstage’ the changes. We can also see what files are to be committed (i.e. have changed)."
  },
  {
    "objectID": "lectures/2.7-Git.html#working-on-a-project-or-file",
    "href": "lectures/2.7-Git.html#working-on-a-project-or-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a Project or File",
    "text": "Working on a Project or File\n\n\n\nTerm\nMeans\n\n\n\n\ncommit\nTo record changes to the repo.\n\n\nbranch\nCreate or delete branches.\n\n\ncheckout\nJump to a different branch.\n\n\n\nFor example:\ngit commit -m \"Added and then renamed the README.\"\ngit status\nYou should see:\n[master (root-commit) e7a0b25] Added and then renamed the README Markdown file.\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 fileA.md\n# ... and then this:\nOn branch master\nnothing to commit, working tree clean\nMake a note of the number after ‘root-commit’!"
  },
  {
    "objectID": "lectures/2.7-Git.html#recovery",
    "href": "lectures/2.7-Git.html#recovery",
    "title": "Getting to Grips with Git",
    "section": "Recovery",
    "text": "Recovery\ngit rm fileA.md\ngit status\ngit commit -m \"Removed file.\"\nls \ngit checkout &lt;number you wrote down earlier&gt;\nls \n\nSo every operation on a file is recorded in the repository: adding, renaming, deleting, and so on. And we can roll back any change at any time. For plain-text files (such as Markdown, Python and R scripts) these changes are recorded at the level of each line of code: so you can jump around through your entire history of a project and trace exactly when and what changes you (or anyone else) made."
  },
  {
    "objectID": "lectures/2.7-Git.html#collaborating-on-a-project",
    "href": "lectures/2.7-Git.html#collaborating-on-a-project",
    "title": "Getting to Grips with Git",
    "section": "Collaborating on a Project",
    "text": "Collaborating on a Project\n\n\n\nTerm\nMeans\n\n\n\n\npull\nTo request changes on a repo from another computer.\n\n\npush\nTo send changes on a repo to another computer.\n\n\n\nFor example:\ngit push"
  },
  {
    "objectID": "lectures/2.7-Git.html#this-is-not-easy",
    "href": "lectures/2.7-Git.html#this-is-not-easy",
    "title": "Getting to Grips with Git",
    "section": "This is not easy1",
    "text": "This is not easy1\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/2.7-Git.html#a-dropbox-analogy",
    "href": "lectures/2.7-Git.html#a-dropbox-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Dropbox Analogy",
    "text": "A Dropbox Analogy\n\nThink of JupyterLab as being like Word or Excel: an application that allows you to read/write/edit notebook files.\nThink of GitHub as being like Dropbox: a place somewhere in the cloud that files on your home machine can be backed up.\n\nBut Dropbox doesn’t have the .gitignore file!\n\nSo why use Git? It gives you a full history of everything for as far back as the project goes and much finer-grained control over files and syncrhonisation than Dropbox. If you don’t add a file to git it can live quite happily in your git repository but will never synchronise.\nLike Dropbox, GitHub offers a lot of ‘value added’ featuers (like simple text editing) on top of the basic service of ‘storing files’.\nDropbox will automatically back up anything that you put in your special Dropbox folder. Git will only back up the things that you tell it to back up, even if they are in the Git folder!"
  },
  {
    "objectID": "lectures/2.7-Git.html#a-note-on-workflow",
    "href": "lectures/2.7-Git.html#a-note-on-workflow",
    "title": "Getting to Grips with Git",
    "section": "A Note on Workflow",
    "text": "A Note on Workflow\nSo your workflow should be:\n\nSave edits to Jupyter notebook.\nRun git add &lt;filename.ipynb&gt; to record changes to the notebook (obviously replace &lt;filename.ipynb&gt; completely with the notebook filename.\nRun git commit -m \"Adding notes based on lecture\" (or whatever message is appropriate: -m means ‘message’).\nThen run git push to push the changes to GitHub.\n\nIf any of those commands indicate that there are no changes being recorded/pushed then it might be that you’re not editing the file that you think you are (this happens to me!).\nOn the GitHub web site you may need to force reload the view of the repository: Shift + Reload button usually does it in most browsers. You may also need to wait 5 to 10 seconds for the changes to become ‘visible’ before reloading. It’s not quite instantaeous."
  },
  {
    "objectID": "lectures/2.7-Git.html#resources",
    "href": "lectures/2.7-Git.html#resources",
    "title": "Getting to Grips with Git",
    "section": "Resources",
    "text": "Resources\n\nUnderstanding Git (Part 1) – Explain it Like I’m Five\nTrying Git\nVisualising Git\nGit Novice\nGit Cheat Sheet: Commands and Best Practices\nAndy’s R-focussed Tutorial\n\n\nI now have everything in Git repos: articles, research, presentations, modules… the uses are basically endless once you start using Markdown heavily (even if you don’t do much coding)."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#iteration",
    "href": "lectures/2.5-Iteration.html#iteration",
    "title": "Iteration",
    "section": "it·er·a·tion",
    "text": "it·er·a·tion\n/itə’rāSHən/\nNoun\nThe repetition of a process or utterance.\n\nrepetition of a mathematical or computational procedure applied to the result of a previous application, typically as a means of obtaining successively closer approximations to the solution of a problem.\na new version of a piece of computer hardware or software. plural noun: iterations\n\n\nMany programmers also call these loops."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#two-types-of-iteration",
    "href": "lectures/2.5-Iteration.html#two-types-of-iteration",
    "title": "Iteration",
    "section": "Two Types of Iteration",
    "text": "Two Types of Iteration\n\n\n‘For’ loops\n\nUsed with finite lists of definite length\nFor each item in this list do something…\n\n\n‘While’ loops:\n\nUsed with unknown or non-finite lists\nWhile a condition is still True, do something to the list…"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#making-the-difference-memorable",
    "href": "lectures/2.5-Iteration.html#making-the-difference-memorable",
    "title": "Iteration",
    "section": "Making the Difference Memorable",
    "text": "Making the Difference Memorable"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#for-loops-1",
    "href": "lectures/2.5-Iteration.html#for-loops-1",
    "title": "Iteration",
    "section": "For Loops",
    "text": "For Loops\nThis ‘simple’ loop allows us to print out every element of the list in turn:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\nNotice the format:\nfor x in list:\n  # ...do something using the current value of x...\n\nNotice that this is the same in that we saw with if 'Batty' in geographers!"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#while-loops-1",
    "href": "lectures/2.5-Iteration.html#while-loops-1",
    "title": "Iteration",
    "section": "While Loops",
    "text": "While Loops\nThis loop does the same thing, but differently:\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )\n  g += 1\nNotice the format:\nwhile &lt;some condition is true&gt;:\n  # ...do something..."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#nesting-loops",
    "href": "lectures/2.5-Iteration.html#nesting-loops",
    "title": "Iteration",
    "section": "Nesting Loops",
    "text": "Nesting Loops\nWe can use one loop ‘inside’ another loop! What do you think this might print?\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  for h in g:\n    print(h)\nHuh??? Let’s puzzle this out…"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#debugging",
    "href": "lectures/2.5-Iteration.html#debugging",
    "title": "Iteration",
    "section": "Debugging",
    "text": "Debugging\nWhen you see something completely new, it’s often good to:\n\nAdd print(...) statements to see how the values of a variable are changing.\nComment out the parts you don’t understand so that you can focus on the parts you do\nThen iteratively add complexity!"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#step-1-the-outer-loop",
    "href": "lectures/2.5-Iteration.html#step-1-the-outer-loop",
    "title": "Iteration",
    "section": "Step 1: The ‘Outer Loop’",
    "text": "Step 1: The ‘Outer Loop’\nSo I would start off with:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\n#   for h in g:\n#   print(h)\nThis prints:\n'Rose'\n'Massey'\n'Jefferson'"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#step-1-summing-up",
    "href": "lectures/2.5-Iteration.html#step-1-summing-up",
    "title": "Iteration",
    "section": "Step 1: Summing Up",
    "text": "Step 1: Summing Up\nOK, so now we know:\n\nThat g is the name of a geographer.\nThe ‘outer’ loop sets g to the name of a new geographer on each iteration.\nSo if g is set to 'Rose' what does for h in g: do?"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#step-2-the-inner-loop",
    "href": "lectures/2.5-Iteration.html#step-2-the-inner-loop",
    "title": "Iteration",
    "section": "Step 2: The ‘Inner’ Loop",
    "text": "Step 2: The ‘Inner’ Loop\nWe know change it like this:\nfor g in geographers:\n  print(g)\n  for h in g:\n    print(h)\n  break # &lt;-- Notice this!\nThis prints:\nRose\nR\no\ns\ne"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#step-2-summing-up",
    "href": "lectures/2.5-Iteration.html#step-2-summing-up",
    "title": "Iteration",
    "section": "Step 2: Summing Up",
    "text": "Step 2: Summing Up\nAnd now we know that:\n\nh is looping over the string, meaning that a string can be treated as a list!\nAnd break is a really useful way to control a loop while you’re testing your code!"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#recap",
    "href": "lectures/2.5-Iteration.html#recap",
    "title": "Iteration",
    "section": "Recap",
    "text": "Recap\n\nfor iterates once over a collection items (e.g. a list).\nwhile keeps going until a condition is False."
  },
  {
    "objectID": "lectures/2.5-Iteration.html#test-yourself",
    "href": "lectures/2.5-Iteration.html#test-yourself",
    "title": "Iteration",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat will this code print? [I’d suggest that you don’t run it!]\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#test-yourself-tricksy-version",
    "href": "lectures/2.5-Iteration.html#test-yourself-tricksy-version",
    "title": "Iteration",
    "section": "Test Yourself (Tricksy Version)",
    "text": "Test Yourself (Tricksy Version)\nHere’s a really tricky one! The following two blocks of code produce the same output, how are they different?\ngeographers = ['Rose','Massey','Jefferson']\ngeographers.reverse()\nfor g in geographers:\n  print(g)\nAnd:\ngeographers = ['Rose','Massey','Jefferson']\ng = len(geographers)-1\nwhile g &gt;= 0:\n  print( geographers[g] )\n  g -= 1"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#one-more-thing",
    "href": "lectures/2.5-Iteration.html#one-more-thing",
    "title": "Iteration",
    "section": "One More Thing…",
    "text": "One More Thing…\nLet’s go back to the Lists examples for a second:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nHave a think about how this code works:\nfor ag in all_geographers:\n  for g in ag:\n    print(g)"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#a-debuggin-manifesto",
    "href": "lectures/2.5-Iteration.html#a-debuggin-manifesto",
    "title": "Iteration",
    "section": "A Debuggin Manifesto!",
    "text": "A Debuggin Manifesto!\n)"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#resources",
    "href": "lectures/2.5-Iteration.html#resources",
    "title": "Iteration",
    "section": "Resources",
    "text": "Resources\n\nWhat is Iteration?\nLoops\nFor Loop\nWhile Loop\n\nWe don’t cover the concept of recursion, but it’s quite a powerful idea and links nicely with Iteration:\n\nWhat is a recursive function?\nDefine recursive functions"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#variables-have-types",
    "href": "lectures/2.3-Python_the_Basics.html#variables-have-types",
    "title": "Python, the Basics",
    "section": "Variables Have Types…",
    "text": "Variables Have Types…\n\n\n\nName\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#but-we-can-change-that",
    "href": "lectures/2.3-Python_the_Basics.html#but-we-can-change-that",
    "title": "Python, the Basics",
    "section": "… But We Can Change That",
    "text": "… But We Can Change That\nMessage starts as a string:\nmsg = '42'\ntype(msg) # str\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg) # change to int\nAnd back to a string:\nmsg = str(msg)\ntype(msg) # back to str\nAnd notice:\nprint(str(int('42'))) # string from int from string\n\nThat last line of code is intended to start familiarising you with Python syntax: programmers rarely do one operation per line of code if they can do more than one, so you’ll often see nested parentheses like this and you need to learn how to read this kind of code starting from the inner-most parentheses (the int()) and working outwards from there to str() and finally print()."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#and-they-are-all-objects",
    "href": "lectures/2.3-Python_the_Basics.html#and-they-are-all-objects",
    "title": "Python, the Basics",
    "section": "…And They Are All Objects",
    "text": "…And They Are All Objects\nOne to remember for the session on objects and classes:\nisinstance(msg,object)     # True\nisinstance(answer,object)  # True\nisinstance(pi,object)      # True\nisinstance(c,object)       # True\nisinstance(correct,object) # True\n\nLike str(), int(), and print(), you see here another command — or ‘function’ in Python terminology — called ininstance. An ‘instance’ of something is just a way of asking ‘is it a type of’: so the first line asks if string msg is a kind of object, and the answer to that is: True. But it’s the same for answer (an integer), pi (a floating point number), and so on. They are all a kind of object, and we’ll see why that is a useful answer in a few weeks’ time."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#variables-have-names",
    "href": "lectures/2.3-Python_the_Basics.html#variables-have-names",
    "title": "Python, the Basics",
    "section": "Variables Have Names…",
    "text": "Variables Have Names…\nRules for variable names:\n\nThey cannot start with a number: item3 is valid, 3items is not.\nWhite space and symbols are not allowed, but _ is allowed: my_variable is valid, my-variable, my$variable, and my variable are not.\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used; but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#but-not-all-words-are-allowed",
    "href": "lectures/2.3-Python_the_Basics.html#but-not-all-words-are-allowed",
    "title": "Python, the Basics",
    "section": "…But Not All Words Are Allowed",
    "text": "…But Not All Words Are Allowed\nDo not try to use any of these as variable names. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#simple-operations-on-variables",
    "href": "lectures/2.3-Python_the_Basics.html#simple-operations-on-variables",
    "title": "Python, the Basics",
    "section": "Simple Operations on Variables",
    "text": "Simple Operations on Variables\nLet’s start with x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n15\n\n\nDifference\nx - y\n5\n\n\nProduct\nx * y\n50\n\n\nQuotient\nx / y\n2.0\n\n\n‘Floored’ Quotient\nx // y\n2\n\n\nRemainder\nx % y\n0\n\n\nPower\npow(x,y) or x**y\n100000\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#strings-are-different",
    "href": "lectures/2.3-Python_the_Basics.html#strings-are-different",
    "title": "Python, the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nWhen you do things with strings the answers can look a little different. Let’s start with x=\"Hello\" and y=\"You\" and z=2…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n'HelloYou'\n\n\nDifference\nx - y\nTypeError\n\n\nProduct\nx * z\nHelloHello\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#using-strings-to-output-information",
    "href": "lectures/2.3-Python_the_Basics.html#using-strings-to-output-information",
    "title": "Python, the Basics",
    "section": "Using Strings to Output Information",
    "text": "Using Strings to Output Information\nPython has no fewer than three ways output information:\n\nstring concatenation using +;\nstring formatting using &lt;str&gt;.format(&lt;variables&gt;); and\nf-strings using f\"{variable_1} some text {variable_n}\".\n\nThere are pros and cons to each:\nx = 24\ny = 'Something'\nprint(\"The value of \" + y + \" is \" + str(x))\nprint(\"The value of {0} is {1}\".format(y, x))\nprint(f\"The value of {y} is {x}\")\n\nObviously, a really common requirement that programmers have is ‘output a nicely formatted string containing information about the variables in my program’.\nI rather like f-strings because they can actually contain any code you like (you could, for instance, write `f”The square root of {y} is {x**(1/2)}” and it would work. However, concatenation is the easiest to learn."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#operators-assemble",
    "href": "lectures/2.3-Python_the_Basics.html#operators-assemble",
    "title": "Python, the Basics",
    "section": "Operators Assemble!",
    "text": "Operators Assemble!\nAlways pay attention to precedence:\nx, y = 10, 5\nx + y * 2         # == 20\n(x + y) * 2       # == 30\nx + y * 2 / 3     # == 13.3333333333334\nx + y * (2/3)     # == 13.3333333333332\n(x + y) * (2/3)   # == 10.0\nAnd here’s a subtle one:\n(x * y) ** 2/3    # == 833.333333333334\n(x * y) ** (2/3)  # == 13.5720880829745\nThe full list is here.\n\nIf you’re a little rusty on exponents, that last example is the cube root of (x*y)**2. But the key point is that formatting is not what matters here, it’s operator precedence: one has parentheses, the other does not, so they are evaluated differently. To make the first one more clear we might use (x * y)**2 / 3 or even ((x*y)**2)/3.\nAlso notice that with the two floats in the first block you do not always get the same result from operations that should give the same answer. Non-terminating decimals (e.g. 1/3) will always be rounded, unpredictably, by the computer because it doesn’t have infinite memory. The process of rounding means that you need to be very careful comparing floats (more on this later)."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#comparing-variables",
    "href": "lectures/2.3-Python_the_Basics.html#comparing-variables",
    "title": "Python, the Basics",
    "section": "Comparing Variables",
    "text": "Comparing Variables\nFor numeric variables comparisons are easy.\n\n\n\nOperator\nInput\nResult\n\n\n\n\n==\n10 == 5\nFalse\n\n\n!=\n10 != 5\nTrue\n\n\n&lt;, &lt;=\n10 &lt; 5\nFalse\n\n\n&gt;, &gt;=\n10 &gt; 5\nTrue\n\n\n\n\nThe result of any (successful) comparison is a Boolean (True/False). We can save the output of this comparison to a new variable (e.g. z = x &gt; y).\nThis last example has to do with the way that strings are compared."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#strings-are-different-1",
    "href": "lectures/2.3-Python_the_Basics.html#strings-are-different-1",
    "title": "Python, the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nBut notice:\nw, x, y, z = '4a','4a','365','42'\nw == x  # True\nw != y  # True\nx &gt; y   # True\nx &gt; z   # True\nWhy is 4a greater than both 365 and 42?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#danger-will-robinson",
    "href": "lectures/2.3-Python_the_Basics.html#danger-will-robinson",
    "title": "Python, the Basics",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nNotice the very subtle visual difference between = and ==!\nx = 5\ny = 10\nx = y   # Probably a mistake: setting x to the value of y\nx == y  # True, because x and z are now both set to 10\nRemember this!\n\nConfusing these two operators is the most common source of mistakes early on when learning to code in Python! One (=) does assignment, the other (==) does comparison."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#common-mistakes",
    "href": "lectures/2.3-Python_the_Basics.html#common-mistakes",
    "title": "Python, the Basics",
    "section": "Common Mistakes",
    "text": "Common Mistakes\nHere’s the output from some attempts at comparison:\nx, y = '42', 42\nx==y   # False\nx&gt;y    # An error!\nThis last line produces:\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: '&gt;' not supported between instances of 'str' and 'int'\nIf we want to compare them then we’ll need to change their type:\nx &gt; str(y)   # False\nint(x) &lt;= y  # True\nx &gt; = str(y) # Also True\n\nA really common mistake is to think that string (str) \"42\" is the same as the integer (int) 42.\nNotice that in the first example we can say that 42 is clearly not the same as ‘42’, but we can’t say whether it’s more or less because that’s non-sensical in this context. So this is a computer being totally logical but not always sensible.\nAlso notice the syntax for this: we have str(&lt;something&gt;) and int(&lt;something&gt;) to convert between types. These are functions, which we’ll spend a lot more time on next week!\nWhy might it be (fractionally) faster to compare integers than strings?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#conditions-consequences",
    "href": "lectures/2.3-Python_the_Basics.html#conditions-consequences",
    "title": "Python, the Basics",
    "section": "Conditions & Consequences",
    "text": "Conditions & Consequences\nThe simplest condition only considers one outcome:\nif &lt;condition is true&gt;:\n    #...do something...\nBut you’ll often needs something a little more sophisticated:\nif &lt;condition is true&gt;:\n    #...do something...\nelif &lt;some other condition is true&gt;:\n    #...do something else...\nelse:\n    #...if no conditions are true...\n\n#...code continues...\n\nBut no matter how complex, conditions always ultimately evaluate to True or False."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#for-example",
    "href": "lectures/2.3-Python_the_Basics.html#for-example",
    "title": "Python, the Basics",
    "section": "For Example",
    "text": "For Example\nif x &lt; y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x &lt; y:\n  print(\"x is less than y\")\nelif x &gt; y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#conditional-syntax",
    "href": "lectures/2.3-Python_the_Basics.html#conditional-syntax",
    "title": "Python, the Basics",
    "section": "Conditional Syntax",
    "text": "Conditional Syntax\nThe most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#all-of-them-together-input",
    "href": "lectures/2.3-Python_the_Basics.html#all-of-them-together-input",
    "title": "Python, the Basics",
    "section": "All of Them Together (Input)!",
    "text": "All of Them Together (Input)!\nif hours &gt;= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#all-of-them-together-output",
    "href": "lectures/2.3-Python_the_Basics.html#all-of-them-together-output",
    "title": "Python, the Basics",
    "section": "All of Them Together (Output)!",
    "text": "All of Them Together (Output)!\nOutput from the Python interpreter:\n&gt;&gt;&gt; if hours &gt;= 0:\n... print(\"Hours were worked.\")\n  File \"&lt;stdin&gt;\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n&gt;&gt;&gt; else\n  File \"&lt;stdin&gt;\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt;     print \"No hours were worked.\")\n  File \"&lt;stdin&gt;\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#thats-better",
    "href": "lectures/2.3-Python_the_Basics.html#thats-better",
    "title": "Python, the Basics",
    "section": "That’s Better!",
    "text": "That’s Better!\nIt’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours &gt; 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#make-your-life-easy-well-easier",
    "href": "lectures/2.3-Python_the_Basics.html#make-your-life-easy-well-easier",
    "title": "Python, the Basics",
    "section": "Make Your Life Easy (Well, Easier)",
    "text": "Make Your Life Easy (Well, Easier)\nAlways comment your code:\n\nSo that you know what is going on.\nSo that you know why it is going on.\nSo that others can read your code.\nTo help you plan your code\n\n\nYou are reminding your future self what your code was for and helping to give it structure (explaining==thinking!)."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#different-comment-styles",
    "href": "lectures/2.3-Python_the_Basics.html#different-comment-styles",
    "title": "Python, the Basics",
    "section": "Different Comment Styles",
    "text": "Different Comment Styles\n# This is a short comment\nprint(\"Foo\")\nprint(\"Bar\") # Also a short comment\n\n# ------- New Section --------\n# You can have comments span multiple\n# lines just by adding more '#' at the \n# start of the line.\n\n# You can keep code from running\n# print(\"Baz\")"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#comments-follow-indentation",
    "href": "lectures/2.3-Python_the_Basics.html#comments-follow-indentation",
    "title": "Python, the Basics",
    "section": "Comments Follow Indentation",
    "text": "Comments Follow Indentation\n# Function for processing occupational data\n# from the 2001 and 2011 Censuses.\ndef occ_data(df):\n  #  Columns of interest\n  cols = ['Managerial','Professional','Technical']\n    \n  # Integrate results into single dataset -- \n  # right now we don't replicate Jordan's approach of\n  # grouping them into 'knowledge worker' and 'other'. \n  for i in df.iterrows():\n    # For each column...\n    for j in cols:\n      # Do something\n      ..."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#easier-multi-line-comments",
    "href": "lectures/2.3-Python_the_Basics.html#easier-multi-line-comments",
    "title": "Python, the Basics",
    "section": "Easier Multi-Line Comments",
    "text": "Easier Multi-Line Comments\nThe below are not real comments, but they can help when you have a really long comment that you want to make. They are also used to help explain what a function does (called a docstring).\n\"\"\"\nSo I was thinking that what we need here is \na way to handle the case where the data is\nincomplete or contains an observation that we\nweren't expecting (e.g. \"N/A\" instead of \"0\").\n\"\"\""
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#tips",
    "href": "lectures/2.3-Python_the_Basics.html#tips",
    "title": "Python, the Basics",
    "section": "Tips",
    "text": "Tips\nSome useful tips for commenting your code:\n\nInclude general information at the top of your programming file.\nAssume the person reading the code is a coder themselves.\nGood commenting is sparse in the sense that it is used judiciously, and concise without being gnomic.\nUse comments to track the logic of your code (especially in conditionals and loops)"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics.html#more-resources",
    "href": "lectures/2.3-Python_the_Basics.html#more-resources",
    "title": "Python, the Basics",
    "section": "More Resources",
    "text": "More Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\n\n\nTypes of Data\nVariables and expressions\nStrings\nThe string type\nCommon string methods\nFormatting strings\nSplitting and joining\nNumeric types\nThe bool type\nStoring Data in Variables\n\n\n\nConditional structures\nIf Statements\nIf-Else Statements\nIf-Elif\nWhitespace and comments\nUsing print()\nConditional syntax\nConditional operators\nConditional assignment"
  },
  {
    "objectID": "lectures/10.1-Classification.html#spot-the-difference",
    "href": "lectures/10.1-Classification.html#spot-the-difference",
    "title": "Classification",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nOn Maps\n\nGroup observations by ‘class’.\nTypically based on 1-D distribution.\nClasses are assigned by user choice.\n\n\nOn Labels\n\nLabel observations by ‘class’.\nTypically based on model outputs.\nLabels are assigned by user feedback.\n\n\n\n\nIn this session we are primarily concerned with the first column — classification as a modelling process is better considered a data science/modelling problem that is beyond the scope of this module."
  },
  {
    "objectID": "lectures/10.1-Classification.html#map-classification-choices",
    "href": "lectures/10.1-Classification.html#map-classification-choices",
    "title": "Classification",
    "section": "Map Classification Choices",
    "text": "Map Classification Choices\n\nAssign classes manually.\nSplit range evenly.\nSplit data evenly\nSplit data according to distribution\nSplit data according to their similarity to each other.\n\n\n\nAccording to some logic/theory/regulatory or policy fact or objective.\nEqual intervals for cases without heavy skew\nQuantiles or HeadTailBreaks for cases with heavy skew\nSD for cases with normal distribution; BoxPlot for others.\nNatural breaks/FIsher Jenks for cases where distribution is discontinuous"
  },
  {
    "objectID": "lectures/10.1-Classification.html#mapclassify",
    "href": "lectures/10.1-Classification.html#mapclassify",
    "title": "Classification",
    "section": "Mapclassify",
    "text": "Mapclassify\nMapclassify (part of PySAL) provides a wide range of classifiers:\n\n\n\nNo Parameters\nk Parameter\n\n\n\n\nBoxPlot\nUserDefined\n\n\nStdMean\nPercentiles\n\n\nMaxP\nQuantiles\n\n\nHeadTailBreaks\nNatural Breaks\n\n\nEqualInterval\nMaximum Breaks\n\n\n\nJenksCaspall/Sampled/Forced\n\n\n\nFisherJenks/Sampled\n\n\n\nk will a user-specified number of classes or binning criterion."
  },
  {
    "objectID": "lectures/10.1-Classification.html#raw",
    "href": "lectures/10.1-Classification.html#raw",
    "title": "Classification",
    "section": "Raw",
    "text": "Raw"
  },
  {
    "objectID": "lectures/10.1-Classification.html#user-defined",
    "href": "lectures/10.1-Classification.html#user-defined",
    "title": "Classification",
    "section": "User Defined",
    "text": "User Defined\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, 125000.00]\n0\n\n\n( 125000.00, 250000.00]\n4\n\n\n( 250000.00, 925000.00]\n865\n\n\n( 925000.00, 1500000.00]\n85\n\n\n(1500000.00, 4500000.00]\n29"
  },
  {
    "objectID": "lectures/10.1-Classification.html#box-plot",
    "href": "lectures/10.1-Classification.html#box-plot",
    "title": "Classification",
    "section": "Box Plot",
    "text": "Box Plot\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -31429.25]\n0\n\n\n( -31429.25, 391267.00]\n246\n\n\n( 391267.00, 495010.00]\n246\n\n\n( 495010.00, 673064.50]\n245\n\n\n( 673064.50, 1095760.75]\n175\n\n\n(1095760.75, 4416659.00]\n70"
  },
  {
    "objectID": "lectures/10.1-Classification.html#standard-deviations",
    "href": "lectures/10.1-Classification.html#standard-deviations",
    "title": "Classification",
    "section": "Standard Deviations",
    "text": "Standard Deviations\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -171366.63]\n0\n\n\n(-171366.63, 216174.43]\n0\n\n\n( 216174.43, 991256.55]\n892\n\n\n( 991256.55, 1378797.61]\n53\n\n\n(1378797.61, 4416659.00]\n38"
  },
  {
    "objectID": "lectures/10.1-Classification.html#max-p",
    "href": "lectures/10.1-Classification.html#max-p",
    "title": "Classification",
    "section": "Max P",
    "text": "Max P\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346594.00]\n142\n\n\n( 346594.00, 461577.00]\n279\n\n\n( 461577.00, 529197.00]\n140\n\n\n( 529197.00, 530662.00]\n3\n\n\n( 530662.00, 613465.00]\n115\n\n\n( 613465.00, 842387.00]\n167\n\n\n( 842387.00, 4416659.00]\n137"
  },
  {
    "objectID": "lectures/10.1-Classification.html#head-tail-breaks",
    "href": "lectures/10.1-Classification.html#head-tail-breaks",
    "title": "Classification",
    "section": "Head Tail Breaks",
    "text": "Head Tail Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 603715.49]\n670\n\n\n( 603715.49, 976290.79]\n218\n\n\n( 976290.79, 1508985.73]\n66\n\n\n(1508985.73, 2257581.55]\n16\n\n\n(2257581.55, 2826007.08]\n9\n\n\n(2826007.08, 3553496.25]\n3\n\n\n(3553496.25, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/10.1-Classification.html#equal-interval",
    "href": "lectures/10.1-Classification.html#equal-interval",
    "title": "Classification",
    "section": "Equal Interval",
    "text": "Equal Interval\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 825125.00]\n842\n\n\n( 825125.00, 1423714.00]\n108\n\n\n(1423714.00, 2022303.00]\n17\n\n\n(2022303.00, 2620892.00]\n10\n\n\n(2620892.00, 3219481.00]\n4\n\n\n(3219481.00, 3818070.00]\n1\n\n\n(3818070.00, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/10.1-Classification.html#quantiles",
    "href": "lectures/10.1-Classification.html#quantiles",
    "title": "Classification",
    "section": "Quantiles",
    "text": "Quantiles\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346009.00]\n140\n\n\n( 346009.00, 405677.86]\n140\n\n\n( 405677.86, 461959.29]\n140\n\n\n( 461959.29, 529612.86]\n141\n\n\n( 529612.86, 639488.86]\n140\n\n\n( 639488.86, 827691.43]\n140\n\n\n( 827691.43, 4416659.00]\n141"
  },
  {
    "objectID": "lectures/10.1-Classification.html#natural-breaks",
    "href": "lectures/10.1-Classification.html#natural-breaks",
    "title": "Classification",
    "section": "Natural Breaks",
    "text": "Natural Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 433543.00]\n356\n\n\n( 433543.00, 605879.00]\n316\n\n\n( 605879.00, 842387.00]\n174\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/10.1-Classification.html#maximum-breaks",
    "href": "lectures/10.1-Classification.html#maximum-breaks",
    "title": "Classification",
    "section": "Maximum Breaks",
    "text": "Maximum Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 1688895.00]\n961\n\n\n(1688895.00, 1926265.50]\n4\n\n\n(1926265.50, 2278155.50]\n5\n\n\n(2278155.50, 2929865.50]\n9\n\n\n(2929865.50, 3349991.00]\n2\n\n\n(3349991.00, 3959682.50]\n1\n\n\n(3959682.50, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/10.1-Classification.html#fisher-jenks",
    "href": "lectures/10.1-Classification.html#fisher-jenks",
    "title": "Classification",
    "section": "Fisher Jenks",
    "text": "Fisher Jenks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 435961.00]\n363\n\n\n( 435961.00, 607480.00]\n310\n\n\n( 607480.00, 842387.00]\n173\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/10.1-Classification.html#jenks-caspall",
    "href": "lectures/10.1-Classification.html#jenks-caspall",
    "title": "Classification",
    "section": "Jenks Caspall",
    "text": "Jenks Caspall\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 365741.00]\n188\n\n\n( 365741.00, 441979.00]\n187\n\n\n( 441979.00, 520791.00]\n167\n\n\n( 520791.00, 638474.00]\n160\n\n\n( 638474.00, 890055.00]\n156\n\n\n( 890055.00, 1626454.00]\n103\n\n\n(1626454.00, 4416659.00]\n22"
  },
  {
    "objectID": "lectures/10.1-Classification.html#summary",
    "href": "lectures/10.1-Classification.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\n\nThe choice of classification scheme should be data- and distribution-led. This is simply a demonstration of how different schemes can shape your understanding of the data."
  },
  {
    "objectID": "lectures/10.1-Classification.html#code-useful-tips",
    "href": "lectures/10.1-Classification.html#code-useful-tips",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the classes:\nkl = 7\ncls = [mapclassify.BoxPlot, ...,  mapclassify.JenksCaspall]\nSetting up the loop:\nfor cl in cls:\n    try: \n        m = cl(ppd.Value, k=kl)\n    except TypeError:\n        m = cl(ppd.Value)\n    \n    f = plt.figure()\n    gs = f.add_gridspec(nrows=2, ncols=1, height_ratios=[1,4])\n\n    ax1 = f.add_subplot(gs[0,0])\n    ...\n\n    ax2 = f.add_subplot(gs[1,0])\n    ..."
  },
  {
    "objectID": "lectures/10.1-Classification.html#code-useful-tips-1",
    "href": "lectures/10.1-Classification.html#code-useful-tips-1",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the distribution:\n    ax1 = f.add_subplot(gs[0,0])\n    sns.kdeplot(ppd.Value, ax=ax1, color='r')\n    ax1.ticklabel_format(style='plain', axis='x') \n\n    y = ax1.get_ylim()[1]\n    for b in m.bins:\n        ax1.vlines(b, 0, y, linestyles='dotted')"
  },
  {
    "objectID": "lectures/10.1-Classification.html#code-useful-tips-2",
    "href": "lectures/10.1-Classification.html#code-useful-tips-2",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nAdjusting the legend text:\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k,v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\nSetting up the map:\n    ax2 = f.add_subplot(gs[1,0])\n    ppd.assign(cl=m.yb).plot(column='cl', k=len(m.bins), categorical=True, legend=True, ax=ax2)\n    \n    mapping = dict([(i,s) for i,s in enumerate(m.get_legend_classes())])\n    ax2.set_axis_off()\n    replace_legend_items(ax2.get_legend(), mapping)"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#principles",
    "href": "lectures/1.2-Tools_of_the_Trade.html#principles",
    "title": "Tools of the Trade",
    "section": "Principles",
    "text": "Principles\n\nSoftware should be free (as far as practicable).\nSoftware should be open (as far as practicable).\nSoftware should run on all platforms.\nSoftware should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#tools-to-make-your-life-easier",
    "href": "lectures/1.2-Tools_of_the_Trade.html#tools-to-make-your-life-easier",
    "title": "Tools of the Trade",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nDropbox: keep your stuff backed up in the cloud.\nSlack: get help (or just tips and tricks) from peers and staff\nDocker: virtualisation platforms to ensure you don’t ‘hose’ your computer.\nPython: how we do ‘data science’.\nGitHub: manage your code, your data, even your essays/reports.\nMarkdown: focus on the right things while you write and treat your essays like code!\nQuarto: convert Markdown+Python to pretty documents/web pages.\nLLMs: assistance in sketching out code snippets/validation."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#dropbox",
    "href": "lectures/1.2-Tools_of_the_Trade.html#dropbox",
    "title": "Tools of the Trade",
    "section": "Dropbox",
    "text": "Dropbox\n\nDropbox is a ‘cloud-based file synchronisation tool’: files placed in the special Dropbox folder are automatically uploaded to their servers, and automatically downloaded to any other computer on which you have set up Dropbox. Changes are also synchronised every time you save the file."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use Dropbox for four reasons:\n\nYou can access your Dropbox files anywhere in the world via the Desktop or Web.\nYou have an backup of all of your work, even if your computer has a complete meltdown.\nYou have limited ‘versioning’ support, so if you accidentally overwrite an essay or file, you can recover a previous version.\nDropbox is how we collaborate, and it’s how many businesses work as well."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nThat you keep all files that aren’t in GitHub in your Dropbox folder. This applies to all your CASA MSc work but could be especially useful for ensuring that data files used as part of your group work are readily accessible!\nDropbox signup: bit.ly/32jhdvN"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#slack",
    "href": "lectures/1.2-Tools_of_the_Trade.html#slack",
    "title": "Tools of the Trade",
    "section": "Slack",
    "text": "Slack\n\nSlack is a “messaging app for teams” that is designed to reduce email, organise conversations & topics of discussion, and pull in relevant data from a host of other services in a flexible, fully-searchable way."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-1",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-1",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use Slack for four reasons:\n\nMoodle is clunky and formal—it works well for one-to-many communication, but not so much for ‘chat’.\nSlack offers a searchable history—you will have access to this archive for as long as you need it.\nYou (and we) can access Slack on every major OS (OSX, Windows, iOS, Android, and Windows Phone) and via a browser quickly.\nSlack is used in the ‘real world’ by everyone from Apple to PayPal and the JPL. This is how developers work."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-1",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-1",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nInstall the Slack client on your phone and on your personal computer and start using it as the way to ask questions, share answers, and generally keep ‘up to date’ on things across the entire MSc.\nSlack signup: casa-students-2022.slack.com\nP.S. Unless a question is personal it should normally be asked in the appropriate module channel."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#docker",
    "href": "lectures/1.2-Tools_of_the_Trade.html#docker",
    "title": "Tools of the Trade",
    "section": "Docker",
    "text": "Docker\n\nDocker “makes development efficient and predictable” because it is “used through the development lifecycle for fast, easy and portable application development”."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-2",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-2",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nDocker is a ‘virtualisation platform’ that allows you to run a second (virtual) computer on your personal computer. We use it for four reasons:\n\nEasier installation than Anaconda Python and everyone has the same versions of every library.\nNo spillover effects since each container is isolated.\nEasy to tidy up when you’re done or add new containers when you start something new (e.g. PostgreSQL).\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-2",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-2",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Docker because configuring a development machine is hard, this makes it simple. If a Docker image works for us then we know 1 it works for you.\nDocker Desktop is your starting point.\nNot always true, alas."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#anaconda-python",
    "href": "lectures/1.2-Tools_of_the_Trade.html#anaconda-python",
    "title": "Tools of the Trade",
    "section": "Anaconda Python",
    "text": "Anaconda Python\n\nAnaconda Python is a ‘flavour’ of Python that comes packaged with useful tools for configuring and management. If virtualisation is too resource-intensive for your computer (e.g. because it’s older or doesn’t have enough RAM) then installing Python directly is the next-best option."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-3",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-3",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe use Anaconda Python for three reasons:\n\nIt is easy to create and configure virtual environments (each research project has its own environment).\nUse of channels allows installation of cutting-edge libraries not yet packaged for ‘regular’ Python (install from GitHub, etc.)\nWidely supported by developers with builds for most Operating Systems and a focus on data science applications.\n\nIt’s what we use on the Docker image as well."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-3",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-3",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nIf Docker doesn’t work on your computer, then this is how we will get you up and running because it’s (fairly) robust and ‘standard issue’. However, we can’t guarantee you’ll get the same versions of every package as installed on the virtualised systems so differences may emerge.\nYou’ll need to download the ‘Individual Edition’: 64-bit Graphical Installer 1\nUnless your computer is very, very old."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#recap",
    "href": "lectures/1.2-Tools_of_the_Trade.html#recap",
    "title": "Tools of the Trade",
    "section": "Recap",
    "text": "Recap\n\nWith Docker we have a way to create a coding environment that is isolated from the computer and highly portable across machiens.\nWith Dropbox we have a place to store, backup, and share files (size limits apply).\nWith Slack we have a place to ask for/provide help.\nWith LLMs we have a personal ‘tutor’ who can help us to learn more quickly and effectively.\n\nLet’s turn to the rest in part 2: Writing Code!"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#resources",
    "href": "lectures/1.2-Tools_of_the_Trade.html#resources",
    "title": "Tools of the Trade",
    "section": "Resources",
    "text": "Resources\n\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nProgram-Aided Language Models\nChain of Thought Prompting\nChatGPT is a blurry JPEG of the Internet 1\nWhy Meta’s latest large language model survived only three days online 2\nGit for Decolonisation3\n\nProbably the best ‘lay person’s’ explanation of how LLMs work/fall apart you’ll ever read.And this one was trained on scientific articles!Part art, part activism, part tech!"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "We all need help from time to time, and while we will always do our best to support you because we know that this module is hard for students who are new to programming, the best way to ‘get help’ will also always be taking steps to ‘help yourself’ first."
  },
  {
    "objectID": "help.html#how-to-help-yourself",
    "href": "help.html#how-to-help-yourself",
    "title": "Getting Help",
    "section": "How to Help Yourself",
    "text": "How to Help Yourself\nHere are at least six things that you can do to ‘help yourself’:\n\nUse the dedicated #fsds channel on Slack–this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours.\nDo the readings–regardless of whether we ask you questions in class about them (or not), the readings are designed to support the module’s learning outcomes, so if you are struggling with a concept or an idea then please look to the week’s readings! You should also review the full bibliography while developing your thinking for the final project.\nUse Google–this is one course where saying “I googled it…” will be taken as a good sign! Probalby the biggest difference between a good programmer and a new programmer is that the good one knows which terms to type into Google to get the answer that they need right away.\nUse Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the right answer right away, but whether you’re a beginner or an expert Stack Overflow is your friend.\nMake use of Drop-in Hours (see also below)–there is no extra credit for struggling in silence, and we can’t help you if we don’t know that you’re lost! That doesn’t mean that we can simply ‘give’ you the answers to challenging questions, but we will do everything that we can to support your learning. Many of the same advice applies here as on our other Getting Help page (with the Soft Skills).\nPractice like it’s a language–set yourself little problems or tasks and see if you can apply what you’ve learned in class to a problem in a different class, or a friend’s problem, or just something you’re curious about! In the same way that practicing your Chinese or French with native speakers will help you to learn those languages, so will practicing your Python.\nSign up for online classes–realistically, you will have a lot on your plate, but if you want or need more practice with Python and are strugging to come up with your own problems to work through then there is a wealth of options out there. You might also find that a different explanation or challenge resonates and gives you new insight into how to code.\n\nRemember: when you are learning to code there is no such thing as a stupid question. Sometimes students have lazy questions when they are frustrated and just want to know ‘the answer’, but anyone finding themselves stuck on a particular problem has a 100% chance that someone else in the class has the same problem as well but hasn’t quite worked up the courage to ask. So please: ask."
  },
  {
    "objectID": "help.html#you-can-book-me",
    "href": "help.html#you-can-book-me",
    "title": "Getting Help",
    "section": "You Can Book Me",
    "text": "You Can Book Me"
  },
  {
    "objectID": "assessments/models.html",
    "href": "assessments/models.html",
    "title": "Model Content",
    "section": "",
    "text": "Although the following examples are all much longer than permitted under the assessment format, they are exemplary in their communication of the data and key findings in a manner that is clear, straightforward, and well-illustrated:\nThe last of these is bit more ‘academic’ in tone but still intended to be very accessible to a lay-reader (i.e. non-expert)."
  },
  {
    "objectID": "assessments/models.html#models-for-the-policy-briefing",
    "href": "assessments/models.html#models-for-the-policy-briefing",
    "title": "Briefing Models & Topics",
    "section": "",
    "text": "Although the following examples are all much longer than permitted under the assessment format, they are exemplary in their communication of the data and key findings in a manner that is clear, straightforward, and well-illustrated:\n\n\n\n\n\n\nTip\n\n\n\nNotice how the format of these is broadly similar but differs from a traditional essay format. So instead of Introduction, Literature, etc. you will see Key Findings, potentially some Recommendations, and two or more sections or chapters in which the evidence is developed in parallel with the background material. This format provides for more flexibility in style and presentation, though you will note that they all refer to a mix of academic and grey literature as well!\n\n\n\nSmith, D.A. (2010), Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London, GLA Economics; URL.\nTravers, T. Sims, S. and Bosetti, N. (2016), Housing and Inequality in London, Centre for London; URL.\nBivens, J. (2019), The economic costs and benefits of Airbnb, Economic Policy Institute; URL.\nWachsmuth, D., Chaney, D., Kerrigan, D. Shillolo, A. and Basalaev-Binder, R. (2018), The High Cost of Short-Term Rentals in New York City, Urban Politics and Governance research group, McGill University; URL.\nChapple, K. (2009), Mapping Susceptibility to Gentrification: The Early Warning Toolkit, Centre for Community Innovation; URL\n\nThe last of these is bit more ‘academic’ in tone but still intended to be very accessible to a lay-reader (i.e. non-expert)."
  },
  {
    "objectID": "assessments/models.html#possible-topics",
    "href": "assessments/models.html#possible-topics",
    "title": "Model Content",
    "section": "Possible Topics",
    "text": "Possible Topics\nThe exact nature of the group’s response to the final questions in the assessment is up to you, but you should reference existing policies, where relevant, and feel free to make recommendations based on the analysis undertaken.\nBelow are some indicative topics and you should feel free to approach the lecturers if you wish to strike out because some other aspect of the question and data interest you:\n\nImpact of Airbnb on local area rental markets — this would require some assumptions about listings and lettings based on available data but as long as these are clearly stated this would be a strong approach; there are good examples of models used in other cities that it may be possible to draw on, or adapt to, London. You may want to consider things like the type of listing and the issues around the Short- and Long-Term Rental markets.\nImpact of Airbnb on London’s Tourism Economy — this would look at the distribution of London’s tourism venues and, possibly, hotels alongside Airbnb listings in order to evaluate the extent to which tourism ‘dollars’ might be spent in ways that positively impact less tourist-oriented areas if we assume (again, detail the assumptions) that some percentage of a tourist’s dollars are spent locally in an area. Again, there may be models developed elsewhere that could be adapted for the London context.\nOpportunities and Risks arising from Covid-19 — it should/may be possible to assess the impact of Covid-19 on London’s short- and long-term rental markets by looking at entry to/exit from the Airbnb marketplace by comparing more than one snapshot of London data. Again, this will require some reasonable assumptions to be drawn (are all flats withdrawn from Airbnb going back on to the Long-Term Rental Sector?) but these can be documented and justified.\nOpportunities for Place- or Listing-Branding — identifying key terms and features/amenities used to market listings by area and using these to identify opportunities for investment or branding. This would benefit from the use of NLP approaches and, potentially, word embeddings to identify distinctive patterns of word use as well as, potentially, One-Hot encoding to identify specific amenities that appear associated in some way with particular areas.\nThe Challenge of Ghost Hotels — evaluating ways to automatically identify ghost hotels from the InsideAirbnb data and then, potentially, assessing their extent and impact on local areas where they dominate either ‘proper’ hotel provision or other types of listings. You will need to consider the way that Airbnb randomly shuffles listings to prevent exactly this type of application and textual similarity via NLP is an obvious application.\nThe Professionalisation of Airbnb — this could be treated either as a regulatory challenge (is Airbnb not benefiting locals) or an investment opportunity (is this a way to ‘scale’ or develop new service offers for small hosts) depending on your interests. You will need to consider the different types of hosts and evaluate ways of distinguishing between them (e.g. number of listings, spatial extent, etc.).\nImpact Profiles — a geodemographic classification of London neighbourhoods based on how they have, or have not, been impacted by Airbnb. This would require you to think about how to develop a classification/clustering of London neighbourhoods and use data to develop ‘pen portraits’ of each so that policy-makers could better-understand the range of environments in which Airbnb operates and why a 1-size-fits-all regulatory approach may be insufficient. Again, this could be argued from either standpoint or even both simultaneously: these areas are already so heavily impacted that regulation is too little, too late, while these other areas are ‘at risk’."
  },
  {
    "objectID": "assessments/models.html#partial-bibliography",
    "href": "assessments/models.html#partial-bibliography",
    "title": "Model Content",
    "section": "Partial Bibliography",
    "text": "Partial Bibliography\nYou will also want to review the partial bibliography available here; this is by no means complete and you will likely find other relevant work ‘out there’ but you may find it useful for spurring your thinking on what to study and how to study it. You might also want to have a look at guidance for London:\n\nKeyNest (2019), Understanding Airbnb regulations in London, KeyNest; URL\nAirbnb (n.d.), I rent out my home in London. What short-term rental laws apply?, Airbnb; URL\nHostmaker (2018), Important Airbnb regulations and laws you should know about in London, Hostmaker; URL Now Houst."
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "Context",
    "section": "",
    "text": "The overall assessment package is intended to test students’ comprehension of, and ability to integrate, technical skills with a broader understanding of, and reflection upon, computational approaches to urban research and spatial data science."
  },
  {
    "objectID": "assessments/index.html#assessment-elements",
    "href": "assessments/index.html#assessment-elements",
    "title": "Context",
    "section": "Assessment Elements",
    "text": "Assessment Elements\nThe assessments are grounded in a mixture of critical reflection and group work that map on to real-world data science challenges, including:\n\nTimed Open Book Examination (30%) to ensure that you are acquiring the fundamentals of data analysis in Python, and simulate the real-world challenge of having to write code to a deadline (due Thursdsay, 23 November 2023 @ 6pm);\nCollaboratively evaluating and analysing a data set (60%) as a group you will be determining the suitability of a data set for tackling an analytical ‘problem’ using a mix of coding, analysis, and presentation skills in a reproducible format Tuesday, 19 December @ 6pm);\nReflecting on the process (10%) to better-understand why a project succeeded/failed so as to improve future outcomes and recognise the contributions of individual members of the group to the success of the project (due Wednesday, 20 December 2023 @ 6pm)."
  },
  {
    "objectID": "assessments/index.html#rationale",
    "href": "assessments/index.html#rationale",
    "title": "Context",
    "section": "Rationale",
    "text": "Rationale\nCollectively, these assessments seek to provide multiple opportunities to ‘shine’ both individually and as part of a group. You do not need to be the best programmer in the class in order to do well on these assessments. Indeed, focussing only on the programming is likely to result in a low mark because it missed the context in which data science and data analysis ‘work’. As a budding data scientist/analyst your job is just as much to understand your audience and their needs: you will work with clients who can’t really articulate what they want or why, so good project management often involves putting yourself in your client’s shoes and working out how to translate what they say they want into what they actually need.\nYou will therefore do poorly on the assessments if you do not do the readings, watch the pre-recorded lectures, or participate in discussions (both online and in-person during practicals and classes). These provide you with context for the work that is being done when you start typing and running code in a Jupyter Notebook. Code is the how. Context is the why."
  },
  {
    "objectID": "assessments/code.html",
    "href": "assessments/code.html",
    "title": "Reproducible Analysis",
    "section": "",
    "text": "The Reproducible Analysis is worth 40% of the Group Assessment.\nThe Reproducible Analysis must be written using Python in a Quarto Markdown Document (QMD file). You are free to draw on concepts and methods covered in both Quantitative Methods and GIS, but must still write the code in Python (e.g. adapting something from R in the GIS module to Python)."
  },
  {
    "objectID": "assessments/code.html#format",
    "href": "assessments/code.html#format",
    "title": "Reproducible Analysis",
    "section": "Format",
    "text": "Format\nYou will be submitting a runnable markdown document (.qmd file) that includes: your group’s name and all student ids. We will supply a template closer to the deadline."
  },
  {
    "objectID": "assessments/code.html#how-we-measure-reproducibility",
    "href": "assessments/code.html#how-we-measure-reproducibility",
    "title": "Reproducible Analysis",
    "section": "How We Measure Reproducibility",
    "text": "How We Measure Reproducibility\nWe will assess reproducibility by selecting “Restart Kernel and Run All” using the jreades/sds:2023 Docker environment. If you have made use of another Docker image then you must clearly signpost this at the start of your notebook so that we know to select a different image. We will not install libraries ‘by-hand’ in an ad hoc manner order to test the reproducibility of your work.\n\n\n\n\n\n\nReproducibility\n\n\n\nTo ensure reproducibility, markers must be able to run quarto render &lt;your file.qmd&gt; and reproduce your entire analysis. This includes downloading and extracting data (we will provide a function for the main data set), cleaning, transformation, clustering… charts, tables, etc.\n\n\n\n\n\n\n\n\nFor Large Workflows\n\n\n\nIf you need to provide supplementary or partially-processed data (see section below) then you can provide this via Dropbox, OneDrive (public sharing link), or some other robust cloud solution that will be accessible from the marker’s system.\n\n\nIf you have made use of one or more libraries that are not part of the Docker image then you can install these using ! pip install; however, if you take this approach then you should also ‘place nice’ by checking first to see if the library is already installed using try... except code that you can find on Stack Overflow and elsewhere (you will need to look this up)."
  },
  {
    "objectID": "assessments/code.html#data-and-resources-used",
    "href": "assessments/code.html#data-and-resources-used",
    "title": "Reproducible Analysis",
    "section": "Data and Resources Used",
    "text": "Data and Resources Used\nIt is also up to you to ensure that all relevant data are available via a valid URL for downloading and running. You may host your data anywhere you like, but please bear in mind that the markers will be based in the U.K. so some servers may be inaccessible. For very small data sets we’d recommend a GitHub repo, but for larger ones a Dropbox or OneDrive link would be more appropriate (you will need to check that the link you’ve created gives permission to anyone to download)."
  },
  {
    "objectID": "assessments/code.html#time-consuming-code",
    "href": "assessments/code.html#time-consuming-code",
    "title": "Reproducible Analysis",
    "section": "Time-Consuming Code",
    "text": "Time-Consuming Code\nIf your analysis has a particularly time-consuming stage (e.g. Named-Entity Recognition or Part-of-Speech tagging) then you can provide partially-processed data: comment out the code up to the point where you have generated the ‘expensive’ data set but leave it in the markdown document. That way we can see how you generated the data without it being part of the reproducibility stage."
  },
  {
    "objectID": "assessments/code.html#other-requirements",
    "href": "assessments/code.html#other-requirements",
    "title": "Reproducible Analysis",
    "section": "Other Requirements",
    "text": "Other Requirements\nYou must maintain a copy of the submission in GitHub so that we can review contributions if necessary."
  },
  {
    "objectID": "assessments/code.html#supporting-documents",
    "href": "assessments/code.html#supporting-documents",
    "title": "Reproducible Analysis",
    "section": "Supporting Documents",
    "text": "Supporting Documents\n\nA marking rubric is available.\nA proposal outline template is available (see: example)."
  },
  {
    "objectID": "assessments/audit.html",
    "href": "assessments/audit.html",
    "title": "Data Audit (30%)",
    "section": "",
    "text": "This individual assessment is due TBC (after Reading Week). The data set for which the ‘biography’ will be written is the Inside Airbnb data for London. This assessment will be quite straightforward if you have been doing the readings and keeping up with the basics of using Markdown, and rather challenging if you have not.\nThis assessment is, in part, a technical ‘test run’ for the policy report: you will not fail this assessment if you decide to write it in Word or some other editor, but now is a good time to check that you understand how Quarto works before beginning work on the other two assessments or on your dissertation!"
  },
  {
    "objectID": "assessments/audit.html#format",
    "href": "assessments/audit.html#format",
    "title": "Data Audit (30%)",
    "section": "Format",
    "text": "Format\nThis is a structured assessment that requires you to answer the following questions:\n\nWho collected the data? 2 points; 25 words; Consider the source of the data and its relation to the underlying data generating process.\nWhy did they collect it? 4 points; 50 words; Consider the purposes for which the data was collected and how this might shape its structure or content.\nHow was it collected? 7 points; 75 words; What was the method by which the data was collected and how might this shape its structure, content, or completeness.\nWhat useful information does it contain? 12 points; 100 words; Discuss how the data might support a range of analyses and note any limitations encountered so far in class or in your own investigations.\nTo what extent is the data ‘complete’? 25 points; 200 words; Reflecting on your earlier answers, and drawing on what you’ve learned about the data so far in class, to what extent is this data a ‘complete’ picture of Airbnb’s operations in London?\nWhat kinds of analysis would this support? 15 points; 200 words; Given the issues identified above, what kinds of analysis would this data support? You do not need to propose a specific analysis and should instead focus on generic classes of analysis.\nWhich of the analyses outlined above are ethical? 35 points; 350 words; Discuss the ethics of these classes of analysis with reference to your earlier answers and to the assigned readings.\n\nA marking rubric is available."
  },
  {
    "objectID": "assessments/audit.html#restrictions",
    "href": "assessments/audit.html#restrictions",
    "title": "Data Audit (30%)",
    "section": "Restrictions",
    "text": "Restrictions\nThe word limit for this assessment is 1,050 words. The per-question word counts above are indicative and are not a ‘hard’ limit.\nTables should be added in an appendix and do not count towards the word total.\nDo not use figures.\nAny works referenced should appear in a Bibliography."
  },
  {
    "objectID": "assessments/audit.html#referencing",
    "href": "assessments/audit.html#referencing",
    "title": "Data Audit (30%)",
    "section": "Referencing",
    "text": "Referencing\nYou will need to reference various documents as part of this submission. It is possible to write these ‘by hand’, but you might find it helpful to also learn about BibTeX fies and Markdown referencing in Quarto. The Docker image that we have provided has Quarto pre-installed, but because of the complexities of specifying fonts in Docker you are likely to find it easier to install it natively for your Operating System.\nYou’ll also need BibDesk (Mac) or JabRef (Mac/Windows) or similar (Zotero shuould also work) to edit the BibTeX file (outside of Docker)."
  },
  {
    "objectID": "assessments/audit.html#the-template",
    "href": "assessments/audit.html#the-template",
    "title": "Data Audit (30%)",
    "section": "The Template",
    "text": "The Template\nOnce you have installed Quarto, you will need the following files:\n\nThe template file (.qmd) that we have provided: Data_Audit.qmd.\nThe CSL file (.csl) that governs how references are done: harvard-cite-them-right.csl.\nThe BibTeX file (.bib) that will contain any references you use: bio.bib\n\n\n\n\n\n\n\nReally Look at the Template!\n\n\n\nYou’ll notice that the template specifies three fonts (mainfont, monofont, sansfont). If you don’t have these fonts installed on your computer then quarto render... will fail. These are installed in Docker, and I’ve added links in the template to show how you can download them for your own computer.\nYou are welcome to change the fonts to something already installed on your computer (this will help you to learn about font-management), or you can follow the links embedded in the template to download the specified fonts and install them for your computer."
  },
  {
    "objectID": "assessments/audit.html#rendering",
    "href": "assessments/audit.html#rendering",
    "title": "Data Audit (30%)",
    "section": "Rendering",
    "text": "Rendering\nThese three files together will enable you to ‘render’ the Markdown template as a PDF for submission. To do this, ensure that all three files above are in the same folder (e.g. ~/git/fsds/bio), in which case you’d see something like:\ncd ~/git/fsds/bio\nls . # Should show a .qmd file, a .csl file, and a .bib file\nquarto render Data_Audit.qmd\nThis should render the PDF with a look something like this example output."
  },
  {
    "objectID": "assessments/briefing.html",
    "href": "assessments/briefing.html",
    "title": "Written Content",
    "section": "",
    "text": "The Content is worth 60% of the Group Assessment. You will be writing for a non-technical audience: imagine that you are writing an outline proposal to undertake a piece of research for the Mayor of London.\nConsequently, through set questions the Content will establish the suitability of data taken from the Inside Airbnb web site for London ifor policy-making and, specifically, for the regulation of Short-Term Lets (STL) in London.\nThe responses to the set questions may be written without substantially new modelling or coding through the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature), but it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was taught in class.\nStudents may use data from more than one time period if they wish, but this is not required."
  },
  {
    "objectID": "assessments/briefing.html#format",
    "href": "assessments/briefing.html#format",
    "title": "Written Content",
    "section": "Format",
    "text": "Format\nThis is not an essay, and students who submit a traditional essay format will see their overall mark impacted as a result. You must preserve the question/response format, and your responses should stress intelligibility to an intelligent, but non-technical audience. This doesn’t mean that you don’t need citations, but you should not employ academic writing styles.\nExamples of the kind of tone and presentation format expected in response to the set questions are provided below and there will be opportunities to discuss the submission during Term. You should refer to the models provided for insight into how to write for the target audience.\n\n\n\n\n\n\nWrite for Your Audience\n\n\n\nWhat makes writing a good briefing hard—and not just about writing good code—is finding the right balance of technical detail and high-level explanation: you can’t just say ‘here are the five types of accommodation we found…’, but you also can’t say ‘we tested clustering solutions in the range 3–50 and found the optimal result at k=12…’ You should have a look at the examples."
  },
  {
    "objectID": "assessments/briefing.html#word-count",
    "href": "assessments/briefing.html#word-count",
    "title": "Written Content",
    "section": "Word Count",
    "text": "Word Count\nThe word limit for this assessment is 2,500 words."
  },
  {
    "objectID": "assessments/briefing.html#preparing-your-submission",
    "href": "assessments/briefing.html#preparing-your-submission",
    "title": "Written Content",
    "section": "Preparing Your Submission",
    "text": "Preparing Your Submission\nYou are expected to use sustainable authorship tools for this submission. You may be asked to provide evidence of this. A template will be provided, and you should also look at the Quarto Guide and, in particular, the PDF options in order to customise your project.\nYou must develop and maintain the submission in GitHub so that we can review contributions if necessary."
  },
  {
    "objectID": "assessments/briefing.html#supporting-documents",
    "href": "assessments/briefing.html#supporting-documents",
    "title": "Written Content",
    "section": "Supporting Documents",
    "text": "Supporting Documents\n\nA marking rubric is available.\nA proposal outline template is available (see: example)."
  },
  {
    "objectID": "assessments/group.html",
    "href": "assessments/group.html",
    "title": "Rationale",
    "section": "",
    "text": "This is a collaborative project (worth 60%) due Tuesday, 19 December @ 6pm that you will undertake in a small group of no more than five students. The project is intended to resemble real-world data science ways of working: you will be part of a small team, you will need to figure out how to work effectively together, you will need to jointly produce an output in which you all have confidence. You will be submitting a reproducible analysis (written for Quarto+Python) that we will be able to run on our own computers in order to generate a PDF output.\nThe focus of this assessment is the student’s ability to make use of concepts and methods covered in class as part of an analytical process to support decision-making in a non-academic context. It is not necessary that you employ every technique covered in class. It is necessary that you justify your choice of approach with reference to relevant academic and ‘grey’ literature, as well as the computational, statistical, and analytical objectives of your submission. It is perfectly possible to complete this assessment without the use of advanced analytical topics (e.g. clustering, NLP, or global/local/LISA autocorrelation methods); however, it is unlikely that you would be able to complete this assessment to a high standard without some graphs and some maps chosen for their ability to advance your argument.\nThe assessment may be completed without substantially new modelling or coding by drawing on the code written in practicals to develop an analysis based on the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature), but it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was taught by selectively deploying more advanced programming techniques.\n\n\n\n\n\n\nGroup Disputes\n\n\n\nIn the event that there is irreconcilable disagreement within a group, we will use GitHub to determine contributions and inform individual marks.\n\n\nThe reproducible analysis must be a runnable QMD (Quarto Markdown Document) file that addresses the set questions provided in class. The QMD file will be assessed on two components:\n\nIts reproducibility (40% of this assessment): do the analyses employed, and outputs created by the group run fully and without errors on a different computer, and do they show evidence of thought in relation to the quality of coding and outputs?\nIts content (60% of this assessment): do the answers written by the group engage through a mix of literature, critical thinking, and data analysis with the set questions?"
  },
  {
    "objectID": "assessments/group.html#rationale",
    "href": "assessments/group.html#rationale",
    "title": "Overview",
    "section": "Rationale",
    "text": "Rationale\nThe focus of this assessment is the student’s ability to make use of concepts and methods covered in class as part of an analytical process to support decision-making in a non-academic context. It is not necessary that you employ every technique covered in class. It is necessary that you justify your choice of approach with reference to relevant academic and ‘grey’ literature, as well as the computational, statistical, and analytical objectives of your briefing paper. It is perfectly possible to complete this assessment without the use of advanced analytical topics (e.g. clustering, NLP, or global/local/LISA autocorrelation methods); however, it is unlikely that you would be able to complete this assessment to a high standard without some graphs and some maps chosen for their ability to advance your argument.\nThe assessment may be completed without substantially new modelling or coding by drawing on the code written in practicals to develop an analysis based on the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature), but it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was taught by selectively deploying more advanced programming techniques.\n\n\n\n\n\n\nGroup Disputes\n\n\n\nIn the event that there is irreconcilable disagreement within a group, we will use GitHub to determine contributions and inform individual marks.\n\n\nThe reproducible analysis must be a runnable QMD (Quarto Markdown Document) file that addresses the set questions provided in class. The QMD file will be assessed on two components:\n\nIts reproducibility (40% of this assessment): do the analyses employed, and outputs created by the group run fully and without errors on a different computer.\nIts content (60% of this assessment): do the answers written by the group engage through a mix of literature, critical thinking, and data analysis with the set questions."
  },
  {
    "objectID": "assessments/group.html#supporting-documents",
    "href": "assessments/group.html#supporting-documents",
    "title": "Rationale",
    "section": "Supporting Documents",
    "text": "Supporting Documents\n\nA marking rubric is available.\nA proposal outline template is available (see: example)."
  },
  {
    "objectID": "assessments/individual.html",
    "href": "assessments/individual.html",
    "title": "Individual Reflection",
    "section": "",
    "text": "This individual assessment due ?var:assess.individual asks you to reflect on the process of ‘doing data science’ as part of a team, since this format is typical of real-world projects. Many companies (e.g. Apple, Google, etc.) employ an Agile project format in which teams undertake a ‘Retrospective’ at the end of a project in order to identify ways to improve how they work in the future. We are not asking you to do this as a group (indeed, it’s an individual reflection), but we hope that this will help you to develop as a budding analyst or data scientist."
  },
  {
    "objectID": "assessments/individual.html#format",
    "href": "assessments/individual.html#format",
    "title": "Individual Reflection",
    "section": "Format",
    "text": "Format\nYou must answer these 3 questions:\n\nWhat factors do you think help to explain what went well/not well in the project? 3 points; 200 words; Critically engaging with challenges faced by the group will not result in a low mark.\nHow do you think the other students in your group would evaluate your contribution to each of these outcomes? 12 points; 400 words; Honesty is the best policy here as we may compare your response with that of the rest of the group.\nBriefly describe one event or experience from the group assessment that gave you new insight into ‘doing’ data science, and explain how and why this will be useful to you in the future. 10 points; 400 words; Try to connect this to either/both of employability and transferrable skills.\n\nA marking rubric is available."
  },
  {
    "objectID": "assessments/individual.html#restrictions",
    "href": "assessments/individual.html#restrictions",
    "title": "Individual Reflection",
    "section": "Restrictions",
    "text": "Restrictions\nThe word limit for this assessment is 1,000 words. The per-question word counts above are indicative and are not a ‘hard’ limit.\nDo not use tables or figures.\nAny works referenced should appear in a Bibliography."
  },
  {
    "objectID": "assessments/individual.html#the-template",
    "href": "assessments/individual.html#the-template",
    "title": "Individual Reflection",
    "section": "The Template",
    "text": "The Template\nYou are encouraged use the Individual Reflection Template as a starting point, but you may also wish to experiment with other templates, styles, or fonts. Make careful note of the expected sections and content. References are not required unless you wish to connect your reflection to specific publications. Note the requirement to include a ‘Sustainable Authorship’ section."
  },
  {
    "objectID": "assessments/restrictions.html",
    "href": "assessments/restrictions.html",
    "title": "Additional Guidance",
    "section": "",
    "text": "Unless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used must be generated by Python code cells included in the markdown file. You may not modify or create figures in another application since this undermines the reproducibility of the analysis."
  },
  {
    "objectID": "assessments/restrictions.html#figures-tables",
    "href": "assessments/restrictions.html#figures-tables",
    "title": "Additional Guidance",
    "section": "",
    "text": "Unless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used must be generated by Python code cells included in the markdown file. You may not modify or create figures in another application since this undermines the reproducibility of the analysis."
  },
  {
    "objectID": "assessments/restrictions.html#word-counts",
    "href": "assessments/restrictions.html#word-counts",
    "title": "Additional Guidance",
    "section": "Word Counts",
    "text": "Word Counts\nEach figure or table in the Policy Briefing counts for 150 words, and so students should give careful consideration to the trade-offs involved: more figures may serve to illustrate your points but leave you with much less space to synthesise and present and argument."
  },
  {
    "objectID": "assessments/restrictions.html#ab-figures",
    "href": "assessments/restrictions.html#ab-figures",
    "title": "Additional Guidance",
    "section": "A/B Figures",
    "text": "A/B Figures\nA figure with A/B/C elements will count as one figure, but only where the parts parts are conceptually related (e.g. before/after; non-spatial/spatial distribution; type 1 and type 2; etc.). The output from PySAL’s LISA analysis library, for instance, is pre-formatted as 3 figures. Seaborn’s jointplot will only be considered to be one plot even though it is technically three because the distribution plots in the margin are related to the scatter plot that is the focus of the plot.\nIn principle, a briefing with 16 figures would have no space for any text or interpretation; this choice is deliberate because its purpose is to focus your attention on which charts and tables best-communicate your findings. In practice, using A/B/C figure layouts then you are looking at up to 48 separate figures before hitting the limit, though you would at this point be producing an infographic and not a briefing."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#how-we-are-running-things",
    "href": "lectures/1.1-Getting_Oriented.html#how-we-are-running-things",
    "title": "Getting Oriented",
    "section": "How We Are Running Things…",
    "text": "How We Are Running Things…\nYour input will guide the class!\n\nPreparation: readings, pre-recorded lectures, quizzes/feedback.\nClasses: discussing readings and lectures; discussing questions and issues arising from the previous week’s practical, and ‘live coding’ in an I do/We do format.\nPractical: working through a weekly ‘programming notebook’ in a small group with support from your PGTAs."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#assessments-logic",
    "href": "lectures/1.1-Getting_Oriented.html#assessments-logic",
    "title": "Getting Oriented",
    "section": "Assessments (Logic)",
    "text": "Assessments (Logic)\n\nTeach and test the most challenging aspects of data science ‘work’ without mastery of Python.\nDiscover transferrability of skills and tools across projects, disciplines, and industries.\nBuild on content from QM (e.g. setting quantitative research questions) and GIS (e.g. spatial statistics).\nDevelop experience with non-academic research formats and writing."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#assessments-details",
    "href": "lectures/1.1-Getting_Oriented.html#assessments-details",
    "title": "Getting Oriented",
    "section": "Assessments (Details)",
    "text": "Assessments (Details)\n\nTimed, Open Book Exam (30% of module grade): A quiz requiring a mix of numeric and textual answers to short data analysis questions for which you must write the code.\nGroup Report (60% of module grade; 2,500 words max): A structured, small-group submission in which students respond to set questions and develop an exploratory analysis of the assigned data set.\nSelf-Evaluation (10% of module grade): A structured individual reflection combined with numerical scoring of peers on their contribution to the group’s outcomes.\n\n\n1 is a Moodule quiz due Thursdsay, 23 November 2023 @ 6pm (after Reading Week) and it will focus on the effective use of the pandas library.\n2 A Quarto document due Tuesday, 19 December @ 6pm (immediately after the end of term) that combines the analysis and outputs in one document with a set of specified questions upon which randomly-selected groups will receive feedback throughout the term.\n3 A reflection and ranking exercise due ?var:assess.peerdate (the day after the Quarto submission).\nI will talk more about these over the course of the term."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#where-does-fsds-fit",
    "href": "lectures/1.1-Getting_Oriented.html#where-does-fsds-fit",
    "title": "Getting Oriented",
    "section": "Where Does FSDS Fit?",
    "text": "Where Does FSDS Fit?\n\nGeographic Information Systems (GIS)\n\nFoundations of spatial analysis\nWorking with geo-data\n\nQuantitative Methods (QM)\n\nFoundations of statistical analysis\nWorking with data\n\nFoundations of Spatial Data Science (FSDS)\n\nFoundations of applied spatial and statistical analysis\nIntegrating and applying concepts from GIS & QM to a problem\nDeveloping programming and practical analysis skills\nSeeing the ‘data science’ pipeline from end to end"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#what-are-we-trying-to-do",
    "href": "lectures/1.1-Getting_Oriented.html#what-are-we-trying-to-do",
    "title": "Getting Oriented",
    "section": "What Are We Trying to Do?",
    "text": "What Are We Trying to Do?\nThis class hopes to achieve four things:\n\nTo teach you the basics of how to code in Python.\nTo teach you the basics of how to think in Python.\nTo teach you how to engage with data critically.\nTo help you integrate concepts taught across Term 1 and prepare you to apply them in Term 2.\n\nThese skills are intended to be transferrable to post-degree employment or research."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#overall-structure",
    "href": "lectures/1.1-Getting_Oriented.html#overall-structure",
    "title": "Getting Oriented",
    "section": "Overall Structure",
    "text": "Overall Structure\n\nPart 1: Foundations: Weeks 1–4 to covering the ‘basics’ and setting out a data science workflow.\nPart 2: Data: Weeks 5–7 looking at the same data through three lenses.\nPart 3: Analysis: Weeks 8–10 making sense of the data by building on concepts covered in QM and GIS.\n\n\n1-4 means tackling the ‘basics’ of Python, foundational concepts in programming, and practicing with the ‘tools of the trade’ for programmers.\n5-7 means different types of data (numeric, spatial and textual) with a view to understanding how such data can be cleaned, processed, and aggregated for use in a subsequent analysis. It is commonly held that 80% of ‘data science’ involves data cleaning, so this is a critical phase in developing an understanding of data.\n8-10 is about visualisation, classification, dimensionality reduction, and clustering. These concepts will have been encountered in other modules, so the intention is that the student will see how these fit into the ‘bigger picture’ of applied spatial analysis."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#also",
    "href": "lectures/1.1-Getting_Oriented.html#also",
    "title": "Getting Oriented",
    "section": "Also…",
    "text": "Also…\nWe hope to convince you t-hat:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#consequences",
    "href": "lectures/1.1-Getting_Oriented.html#consequences",
    "title": "Getting Oriented",
    "section": "Consequences…",
    "text": "Consequences…\nDon’t take my word for it, Prat et al. (2020) in Nature link language learning to programming language learning!\nSo…\n\nIf you only code during the practical session then you will not learn how to code.\nIf you cram the night before then you will not learn how to code.\nIf you practice for 45 minutes a day then you will learn how to code.\n\n\nThis said, I do hope to convince you that:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#a-bit-of-perspective",
    "href": "lectures/1.1-Getting_Oriented.html#a-bit-of-perspective",
    "title": "Getting Oriented",
    "section": "A Bit of Perspective",
    "text": "A Bit of Perspective\n\n\nStudents face the risks of the de-skilling of geography and planning at one end, and being subsumed by data science at the other…"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-challenges",
    "href": "lectures/1.1-Getting_Oriented.html#the-challenges",
    "title": "Getting Oriented",
    "section": "The Challenges",
    "text": "The Challenges\n\nDifferent style of learning from what you might be used to (“I didn’t anticipate, or rather factor into my schedule, the amount of out-of-hours practice that was required to stay up to date.”).\nDoing stats and programming at the same time and connecting this all back to the bigger picture.\nDelayed gratification (you have to walk before you can run).\nEasy to fall behind, but hard to catch up (“the pace is relentless”)."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-rewards",
    "href": "lectures/1.1-Getting_Oriented.html#the-rewards",
    "title": "Getting Oriented",
    "section": "The Rewards",
    "text": "The Rewards\n\nSkills that are highly transferrable and highly sought-after professionally.\nProblem-solving and practical skills that are valued by the private and public sectors.\nA whole new way of seeing the world and interacting with it.\nLots of support along the way… if you remember to ask for it!\n\nSee this thread on moving from academia to data science."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-implications",
    "href": "lectures/1.1-Getting_Oriented.html#the-implications",
    "title": "Getting Oriented",
    "section": "The Implications",
    "text": "The Implications\nYou will learn to code best if you treat it like learning a new language:\n\nStart simple and work up.\nGoogle is your friend (really).\nTalk with your friends (i.e. Slack).\nImmerse yourself and practice regularly.\nDo the readings even if we don’t address them specifically.\nLearn how to ask questions (i.e. Search Stack Overflow).\nSubscribe to a ‘magazine’ or two (e.g. Medium or Pocket)."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#study-aids",
    "href": "lectures/1.1-Getting_Oriented.html#study-aids",
    "title": "Getting Oriented",
    "section": "Study Aids",
    "text": "Study Aids\nWhen you need an answer right now:\n\nGoogle\nStack Overflow\nSlack\n\nWhen you want to learn more:\n\nMedium\nPocket\n\n\nGoogle will become more useful as you learn more and this is definitely one class in which “I Googled it” is a good answer.\nAs of early September 2020, Stack Overflow contains over 1.5 million Python questions alone! Chances are someone else has had your question before."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#study-right",
    "href": "lectures/1.1-Getting_Oriented.html#study-right",
    "title": "Getting Oriented",
    "section": "Study ‘Right’",
    "text": "Study ‘Right’\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time, as well as how to read and how to think.\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’.\n\nIf you have a trick or technique that works for you then I want to hear about it! And I’d encourage you all to share with your peers anything that helps you to stay focussed but also relaxed!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#before-you-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#before-you-ask-for-help",
    "title": "Getting Oriented",
    "section": "Before You Ask for Help",
    "text": "Before You Ask for Help\nFrom the Computer Science Wiki:\n\nDraw a picture of the problem\nExplain the problem to a rubber duck, teddy bear or whatever (really!)\nForget about a computer; how would you solve this with a pencil and paper?\nThink out loud\nExplain the problem to a friend\n\nTo which I would add:\n\nUse print(variable) statements liberally in your code!\n\n\nWe’ll cover this last bit as we get more used to coding!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#where-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#where-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "Where to Ask for Help",
    "text": "Where to Ask for Help\nThere is no shame in asking for help. None. We are here to support your learning and we have chosen a range of tools to support that:\n\nSlack: use public #fsds channel for help with coding, practical, and related course questions.\nDrop-in Hours: TBC (always good to message me in advance).\nOut-of-Hours: use email to raise personal circumstances and related issues for focussed support. Make use of Professional Services support as-needed to preserve privacy and for Extenuating Circumstances.\n\n\nWe’ll talk about Slack more later, but we think that this is the best way to get help when you need it. Slack enables us to support you as a community of learners across computer / tablet / phone.\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time, as well as how to read and how to think."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#when-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#when-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "When to Ask for Help",
    "text": "When to Ask for Help\n\nWhen you get warning messages from your computer’s Operating System.\nWhen you cannot get the coding environment to run at all.\nWhen even simple commands return line after line of error code.\nWhen you have no clue what is going on or why.\nWhen you have been wrestling with a coding question for more than 20 minutes (but see: How to Ask for Help!)\n\n\nIn order to learn you do need to struggle, but only up to a point! So we don’t think that giving you the answer to a coding question as soon as you get stuck is a good way for you to learn. At the same time, I remain sad to this day that one of the most insightful students I’ve ever taught in a lecture context dropped out of our module because they were having trouble with their computer and thought it was their fault nothing was working right. By we had realised what was going on it was too late: they were so far behind that they didn’t feel able to catch up. We’d rather that you asked and we said “Close, but try it again” than you didn’t ask and checked out thinking that you couldn’t ‘do’ programming."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#how-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#how-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nIn addition to what I have provided, I like the “How to ask programming questions” page provided by ProPublica:\n\nDo some research first.\nBe specific.\nRepeat.\nDocument and share.\n\nIf you find yourself wanting to ask a question on Stack Exchange then they also have a guide, and there are plenty of checklists.\n\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#learn-from-your-mistakes",
    "href": "lectures/1.1-Getting_Oriented.html#learn-from-your-mistakes",
    "title": "Getting Oriented",
    "section": "Learn from Your Mistakes",
    "text": "Learn from Your Mistakes"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#one-more-thing",
    "href": "lectures/1.1-Getting_Oriented.html#one-more-thing",
    "title": "Getting Oriented",
    "section": "One More Thing…",
    "text": "One More Thing…\nYou will get things wrong. I will get things wrong.\nWe will assume that you are trying your best. Please assume the same about us!\nIt’s going to be messy, but I’m really excited about it!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#and-finally",
    "href": "lectures/1.1-Getting_Oriented.html#and-finally",
    "title": "Getting Oriented",
    "section": "And Finally…",
    "text": "And Finally…\n\n\n\n\n\n\nAuto-Updates\n\n\nDo not allow your computer to auto-update during term. Inevitably, major upgrades will break developer tools. Do this by choice only when you have time. MacOS Sonoma is out 26 September, do not install it!\n\n\n\n\nMany students allowed their computer to update to Big Sur last year and it broke their entire computing environment. Some did this shortly before a submission was due. Do not do this!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#additional-resources",
    "href": "lectures/1.1-Getting_Oriented.html#additional-resources",
    "title": "Getting Oriented",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSee the GIS&T Body of Knowledge (BoK) for quick overview of concepts, techniques, and tools: gistbok.ucgis.org.\nA degree of ‘plagiarism’ is acceptable in code since that’s how we learn; however, mindless copy+pasting of Stack Overflow code leads to sphagetti and, often, incorrect results or difficult-to-squash bugs. Think of it like paraphrasing.\nTo distinguish between plagiarism and paraphrasing here’s a nice tutorial that you can also use to help you with your ‘regular’ writing."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#three-phases-of-coding",
    "href": "lectures/1.3-Writing_Code.html#three-phases-of-coding",
    "title": "Writing Code",
    "section": "Three Phases of Coding",
    "text": "Three Phases of Coding\n\nManaging it\nDocumenting it\nWriting it\n\nNotice that writing comes last."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#git",
    "href": "lectures/1.3-Writing_Code.html#git",
    "title": "Writing Code",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#github",
    "href": "lectures/1.3-Writing_Code.html#github",
    "title": "Writing Code",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#gitgithub-is-for-anything",
    "href": "lectures/1.3-Writing_Code.html#gitgithub-is-for-anything",
    "title": "Writing Code",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course (minus videos and assessments) is on GitHub."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#additional-stylings",
    "href": "lectures/1.3-Writing_Code.html#additional-stylings",
    "title": "Writing Code",
    "section": "Additional Stylings",
    "text": "Additional Stylings\nFor those who know how to do it, you can also insert bits of real HTML and CSS (the ‘languages’ of web sites) as well.\n\nThis content has HTML formatting attached."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#why-use-it",
    "href": "lectures/1.3-Writing_Code.html#why-use-it",
    "title": "Writing Code",
    "section": "Why Use It?",
    "text": "Why Use It?\nThe features that make it easy to use have real advantages for you:\n\nYou will spend less time wrestling with Microsoft Word and its formatting; this means that…\nYou will spend more time focussing on the important stuff: writing and coding!\nYou will be able to combine Code and Documentation easily because Python/R and Markdown all coexist happily on GitHub."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#markdown-examples",
    "href": "lectures/1.3-Writing_Code.html#markdown-examples",
    "title": "Writing Code",
    "section": "Markdown Examples",
    "text": "Markdown Examples\nSee CommonMark and the Markdown Guide for more :\n\n\n\n\nFormat\n\n\nOutput\n\n\n\n\nPlain text…\n\n\nPlain text\n\n\n\n\n## A Large Heading\n\n\n\nA Large Heading\n\n\n\n\n\n### A Medium Heading\n\n\n\nA Medium Heading\n\n\n\n\n\n- A list- More list\n\n\n\n\nA list\n\n\nMore list\n\n\n\n\n\n\n1. An ordered list2. More ordered list\n\n\n\n\nAn ordered list\n\n\nMore ordered list\n\n\n\n\n\n\n[A link](http://casa.ucl.ac.uk)\n\n\nA link\n\n\n\n\n\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#more-markdown",
    "href": "lectures/1.3-Writing_Code.html#more-markdown",
    "title": "Writing Code",
    "section": "More Markdown…",
    "text": "More Markdown…\nHere are some more resources:\n\nGetting Started\nAn online interactive tutorial\nCheatsheet\n\nAnd once you’re ready to get ‘serious’, check out this tutorial on Sustainable Authorship in Plain Text using Pandoc and Markdown from The Programming Historian! That’s what actually underpins Knitr and Quarto, but you can do so much more…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#literate-programming",
    "href": "lectures/1.3-Writing_Code.html#literate-programming",
    "title": "Writing Code",
    "section": "Literate Programming",
    "text": "Literate Programming\nIdeally, we want to write code in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct.\n\n\n– Donald Knuth, Selected Papers on Computer Science"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#key-tenets",
    "href": "lectures/1.3-Writing_Code.html#key-tenets",
    "title": "Writing Code",
    "section": "Key Tenets",
    "text": "Key Tenets\nWhat we want:\n\nWeaving: the code and its documentation are together in one file.\nTangling: the code can be run directly from this file.\n\nWhy do we want this?"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-notebooks",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-notebooks",
    "title": "Writing Code",
    "section": "Jupyter(Lab) & Notebooks",
    "text": "Jupyter(Lab) & Notebooks\n\nModern Browser + Jupyter == Tangled, Woven code in (m)any languages\nIncluding maths:\n\\[\nf(a) = \\frac{1}{2\\pi i} \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n\\]\n\nNote: you can set equations in Markdown too!"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-python",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-python",
    "title": "Writing Code",
    "section": "JupyterLab + Python",
    "text": "JupyterLab + Python"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#all-kinds-of-features",
    "href": "lectures/1.3-Writing_Code.html#all-kinds-of-features",
    "title": "Writing Code",
    "section": "All Kinds of Features",
    "text": "All Kinds of Features\nJupyterLab is basically a web application:"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-uses-markdown",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-uses-markdown",
    "title": "Writing Code",
    "section": "JupyterLab Uses Markdown",
    "text": "JupyterLab Uses Markdown"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#and-so-does-quarto",
    "href": "lectures/1.3-Writing_Code.html#and-so-does-quarto",
    "title": "Writing Code",
    "section": "And so Does Quarto",
    "text": "And so Does Quarto\n\n\n\n\nQuarto is intended to:\n\nExtend power of RMarkdown/knitr to all languages\nMake it easy to create web sites, documents, and articles.\nMake it easy to integrate code with text and images."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#resources",
    "href": "lectures/1.3-Writing_Code.html#resources",
    "title": "Writing Code",
    "section": "Resources",
    "text": "Resources\n\nStack Overflow\nVersion Control with Git\nSetting up and managing your GitHub user account\nPersonal Access Tokens on Git\nGit Cheat Sheet"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#spot-the-difference",
    "href": "lectures/10.2-Clustering.html#spot-the-difference",
    "title": "Clustering",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nClassification\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms (A) present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in 1–2 dimensions\n\n\nClustering\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms A present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in &lt; 9 dimensions\n\n\n\n\nClustering algorithms can suffer from the ‘curse of dimensionality’ such that high-dimensional spaces cluster poorly without either dimensionality reduction or the use of specialist algorithms such as Spherical k-Means."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#the-first-geodemographic-classification",
    "href": "lectures/10.2-Clustering.html#the-first-geodemographic-classification",
    "title": "Clustering",
    "section": "The First Geodemographic Classification?",
    "text": "The First Geodemographic Classification?\n\nSource: booth.lse.ac.uk/map/"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#more-than-100-years-later",
    "href": "lectures/10.2-Clustering.html#more-than-100-years-later",
    "title": "Clustering",
    "section": "More than 100 Years Later",
    "text": "More than 100 Years Later\n\nSource: vis.oobrien.com/booth/"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#intimately-linked-to-rise-of-the-state",
    "href": "lectures/10.2-Clustering.html#intimately-linked-to-rise-of-the-state",
    "title": "Clustering",
    "section": "Intimately Linked to Rise of The State",
    "text": "Intimately Linked to Rise of The State\n\nGeodemographics only possible in context of a State – without a Census it simply wouldn’t work… until now?\nClearly tied to social and economic ‘control’ and intervention: regeneration, poverty & exclusion, crime, etc.\nPresumes that areas are the relevant unit of analysis; in geodemographics these are usually called neighbourhoods… which should ring a few bells.\nIn practice, we are in the realm of ‘homophily’, a.k.a. Tobler’s First Law of Geography"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#where-is-it-used",
    "href": "lectures/10.2-Clustering.html#where-is-it-used",
    "title": "Clustering",
    "section": "Where is it used?",
    "text": "Where is it used?\nAnything involving grouping individuals, households, or areas into larger ‘groups’…\n\nStrategic marketing (above the line, targeted, etc.)\nRetail analysis (store location, demand modelling, etc.)\nPublic sector planning (resource allocation, service development, etc.)\n\nCould see it as a subset of customer segmentation."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#problem-domains",
    "href": "lectures/10.2-Clustering.html#problem-domains",
    "title": "Clustering",
    "section": "Problem Domains",
    "text": "Problem Domains\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nSupervised\nRegression\nClassification\n\n\nUnsupervised\nDimensionality Reduction\nClustering\n\n\n\n\n\nIn classification we ‘know’ the answer (we test against labels).\nIn clustering we don’t ‘know’ the answer (we look for clusters)."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#measuring-fit",
    "href": "lectures/10.2-Clustering.html#measuring-fit",
    "title": "Clustering",
    "section": "Measuring ‘Fit’",
    "text": "Measuring ‘Fit’\n\nUsually working towards an ‘objective criterion’ for quality… these are known as cohesion and separation measures."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#how-your-data-looks",
    "href": "lectures/10.2-Clustering.html#how-your-data-looks",
    "title": "Clustering",
    "section": "How Your Data Looks…",
    "text": "How Your Data Looks…\nClustering is one area where standardisation (and, frequently, normalisation) are essential:\n\nYou don’t (normally) want scale in any one dimension to matter more than scale in another.\nYou don’t want differences between values in one dimension to matter more than differences in another.\nYou don’t want skew in one dimension to matter more than skew in another.\n\nYou also want uncorrelated variables… why?"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#first-steps",
    "href": "lectures/10.2-Clustering.html#first-steps",
    "title": "Clustering",
    "section": "First Steps",
    "text": "First Steps\nYou will normally want a continuous variable… so these types of data are especially problematic:\n\nDummies / One-Hot Encoded\nCategorical / Ordinal\nPossible solutions: k-modes, CCA, etc."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#performance",
    "href": "lectures/10.2-Clustering.html#performance",
    "title": "Clustering",
    "section": "Performance",
    "text": "Performance\nTypically about trade-offs between:\n\n\n\nAccuracy\nGeneralisation"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#trade-offs",
    "href": "lectures/10.2-Clustering.html#trade-offs",
    "title": "Clustering",
    "section": "Trade-Offs",
    "text": "Trade-Offs\nNeed to balance:\n\nAbility to cluster at speed.\nAbility to replicate results.\nAbility to cope with fuzzy/indeterminate boundaries.\nAbility to cope with curse of dimensionality.\nUnderlying representation of group membership…"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#visualising-the-trade-offs",
    "href": "lectures/10.2-Clustering.html#visualising-the-trade-offs",
    "title": "Clustering",
    "section": "Visualising the Trade-Offs",
    "text": "Visualising the Trade-Offs\n\n\n\nNotice the limitations to k-means: it may be fast but it’s got problems if your data is non-linear/non-Gaussian.\nAnd this doesn’t even include options like HDBSCAN, HAC/Hierarchical Clustering, and many more!\n\n\nDetails on scikit-learn.org."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#putting-it-all-into-context",
    "href": "lectures/10.2-Clustering.html#putting-it-all-into-context",
    "title": "Clustering",
    "section": "Putting it All into Context",
    "text": "Putting it All into Context"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#space-adds-complexity",
    "href": "lectures/10.2-Clustering.html#space-adds-complexity",
    "title": "Clustering",
    "section": "Space Adds Complexity",
    "text": "Space Adds Complexity\nWe now have to consider two more types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#trade-offs-again",
    "href": "lectures/10.2-Clustering.html#trade-offs-again",
    "title": "Clustering",
    "section": "Trade-offs (Again)…",
    "text": "Trade-offs (Again)…\nConsider:\n\nClustering algorithms are inherently spatial.\nClustering algorithms do not take space geography into account.\n\nDoes this matter?\n\nAll clustering algorithms are about inter-observation and intra-cluster distances so they have some conceptualisation of ‘space’.\nSpatially-aware clustering algorithms exist but are generally much more computationally-intensive than ‘regular ones’."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#different-approaches",
    "href": "lectures/10.2-Clustering.html#different-approaches",
    "title": "Clustering",
    "section": "Different Approaches",
    "text": "Different Approaches\n\n\n\nAlgorithm\nPros\nCons\nGeographically Aware?\n\n\n\n\nk-Means\nFast. Deterministic.\nEvery observation to a cluster.\nN.\n\n\nDBSCAN\nAllows for clusters and outliers.\nSlower. Choice of \\(\\epsilon\\) critical. Can end up with all outliers.\nN, but implicit in \\(\\epsilon\\).\n\n\nOPTICS\nFewer parameters than DBSCAN.\nEven slower.\nN, but implicit in \\(\\epsilon\\).\n\n\nHierarchical/ HDBSCAN\nCan cut at any number of clusters.\nNo ‘ideal’ solution.\nY, with connectivity parameter.\n\n\nADBSCAN\nScales. Confidence levels.\nMay need large data set to be useful. Choice of \\(\\epsilon\\) critical.\nY.\n\n\nMax-p\nCoherent regions returned.\nVery slow if model poorly specified.\nY."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#setting-the-relevant-distance",
    "href": "lectures/10.2-Clustering.html#setting-the-relevant-distance",
    "title": "Clustering",
    "section": "Setting the Relevant Distance",
    "text": "Setting the Relevant Distance\nMany clustering algorithms rely on a distance specification (usually \\(\\epsilon\\)). So to set this threshold:\n\nIn high-dimensional spaces this threshold will need to be large.\nIn high-dimensional spaces the scale will be meaningless (i.e. not have a real-world meaning, only an abstract one).\nIn 2- or 3-dimensional (geographical) space this threshold could be meaningful (i.e. a value in metres could work)."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#choosing-a-distance-metric",
    "href": "lectures/10.2-Clustering.html#choosing-a-distance-metric",
    "title": "Clustering",
    "section": "Choosing a Distance Metric",
    "text": "Choosing a Distance Metric\n\n\n\n\n\n\n\n\nn Dimensions\nHow to Set\nExamples\n\n\n\n\n2 or 3\nTheory/Empirical Data\nWalking speed; Commute distance\n\n\n2 or 3\nK/L Measures\nPlot with Simulation for CIs to identify significant ‘knees’.\n\n\n3\nMarked Point Pattern?\n\n\n\n&gt; 3\nkNN\nCalculate average kNN distance based on some expectation of connectivity.\n\n\n\n\nRemember: inter-observation distance increases with dimensionality!"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#experian",
    "href": "lectures/10.2-Clustering.html#experian",
    "title": "Clustering",
    "section": "Experian",
    "text": "Experian\nSpecialist in consumer segmentation and geodemographics (bit.ly/2jMRhAW).\n\nMarket cap: £14.3 billion.\nMosaic: “synthesises of 850 million pieces of information… to create a segmentation that allocates 49 million individuals and 26 million households into one of 15 Groups and 66 detailed Types.””\nMore than 450 variables used.\n\nMost retail companies will have their own segmentation scheme. Competitors: CACI, Nielsen, etc."
  },
  {
    "objectID": "lectures/10.2-Clustering.html#experian-groups",
    "href": "lectures/10.2-Clustering.html#experian-groups",
    "title": "Clustering",
    "section": "Experian Groups",
    "text": "Experian Groups"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#experian-mapping",
    "href": "lectures/10.2-Clustering.html#experian-mapping",
    "title": "Clustering",
    "section": "Experian Mapping",
    "text": "Experian Mapping"
  },
  {
    "objectID": "lectures/10.2-Clustering.html#output-area-classification",
    "href": "lectures/10.2-Clustering.html#output-area-classification",
    "title": "Clustering",
    "section": "Output Area Classification",
    "text": "Output Area Classification\nOAC set up as ‘open source’ alternative to Mosaic:\n\nWell documented (UCL Geography a major contributor)\nDoesn’t require a license or payment\nCan be tweaked/extended/reweighted by users as needed"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#what-about-computational-thinking",
    "href": "lectures/2.2-Principles_of_Programming.html#what-about-computational-thinking",
    "title": "Principles of Programming",
    "section": "What About Computational Thinking?",
    "text": "What About Computational Thinking?\n\nComputational thinking is not thinking like a Computer Scientist. It is about recognising how to code can help us to understand, and manipulate, the world."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#key-features",
    "href": "lectures/2.2-Principles_of_Programming.html#key-features",
    "title": "Principles of Programming",
    "section": "Key Features",
    "text": "Key Features\nAspects of computational thinking include:\n\nRecognising how one problem connects to other problems.\nRecognising when and how to make things simpler and faster\nRecognising how different ways of tackling a problem gives you power to tackle new problems.\n\nSee this keynote by Lorena Barba (2014); esp. from 52:00 onwards.\n\nYou already do a lot of this when you generalise from your readings to your ideas/understanding!"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#why-are-these-virtues",
    "href": "lectures/2.2-Principles_of_Programming.html#why-are-these-virtues",
    "title": "Principles of Programming",
    "section": "Why Are These Virtues?",
    "text": "Why Are These Virtues?\nAccording to Larry Wall the three virtues of the programmer are:\n\nLaziness\nImpatience\nHubris\n\nThese are not to be taken literally (see Larry Wall’s “Three Virtues of a Programmer” are Utter Bull💩).\n\nAutomate the boring stuff, focus on the interesting bits! And it’s not about quantity of code, it’s about quality.\nUse code to save time, but don’t just jump head-first into problems.\nWhen something isn’t working well you want to make it work better/faster/more efficiently…"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#four-quotes-to-remember",
    "href": "lectures/2.2-Principles_of_Programming.html#four-quotes-to-remember",
    "title": "Principles of Programming",
    "section": "Four Quotes to Remember",
    "text": "Four Quotes to Remember\n\nComputers make very fast, very accurate mistakes.\nA computer program does what you tell it to do, not what you want it to do.\nOnly half of programming is coding. The other 90% is debugging.\nWeeks of coding can save you hours of planning."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#and-one-more",
    "href": "lectures/2.2-Principles_of_Programming.html#and-one-more",
    "title": "Principles of Programming",
    "section": "And One More…",
    "text": "And One More…\n\nExperience is the name everyone gives to their mistakes.\nOscar Wilde"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#following-a-recipe-is-easy-right",
    "href": "lectures/2.2-Principles_of_Programming.html#following-a-recipe-is-easy-right",
    "title": "Principles of Programming",
    "section": "Following a Recipe is Easy, Right?",
    "text": "Following a Recipe is Easy, Right?\n\nSource"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#calculating-the-mean",
    "href": "lectures/2.2-Principles_of_Programming.html#calculating-the-mean",
    "title": "Principles of Programming",
    "section": "Calculating the Mean",
    "text": "Calculating the Mean\nGiven these numbers, what’s the average?\n1, 4, 7, 6, 4, 2"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#as-a-recipe",
    "href": "lectures/2.2-Principles_of_Programming.html#as-a-recipe",
    "title": "Principles of Programming",
    "section": "As a Recipe",
    "text": "As a Recipe\n\nTake a list of numbers\nStart a count of numbers in the list at 0\nStart a sum of numbers in the list at 0\nTake a number from the list:\n\nAdd 1 to the count\nAdd the value of the number to the sum\n\nRepeat step #4 until no numbers are left in the list.\nDivide the sum by the count\nReport this number back to the user"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#as-python",
    "href": "lectures/2.2-Principles_of_Programming.html#as-python",
    "title": "Principles of Programming",
    "section": "As Python",
    "text": "As Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#why-we-still-havent-solved-it",
    "href": "lectures/2.2-Principles_of_Programming.html#why-we-still-havent-solved-it",
    "title": "Principles of Programming",
    "section": "Why We Still Haven’t ‘Solved It’",
    "text": "Why We Still Haven’t ‘Solved It’\n\nSource"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#languages",
    "href": "lectures/2.2-Principles_of_Programming.html#languages",
    "title": "Principles of Programming",
    "section": "Languages",
    "text": "Languages\nComputer languages come with all of the ‘baggage’ of human languages; they have:\n\nA vocabulary (reserved words)\nA grammar (syntax)\nRules about the kinds of things you can say (grammar)\nStyles and idiosyncrasies all their own (history)\n\nIn this module we will use the Python programming language. We could also teach this same content in R."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#python",
    "href": "lectures/2.2-Principles_of_Programming.html#python",
    "title": "Principles of Programming",
    "section": "Python",
    "text": "Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count += 1 # An alternative to count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#r",
    "href": "lectures/2.2-Principles_of_Programming.html#r",
    "title": "Principles of Programming",
    "section": "R",
    "text": "R\nnumbers = c(1, 4, 7, 6, 4, 2)\ntotal   = 0\ncount   = 0\nfor (num in numbers) {\n  total = total + num\n  count = count + 1\n}\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#finally-style",
    "href": "lectures/2.2-Principles_of_Programming.html#finally-style",
    "title": "Principles of Programming",
    "section": "Finally: Style",
    "text": "Finally: Style\nAlthough all programmers develop their own style (see: writing in any language), Python encourages coders to use a consistent style so that others can pick up your code and make sense of what’s going on (see: comments!).\nTwo useful resources:\n\nThe Hitchhiker’s Guide to Python\nA summary of Python code style conventions"
  },
  {
    "objectID": "lectures/2.4-Lists.html#whats-in-a-list",
    "href": "lectures/2.4-Lists.html#whats-in-a-list",
    "title": "Lists",
    "section": "What’s in a List?",
    "text": "What’s in a List?\nIn the same way that a paper shopping list holds many ‘types’ of shopping in one place, a Python list holds many ‘types’ of data in one place.\nmyList = [1, 3, 5, 7]     # homogenous list\nmyList = [1, \"dog\", 7.01] # heterogenous list \nmyList = []               # empty list\nPython lists are always recognisable by their “square brackets”: [...]"
  },
  {
    "objectID": "lectures/2.4-Lists.html#whats-in-a-list-part-2",
    "href": "lectures/2.4-Lists.html#whats-in-a-list-part-2",
    "title": "Lists",
    "section": "What’s in a List? (Part 2)",
    "text": "What’s in a List? (Part 2)\nIn fact, when I say lists can hold many types of data, I should have said that they can hold any type of data:\nx = 3\ny = \"Foo\"\nz = [\"A\", \"list\", 42]\n\na = [x, y, z] # Holds x, y, *and* list z\nThe output of print(a) is:\n[3, 'Foo', ['A', 'list', 42]]\n\nWe’re going to come back to this a lot later, but for now notice that a list can hold lists!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#using-list-indexes",
    "href": "lectures/2.4-Lists.html#using-list-indexes",
    "title": "Lists",
    "section": "Using List Indexes",
    "text": "Using List Indexes\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nLists are ‘indexed’ numerically from the zero-th element:\n\n\n\n\n\n\n\n\n\n\ngeographers [\n0\n1\n2\n]\n\n\n\n\n\nMassey 1\nHarvey 2\nRose 3\n\n\n\n\nprint(geographers[1]) # Harvey\nprint(geographers[2]) # Rose\nprint(geographers[3]) # Error: List index out of range\n\nAnd notice this error: Python tells you waht the problem is. The issue is understanding what the message means if you don’t know the vocabulary.\n\nhttps://en.wikipedia.org/wiki/Doreen_Massey_(geographer)https://en.wikipedia.org/wiki/David_Harveyhttps://en.wikipedia.org/wiki/Gillian_Rose_(geographer)"
  },
  {
    "objectID": "lectures/2.4-Lists.html#interpolation",
    "href": "lectures/2.4-Lists.html#interpolation",
    "title": "Lists",
    "section": "Interpolation",
    "text": "Interpolation\nWe can also use variables as list indexes:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 0\nprint(geographers[i]) # Massey\nAnything that evaluates (i.e. resolves) to a number can be used as an index:\ni = 1\nprint(geographers[i+1]) # Rose\nprint(geographers[ (i-2+1)*2 ]) # Massey"
  },
  {
    "objectID": "lectures/2.4-Lists.html#countdown",
    "href": "lectures/2.4-Lists.html#countdown",
    "title": "Lists",
    "section": "Countdown!",
    "text": "Countdown!\nWe can ‘count’ backwards from the end of the list using negative numbers:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[-1] ) # Rose\nprint( geographers[-2] ) # Harvey"
  },
  {
    "objectID": "lectures/2.4-Lists.html#does-not-compute",
    "href": "lectures/2.4-Lists.html#does-not-compute",
    "title": "Lists",
    "section": "Does Not Compute!",
    "text": "Does Not Compute!\nErrors can be scary… but informative!\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[4] )\n\nIndexError: list index out of range\n\nAnd then try:\nprint( geographers[1.25] )\n\nTypeError: list indices must be integers or slices, not float\n\nNotice that Python gives us important hints about the source of the problem!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#slicing-dicing-lists",
    "href": "lectures/2.4-Lists.html#slicing-dicing-lists",
    "title": "Lists",
    "section": "Slicing & Dicing Lists",
    "text": "Slicing & Dicing Lists\nYou can access more than one element at a time using a slice:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[0:2] ) # ['Massey','Harvey']\nprint( geographers[1:] )  # ['Harvey', 'Rose']\nprint( geographers[-2:] ) # ['Harvey', 'Rose']\nThe syntax for a slice is: list[ &lt;start_idx&gt;, &lt;end_idx&gt; ], but end_idx is not included in the slice. And notice:\nprint( geographers[1:2] ) # ['Harvey']\nprint( geographers[1] )   #   Harvey\n\nIt’s really subtle, but notice that a slice always returns a list, even if it’s just a list containing one thing. So geographers[1]=='Harvey' but geographers[1:2]==['Harvey']. Not the same thing!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself",
    "href": "lectures/2.4-Lists.html#test-yourself",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat do you think this will produce?\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 2\nprint( geographers[ (i-3)**2-4:-1 ] )\nSee if you can work out in your head before typing it!"
  },
  {
    "objectID": "lectures/2.4-Lists.html#wheres-wally",
    "href": "lectures/2.4-Lists.html#wheres-wally",
    "title": "Lists",
    "section": "Where’s Wally?",
    "text": "Where’s Wally?\nlist.index(...) tells you where something can be found in a list:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index(\"Harvey\") # 1\ngeographers.index(\"Massey\") # 0\nCombining ideas that will become very useful later:\nprint(geographers[ geographers.index(\"Massey\") ])\nWhat do you think this prints? Why does it work at all?\n\nThis last example looks a little strange, but what if I had a separate list with first names, or Wikipedia links, or other information about these geographers? Because list.index(x) returns an integer we can use it as an index for accessing another list."
  },
  {
    "objectID": "lectures/2.4-Lists.html#wheres-wally-part-2",
    "href": "lectures/2.4-Lists.html#wheres-wally-part-2",
    "title": "Lists",
    "section": "Where’s Wally (Part 2)",
    "text": "Where’s Wally (Part 2)\nlist.index(...) has one flaw:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index('Batty')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: 'Batty' is not in list\nIf ‘throwing an error’ is overkill, then here’s another way:\nif 'Batty' in geographers:\n    print(\"Found Mike!\")\nelse:\n    print(\"Not a geographer!\")"
  },
  {
    "objectID": "lectures/2.4-Lists.html#sorting",
    "href": "lectures/2.4-Lists.html#sorting",
    "title": "Lists",
    "section": "Sorting",
    "text": "Sorting\nWe can sort lists in alpha-numerical order:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.sort()\nprint(geographers) # ['Harvey', 'Massey', 'Rose']\nAnd we can reverse-sort too:\ngeographers.sort(reverse=True)\nprint(geographers) # ['Rose', 'Massey', 'Harvey']"
  },
  {
    "objectID": "lectures/2.4-Lists.html#lists-are-mutable",
    "href": "lectures/2.4-Lists.html#lists-are-mutable",
    "title": "Lists",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\nMutable == “liable or subject to change or alteration”\nLet’s replace Rose with Jefferson1 in the list.\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers[2] = \"Jefferson\"\nprint(geographers) # ['Massey','Harvey','Jefferson']\nthe-women-cartographers-who-mapped-art-and-science-in-the-20th-century"
  },
  {
    "objectID": "lectures/2.4-Lists.html#addingremoving-items",
    "href": "lectures/2.4-Lists.html#addingremoving-items",
    "title": "Lists",
    "section": "Adding/Removing Items",
    "text": "Adding/Removing Items\nWhen we insert() items into, or pop() items out of, a list we normally need to specify the index.\ngeographers = [\"Massey\", \"Harvey\", \"Jefferson\"]\ngeographers.insert(0,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.insert(3,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'von Humboldt', 'Jefferson']\nAnd in ‘reverse’:\ngeographers.pop(3) # 'von Humboldt'\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\n\nNotice also that insert modifies the list and returns nothing, while pop modifies the list and returns the value that you ‘popped’."
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself-1",
    "href": "lectures/2.4-Lists.html#test-yourself-1",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nThere are two ways to remove David Harvey from the list of geographers without writing this:\ngeographers = ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.pop(2) # Do not use this answer!\n\nYou can adapt an example we saw earlier in ‘Finding Things’.\nYou can use Google to see if there are list operations we’ve not covered.\n\n\nHints: remove and del are both options for 2."
  },
  {
    "objectID": "lectures/2.4-Lists.html#concatenating",
    "href": "lectures/2.4-Lists.html#concatenating",
    "title": "Lists",
    "section": "Concatenating",
    "text": "Concatenating\nWe combine lists using addition:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = female_geographers + male_geographers\nprint(all_geographers)    # ['Rose', ..., 'Hägerstrand']\nprint(all_geographers[0]) # Rose"
  },
  {
    "objectID": "lectures/2.4-Lists.html#appending",
    "href": "lectures/2.4-Lists.html#appending",
    "title": "Lists",
    "section": "Appending",
    "text": "Appending\nNote that this is not the same!\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers   = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\n all_geographers.append(male_geographers)\nprint(all_geographers) # [['Rose',...], [..., 'Hägerstrand']]\nprint(all_geographers[0]) # ['Rose', ..., 'Jefferson']\nWhat do you think has happened here?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself-2",
    "href": "lectures/2.4-Lists.html#test-yourself-2",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nmale_geographers.append('Batty')\nprint(male_geographers)\nWhat do you think this will produce? And why do you think that append appears to do something different in these two examples?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#how-many-geographers-do-i-know",
    "href": "lectures/2.4-Lists.html#how-many-geographers-do-i-know",
    "title": "Lists",
    "section": "How many geographers do I know?",
    "text": "How many geographers do I know?\nlen(...) gives you the length of ‘countable’ things:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nlen(geographers) # 3\nBut…\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nprint( len(all_geographers) ) # 2"
  },
  {
    "objectID": "lectures/2.4-Lists.html#whos-on-the-list",
    "href": "lectures/2.4-Lists.html#whos-on-the-list",
    "title": "Lists",
    "section": "Who’s on the List?",
    "text": "Who’s on the List?\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers) # True\nprint(\"Batty\" in geographers)  # False\nBut…\ngeographers.index('Batty')\nis a ValueError that causes your Python code to fail.\n\nWhy might you choose one of these over the other?"
  },
  {
    "objectID": "lectures/2.4-Lists.html#test-yourself-3",
    "href": "lectures/2.4-Lists.html#test-yourself-3",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nHow would you change this code:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers)\nprint(\"Batty\" in geographers)\nSo that it prints:\nFalse\nTrue\nYou will have seen the answer to this in Code Camp, but you can also Google it†!\n\n\n† I’d suggest looking first at Stack Overflow answers in most cases."
  },
  {
    "objectID": "lectures/2.4-Lists.html#tuples-not-actually-a-list",
    "href": "lectures/2.4-Lists.html#tuples-not-actually-a-list",
    "title": "Lists",
    "section": "Tuples: Not Actually a List",
    "text": "Tuples: Not Actually a List\nBecause they come up a lot in geo-data, it’s worth knowing about tuples, which are basically immutable lists:\nt = (52.124021, -0.0012012)\nprint(type(t)) # &lt;class 'tuple'&gt;\nprint(t)       # (52.124021, -0.0012012)\nprint(t[0])    # 52.124021\nBut this…\nt[0] = 25.1203210\nwill throw an error:\n\nTypeError: ‘tuple’ object does not support item assignment"
  },
  {
    "objectID": "lectures/2.4-Lists.html#more-resources",
    "href": "lectures/2.4-Lists.html#more-resources",
    "title": "Lists",
    "section": "More Resources",
    "text": "More Resources\n\nLists in Python\nTuples in Python\nRange and lists\nSequence types"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#as-in",
    "href": "lectures/2.6-The_Command_Line.html#as-in",
    "title": "The Command Line",
    "section": "As in…",
    "text": "As in…\n\nWhy are you torturing me with this arcane knowledge?\nWhy do I need to do this when we have slick IDEs now?"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#the-answer",
    "href": "lectures/2.6-The_Command_Line.html#the-answer",
    "title": "The Command Line",
    "section": "The Answer?",
    "text": "The Answer?\nNo matter how long you try to avoid it, eventually you’ll find things that can only be solved (or that can be much more quickly solved) using the Command Line Interface (CLI).\nThings like:\n\nInteracting with git is actually easier on the Command Line.\nMaking the most of developer-oriented tools (e.g. docker, GDAL, proj4/6).\nPeeking and poking at (large) files efficiently…\nAutomating things that would be hard/annoying to do manually…\n\nA lot of this ties back to data and servers.\n\nTrue story: 25 years ago I used to process more than 40GB of compressed plain-text data every day from my Titanium PowerBook. But that’s because it was all running on a server in New Jersey while I was in Manhattan. Everything was done using the Command Line and SSH (secure shell).\nMore recently, processing OSM data for the entire UK was possible on my MacBook Air using GDAL and bash scripts but not possible using R/RStudio directly. Basically, the work took so long (&gt; 13 hours) that RStudio thought the script had died and tried to kill it."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#anatomy-of-a-command",
    "href": "lectures/2.6-The_Command_Line.html#anatomy-of-a-command",
    "title": "The Command Line",
    "section": "Anatomy of a Command",
    "text": "Anatomy of a Command\ncurl -L http://bit.ly/2vrUFKi | \n  head -3 |  \n  awk -F\",\" '{ print $2, $4, $6; }' &gt; results.txt\nThis command does four things in one ‘line’ on the CLI:\n\ncurl downloads the file and passes the contents to…\nhead which takes the first three rows and passes those to…\nawk which splits the rows on \",\" and takes the 2nd, 4th, and 6th fields and directs them into…\nA file called results.txt\n\n\nNote that results.txt is created if it doesn’t already exist, or overwritten if it does.\nIf you wanted to append to an existing file you would use &gt;&gt; instead of &gt;."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#interacting-with-files",
    "href": "lectures/2.6-The_Command_Line.html#interacting-with-files",
    "title": "The Command Line",
    "section": "Interacting with Files",
    "text": "Interacting with Files\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nls\nList\nls .\n\n\ncd\nChange Directory\ncd $HOME or cd ~\n\n\npwd\nPrint Working Directory\npwd\n\n\nmv\nRename/Move file a to b\nmv a.txt b.txt\n\n\nfind\nFind files matching some criteria\nfind . -name \"*.md\"\n\n\n\n\nNotice that most commands on the Command Line involve typing mnemonics (the shortest possible combination of letters that is unique memorable)."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#common-shortcuts",
    "href": "lectures/2.6-The_Command_Line.html#common-shortcuts",
    "title": "The Command Line",
    "section": "Common Shortcuts",
    "text": "Common Shortcuts\n\n\n\n\n\n\n\n\nShortcut\nMeans\nExample\n\n\n\n\n.\nThe current working directory\nls .\n\n\n..\nThe directory above the current working one\ncd ..\n\n\n~1\nThe current user’s home directory.\ncd ~\n\n\n/\nThe ‘root’ directory for the entire computer\nls /\n\n\n\"*\"\nA ‘wildcard’ meaning any number of characters in a filename\nfind . -name \"*.md\"\n\n\n\"?\"\nA ‘wildcard’ meaning one character in a filename\nfind . -name \"2.?-*.md\"\n\n\n\n\nThe main reason we care about all this is that all data is stored somewhere and all code executes somewhere. So we want a way to traverse the device efficiently when looking in directories, creating new files, writing different types of data to different places, and so forth. These shortcuts therefore crop up all over the place ‘in the wild’–if you don’t know what they’re telling you then you’ll wonder why your code doesn’t run or you can’t find the data you saved!\n\nThis may be easier to remember and write as cd $HOME, which does the same thing."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "href": "lectures/2.6-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "title": "The Command Line",
    "section": "A Simulated Walk Across My Laptop",
    "text": "A Simulated Walk Across My Laptop\ncd /\npwd\n&gt; /\nls\n&gt; Applications  Library  System  Users Volumes ...\ncd $HOME\npwd\n&gt; /Users/casa\nls\n&gt; Applications  Desktop  Dropbox  ...\ncd Dropbox\npwd\n&gt; /Users/casa/Dropbox\nls\n&gt; CASA  Lectures  Practicals ...\n\nModern computers (especially if you’ve grown up around iPhones and Android phones/tablets) are really good at hiding this fact, but that’s because people using phones or tablets really don’t want to be thinking about where their data is being stored, they just want to click save. But when you start coding then you need to start caring a lot more about where something is happening."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#finding-things-in-files",
    "href": "lectures/2.6-The_Command_Line.html#finding-things-in-files",
    "title": "The Command Line",
    "section": "Finding Things in Files",
    "text": "Finding Things in Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nless\nPeek at contents of a text file\nless file.txt\n\n\ngrep\nFind lines matching a ‘pattern’ in a file\ngrep 'pattern' file.txt\n\n\nhead\nPeek at first x rows of a text file\nhead -n 10 file.txt\n\n\ntail\nPeek at last x rows of a text file\ntail -n 10 file.txt\n\n\nwc\nCount things (rows, words, etc.)\nwc -l file.txt\n\n\nsed/awk\nComplicated, but powerful, things\nawk -F\",\" '{ print $1, $3; }' file.csv\n\n\n\n\nThe really crucial thing about all of these utilities is that they don’t load the entire file into memory. So you can ‘peek’ into a 15GB text file instantly without waiting four hours for it to load into memory (and then crash your machine). It’s kind of like the anti-Excel."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#time-to-escape",
    "href": "lectures/2.6-The_Command_Line.html#time-to-escape",
    "title": "The Command Line",
    "section": "Time to Escape!",
    "text": "Time to Escape!\nSome characters are ‘special’ and need to be escaped. You’ll encounter these both in the shell (a.k.a. command line) and in Python:\n\n\n\n\n\n\n\n\nEscape\nDoes\nExample\n\n\n\n\n\\\nAllows spaces in file names\nless My\\ File\\ with\\ Spaces.txt\n\n\n\\t\nCreates/matches a tab character\n\\tThe start of a paragraph...\n\n\n\\n\nCreates/matches a newline character\nThe end of a row/para...\\n\n\n\n\\r\nCreates/matches a carriange return\nThe end of a row/para...\\r\\n\n\n\n\\$\nLiteral dollar sign (since $ often marks a variable)\nIt costs \\$1,000,000\n\n\n\\!\nLiteral exclamation mark (since ! can mean a number of things)\nDon't forget me\\!\n\n\n\nThis also becomes relevant when you’re dealing with quotes:\n\"\"This is a problem,\" she said.\"\nvs. \n\"\\\"This is a problem,\\\" she said.\"\n\nThe carriage return is only ever encountered on files that have been opened on Windows machines."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#compressingdecompressing-files",
    "href": "lectures/2.6-The_Command_Line.html#compressingdecompressing-files",
    "title": "The Command Line",
    "section": "Compressing/Decompressing Files",
    "text": "Compressing/Decompressing Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\ngzip\nCompress/Decompress files\ngzip file.txt\n\n\ngunzip\nDecompress files\ngunzip file.txt.gz1\n\n\n\nThis can also be done using ‘switches’ passed to gzip: gzip -cd (where -d means ‘decompress’)."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#chaining-commands",
    "href": "lectures/2.6-The_Command_Line.html#chaining-commands",
    "title": "The Command Line",
    "section": "Chaining Commands",
    "text": "Chaining Commands\nThe CLI becomes much useful with command chaining:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\"\nThe ‘pipe’ (|) takes output from command and ‘pipes’ (aka. passes) it to another.\n\nThis will give you an ‘answer’ much, much, much faster than trying to open the whole file in, say, Excel, Numbers, or even Python."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#redirecting-output",
    "href": "lectures/2.6-The_Command_Line.html#redirecting-output",
    "title": "The Command Line",
    "section": "Redirecting Output",
    "text": "Redirecting Output\nWe can redirect outputs in to new files with &gt;, and inputs out of existing files using &lt;:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\" &gt; matches.txt\nSo the output from the previous commands goes into matches.txt as plain-text. The reverse &lt; is only used in very special circumstances so you probably won’t encounter it very often."
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#a-complex-example",
    "href": "lectures/2.6-The_Command_Line.html#a-complex-example",
    "title": "The Command Line",
    "section": "A (Complex) Example",
    "text": "A (Complex) Example\nI do not expect you to understand this, but I do want you to understand why this is important:\ndocker run -v conda:/home/jovyan/work --rm ${DOCKER_NM} start.sh \\\n   conda env export -n ${ENV_NM} | sed '1d;$d' | sed '$d' \\\n   | perl -p -e 's/^([^=]+=)([^=]+)=.+$/$1$2/m' \\\n   | grep -Ev '\\- _|cpp|backports|\\- lib|\\- tk|\\- xorg' &gt; conda/environment_py.yml\n\nThis is how I generated the YAML file used by Anaconda Python installers: it is running a command on a virtual machine, collecting the output, filtering out lines by both row number and textual pattern, and directing this all in the environment_py.yml file. This can be run as part of my ‘build’ of the programming environment. It’s all automated!"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#getting-help",
    "href": "lectures/2.6-The_Command_Line.html#getting-help",
    "title": "The Command Line",
    "section": "Getting Help",
    "text": "Getting Help\nThe Software Carpentry people have a whole set of lessons around working with ‘the shell’ (a.k.a. Command Line) that might help you.\n\nThe UNIX Shell.\nMIT’s ‘Missing Semester’ on Vim\n\nIndeed all of MIT’s Missing Semester content could be useful!"
  },
  {
    "objectID": "lectures/2.6-The_Command_Line.html#useful-videos",
    "href": "lectures/2.6-The_Command_Line.html#useful-videos",
    "title": "The Command Line",
    "section": "Useful Videos",
    "text": "Useful Videos\n\n\nThe Shell/Terminal in general:\n\nAbsolute BEGINNER Guide to the Mac OS Terminal\nLinux Bash Shell for Beginners: Tutorial 1\nBeginner’s Guide to the Bash Terminal\nShell Novice\nHow to use the Command Line\n\n\nSome specific commands:\n\nCat\nGzip/Tar (also a good point about spaces in a file name!)\nGrep\nFind\n\n\n\nAnd lots more here on using the file system and shell commands"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#so-key---value",
    "href": "lectures/3.1-Dictionaries.html#so-key---value",
    "title": "Dictionaries",
    "section": "So: Key -> Value",
    "text": "So: Key -&gt; Value\nThe key can be almost anything that is immutable (aka. hashable). So these are all ‘legit’:\nlookup[1]             # Int\nlookup(52.1)          # Float\nlookup['1']           # String\nlookup['Jon Reades']  # String\nk = 'Jon Reades'\nlookup[k]             # String variable\nlookup[(52.1, -0.04)] # Tuple\nBut this is not:\nlookup[['Jon','Reades']] # Error, unhashable type\nThat’s because a list is not immutable.\n\nAgain, just like a real dictionary: you don’t have multiple entries for ‘dog’, otherwise the dictionary wouldn’t work. You might have multiple definitions: which is to say, the key might return multiple values."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "href": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "title": "Dictionaries",
    "section": "Deliberately Similar",
    "text": "Deliberately Similar\nNotice the differences when creating them, and the absence of difference when accessing them.\n\n\nList\ncities = [\n  'San Francisco',\n  'London',\n  'Paris',\n  'Beijing']\n  \n# Prints London\nprint(cities[2]) \n\nDict\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\n# Prints pop of London\nprint(cities['London'])\n\n\n\nSo why might we prefer the dictionary?\n\nDicts are created using: d = { key: value, key: value }\nDicts are accessed using: d[key]\n\nSo the only difference between lists and dicts is: [...] and {...} when they are created.\nOver the next couple of weeks we’ll see ways that you can store more information in a list and also why lists are sometimes more powerful than you realise… if you can think about your data in an entirely new way. But for simple key/value stuff it’s hard to beat a dictionary!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "href": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "title": "Dictionaries",
    "section": "It’s All About Access",
    "text": "It’s All About Access\nSpecifically: do we need sequential or random access?\n\n\nList\n\n\n\nindex\nvalue\n\n\n\n\n0\nSan Francisco\n\n\n1\nLondon\n\n\n2\nParis\n\n\n3\nBeijing\n\n\n\n\nDict\n\n\n\nkey\nvalue\n\n\n\n\nSan Francisco\n837442\n\n\nLondon\n8673713\n\n\nParis\n2229621\n\n\nBeijing\n21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values",
    "href": "lectures/3.1-Dictionaries.html#getting-values",
    "title": "Dictionaries",
    "section": "Getting Values",
    "text": "Getting Values\nThere are two ways to retrieve values from a dictionary:\n\ncities['Beijing']\ncities.get('Beijing')\n\nWhy have two? Consider:\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nprint(cities['Sao Paulo'])     # Throws KeyError\nprint(cities.get('Sao Paulo')) # Returns None\nprint(cities.get('Sao Paulo','No Data')) # Returns 'No Data'\n\nThe first triggers an error, the second returns None. Errors can be unfriendly: do you want your entire Python program to fail because a single city is missing, or would you rather than it did something a little more sensible such as… skipping the row or returning a sensible default?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "href": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "title": "Dictionaries",
    "section": "Getting Values (cont’d)",
    "text": "Getting Values (cont’d)\nIf we want to think about whether a value is in the dictionary (as opposed to just retrieving it) then notice these options:\nc = cities.get('Sao Paulo')\nif not c:\n  print(\"Sorry, no city by that name.\")\n\nif 'Beijing' in cities:\n  print(\"Found Beijing!\")\n\nThe first example works because cities.get returns None, which is the same as ‘undefined’ for Python. So we can use ‘not’ to imply ‘if c is not defined then do something…’\nThe second example works because we are implicitly treating the keys in the cities dictionary as a list and looking to see if Beijing is one of the values in that list.\nPython often benefits and suffers from TMTOWTDI (There’s More Than One Way To Do It): think of these as being different ways to say the same thing, but depending on where you want to put the emphasis you would choose one or the other."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#setting-values",
    "href": "lectures/3.1-Dictionaries.html#setting-values",
    "title": "Dictionaries",
    "section": "Setting Values",
    "text": "Setting Values\nIt’s the same process to update an existing value or create a new one:\ncities = {}  # Empty dictionary\ncities['Beijing'] = 21716620    # Sets key-&gt;value\ncities['Toronto'] = 2930000     # Sets key-&gt;value\n\nprint(cities['Toronto'])        # Prints 2930000\ndel cities['Toronto']           # Deletes Toronto key (and value)\ncities.pop('Toronto','Default') # Prints 'Default' b/c key not found\nprint(cities)\nThis last command outputs:\n{'Beijing': '0.17'16620}"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#iterating",
    "href": "lectures/3.1-Dictionaries.html#iterating",
    "title": "Dictionaries",
    "section": "Iterating",
    "text": "Iterating\nSimilar to iterating over lists but…\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nfor c in cities:\n  print(c)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nOne really important point to note: here, the cities are printed out in the same order that they were added to the dictionary, but that is not guaranteed! Unlike lists, dictionaries are unordered.\nAlso, how would we print out the population of each city?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#section",
    "href": "lectures/3.1-Dictionaries.html#section",
    "title": "Dictionaries",
    "section": "",
    "text": "Keys\nfor k in cities.keys():\n  print(k)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nValues\nfor v in cities.values():\n  print(v)\nPrints:\n837442\n8673713\n2229621\n21716620\n\nBoth\nfor k,v in cities.items():\n  print(f\"{k} -&gt; {v}\")\nPrints:\nSan Francisco -&gt; 837442\nLondon -&gt; 8673713\nParis -&gt; 837442\nBeijing -&gt; 21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "href": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "title": "Dictionaries",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\n\nDictionaries are unordered key/value pairs. There is no guarantee that things come out in the same order they went in! They complement ordered lists, they don’t replace them!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#a-final-note",
    "href": "lectures/3.1-Dictionaries.html#a-final-note",
    "title": "Dictionaries",
    "section": "A Final Note!",
    "text": "A Final Note!\nValues can be almost anything, including a dictionary or list! This opens up some interesting possibilities:\n\n\ncities = {\n  'San Francisco': \n    [37.77, -122.43, 'SFO']\n}\n\ncities = {\n  'San Francisco': {\n    'lat': 37.77,\n    'lon': -122.43,\n    'airport':'SFO'}\n}\nprint(cities['San Francisco']['lat'])\n\n\nSpoiler: you’re going to encounter this kind of thing a lot.\n\nWhat is this starting to look like? This is basically what JSON is."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#resources",
    "href": "lectures/3.1-Dictionaries.html#resources",
    "title": "Dictionaries",
    "section": "Resources",
    "text": "Resources\n\nDictionaries and sets\nComprehensions\nThe Complete Guide to Dictionaries (by a CASA alum)"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nConsider how these two data structures differ:\ncities = [\n  {'name': 'London', 'loc': [51.5072, 0.1275], 'tz': +0}, \n  {'name': 'New York', 'loc': [40.7127, 74.0059], 'tz': -5}, \n  {'name': 'Tokyo', 'loc': [35.6833, 139.6833], 'tz': +8}\n]\nOr:\ncities = {\n  'London': {'loc': [51.5072, 0.1275], 'tz': +0}, \n  'New York': {'loc': [40.7127, 74.0059], 'tz': -5}, \n  'Tokyo': {'loc': [35.6833, 139.6833], 'tz': +8}\n}\n\nWhy don’t you copy the code and then see how to access different fields/values? What might be the pros and cons of each?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#implications",
    "href": "lectures/3.3-DOLs_to_Data.html#implications",
    "title": "Data Structures",
    "section": "Implications",
    "text": "Implications\n\nSo we can mix and match dictionaries and lists in whatever way we need to store… ‘data’. The question is then: what’s the right way to store our data?\n\n\nAnswer: the way that makes the most sense to a human while also being the most robust for coding."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nHow do these data structures differ?\n\n\nOption 1\nds1 = [\n  ['lat','lon','name','tz'],\n  [51.51,0.13,'London',+0],\n  [40.71,74.01,'New York',-5],\n  [35.69,139.68,'Tokyo',+8]\n]\n\nOption 2\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\n\n\nTo understand why I’m asking this question, here are two example questions I’d like you to try to answer:\n\nWhat’s the average latitude of these three cities?\nWhat’s the time zone of Tokyo?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "href": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "title": "Data Structures",
    "section": "Thinking it Through",
    "text": "Thinking it Through\nWhy does this work for both computers and people?\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nWe are doing away with the idea that the order of columns matters (humans don’t care that a city’s name is in the first column, and a city’s latitude in the second). We just want to find the column. But because we have a dictionary-of-lists we can ensure that the row order is preserved. Let’s see this in action."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#examples",
    "href": "lectures/3.3-DOLs_to_Data.html#examples",
    "title": "Data Structures",
    "section": "Examples",
    "text": "Examples\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nprint(ds2['name'][0]) # London\nprint(ds2['lat'][0])  # 51.51\nprint(ds2['tz'][0])   # 0\nSo 0 always returns information about London, and 2 always returns information about Tokyo. But it’s also easy to ask for the latitude (ds2['lat'][0]) or time zone (ds2['tz'][0]) value once you know that 0 is London!\n\nBut there’s another advantage that’s not quite so obvious: for the computer because everything of type ‘lat’ is a float, everything of type ‘tz’ is an integer, and everything of type ‘name’ is a string, it’s a lot easier to work with each column as data."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "href": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "title": "Data Structures",
    "section": "How is that easier???",
    "text": "How is that easier???\nRemember that we can use any immutable ‘thing’ as a key. This means…\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ncity_nm = 'Tokyo'\ncity_idx = ds2['name'].index(city_nm)\n\nprint(f\"The time zone of {city_nm} is {ds2['tz'][city_idx]}\")\nWe can re-write this into a single line as:\ncity_nm = 'New York'\nprint(f\"The time zone of {city_nm} is {ds2['tz'][ ds2['name'].index(city_nm)]}\")\n\nThis achieves several useful things:\n\nIt is fast: faster than iterating over a list-of-lists or dictionary-of-dictionaries. In other words, there is no iteration at all!\nAll data in a list is of the same type so we can easily add checks to make sure that it’s valid.\nWe can also easily calculate an average/max/min/median and so on (as we’ll see later) without even having to look at any other columns!\nWe can add more columns instantly and the process of finding something is just as fast as it is now. And adding more rows doesn’t make it much slower either!\n\nAlso, notice how in these two examples we don’t try to write the second example in one go: first, we work it out as a set of steps: how do we figure out what ‘row’ (position in the list) Tokyo is in? Now that we’ve got that, how do we retrieve the time zone value for Tokyo? We know that code works, now let’s do variable substitution, as we would if we were doing maths: we can replace the city_idx in the time zone lookup with ds2['name'].index('Tokyo')."
  },
  {
    "objectID": "lectures/3.5-Packages.html#popular-packages",
    "href": "lectures/3.5-Packages.html#popular-packages",
    "title": "Packages",
    "section": "Popular Packages",
    "text": "Popular Packages\n\n\n\nPackage\nDoes\n\n\n\n\nos\nOperating System stuff (paths, processes, etc.)\n\n\ncsv\nReading and Writing CSV files.\n\n\nmath\nMaths constants and functions.\n\n\nstatistics\nSimple statistical tests & measures.\n\n\nurllib\nURL Reading (e.g. web pages).\n\n\nnumpy\nNumerical Python (scientific computing).\n\n\nscipy\nScientific Python (more scientific computing)\n\n\nsklearn\nMachine learning, clustering, etc."
  },
  {
    "objectID": "lectures/3.5-Packages.html#importing-a-package",
    "href": "lectures/3.5-Packages.html#importing-a-package",
    "title": "Packages",
    "section": "Importing a Package",
    "text": "Importing a Package\nIf a package is installed, then it’s as simple as:\nimport &lt;packagename&gt;\nYou normally do this at the start of a program so that it’s easy to see what the program requires to run:\nimport math\nprint(math.pi) # Prints 3.141592653589793"
  },
  {
    "objectID": "lectures/3.5-Packages.html#what-can-a-package-do",
    "href": "lectures/3.5-Packages.html#what-can-a-package-do",
    "title": "Packages",
    "section": "What Can a Package Do?",
    "text": "What Can a Package Do?\nThere are many ways to find this out:\n\nRead the documentation\nSearch Google, and\nSearch StackOverflow\n\nThere’s even a web site python.readthedocs.io.\nBut we can also ask the package:\nimport math\ndir(math)\n# ['__doc__', '__file__', '__name__', '__package__', ..., \n# 'log', 'log10', 'log1p', 'modf', 'pi', ...]\nhelp(math.log10)\n\nSo, remember what I said in the talk about functions: that the numbers and total variables in the function aren’t the same as the ones outside the function? I said the answer had to do with something called the namespace. Here is the math namespace."
  },
  {
    "objectID": "lectures/3.5-Packages.html#so",
    "href": "lectures/3.5-Packages.html#so",
    "title": "Packages",
    "section": "So…",
    "text": "So…\n\ndir(&lt;package name&gt;) lists all ‘things’ that &lt;package&gt; contains.\nBy convention, things that start with __ are ‘private’ (you shouldn’t change them) and things that start and end with __ are metadata (e.g. __version__).\nEverything else you can interrogate with help(&lt;package name&gt;.&lt;thing in package&gt;).\n\nWith help(math.log10) you get an answer like this:\nHelp on built-in function log10 in module math:\n\nlog10(x, /)\n    Return the base 10 logarithm of x.\nWith help(math.pi) you get an answer Help on float object…\n\nThis tells you that pi is a float, it doesn’t tell you what Pi is (an irrational number). So here’s another case where the computer gives you a technically correct but not always helpful answer. In the context of the math package, Pi is a float constant."
  },
  {
    "objectID": "lectures/3.5-Packages.html#why-namespaces-matter",
    "href": "lectures/3.5-Packages.html#why-namespaces-matter",
    "title": "Packages",
    "section": "Why Namespaces Matter",
    "text": "Why Namespaces Matter\nConsider this:\nimport math\npi = 4\nprint(math.pi)\nprint(pi)\nSo math.pi and pi are not the same variable!\n\nThe latter is implicitly main.pi but Python doesn’t have a ‘main’ programme namespace. Everything is ‘main’ unless you put it in a package."
  },
  {
    "objectID": "lectures/3.5-Packages.html#more-laziness-aliases",
    "href": "lectures/3.5-Packages.html#more-laziness-aliases",
    "title": "Packages",
    "section": "More Laziness: Aliases",
    "text": "More Laziness: Aliases\nProgrammers hate typing more than they have to:\nimport math\nr = 5\narea = math.pi * r**2\nln = math.log(area)\nprint(ln)\nSo we can use an alias instead:\nimport math as m\nr = 5\narea = m.pi * r**2\nln = m.log(area)\nprint(ln)\nYou will see this used a lot with more complex libraries like Pandas (pd), Geopandas (gpd), and PySAL (ps)."
  },
  {
    "objectID": "lectures/3.5-Packages.html#importing-part-of-a-package",
    "href": "lectures/3.5-Packages.html#importing-part-of-a-package",
    "title": "Packages",
    "section": "Importing Part of a Package",
    "text": "Importing Part of a Package\nSometimes even that is too much typing… or sometimes we only really want one or two things from a much larger package. In that case we can select these specifically:\nfrom math import pi, log10\nprint(pi)\nhelp(log10)\nThis import pi and log10 from math into the ‘main’ namespace."
  },
  {
    "objectID": "lectures/3.5-Packages.html#gotcha",
    "href": "lectures/3.5-Packages.html#gotcha",
    "title": "Packages",
    "section": "Gotcha!",
    "text": "Gotcha!\nNotice the subtle differences here:\n\n\nApproach 1\npi = 3.1415\nprint(pi)      # 3.1415\n\nimport math as m\nprint(m.pi)    # 3.141592...\nprint(pi)      # 3.1415\nprint(math.pi) # Error!\n\nApproach 2\npi = 3.1415\nprint(pi) # 3.1415\n\nfrom math import pi\nprint(pi)      # 3.141592...\nprint(m.pi)    # Error!\nprint(math.pi) # Error!"
  },
  {
    "objectID": "lectures/3.5-Packages.html#packages-make-your-life-easier",
    "href": "lectures/3.5-Packages.html#packages-make-your-life-easier",
    "title": "Packages",
    "section": "Packages Make Your Life Easier",
    "text": "Packages Make Your Life Easier"
  },
  {
    "objectID": "lectures/3.5-Packages.html#resources",
    "href": "lectures/3.5-Packages.html#resources",
    "title": "Packages",
    "section": "Resources",
    "text": "Resources\nA bit of a mish-mash of different explanations:\n\n\n\nLearnPython.org\nRealPython: Namespaces and Scope\nReal Python: Modules and Packages\nProgramiz\nPythonCourse.eu\nTutorialsTeacher.com\nTutorialsPoint.com\n\n\n\nHow to Make a Package in Python\nHow to Create Your First Python Package\nCreate Python Packages for Your Python Code [This is more for distributing your own packages via Pip]"
  },
  {
    "objectID": "lectures/4.2-Classes.html#whats-an-object",
    "href": "lectures/4.2-Classes.html#whats-an-object",
    "title": "Classes",
    "section": "What’s an Object?",
    "text": "What’s an Object?\nObjects are instantiated versions of classes:\n\n\"hello world\" is an instance of a string, and\n['A','B',1,3] is an instance of a list.\n\nThe class is your recipe, the object is your 🍕…"
  },
  {
    "objectID": "lectures/4.2-Classes.html#really-like-a-pizza",
    "href": "lectures/4.2-Classes.html#really-like-a-pizza",
    "title": "Classes",
    "section": "Really… Like a Pizza!",
    "text": "Really… Like a Pizza!\nclass pizza(object):\n  base = 'sourdough'\n  \n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\n    \n  def add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\n  \n  def get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/4.2-Classes.html#class-definition",
    "href": "lectures/4.2-Classes.html#class-definition",
    "title": "Classes",
    "section": "Class Definition",
    "text": "Class Definition\nclass pizza(object):\n\n    base = 'sourdough'\n    ...\nFollows the pattern: class &lt;name&gt;(&lt;parent class&gt;).\nYou can find many examples in: /opt/conda/envs/sds2020/lib/python3.7/site-packages (Docker)."
  },
  {
    "objectID": "lectures/4.2-Classes.html#the-constructor",
    "href": "lectures/4.2-Classes.html#the-constructor",
    "title": "Classes",
    "section": "The Constructor",
    "text": "The Constructor\n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce    = sauce\n    self.cheese   = cheese\nFollows the pattern: def __init__(self, &lt;params&gt;)\n\n\nNotice also the namespace: the parameters sauce and cheese are the same as the instance variables self.sauce and self.cheese because they occupy different namespaces."
  },
  {
    "objectID": "lectures/4.2-Classes.html#adding-toppings",
    "href": "lectures/4.2-Classes.html#adding-toppings",
    "title": "Classes",
    "section": "Adding Toppings",
    "text": "Adding Toppings\ndef add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\nFollows the pattern: def &lt;function&gt;(self, &lt;params&gt;):"
  },
  {
    "objectID": "lectures/4.2-Classes.html#getting-the-pizza",
    "href": "lectures/4.2-Classes.html#getting-the-pizza",
    "title": "Classes",
    "section": "Getting the Pizza",
    "text": "Getting the Pizza\ndef get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/4.2-Classes.html#pizza-in-action",
    "href": "lectures/4.2-Classes.html#pizza-in-action",
    "title": "Classes",
    "section": "Pizza in Action",
    "text": "Pizza in Action\np = pizza(sauce='white')\np.add_topping('peppers')\np.add_topping('chillis')\np.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chillis']"
  },
  {
    "objectID": "lectures/4.2-Classes.html#check-it-out",
    "href": "lectures/4.2-Classes.html#check-it-out",
    "title": "Classes",
    "section": "Check it Out",
    "text": "Check it Out\np1 = pizza(sauce='white')\np1.add_topping('peppers')\np1.add_topping('chilis')\n\np2 = pizza()\np2.base = \"Plain old base\"\np2.add_topping('pineapple')\np2.add_topping('ham')\n\np1.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()\n&gt; ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']"
  },
  {
    "objectID": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza",
    "href": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza",
    "text": "Recap: How to Make a Pizza\nA class is defined by:\nclass &lt;name&gt;(&lt;parent class):\n  ...\nA class is initialised by:\n  def __init__(self, &lt;any_parameters&gt;):\n    ...\nAll methods have to have this:\n  def &lt;method&gt;(self, &lt;any_parameters&gt;):\n    ..."
  },
  {
    "objectID": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "href": "lectures/4.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza (cont’d)",
    "text": "Recap: How to Make a Pizza (cont’d)\nThis is an instance variable:\n  self.&lt;var&gt; = &lt;something&gt;\nThis is a class variable (in the class definition):\n  &lt;var&gt; = &lt;something&gt;"
  },
  {
    "objectID": "lectures/4.2-Classes.html#resources",
    "href": "lectures/4.2-Classes.html#resources",
    "title": "Classes",
    "section": "Resources",
    "text": "Resources\n\n\n\nClasses\nObjects\nBasic class definition\nInstance methods and attributes\nChecking instance types\nClass methods and members\n\n\n\nCreating a class\nConstructing an object\nClass methods\nClass vs Instance Variables\nObject data\nInheritance\nAn Introduction to Object-Oriented Programming (by a CASA alum)"
  },
  {
    "objectID": "lectures/4.4-Errors.html#helpfully",
    "href": "lectures/4.4-Errors.html#helpfully",
    "title": "Errors",
    "section": "Helpfully…",
    "text": "Helpfully…\nPython will always try to tell you what it thinks went wrong: “I didn’t understand what you meant by this…” or “I’m sorry, I can’t let you do that Dave…”\nThe challenges are:\n\nPython tends to give you a lot of information about the error: this can be very helpful for programmers dealing with complex problems and totally overwhelming for beginners.\nThat what Python thinks the problem is doesn’t always line up with where the problem actually is. In cases of syntax, for instance, the problem could be an unclosed parenthesis three lines earlier!"
  },
  {
    "objectID": "lectures/4.4-Errors.html#challenge-1",
    "href": "lectures/4.4-Errors.html#challenge-1",
    "title": "Errors",
    "section": "Challenge 1",
    "text": "Challenge 1\nThat the ‘error’ isn’t always the error…\ntotal = 0\nprint(\"About to start loop\"\nfor i in range(1,10):\n  total += i\nprint(total)\nThis outputs:\nprint(\"About to start loop\"\n... for i in range(1,10):\n  File \"&lt;stdin&gt;\", line 2\n    for i in range(1,10):\n                        ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "lectures/4.4-Errors.html#errors-have-types",
    "href": "lectures/4.4-Errors.html#errors-have-types",
    "title": "Errors",
    "section": "Errors Have Types",
    "text": "Errors Have Types\nIn the same way that variables have types, so do errors:\n\nModuleNotFoundError\nIndexError\nKeyError\nOSError\n…\n\nWe can add our own messages:\nraise Exception(\"Sorry, I can't let you do that, Dave.\")\n\n\nWhy might different types of errors be useful?\nWe might reasonably want to distinguish between errors that we could reasonably expect or that are not serious, from those that we did not expect or that call the results of the program into question.\n\n\nLots more built-in error types in the Python documentation and imported packages will provide their own as well."
  },
  {
    "objectID": "lectures/4.4-Errors.html#custom-errors",
    "href": "lectures/4.4-Errors.html#custom-errors",
    "title": "Errors",
    "section": "Custom Errors",
    "text": "Custom Errors\nWe can create our own types (classes) of error:\nclass CustomError(Exception):\n  pass # We do nothing except create a new type\nThis can then be triggered with:\nraise CustomError(\"Our custom error\")\nAnd (very importantly) this can be caught with:\nexcept CustomError: \n  #... do something with CustomError ...\nThis means that exceptions could accept custom arguments, perform tidying-up or rollback operations, etc.\n\nSome of the intricacies of errors can seem quite confusing. What’s the point of having a finally (which we’ll get to in a moment), for instance? Well, if your application is connected to a database then finally gives your application a chance to disconnect cleanly (freeing up resources for the database) and even to rollback incomplete changes (e.g. a new user whose details were only partially inputted when the application crashed)."
  },
  {
    "objectID": "lectures/4.4-Errors.html#so-errors-can-be-trapped",
    "href": "lectures/4.4-Errors.html#so-errors-can-be-trapped",
    "title": "Errors",
    "section": "So Errors can be Trapped",
    "text": "So Errors can be Trapped\nPython calls errors exceptions, so this leads to:\ntry:\n  #... some code that might fail...\nexcept &lt;Named_Error_Type&gt;:\n  #... what do it if it fails for a specific reason...\nexcept:\n  #... what to do if it fails for any other reason...\nfinally:\n  #... always do this, even if it fails...\nYou can use any or all of these together: you can have multiple named excepts to handle different types of errors from a single block of code; you do not have to have a catch-all except or a finally.\n\nSo it makes sense to think: “Well, let’s try this and see what happens. If we have a problem of this type then it’s not serious and we should carry on. But if we have a problem that type then we need to stop what we’re doing right away.”"
  },
  {
    "objectID": "lectures/4.4-Errors.html#trapping-errors",
    "href": "lectures/4.4-Errors.html#trapping-errors",
    "title": "Errors",
    "section": "Trapping Errors",
    "text": "Trapping Errors\nThis code fails:\nx,y = 10,0\nprint(x/y)\nAnd it generates this error:\n&gt; Traceback (most recent call last):\n&gt;   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n&gt; ZeroDivisionError: division by zero"
  },
  {
    "objectID": "lectures/4.4-Errors.html#trapping-errors-contd",
    "href": "lectures/4.4-Errors.html#trapping-errors-contd",
    "title": "Errors",
    "section": "Trapping Errors (cont’d)",
    "text": "Trapping Errors (cont’d)\nBut if you ‘trap’ the error using except then:\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\nexcept:\n  print(\"Something has gone very wrong.\")\nfinally: \n  print(\"Division is fun!\")\nThis will print\n&gt; You can't divide by zero!\n&gt; Division is fun!\n\n\nNote: if we need to access the actual exception: except ZeroDivisionError as e:"
  },
  {
    "objectID": "lectures/4.4-Errors.html#raising-hell",
    "href": "lectures/4.4-Errors.html#raising-hell",
    "title": "Errors",
    "section": "Raising Hell",
    "text": "Raising Hell\nYou can trigger your own exceptions using raise.\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")"
  },
  {
    "objectID": "lectures/4.4-Errors.html#understanding-multiple-errors",
    "href": "lectures/4.4-Errors.html#understanding-multiple-errors",
    "title": "Errors",
    "section": "Understanding Multiple Errors",
    "text": "Understanding Multiple Errors\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")\n\nThe code we try triggers the ZeroDivisionError block.\nThis prints \"You can't divide by zero!\"\nWe then raise a new exception that is not caught.\nThe finally code executes because it always does before Python exits.\nPython exits with the message from our newly raised Exception.\n\nThus: ‘During handling of above (ZeroDivisionError) another exception (our Exception) occurred…’"
  },
  {
    "objectID": "lectures/4.4-Errors.html#test-based-development",
    "href": "lectures/4.4-Errors.html#test-based-development",
    "title": "Errors",
    "section": "Test-Based Development",
    "text": "Test-Based Development\nWe can actually think of exceptions as a way to develop our code.\nHere’s some ‘pseudo-code’:\n# Testing the 'addition' operator\ntest(1+1, 2)           # Should equal 2\ntest(1+'1', TypeError) # Should equal TypeError\ntest('1'+'1', '11')    # Should equal '11'\ntest(-1+1, 0)          # Should equal 0 \nOur test(A,B) function takes an input (A) and the expected output (B) and then compares them. The test returns True if A==B and False otherwise."
  },
  {
    "objectID": "lectures/4.4-Errors.html#unit-tests",
    "href": "lectures/4.4-Errors.html#unit-tests",
    "title": "Errors",
    "section": "Unit Tests",
    "text": "Unit Tests\nEach test is a Unit Test because it tests one thing and one thing only. So if you had three functions to ‘do stuff’ then you’d need at least three unit tests.\nA Unit Test may be composed of one or more assertions. Our pseudo-code on the previous slide contained 4 assertions.\nA Unit Test does not mean that your code is correct or will perform properly under all circumstances. It means that your code returns the expected value for a specified input.\nPython considers this approach so important that it’s built in."
  },
  {
    "objectID": "lectures/4.4-Errors.html#approach-1",
    "href": "lectures/4.4-Errors.html#approach-1",
    "title": "Errors",
    "section": "Approach 1",
    "text": "Approach 1\nThis is an explict assertion to test fun:\nimport unittest\n\ndef fun(x):\n  return x + 1\n\nclass MyTest(unittest.TestCase):\n  def test(self):\n    self.assertEqual(fun(3), 4)\n    print(\"Assertion 1 passed.\")\n    self.assertEqual(fun(3), 5)\n    print(\"Assertion 2 passed.\")\n\nm = MyTest()\nm.test()\nThe critical output is:\nAssertionError: 4 != 5"
  },
  {
    "objectID": "lectures/4.4-Errors.html#approach-2",
    "href": "lectures/4.4-Errors.html#approach-2",
    "title": "Errors",
    "section": "Approach 2",
    "text": "Approach 2\nThis approach uses the ‘docstring’ (the bits between \"\"\") to test the results of the function. This is intended to encourage good documentation of functions using examples:\ndef square(x):\n    \"\"\"Return the square of x.\n\n    &gt;&gt;&gt; square(2)\n    4\n    &gt;&gt;&gt; square(-2)\n    4\n    &gt;&gt;&gt; square(-1)\n    2\n    \"\"\"\n    return x * x\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()\n\n\nNotice the __name__ (what could that possibly mean???) is tested to see if it’s '__main__'. The doctest code only runs when this code is executed in '__main__'."
  },
  {
    "objectID": "lectures/4.4-Errors.html#collaboration-continuous-integration",
    "href": "lectures/4.4-Errors.html#collaboration-continuous-integration",
    "title": "Errors",
    "section": "Collaboration & Continuous Integration",
    "text": "Collaboration & Continuous Integration\nThe Unit Test approach is often used on collaborative projects, especially in the Open Source world. PySAL, for instance, asks for unit tests with every new feature or integration.\nThe running of all tests for multiple components is called ‘integration testing’.\nA commit, merge, or pull on GitHub can trigger the unit testing process for the entire software ‘stack’. This is known as Continuous Integration because you are always checking that the code works as expected, rather than leaving testing to the end.\n\nThis is heavily used by PySAL and other robust FOSS projects since TravisCI is free for FOSS projects!"
  },
  {
    "objectID": "lectures/4.4-Errors.html#resources",
    "href": "lectures/4.4-Errors.html#resources",
    "title": "Errors",
    "section": "Resources",
    "text": "Resources\n\n\n\nHandling exceptions\nReporting errors\nPython Custom Exceptions\nWriting and Using Custom Exceptions in Python\nPython Documentation\nHow to Define Custom Exception Classes\n\n\n\nUnit Testing in Python\nUnderstanding Unit Testing\nTesting Your Code\nGetting Started with Testing in Python\nPython’s unittest Library\nVideo: Unit Testing Your Code"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#reproducibility-good-or-bad",
    "href": "lectures/5.2-Randomness.html#reproducibility-good-or-bad",
    "title": "Randomness",
    "section": "Reproducibility: Good or Bad?",
    "text": "Reproducibility: Good or Bad?\nDepends on the problem:\n\nBanking and encryption?\nSampling and testing?\nReproducing research/documentation?\n\n\nOK, technically, even encryption needs to be reproducible to allow for decryption, but you sure don’t want it to be easy."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#not-very-good-encryption",
    "href": "lectures/5.2-Randomness.html#not-very-good-encryption",
    "title": "Randomness",
    "section": "Not Very Good Encryption",
    "text": "Not Very Good Encryption\n\n\n\nCyphertext\nOutput\n\n\n\n\nROT0\nTo be or not to be, That is the question\n\n\nROT1\nUp cf ps opu up cf, Uibu jt uif rvftujpo\n\n\nROT2\nVq dg qt pqv vq dg, Vjcv ku vjg swguvkqp\n\n\n…\n…\n\n\nROT9\nCx kn xa wxc cx kn, Cqjc rb cqn zdnbcrxw\n\n\n\nROT is known as the Caesar Cypher, but since the transformation is simple (A..Z+=x) decryption is easy now. How can we make this harder?"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#python-is-random",
    "href": "lectures/5.2-Randomness.html#python-is-random",
    "title": "Randomness",
    "section": "Python is Random",
    "text": "Python is Random\nimport random\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nSee also: random.randrange, random.choice, random.sample, random.random, random.gauss, etc."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#and-repeat",
    "href": "lectures/5.2-Randomness.html#and-repeat",
    "title": "Randomness",
    "section": "And Repeat…",
    "text": "And Repeat…\nimport random\nsize = 10\nresults = [0] * size\n\ntests = 100000\nwhile tests &gt; 0:\n    results[random.randint(0,len(results)-1)] += 1\n    tests -= 1\n\nfor i in range(0,len(results)):\n    print(f\"{i} -&gt; {results[i]}\")\n\nWhat will this return?\nWill it hold for more than 10 numbers?"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat",
    "href": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat\nimport random \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nsize = 1000\ndata = [0] * size\n\ntests = 10000000\nwhile tests &gt; 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nfig = plt.figure()\nplt.bar(np.arange(0,len(data)), data)\nfig.savefig('Random.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "href": "lectures/5.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#setting-a-seed",
    "href": "lectures/5.2-Randomness.html#setting-a-seed",
    "title": "Randomness",
    "section": "Setting a Seed",
    "text": "Setting a Seed\nTwo main libraries where seeds are set:\nimport random\nrandom.seed(42)\n\nimport numpy as np\nnp.random.seed(42)\n\nWhy do you often see 42 used as a seed?"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#seeds-and-state",
    "href": "lectures/5.2-Randomness.html#seeds-and-state",
    "title": "Randomness",
    "section": "Seeds and State",
    "text": "Seeds and State\nimport random\nrandom.seed(42)\nst = random.getstate()\nfor r in range(0,3):\n    random.setstate(st)\n    print(f\"Repetition {r}:\")\n    ints = []\n    for i in range(0,10):\n        ints.append(random.randint(0,10))\n    print(f\"\\t{ints}\")"
  },
  {
    "objectID": "lectures/5.2-Randomness.html#hashing",
    "href": "lectures/5.2-Randomness.html#hashing",
    "title": "Randomness",
    "section": "Hashing",
    "text": "Hashing\nChecking for changes (usally in a security context).\nimport hashlib # Can take a 'salt' (similar to a 'seed')\n\nr1 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r1 is: {r1.hexdigest()}\")\n\nr2 = hashlib.md5('CASA Intro to Programming '.encode())\nprint(f\"The hashed equivalent of r2 is: {r2.hexdigest()}\")\n\nr3 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r3 is: {r3.hexdigest()}\")\nOutputs:\n\"The hashed equivalent of r1 is: acd601db5552408851070043947683ef\"\n\"The hashed equivalent of r2 is: 4458e89e9eb806f1ac60acfdf45d85b6\"\n\"The hashed equivalent of r3 is: acd601db5552408851070043947683ef\"\n\nThis is like generating a ‘fingerprint’ of an application or file. In fact, it’s what is going on behind the scenes when you download something to install on macOS or Windows and you’re told that the installer is being ‘verified’ before it will run: the computer is generating a hash of the application’s codebase, and sending that to the Apple Store or Windows Store or direct to the developer in order to check that the file hasn’t been tampered with."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#and-note",
    "href": "lectures/5.2-Randomness.html#and-note",
    "title": "Randomness",
    "section": "And Note…",
    "text": "And Note…\nimport requests\nnight = requests.get(\"http://www.gutenberg.org/ebooks/1514.txt.utf-8\")\nprint(f\"The text is {night.text[30:70]}\")\nprint(f\"The text is {len(night.text):,} characters long\")\nhash = hashlib.md5(night.text.encode())\nprint(f\"This can be hashed into: {hash.hexdigest()}\")\nOutputs:\n\"The text is A Midsummer Night's Dream by Shakespeare\"\n\"The text is 112,127 characters long\"\n\"This can be hashed into: cce0d35b8b2c4dafcbde3deb983fec0a\"\n\nCan be applied to anything: even one byte’s difference (e.g. in a application) can lead to a different hash output.\nBut notice that hashes are always the same length. This property is quite useful for databases and verifying the integrity of applications (MD5 Checksums)."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#jupyterlab-password",
    "href": "lectures/5.2-Randomness.html#jupyterlab-password",
    "title": "Randomness",
    "section": "JupyterLab Password",
    "text": "JupyterLab Password\nYou may have noticed this in Docker:\n'sha1:5b1c205a53e14e:0ce169b9834984347d62b20b9a82f6513355f72d'\nHow this was generated:\nimport uuid, hashlib\nsalt = uuid.uuid4().hex[:16] # Truncate salt\npassword = 'casa2021'        # Set password\n\n# Here we combine the password and salt to \n# 'add complexity' to the hash\nhashed_password = hashlib.sha1(password.encode() + \n                  salt.encode()).hexdigest()\nprint(':'.join(['sha1',salt,hashed_password]))\n\n\nThen you can replace the JUPYTER_PWD parameter in the start-up string for Docker if you want to set a password.\n\n\nDon’t set your passwords this way."
  },
  {
    "objectID": "lectures/5.2-Randomness.html#encryption-security",
    "href": "lectures/5.2-Randomness.html#encryption-security",
    "title": "Randomness",
    "section": "Encryption & Security",
    "text": "Encryption & Security\nSimple hashing algorithms are not normally secure enough for full encryption. Genuine security training takes a whole degree + years of experience.\nAreas to look at if you get involved in applications:\n\nPublic and Private Key Encryption (esp. OpenSSL)\nPrivileges used by Applications (esp. Docker)\nRevocable Tokens (e.g. for APIs)\nInjection Attacks (esp. for SQL using NULL-byte and similar)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#why-pandas",
    "href": "lectures/5.4-Pandas.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy, numpy, and sklearn) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-data-frame",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-index",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-series",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#pandas-terminology-slice",
    "href": "lectures/5.4-Pandas.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#using-pandas",
    "href": "lectures/5.4-Pandas.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nHere’s code to read a (remote) CSV file:\nimport pandas as pd      # import package\n# Bitly: https://raw.githubusercontent.com/jreades/fsds/master/data/2019-sample-crime.csv\nurl='https://bit.ly/39SJpfp'\ndf = pd.read_csv(url)     # load a (remote) CSV\nprint(type(df))           # not simple data type\nprint(df.columns.values)  # column names\nOutput:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n['ID' 'Case Number' 'Date' 'Primary Type' 'Description'\n 'Location Description' 'Arrest' 'Domestic' 'Year' 'Latitude' 'Longitude']"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#summarise-a-data-frame",
    "href": "lectures/5.4-Pandas.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\ndf.describe() # Information about each Series\ndf.info()     # Information about each Series and the df\ndf.info is more about data types and memory usage. df.describe is for summarising information about the distribution of values in every series."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#familiar",
    "href": "lectures/5.4-Pandas.html#familiar",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\nThis should be looking eerily familiar:\nprint(type(df['Latitude']))          # type for column\nprint(type(df['Latitude'].array))    # type for values\nprint(df['Latitude'].array[:5])     # first five values\nprint(f\"1: {df['Latitude'].mean()}\") # summarise a series/column\nprint(f\"2: {df.Latitude.mean()}\")    # if no spaces in name\nProduces:\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'numpy.ndarray'&gt;\n[41.75130706 41.90399688 41.88032861 41.92438396 41.75579713]\n1: 41.84550008439\n2: 41.84550008439\n\nNotice that we’ve got two ways of accessing a pandas Series:\n\nThe dictionary-like way: df['Latitude']; this works for all columns, always.\nThe method-like way: df.Latitude; this works for ‘reading’ columns without spaces in their names."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#jupyter-formatting",
    "href": "lectures/5.4-Pandas.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter in ‘fancy’ ways:"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#familiar-1",
    "href": "lectures/5.4-Pandas.html#familiar-1",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                       # First 3 rows of df\ndf[['ID','Date','Year']].tail(3) # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nOn one level, this is what we’ve been building towards! We’ve got head and tail which we saw in the Command Line lecture. We’ve got random sampling with seeds which we saw in the Randomness lecture. We’ve even got LoLs, which we saw way back in the Lists of Lists lecture!"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#data-frames-vs-series",
    "href": "lectures/5.4-Pandas.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nPandas operates on two principles:\n\nAny operation on a Data Frame returns a Data Frame.\nAny operation on a Series returns a Series.\n\n\nWe’ll see in a moment why this is useful!"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#shallow-copies",
    "href": "lectures/5.4-Pandas.html#shallow-copies",
    "title": "Pandas",
    "section": "‘Shallow’ Copies",
    "text": "‘Shallow’ Copies\nMore subtly, operations on a Series or Data Frame return a shallow copy, which is like a ‘view’ in a database…\n\nThe original is unchanged unless you specify inplace=True (where supported).\nAttempts to change a subset of the data frame will often trigger a SettingWithCopyWarning warning.\n\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy()).\n\n\nDataQuest has a nice overview of how SettingWithCopyWarning is triggered and what to do about it."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#putting-these-ideas-together",
    "href": "lectures/5.4-Pandas.html#putting-these-ideas-together",
    "title": "Pandas",
    "section": "Putting These Ideas Together",
    "text": "Putting These Ideas Together\n# Returns a series but not a column\ndf.Latitude - 1 \n# Saves returned series as a new column\ndf['lat'] = df.Latitude - 1\n# Returns a new data frame w/o 'lat' \ndf.drop(columns=['lat']) \n# Modifies df directly\ndf.drop(columns=['lat'], inplace=True) \n# Try to modify a view of df (triggers warning)\ndf[df['Primary Type']=='BURGLARY'].Latitude = 41.7"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#chaining",
    "href": "lectures/5.4-Pandas.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\ndf.sort_values(by=['Year','ID'], ascending=False).sample(frac=0.5).head(20).median()"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#selection",
    "href": "lectures/5.4-Pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n# Returns a selection (Boolean series)\ndf['Primary Type']=='ASSAULT'\n\n# All rows where Primary Type is ASSAULT\ndf[ df['Primary Type']=='ASSAULT' ]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['Primary Type']=='ASSAULT'][['Longitude','Latitude']].mean()\n\n# Two conditions with a bit-wise AND\ndf[\n  (df['Primary Type']=='ASSAULT') &\n  (df['Description']=='AGGRAVATED: HANDGUN')\n]\n\n# Two conditions with a bit-wise OR\ndf[\n  (df['Primary Type']=='ASSAULT') |\n  (df['Primary Type']=='THEFT')\n]"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#dealing-with-types",
    "href": "lectures/5.4-Pandas.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\n\n\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns (including arrays)\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#changing-the-type",
    "href": "lectures/5.4-Pandas.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\nprint(df['Primary Type'].unique())   # Find unique values\nprint(df['Primary Type'].dtype.name) # Confirm is 'object'\ndf['Primary Type'] = df['Primary Type'].astype('category')\nprint(df['Primary Type'].dtype.name) # Confirm is 'category'\nprint(df['Primary Type'].describe()) # Category column info\nOutputs:\n['BURGLARY' 'DECEPTIVE PRACTICE' 'BATTERY'...]\nobject   # &lt; before `as type`\ncategory # &lt; after `as type`\ncount       100\nunique       15\ntop       THEFT\nfreq         28\nName: Primary Type, dtype: object # category==special class of object"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#datetime-data",
    "href": "lectures/5.4-Pandas.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\nprint(df.Date.dtype.name)\n# object\ndf.Date.to_list()[:3]\n# ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nThis shows that Date is currently a string of dates+times.\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.values, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nprint(df.dt.dtype.name)\n# datetime64[ns]\ndf.dt.to_list()[:3]\n# [Timestamp('2019-04-20 11:00:00'), Timestamp('2019-12-02 10:35:00'), Timestamp('2019-10-06 04:50:00')]\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#datetime-formats",
    "href": "lectures/5.4-Pandas.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM\n\n\n\nSo that is why:\npd.to_datetime(df.Date.array, format=\"%m/%d/%Y %H:%M:%S %p\")\nNote the other things happening here:\n\npd.to_datetime(...) is not a method, it’s a function from the pandas package.\ndf.Date.array (and df.Date.to_numpy() and df.Data.tolist()) gives access to the data directly, whereas df.Date gives access to the Series."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#deprecation-warning",
    "href": "lectures/5.4-Pandas.html#deprecation-warning",
    "title": "Pandas",
    "section": "Deprecation Warning!",
    "text": "Deprecation Warning!\nFrom time to time, real-world software projects will change the way things work. Pandas is just such a project!\n\n\n\n\n\n\nWarning\n\n\nWe recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array. See API Documenation.\n\n\n\nSo while Series.values still works, and will continue to work for some time, you are being advised to start using Series.array or Series.to_numpy() instead. Meaning, we should consider using df.Date.array."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#tidying-up",
    "href": "lectures/5.4-Pandas.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many options and subtleties…\n# Fix categories\nmapping = {}\n\n# df['Primary Type'].unique().to_list() also works\nfor x in df['Primary Type'].cat.categories.to_list():\n  mapping[x]=x.title()\n\n# And update\ndf['Primary Type'] = df['Primary Type'].cat.rename_categories(mapping)\nHow would you work out what this code does? 1\nTo deal with pricing information treated as a string:\ndf2['price'].str.replace('$','').astype(float)\nMany more examples accessible via Google!\n\nAnother thing you might notice here: adding .cat allows us to access category methods for the Series; adding .str allows us to access string methods for the Series.\n\nThere are at least two ways: 1) print out mapping; 2) before running the code comment out the ‘update’ line and print out x and x.title(); 3) search for title python."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#dropping-rows-and-columns",
    "href": "lectures/5.4-Pandas.html#dropping-rows-and-columns",
    "title": "Pandas",
    "section": "Dropping Rows and Columns",
    "text": "Dropping Rows and Columns\nThere are multiple ways to drop ‘stuff’:\ndf2 = df.copy()\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf2.drop(index=range(5,10), inplace=True) # Row 'numbers' or index values\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf.drop(columns=['Year'], inplace=True)   # Column name(s)\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\nThere is also df.dropna() which can apply to rows or columns with NULL or np.nan values.\nI often prefer df = df[df.index &gt; 15] (negative selection) to df.drop(index=range(0,14)) (positive selection).\n\nWhy might you want the default to not be in_place?"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#accessing-data-by-location",
    "href": "lectures/5.4-Pandas.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index:\n\ndf.loc returns the rows labeled 0, 1, and 2 ([0..2]), whereas\ndf.iloc returns the range 0..2 ([0..2))!"
  },
  {
    "objectID": "lectures/5.4-Pandas.html#indexes",
    "href": "lectures/5.4-Pandas.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you print out the head:\ndf.head(3)\nThe left-most column (without) a name is the index.\ndf.set_index('ID', inplace=True)\ndf.head(3)\nNow we see:\n         Case Number                    Date  ...  Longitude                  dt\nID                                            ...\n11667185    JC237601  04/20/2019 11:00:00 PM  ... -87.603468 2019-04-20 11:00:00\n11909178    JC532226  12/02/2019 10:35:00 AM  ... -87.643230 2019-12-02 10:35:00\n11852571    JC462365  10/06/2019 04:50:00 PM  ... -87.758473 2019-10-06 04:50:00\n\nSo ID is now the index and is not accessible as a column: df.ID will now throw an error because it’s not longer part of the Column Index."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#indexes-contd",
    "href": "lectures/5.4-Pandas.html#indexes-contd",
    "title": "Pandas",
    "section": "Indexes (cont’d)",
    "text": "Indexes (cont’d)\nNotice the change to the data frame:\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nAnd now:\nprint(df.loc[11667185,:])\nprint(df.loc[11667185:11852571,'Case Number':'Date'])\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#saving",
    "href": "lectures/5.4-Pandas.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, here are some of the more popular ones:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(&lt;path&gt;)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(&lt;path&gt;)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_feather(&lt;path&gt;)\nDirectly usable by R. Requires pyarrow to be installed to access the options.\n\n\ndf.to_parquet(&lt;path&gt;)\nDirectly usable by many languages. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(&lt;path&gt;))\nWrite a LaTeX-formatted table to a file. Display requires booktabs. Could do copy+paste with print(df.to_latex()).\n\n\ndf.to_markdown(&lt;path&gt;)\nWrite a Markdown-formatted table to a file. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is detected automatically (e.g. df.to_csv('file.csv.gz')) but you can also specify it (e.g. df.to_csv('file.csv.gz', compression='gzip')).1\nFor instance, a bit.ly link to a Gzipped file requires compression='gzip' because there’s nothing in the link itself to tell Pandas what to expect."
  },
  {
    "objectID": "lectures/5.4-Pandas.html#resources",
    "href": "lectures/5.4-Pandas.html#resources",
    "title": "Pandas",
    "section": "Resources",
    "text": "Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#reading-writing",
    "href": "lectures/6.2-Geopandas.html#reading-writing",
    "title": "Geopandas",
    "section": "Reading & Writing",
    "text": "Reading & Writing\nSupported file formats:\n\n\n\nType\nExtension\nNotes\n\n\n\n\nShape\n.shp (etc.)\nMaximum compatibility\n\n\nGeoPackage\n.gpkg\nGood default choice\n\n\nGeoJSON\n.geojson\nFor web mapping\n\n\nZip\n.zip\nFor use with Shapefiles\n\n\nWKT\n.txt\nPlain-text & SQL\n\n\n\nAdditionally, it is possible to read only subsets of the data using row, column, geometry, and bbox filters."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#reading-remote-files",
    "href": "lectures/6.2-Geopandas.html#reading-remote-files",
    "title": "Geopandas",
    "section": "Reading (Remote Files)",
    "text": "Reading (Remote Files)\nAgain, depending on file size you may want to save these locally, but…\nimport geopandas as gpd\ngpkg_src = 'https://bit.ly/2K4JcsB'\nworld = gpd.read_file(gpkg_src)\n# The ';' suppresses matplotlib output\nworld.plot(facecolor='white', edgecolor='darkblue');"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#writing-local-files",
    "href": "lectures/6.2-Geopandas.html#writing-local-files",
    "title": "Geopandas",
    "section": "Writing (Local Files)",
    "text": "Writing (Local Files)\nWrite any OGR-supported vector drivers.\nworld.to_file('world.gpkg', driver='GPKG')\nworld.to_file('world.shp', driver='ESRI Shapefile')\nworld.to_file('world.geojson', driver='GeoJSON')\n\nIf you forget to specify the driver it writes shapefiles by default. This is mainly an issue if you try to write a GeoPackage or GeoJSON file but then end up writing a shapefile to a directory called world.gpkg!"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#data-structures",
    "href": "lectures/6.2-Geopandas.html#data-structures",
    "title": "Geopandas",
    "section": "Data Structures",
    "text": "Data Structures\nGeoPandas does all this by adding just two new classes:\n\nGeoDataFrame\nGeoSeries\n\nIn principle, a GeoSeries can contain multiple geo-data types, but in practice you’ll want to be one of the following shapely classes:\n\nPoints / Multi-Points\nLines / Multi-Lines\nPolygons / Multi-Polygons"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#consider",
    "href": "lectures/6.2-Geopandas.html#consider",
    "title": "Geopandas",
    "section": "Consider",
    "text": "Consider\nRecall that we can ask if a particular object is an instance of any given class:\nimport pandas as pd\nprint(isinstance(world, str))\nprint(isinstance(world, pd.DataFrame))\nprint(isinstance(world, gpd.GeoDataFrame))\nPrints: False, True, True.\nimport pandas as pd\nprint(isinstance(world.geometry, str))\nprint(isinstance(world.geometry, pd.Series))\nprint(isinstance(world.geometry, gpd.GeoSeries))\nAlso prints: False, True, True.\n\nSo converting from Pandas to GeoPandas works well because GeoPandas knows all about Pandas.\nYou can use a GeoDataFrame anywhere you’d use a DataFrame with no loss of functionality! Same for a GeoSeries, though in this case a GeoSeries cannot perform the same statistical operations."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#projections",
    "href": "lectures/6.2-Geopandas.html#projections",
    "title": "Geopandas",
    "section": "Projections",
    "text": "Projections\nDepending on your data source, you may or may not have projection information attached to your GeoDataFrame:\nprint(world.crs)\noutputs epsg:4326, but:\nworld.crs\noutputs:\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#finding-projections",
    "href": "lectures/6.2-Geopandas.html#finding-projections",
    "title": "Geopandas",
    "section": "Finding Projections",
    "text": "Finding Projections\nYou’ll have already covered this in GIS, but you can find nearly any EPSG you might need at epsg.io. By far the most commonly-used here are:\n\nEPSG:4326 for the World Geodetic System 84 used in GPS.\nEPSG:27700 for OSGB 1936/British National Grid used in the UK.\n\nNote: recall that large territories (such as Canada, China and Russia) may well have multiple projections at the state of provincial level."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#reprojection",
    "href": "lectures/6.2-Geopandas.html#reprojection",
    "title": "Geopandas",
    "section": "Reprojection",
    "text": "Reprojection\nFor data sets without projection information (i.e. often anything loaded from a shapefile) you must gdf.set_crs(&lt;spec&gt;). For all others you should gdf.to_crs(&lt;spec&gt;).\nworld2 = world.to_crs('ESRI:54030')\nworld2.plot();\n\n\nUnlike a shapefile, you can have more than one geometry column, each with a different projection. However, only one will be plotted (the one named geometry or specified via set_geometry())."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#the-spatial-index",
    "href": "lectures/6.2-Geopandas.html#the-spatial-index",
    "title": "Geopandas",
    "section": "The Spatial Index",
    "text": "The Spatial Index\nWe can use GeoSeries’ spatial index directly to perform simple spatial queries:\nimport matplotlib.pyplot as plt\nwslice = world.cx[-50:50, -20:20] # cx = coordinate index\nax = wslice.plot()\nplt.axvline(-50, linestyle='--', color='red')\nplt.axvline(50, linestyle='--', color='red')\nplt.axhline(-20, linestyle='--', color='red')\nplt.axhline(20, linestyle='--', color='red');"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#attributes",
    "href": "lectures/6.2-Geopandas.html#attributes",
    "title": "Geopandas",
    "section": "Attributes",
    "text": "Attributes\nA GeoSeries has attributes like any other Series, but also includes some spatially-specifc ones:\n\narea — if a polygon\nbounds — for each feature\ntotal_bounds — for each GeoSeries\ngeom_type — if you don’t already know\nis_valid — if you need to test"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#methods",
    "href": "lectures/6.2-Geopandas.html#methods",
    "title": "Geopandas",
    "section": "Methods",
    "text": "Methods\nAdditional GeoSeries methods icnlude:\n\ndistance() — returns Series measuring distances to some other feature (called as: &lt;GeoSeries&gt;.distance(&lt;feature&gt;))\ncentroid — returns GeoSeries of strict centroids (called as: &lt;GeoSeries&gt;.centroid)\nrepresentative_point() — returns GeoSeries of points within features\nto_crs() and plot(), which you’ve already seen.\n\n\nNote that centroid is not called with parentheses. Technically it’s more like an attribute than a method."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#relationship-tests",
    "href": "lectures/6.2-Geopandas.html#relationship-tests",
    "title": "Geopandas",
    "section": "Relationship Tests",
    "text": "Relationship Tests\nSimple geographical tests:\n\ngeom_almost_equals() — tries to deal with rounding issues when comparing two features.\ncontains() — is shape contained within some other features.\nintersects() — does shape intersect some other features."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#converting-non-spatial-data-1",
    "href": "lectures/6.2-Geopandas.html#converting-non-spatial-data-1",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 1",
    "text": "Converting Non-Spatial Data 1\nLat/Long and Northing/Easting benefit from a helper function gpd.points_from_xy():\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url)\n\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.plot()\n\nYou can also use list comprehensions ([x for x in list]) and zip to combine two lists but then need to specify the CRS as a separate step!"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#csv-to-points-in-3-lines",
    "href": "lectures/6.2-Geopandas.html#csv-to-points-in-3-lines",
    "title": "Geopandas",
    "section": "CSV to Points in 3 Lines!",
    "text": "CSV to Points in 3 Lines!\n\n\nNotice that the default plot from a GeoDataFrame is… a map!"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#converting-non-spatial-data-2",
    "href": "lectures/6.2-Geopandas.html#converting-non-spatial-data-2",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 2",
    "text": "Converting Non-Spatial Data 2\nOther feature types need to be in some kind of regular format such as Well-Known Text (WKT), GeoJSON, or something readable as a Shapely geometry.\nfrom shapely import wkt\n\n# Notice coordinate pairs and last point is same as first one\nbbox = 'POLYGON((5000000.0 2500000.0, 5000000.0 -2500000.0, -5000000.0 -2500000.0, -5000000.0 2500000.0, 5000000.0 2500000.0))'\n\n# Create GeoPandas from dict just like Pandas\nbgdf = gpd.GeoDataFrame({'id':[0], 'coordinates':bbox})\n\n# Turn it into a geometry\nbgdf['geometry'] = bgdf.coordinates.apply(wkt.loads)\nbgdf = bgdf.set_crs('ESRI:54030')\nbgdf.plot() # Not very interesting but...\n\nThese are more rarely used for our purposes but knowing that they exist is useful."
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#from-text-to-bounding-box",
    "href": "lectures/6.2-Geopandas.html#from-text-to-bounding-box",
    "title": "Geopandas",
    "section": "From Text to Bounding Box",
    "text": "From Text to Bounding Box\nscale = int(float('1e7'))\nf,ax=plt.subplots(figsize=(8,4))\nworld2.plot(ax=ax)\nbgdf.plot(ax=ax, color='none', edgecolor='r')\nax.set_xlim([-0.75*scale, +0.75*scale])\nax.set_ylim([-3*scale/10, +3*scale/10])"
  },
  {
    "objectID": "lectures/6.2-Geopandas.html#resources",
    "href": "lectures/6.2-Geopandas.html#resources",
    "title": "Geopandas",
    "section": "Resources",
    "text": "Resources\n\nI Hate Coordinate Systems\nGeoPandas on ReadTheDocs\nDani’s GDS Course\nDani’s Web Mapping Course"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#getting-spatial-with-boroughs",
    "href": "lectures/6.4-ESDA.html#getting-spatial-with-boroughs",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Getting Spatial (with Boroughs)",
    "text": "Getting Spatial (with Boroughs)\nimport geopandas as gpd\nurl = 'https://bit.ly/3neINBV'\nboros = gpd.read_file(url, driver='GPKG')\nboros.plot(color='none', edgecolor='red');"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#convex-hull",
    "href": "lectures/6.4-ESDA.html#convex-hull",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Convex Hull",
    "text": "Convex Hull\nboros['hulls'] = boros.geometry.convex_hull\nboros = boros.set_geometry('hulls')\nboros.plot(column='NAME', categorical=True, alpha=0.5);"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#dissolve",
    "href": "lectures/6.4-ESDA.html#dissolve",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Dissolve",
    "text": "Dissolve\nboros['region'] = 'London'\nboros = boros.set_geometry('geometry') # Set back to original geom\nldn   = boros.dissolve(by='region')    # And dissolve to a single poly\n\nf,ax = plt.subplots(figsize=(10,8))    # New plot\nldn.plot(ax=ax)                        # Add London layer to axis"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#simplify",
    "href": "lectures/6.4-ESDA.html#simplify",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Simplify",
    "text": "Simplify\nldn.simplify(500).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#buffer",
    "href": "lectures/6.4-ESDA.html#buffer",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Buffer",
    "text": "Buffer\nldn.buffer(500).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#buffer-simplify",
    "href": "lectures/6.4-ESDA.html#buffer-simplify",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Buffer & Simplify",
    "text": "Buffer & Simplify\nldn.buffer(1000).simplify(1000).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#difference",
    "href": "lectures/6.4-ESDA.html#difference",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Difference",
    "text": "Difference\nAnd some nice chaining…\nldn.buffer(3000).simplify(2500).difference(ldn.geometry).plot()"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#legendgrams",
    "href": "lectures/6.4-ESDA.html#legendgrams",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Legendgrams",
    "text": "Legendgrams\n\n\n\nCode begins on next slide."
  },
  {
    "objectID": "lectures/6.4-ESDA.html#handy-recall",
    "href": "lectures/6.4-ESDA.html#handy-recall",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Handy Recall",
    "text": "Handy Recall\nLoad the CSV file and convert to GeoDataFrame:\nimport pandas as pd\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url) \ndf['price'] = df.price.str.replace('$','',regex=False).astype('float')\ndf.set_index('id',inplace=True)\ngdf = gpd.GeoDataFrame(df, \n        geometry=gpd.points_from_xy(\n          df['longitude'], df['latitude'], crs='epsg:4326'\n        )\n      )\ngdf = gdf.to_crs('epsg:27700')"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-legendgrams",
    "href": "lectures/6.4-ESDA.html#implementing-legendgrams",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing Legendgrams",
    "text": "Implementing Legendgrams\nimport pysal as ps\n# https://github.com/pysal/mapclassify\nimport mapclassify as mc\n# https://jiffyclub.github.io/palettable/\nimport palettable.matplotlib as palmpl\nfrom legendgram import legendgram\n\nf,ax = plt.subplots(figsize=(10,8))\ngdf.plot(column='price', scheme='Quantiles', cmap='magma', k=5, ax=ax)\nq = mc.Quantiles(gdf.price.array, k=5)\n\n# https://github.com/pysal/legendgram/blob/master/legendgram/legendgram.py\nlegendgram(f, ax, \n               gdf.price, q.bins, pal=palmpl.Magma_5,\n               legend_size=(.4,.2), # legend size in fractions of the axis\n               loc = 'upper left', # mpl-style legend loc\n               clip = (0,500), # clip range of the histogram\n               frameon=True)\n\nNote that the number of colours need to match k, which is 5 in this case.\nIt should be possible to set up the colormap and bins such that they can be passed to both GeoPandas and Legendgram."
  },
  {
    "objectID": "lectures/6.4-ESDA.html#knn-weights",
    "href": "lectures/6.4-ESDA.html#knn-weights",
    "title": "Exploratory Spatial Data Analysis",
    "section": "KNN Weights",
    "text": "KNN Weights"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-knn",
    "href": "lectures/6.4-ESDA.html#implementing-knn",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing KNN",
    "text": "Implementing KNN\nfrom pysal.lib import weights\nw = weights.KNN.from_dataframe(gdf, k=3)\ngdf['w_price'] = weights.lag_spatial(w, gdf.price)\ngdf[['name','price','w_price']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nname\nprice\nw_price\n\n\n\n\n83\nSouthfields Home\n85.0\n263.0\n\n\n53\nFlat in Islington, Central London\n55.0\n190.0\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\n470.0\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\n186.0\n\n\n44\nAvni Kensington Hotel\n430.0\n821.0"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#spatial-lag-of-distance-band",
    "href": "lectures/6.4-ESDA.html#spatial-lag-of-distance-band",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Spatial Lag of Distance Band",
    "text": "Spatial Lag of Distance Band"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-db",
    "href": "lectures/6.4-ESDA.html#implementing-db",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing DB",
    "text": "Implementing DB\nw2 = weights.DistanceBand.from_dataframe(gdf, threshold=2000, alpha=-0.25)\ngdf['price_std'] = (gdf.price - gdf.price.mean()) / gdf.price.std()\ngdf['w_price_std'] = weights.lag_spatial(w2, gdf.price_std)\ngdf[['name','price_std','w_price_std']].sample(5, random_state=42)\n\n\n\n\nname\nprice_std\nw_price_std\n\n\n\n\n83\nSouthfields Home\n-0.27\n0.00\n\n\n53\nFlat in Islington, Central London\n-0.51\n-0.58\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n0.83\n0.46\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n-0.07\n-0.82\n\n\n44\nAvni Kensington Hotel\n2.52\n3.25"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#morans-i",
    "href": "lectures/6.4-ESDA.html#morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Moran’s I",
    "text": "Moran’s I\nmi = esda.Moran(gdf['price'], w)\nprint(f\"{mi.I:0.4f}\")\nprint(f\"{mi.p_sim:0.4f}\")\nmoran_scatterplot(mi)"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#local-morans-i",
    "href": "lectures/6.4-ESDA.html#local-morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#implementing-local-morans-i",
    "href": "lectures/6.4-ESDA.html#implementing-local-morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing Local Moran’s I",
    "text": "Implementing Local Moran’s I\nlisa = esda.Moran_Local(gdf.price, w)\n# Break observations into significant or not\ngdf['sig'] = lisa.p_sim &lt; 0.05\n# Store the quadrant they belong to\ngdf['quad'] = lisa.q\ngdf[['name','price','sig','quad']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nname\nprice\nsig\nquad\n\n\n\n\n83\nSouthfields Home\n85.0\nFalse\n3\n\n\n53\nFlat in Islington, Central London\n55.0\nFalse\n3\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\nFalse\n1\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\nFalse\n3\n\n\n44\nAvni Kensington Hotel\n430.0\nFalse\n1"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#full-lisa",
    "href": "lectures/6.4-ESDA.html#full-lisa",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Full LISA",
    "text": "Full LISA\nplot_local_autocorrelation(lisa, gdf, 'price')"
  },
  {
    "objectID": "lectures/6.4-ESDA.html#resources",
    "href": "lectures/6.4-ESDA.html#resources",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Resources",
    "text": "Resources\nThere’s so much more to find, but:\n\nPandas Reference\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nVisualising Missing Data\nChoosing Map Colours"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#can-we-describe-text",
    "href": "lectures/7.2-Patterns_in_Text.html#can-we-describe-text",
    "title": "Patterns in Text",
    "section": "Can We Describe Text?",
    "text": "Can We Describe Text?\nConsider the following character sequences:\n\nfoo@bar.com\nhttps://www.ucl.ac.uk/bartlett/casa/\n(555) 102-1111\nE17 5RS\nNow, fair Hippolyta, our nuptial hour / Draws on apace. Four happy days bring in / Another moon. But, oh, methinks how slow / This old moon wanes. She lingers my desires, / Like to a stepdame or a dowager / Long withering out a young man’s revenue. (I.i.)\n\n\nWe need ways to distinguish: Upper and Lower Case, Digits, Space Characters, Other Characters, Repetition, Type… Can you do those with strings alone?"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "href": "lectures/7.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "title": "Patterns in Text",
    "section": "Strings Methods are Not Enough",
    "text": "Strings Methods are Not Enough\n'123foo456'.index('foo') # 2\n'123foo456'.split('foo') # ['123', '456']\n' 123 foo 456 '.strip()  # '123 foo 456'\n'HOW NOW BROWN COW?'.lower() # 'how now brown cow?'\n'How now brown cow?'.replace('brown ','green-')\n# 'How now green-cow?'\nSee: dir(str) for full list of string methods."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#regular-expressions",
    "href": "lectures/7.2-Patterns_in_Text.html#regular-expressions",
    "title": "Patterns in Text",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegexes are a way for talking about patterns observed in text, although their origins are rooted in philosophy and linguistics.\nImplemented in Python as:\nimport re\n# re.search(&lt;regex&gt;, &lt;str&gt;)\ns = '123foo456'\nif re.search('123',s):\n  print(\"Found a match.\")\nelse:\n  print(\"No match.\")\nPrints 'Found a match.'"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#capturing-matches",
    "href": "lectures/7.2-Patterns_in_Text.html#capturing-matches",
    "title": "Patterns in Text",
    "section": "Capturing Matches",
    "text": "Capturing Matches\nm = re.search('123',s)\nprint(m.start())\nprint(m.end())\nprint(m.span())\nprint(m.group())\nOutputs:\n0\n3\n(0,3)\n123\n\nSo, we have None if a search fails, but if it succeeds then we have attributes of the match objection like start, end, span, and group (this last is going to be particularly interesting since it tells us what matched)."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#configuring-matches",
    "href": "lectures/7.2-Patterns_in_Text.html#configuring-matches",
    "title": "Patterns in Text",
    "section": "Configuring Matches",
    "text": "Configuring Matches\ns = '123foo456'\nm = re.search('FOO',s)\nprint(m)\nm = re.search('FOO',s,re.IGNORECASE)\nprint(m)\nOutputs:\nNone\n&lt;re.Match object; span=(3, 6), match='foo'&gt;\nThe third parameter allows us to: match newlines (re.DOTALL), ignore case (re.IGNORECASE), take language into account (re.LOCALE), match across lines (re.MULTILINE), and write patterns across multiple lines (re.VERBOSE). If you need multiple options it’s re.DOTALL | re.IGNORECASE. Bitwise again!"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#more-than-one-match",
    "href": "lectures/7.2-Patterns_in_Text.html#more-than-one-match",
    "title": "Patterns in Text",
    "section": "More Than One Match",
    "text": "More Than One Match\ns = '123foo456foo789'\nlst = re.findall('foo',s)\nprint(lst)\nlst = re.finditer('foo',s)\n[x for x in lst]\nrs  = re.sub('foo',' ',s)\nprint(rs)\nrs  = re.split(' ',rs)\nprint(rs)\nOutputs:\n['foo','foo']\n[&lt;re.Match object; span=(3, 6), match='foo'&gt;, &lt;re.Match object; span=(9, 12), match='foo'&gt;]\n'123 456 789'\n['123', '456', '789']"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "href": "lectures/7.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "title": "Patterns in Text",
    "section": "Regular Expressions Do Much More",
    "text": "Regular Expressions Do Much More\nimport re\nm = re.search(r'\\$((\\d+,){2,}\\d+)',\n        \"'That will be $1,000,000 he said...'\")\nprint(m.group(1)) # '1,000,000'\nThis looks for sequences of 1-or-more digits followed by a comma… and for those sequences to repeat two or more times:\n# Look for a literal '$'\nre.search(r'\\$') \n# Group of &gt;=1 digits followed by a comma...\nre.search(r'(\\d+,)') \n# Repeated two or more times...\nre.search(r'(\\d+,){2,}') \n\n\nAlso notice the r'&lt;regex&gt;' with an r in front of the string. This means ‘raw’ and is often a required modifier for regular expression patterns. Simple ones don’t need it, but from here on out you will."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#character-classes",
    "href": "lectures/7.2-Patterns_in_Text.html#character-classes",
    "title": "Patterns in Text",
    "section": "Character Classes",
    "text": "Character Classes\n\n\n\n\n\n\n\n\nCharacters\nRegex Meta Class Options\n‘Antonyms’\n\n\n\n\na…z\n[a-z], \\w (word-like characters)\n[^a-z], \\W\n\n\nA…Z\n[A-Z], \\w (word-like characters)\n[^A-Z], \\W\n\n\n0…9\n[0-9], \\d (digits)\n[^0-9], \\D\n\n\n' ', \\n, \\t, \\r, \\f, \\v\n\\s\n\\S\n\n\n., [, ], +, $, ^, \\|, {, }, *, (, ), ?\nFor safety always precede character with a \\.\nNone\n\n\n\n\n\n\\w will include _. And \\ is, once again, important as it ‘escapes’ various characters, and options."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#metacharacters",
    "href": "lectures/7.2-Patterns_in_Text.html#metacharacters",
    "title": "Patterns in Text",
    "section": "Metacharacters",
    "text": "Metacharacters\n\n\n\nMetacharacter\nMeaning\nExample\n\n\n\n\n.\nAny character at all\nc.t\n\n\n^\nStart of a string/line\n^start\n\n\n$\nEnd of a string/line\nend$\n\n\n*\n0 or more of something\n-*\n\n\n+\n1 or more of something\n-+\n\n\n?\n0 or 1 of something; also lazy modifier\n,?\n\n\n{m,n}\nRepeat between m and n times\n\\d{1,4}\n\n\n[ ]\nA set of character literals\n[1-5]\n\n\n( )\nGroup/remember this sequence of characters\n(\\d+)\n\n\n|\nOr\n(A|B)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#building-blocks",
    "href": "lectures/7.2-Patterns_in_Text.html#building-blocks",
    "title": "Patterns in Text",
    "section": "Building Blocks",
    "text": "Building Blocks\n\n\n\n\n\n\n\nRegex\nInterpretation\n\n\n\n\nr'\\s*'\n0 or more spaces\n\n\nr'\\d+'\n1 or more digits\n\n\nr'[A-Fa-f0-7]{5}'\nExactly 5 hexadecimal ‘digits’\n\n\nr'\\w+\\.\\d{2,}'\n1 or more ‘wordish’ characters, followed by a full-stop, then 2 or more digits\n\n\nr'^[^@]+@\\w+'\nOne more non-@ characters at the start of a line, followed by a ‘@’ then 1 or more ‘wordish’ characters.\n\n\nr'(uk\\|eu\\|fr)$'\nThe characters ‘uk’ or ‘eu’ or ‘fr’ at the end of a line."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^[^@]+@([a-z0-9\\-]+\\.){1,5}[a-z0-9\\-]+$', s)\n\ns should be replaced with any string you want to check."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-1",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-1",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'\\d{4}-\\d{2}-\\d{2}', s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-2",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-2",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^\\s*$', s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-3",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-3",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^(http|https|ftp):[\\/]{2}([a-zA-Z0-9\\-]+\\.){1,4}[a-zA-Z]{2,5}(:[0-9]+)?\\/?([a-zA-Z0-9\\-\\._\\?\\'\\/\\\\\\+\\&\\%\\$#\\=~]*)',s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#whats-this-4",
    "href": "lectures/7.2-Patterns_in_Text.html#whats-this-4",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})',s)"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#to-help",
    "href": "lectures/7.2-Patterns_in_Text.html#to-help",
    "title": "Patterns in Text",
    "section": "To Help…",
    "text": "To Help…\nre.VERBOSE to the rescue:\nregex = r\"\"\"\n([GIR] 0[A]{2})|    # Girobank \n(\n  (\n    ([A-Z][0-9]{1,2})| # e.g A00...Z99\n      (\n        ([A-Z][A-HJ-Y][0-9]{1,2})|  # e.g. AB54...ZX11\n          (([A-Z][0-9][A-Z])|  # e.g. A0B...Z9Z \n          ([A-Z][A-HJ-Y][0-9][A-Z]?))  # e.g. WC1 or WC1H\n        )\n      )\n    \\s?[0-9][A-Z]{2} # e.g. 5RX\n  )\n\"\"\"\nre.match(regex,s,re.VERBOSE|re.IGNORECASE) # Can also use: re.X|re.I\n\nThis is the government’s own regex but is probably not 100% accurate."
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "href": "lectures/7.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "title": "Patterns in Text",
    "section": "Applications of Regular Expressions",
    "text": "Applications of Regular Expressions\nIf our problem follows some set of articulable rules about permissible sequences of characters then we can probably validate it using a regex:\n\n\n\n\n\n\n\nExamples\nMore Examples\n\n\n\n\nEmail\nPassword\n\n\nPostcode\nPhone number\n\n\nDate\nCredit cards\n\n\nWeb scraping\nSyntax highlighting\n\n\nSentence structure\nData wrangling\n\n\nSearching for/withinfiles/content\nLexical analysis/Language detection\n\n\n\n\nThese are all good problems…"
  },
  {
    "objectID": "lectures/7.2-Patterns_in_Text.html#resources",
    "href": "lectures/7.2-Patterns_in_Text.html#resources",
    "title": "Patterns in Text",
    "section": "Resources",
    "text": "Resources\n\nPython Documentation\nReal Python: Regular Expressions 1\nReal Python: Regular Expressions 2\nData Camp RegEx Tutorial\nIntroduction to Regex\nUnderstanding RegExes in Python\nDemystifying RegExes in Python\nPython RegExes\nMastering String Methods in Python\n\nThanks to Yogesh Chavan and Nicola Pietroluongo for examples."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#dummy-variables",
    "href": "lectures/7.4-Analysing_Text.html#dummy-variables",
    "title": "Analysing Text",
    "section": "Dummy Variables",
    "text": "Dummy Variables\nThe concept of ‘dummy variables’ in economics/regression is a useful point to start thinking about text:\n\n\n\nTopic\nDummy\n\n\n\n\nNews\n0\n\n\nCulture\n1\n\n\nPolitics\n2\n\n\nEntertainment\n3\n\n\n\n\nWhat’s the problem with this approach when you’re thinking about the topics in a document? You either have to assign each document to one, and only one, topic, or you need a lot of dummies."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#one-hot-encoders",
    "href": "lectures/7.4-Analysing_Text.html#one-hot-encoders",
    "title": "Analysing Text",
    "section": "One-Hot Encoders",
    "text": "One-Hot Encoders\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n1\n1\n0\n1\n\n\nCulture item\n0\n1\n1\n0\n\n\nPolitics item\n1\n0\n0\n1\n\n\nEntertainment item\n1\n1\n1\n1\n\n\n\n\nOne-Hot encoders are not often used this way, but for keyword detection or keyword-based classification this might be appropriate: i.e. this keyword was used in this document!\nSo the big difference is One Hot == \\(n\\) variables, Dummy == \\(n-1\\).\nDefinitely some ‘gotchas’ in deployment: one-hot models shouldn’t have an intercept unless you apply a ‘ridge shrinkage penalty’. Standardisation affects whether or not an intercept is needed."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#the-bag-of-words",
    "href": "lectures/7.4-Analysing_Text.html#the-bag-of-words",
    "title": "Analysing Text",
    "section": "The ‘Bag of Words’",
    "text": "The ‘Bag of Words’\nJust like a one-hot (binarised approach) on preceding slide but now we count occurences:\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n4\n2\n0\n6\n\n\nCulture item\n0\n4\n7\n0\n\n\nPolitics item\n3\n0\n0\n3\n\n\nEntertainment item\n3\n4\n8\n1"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#bow-in-practice",
    "href": "lectures/7.4-Analysing_Text.html#bow-in-practice",
    "title": "Analysing Text",
    "section": "BoW in Practice",
    "text": "BoW in Practice\nEnter, stage left, scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\n# Non-reusable transformer\nvectors = vectorizer.fit_transform(texts)\n\n# Reusable transformer\nvectorizer.fit(texts)\nvectors1 = vectorizer.transform(texts1)\nvectors2 = vectorizer.transform(texts2)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'All vectors: {vectors.toarray()}')"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#tfidf",
    "href": "lectures/7.4-Analysing_Text.html#tfidf",
    "title": "Analysing Text",
    "section": "TF/IDF",
    "text": "TF/IDF\nBuilds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:\n\\[\nW(t,d) = TF(t,d) / log(N/DF_{t})\n\\]\nFor example:\n\nIf the term ‘cat’ appears 3 times in a document of 100 words then Term Frequency given by: \\(TF(t,d)=3/100\\), and\nIf there are 10,000 documents and cat appears in 1,000 documents then Normalised Document Frequency given by: \\(N/DF_{t}=10000/1000\\) so the Inverse Document Frequency is \\(log(10)=1\\),\nSo IDF=1 and TF/IDF=0.03."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#tfidf-in-practice",
    "href": "lectures/7.4-Analysing_Text.html#tfidf-in-practice",
    "title": "Analysing Text",
    "section": "TF/IDF in Practice",
    "text": "TF/IDF in Practice\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Non-reusable form:\nvectors=vectorizer.fit_transform(texts)\n\n# Reusable form:\nvectorizer.fit(texts)\nvectors = vectorizer.transform(texts)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'Full vector: {vectors.toarray()}')\n\nWhat do you notice about how this code differs from the CountVectorizer?"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "href": "lectures/7.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "title": "Analysing Text",
    "section": "Term Co-Occurence Matrix (TCM)",
    "text": "Term Co-Occurence Matrix (TCM)\nThree input texts with a distance weighting (\\(d/2\\), where \\(d&lt;3\\)):\n\nthe cat sat on the mat\nthe cat sat on the fluffy mat\nthe fluffy ginger cat sat on the mat\n\n\n\n\n\nfluffy\nmat\nginger\nsat\non\ncat\nthe\n\n\n\n\nfluffy\n\n1\n1\n\n0.5\n0.5\n2.0\n\n\nmat\n\n\n\n\n0.5\n\n1.5\n\n\nginger\n\n\n\n0.5\n0.5\n1.0\n1.5\n\n\nsat\n\n\n\n\n3.0\n3.0\n2.5\n\n\non\n\n\n\n\n\n1.5\n3.0\n\n\ncat\n\n\n\n\n\n\n2.0\n\n\nthe"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#how-big-is-a-tcm",
    "href": "lectures/7.4-Analysing_Text.html#how-big-is-a-tcm",
    "title": "Analysing Text",
    "section": "How Big is a TCM?",
    "text": "How Big is a TCM?\nThe problem:\n\nA corpus with 10,000 words has a TCM of size \\(10,000^{2}\\) (100,000,000)\nA corpus with 50,000 words has a TCM of size \\(50,000^{2}\\) (2,500,000,000)\n\nCleaning is necessary, but it’s not sufficient to create a tractable TCM on a large corpus.\n\nAlthough usually used in the context of clustering, there’s also a curse of dimensionality here!"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#enter-document-embeddings",
    "href": "lectures/7.4-Analysing_Text.html#enter-document-embeddings",
    "title": "Analysing Text",
    "section": "Enter Document Embeddings",
    "text": "Enter Document Embeddings\nTypically, some kind of 2 or 3-layer neural network that ‘learns’ how to embed the TCM into a lower-dimension representation: from \\(m \\times m\\) to \\(m \\times n, n &lt;&lt; m\\).\nSimilar to PCA in terms of what we’re trying to achieve, but the process is utterly different.\n\nMany different approaches, but GloVe (Stanford), word2vec (Google), fastText (Facebook), and ELMo (Allen) or BERT (Google) are probably the best-known."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#sentiment-analysis",
    "href": "lectures/7.4-Analysing_Text.html#sentiment-analysis",
    "title": "Analysing Text",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nRequires us to deal in great detail with bi- and tri-grams because negation and sarcasm are hard. Also tends to require training/labelled data.\n\nSource."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#clustering",
    "href": "lectures/7.4-Analysing_Text.html#clustering",
    "title": "Analysing Text",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\nCluster\nGeography\nEarth Science\nHistory\nComputer Science\nTotal\n\n\n\n\n1\n126\n310\n104\n11,018\n11,558\n\n\n2\n252\n10,673\n528\n126\n11,579\n\n\n3\n803\n485\n6,730\n135\n8,153\n\n\n4\n100\n109\n6,389\n28\n6,626\n\n\nTotal\n1,281\n11,577\n13,751\n11,307\n37,916"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#topic-modelling",
    "href": "lectures/7.4-Analysing_Text.html#topic-modelling",
    "title": "Analysing Text",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nLearning associations of words (or images or many other things) to hidden ‘topics’ that generate them:"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#word-clouds",
    "href": "lectures/7.4-Analysing_Text.html#word-clouds",
    "title": "Analysing Text",
    "section": "Word Clouds",
    "text": "Word Clouds"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#resources",
    "href": "lectures/7.4-Analysing_Text.html#resources",
    "title": "Analysing Text",
    "section": "Resources",
    "text": "Resources\n\nOne-Hot vs Dummy Encoding\nCategorical encoding using Label-Encoding and One-Hot-Encoder\nCount Vectorization with scikit-learn\nTFIDF.com\nThe TF*IDF Algorithm Explained\nHow to Use TfidfTransformer and TfidfVectorizer\nSciKit Learn Feature Extraction\nYour Guide to LDA\nMachine Learning — Latent Dirichlet Allocation LDA\nA Beginner’s Guide to Latent Dirichlet Allocation(LDA)\nAnalyzing Documents with TF-IDF\n\nBasically any of the lessons on The Programming Historian."
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#more-resources",
    "href": "lectures/7.4-Analysing_Text.html#more-resources",
    "title": "Analysing Text",
    "section": "More Resources",
    "text": "More Resources\n\n::: {column width=“50%”} - Introduction to Word Embeddings - The Current Best of Universal Word Embeddings and Sentence Embeddings - Using GloVe Embeddings - Working with Facebook’s FastText Library - Word2Vec and FastText Word Embedding with Gensim - Sentence Embeddings. Fast, please!\n\n::: {column width=“50%”} - PlasticityAI Embedding Models - Clustering text documents using k-means - Topic extraction with Non-negative Matrix Factorization and LDA - Topic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison ::: ::::"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#what-is-it",
    "href": "lectures/8.2-Transformation.html#what-is-it",
    "title": "Transformation",
    "section": "What is it?",
    "text": "What is it?\nData transformation just means changing the raw data in some way to make it more tractable for analysis.\nFor example:\n\nCentering the mean on zero is an obvious example.\nBut we can also do quite complicated things (with caution!) in order to get a distribution that we can work with using statistical tests that have certain expectations about the data we’re feeding them.\n\n\nSo, even though we add or subtract, multiply or divide, square or log the data, because we are doing the same thing to every observation the underlying relationships between the data are ‘unchanged’."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#transformation-in-1d",
    "href": "lectures/8.2-Transformation.html#transformation-in-1d",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D\n\n\n\\[\nx-\\bar{x}\n\\]\n\n\n\nInput\nOutput\n\n\n\n\n12\n-2\n\n\n13\n-1\n\n\n14\n0\n\n\n15\n+1\n\n\n16\n+2\n\n\n\n\n\nHow is this any different?"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#so",
    "href": "lectures/8.2-Transformation.html#so",
    "title": "Transformation",
    "section": "So…",
    "text": "So…\n\nTransformations are mathematical operations applied to every observation in a data set that preserve some of the relationships between them."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#for-example",
    "href": "lectures/8.2-Transformation.html#for-example",
    "title": "Transformation",
    "section": "For Example",
    "text": "For Example\nIf we subtract the mean from everyone’s height then we can immediately tell if someone is taller or shorter than we would expect.\nIf we subtract the mean from everyone’s income then we cannot immediately tell if someone is earning more or less that we would expect.\nSo what is a useful transformation in one context, may not be in another!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#fleshing-this-out",
    "href": "lectures/8.2-Transformation.html#fleshing-this-out",
    "title": "Transformation",
    "section": "Fleshing This Out",
    "text": "Fleshing This Out\nQuestion: How can you tell if you did better than everyone else on the Quiz or on the Final Report?\n\nAnswer: Just subtracting the mean is not enough because the distributions are not the same. For that we also need to standardise the data in some way.\n\\[\nz = \\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nDivide through by the distribution!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#z-score-standardisation",
    "href": "lectures/8.2-Transformation.html#z-score-standardisation",
    "title": "Transformation",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\\[\n\\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nprint(scaler.mean_)\nscaler.transform(x)\n\nThe important thing to note is that if transform data that has not been fit and you get values outside the range used for fitting then you can no longer assume a standard normal."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#interquartile-standardisation",
    "href": "lectures/8.2-Transformation.html#interquartile-standardisation",
    "title": "Transformation",
    "section": "Interquartile Standardisation",
    "text": "Interquartile Standardisation\n\\[\n\\dfrac{x_{i}-x_{Q2}}{x_{Q3}-x_{Q1}}\n\\]\nfrom sklearn.preprocessing import RobustScaler\ntrf = RobustScaler(\n        quantile_range=(25.0,75.0)).fit(x)\ntrf.transform(x)"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#interdecile-standardisation",
    "href": "lectures/8.2-Transformation.html#interdecile-standardisation",
    "title": "Transformation",
    "section": "Interdecile Standardisation",
    "text": "Interdecile Standardisation\n\\[\n\\dfrac{x_{i}-x_{50^{th}}}{x_{90^{th}}-x_{10^{th}}}\n\\]\nprint(\"You've got this...\")\n\nWhy standardise:\n\nWe understand the properties of normal-ish distributions, and can simulate them easily.\nMore ‘power’ in the statistical tools available.\nMany analyses assume that ‘error’ is random and symmetric (homoscedastic, not skewed)."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#group-standardisation",
    "href": "lectures/8.2-Transformation.html#group-standardisation",
    "title": "Transformation",
    "section": "Group Standardisation",
    "text": "Group Standardisation\n\\[\nx'_{a,i} = \\dfrac{x_{ai}}{\\sum_{g} r_{N,g} P_{a,g}}\n\\]\n\nDetails:\n\n\\(x_{a,i}\\) = Value of attribute i in area a.\n\\(P_{a,g}\\) = Population of group g in area a.\n\\(r_{N,g}\\) = National ratio N of group g\n\\(\\sum\\) = Sum for all groups.\n\\(x'_{a,i}\\) = Standardised value of i in area a."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#proportional-normalisation",
    "href": "lectures/8.2-Transformation.html#proportional-normalisation",
    "title": "Transformation",
    "section": "Proportional Normalisation",
    "text": "Proportional Normalisation\n\\[\n\\dfrac{x_{i}}{\\sum{x}_{i=1}^{n}}\n\\]\nimport numpy as np\nx/np.sum(x)\n\nNumpy has a fair few options for implementing this, but note that log means natural log, not log10!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#range-normalisation",
    "href": "lectures/8.2-Transformation.html#range-normalisation",
    "title": "Transformation",
    "section": "Range Normalisation",
    "text": "Range Normalisation\n\\[\n\\dfrac{x_{i}-x_{min}}{x_{max}-x_{min}}\n\\]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(x)\nprint(scaler.data_max_)\nscaler.transform(x)\n\nNormalisation helps in several ways:\n\nScaling is important for comparability\nClustering is particularly sensitive to scale"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#log-transformation",
    "href": "lectures/8.2-Transformation.html#log-transformation",
    "title": "Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nRecall: logs are the inverse of exponentiation!\n\nSo if \\(10^{3} = 1,000\\) then \\(log_{10}(1,000) = 3\\).\nAnd if \\(10^{0} = 1\\) then \\(log_{10}(1) = 0\\)\n\nimport numpy as np\nxhat = np.log(x)\n\n\nWhy is this so common? Esp. in social sciences?\n\n\nNote that numpy uses the natural log in base \\(e\\)."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#why-log-transform",
    "href": "lectures/8.2-Transformation.html#why-log-transform",
    "title": "Transformation",
    "section": "Why Log Transform?",
    "text": "Why Log Transform?\nLet’s assume that \\(x = \\{10, 100, 1000, 10000\\}\\), consider what happens if:\n\nThe formula for the mean is \\(\\frac{\\sum{x}}{n}\\).\nThe formula for variance is \\(\\frac{(x-\\bar{x})^{2}}{n}\\).\n\nThe Natural Log (\\(e\\)) has certain advantages over other logs and should probably be your default choice for log transformations."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#other-transforms",
    "href": "lectures/8.2-Transformation.html#other-transforms",
    "title": "Transformation",
    "section": "Other Transforms…",
    "text": "Other Transforms…\n\nQuantile (maps the PDF of each feature to a uniform distribution)\nSquare Root (often with count data)\nArcsine/Angular (with percentages, proportions, text)\nRank (with care on extreme distributions)\nBox-Cox and Yeo-Johnson (arbitrary power transformations)\n\n\n\nTo report measures of central tendency it’s usually helpful to convert back to the original units.\nThe more extreme the transformation the less meaningful measures of dispersion.\nCorrelation can be significantly affected in either direction.\nCount data can be tricky because you should not have negative values (especially \\(-\\infty\\))."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#when-transforms-dont-help",
    "href": "lectures/8.2-Transformation.html#when-transforms-dont-help",
    "title": "Transformation",
    "section": "When Transforms Don’t Help",
    "text": "When Transforms Don’t Help\nArbitrarily transforming data isn’t a panacea. ‘Robust’ tests can be another approach when all else fails and two common approaches are:\n\nTrimming: cutting off, say, the top and bottom 5% of scores would start to remove skew and offer a more useful view of the central tendency of the data.\nBootstrapping: taking many sub-samples (usually of \\(n-1\\) data points or similar) we can build a picture of how certain metrics vary."
  },
  {
    "objectID": "lectures/8.2-Transformation.html#one-last-note",
    "href": "lectures/8.2-Transformation.html#one-last-note",
    "title": "Transformation",
    "section": "One Last Note",
    "text": "One Last Note\n\nThe term normalization is used in many contexts, with distinct, but related, meanings. Basically, normalizing means transforming so as to render normal. When data are seen as vectors, normalizing means transforming the vector so that it has unit norm. When data are though of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be normal, normalizing means transforming to unit variance.\n\nSource: Stack Exchange"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#whats-wrong-with-this-map",
    "href": "lectures/8.2-Transformation.html#whats-wrong-with-this-map",
    "title": "Transformation",
    "section": "What’s Wrong with this Map?",
    "text": "What’s Wrong with this Map?"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#thats-better",
    "href": "lectures/8.2-Transformation.html#thats-better",
    "title": "Transformation",
    "section": "That’s Better!",
    "text": "That’s Better!"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#whats-a-projection",
    "href": "lectures/8.2-Transformation.html#whats-a-projection",
    "title": "Transformation",
    "section": "What’s a Projection?",
    "text": "What’s a Projection?\n\nSource"
  },
  {
    "objectID": "lectures/8.2-Transformation.html#resources",
    "href": "lectures/8.2-Transformation.html#resources",
    "title": "Transformation",
    "section": "Resources",
    "text": "Resources\n\nNormalisation vs Standardisation – Quantitative analysis\nTransforming Data with R\nData Transformation and Normality Testing\nIntroduction to Logarithms\nWhat is ‘e’ and where does it come from?\nLogarithms - What is e?\nsklearn API reference\nCompare effects of different scalers on data with outliers\nThe things you’ll find in higher dimensions (useful brief discussion of manifolds)"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\npd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join",
    "href": "lectures/9.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart\n\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join-result",
    "href": "lectures/9.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\npd.merge(df1, df2, \n         how = 'inner',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#but-what-if",
    "href": "lectures/9.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then:\npd.merge(df1, df2, \n         how      = 'inner',\n         left_on  = 'SensorID',\n         right_on = 'SensorKey')\n \nWe will get an ‘extra’ field:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#outer-join",
    "href": "lectures/9.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\npd.merge(df1, df2,\n         how = 'outer',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#left-join",
    "href": "lectures/9.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\npd.merge(df1, df2, \n        how = 'left',\n        on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append-concat",
    "href": "lectures/9.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#concat",
    "href": "lectures/9.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append",
    "href": "lectures/9.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 0, 'Parameter': 5,  'Humidity', 'Value': 0.45},\n    {'SensorID': 1, 'Parameter': 5,  'Humidity', 'Value': 0.31},\n    {'SensorID': 2, 'Parameter': 4, 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 3, 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#resources",
    "href": "lectures/9.1-Linking_Data.html#resources",
    "title": "Linking Data",
    "section": "Resources",
    "text": "Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#useful-but-limited",
    "href": "lectures/9.3-Grouping_Data.html#useful-but-limited",
    "title": "Grouping Data",
    "section": "Useful, But Limited?",
    "text": "Useful, But Limited?\n\n\n\nMethod\nAchieves\n\n\n\n\ncount()\nTotal number of items\n\n\nfirst(), last()\nFirst and last item\n\n\nmean(), median()\nMean and median\n\n\nmin(), max()\nMinimum and maximum\n\n\nstd(), var()\nStandard deviation and variance\n\n\nmad()\nMean absolute deviation\n\n\nprod()\nProduct of all items\n\n\nsum()\nSum of all items\n\n\n\n\nHere are a bunch of pandas functions that have to do with aggregating data in some way. Some of these you’ll have seen before, some you may not. However, up to this point if you wanted to to know the median price of each type of Airbnb listing, or the sum of each type of vehicle sold, you’d have had to select out one listing type or vehicle type, call median or sum, and then remember the result. Let’s change that."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#grouping-operations",
    "href": "lectures/9.3-Grouping_Data.html#grouping-operations",
    "title": "Grouping Data",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#in-practice",
    "href": "lectures/9.3-Grouping_Data.html#in-practice",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(&lt;fields&gt;).&lt;function&gt;\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "href": "lectures/9.3-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "title": "Grouping Data",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#pivot-tables",
    "href": "lectures/9.3-Grouping_Data.html#pivot-tables",
    "title": "Grouping Data",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(&lt;series&gt;, &lt;bins&gt;) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#in-practice-1",
    "href": "lectures/9.3-Grouping_Data.html#in-practice-1",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#deriving-measures-of-diversity",
    "href": "lectures/9.3-Grouping_Data.html#deriving-measures-of-diversity",
    "title": "Grouping Data",
    "section": "Deriving Measures of Diversity",
    "text": "Deriving Measures of Diversity\n\nOne of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few… Location Quotient (LQ), Herfindah-Hirschman Index (HHI), Shanon Entropy.\n\n\nI like easy measures."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#location-quotient",
    "href": "lectures/9.3-Grouping_Data.html#location-quotient",
    "title": "Grouping Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R. \\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[&lt; 1\\]\n\n\nLow Regional Share\n\\[&gt; 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#herfindahl-hirschman-index",
    "href": "lectures/9.3-Grouping_Data.html#herfindahl-hirschman-index",
    "title": "Grouping Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry: \\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[&lt; 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/9.3-Grouping_Data.html#shannon-entropy",
    "href": "lectures/9.3-Grouping_Data.html#shannon-entropy",
    "title": "Grouping Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure: \\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "This page contains all articles from the weekly readings and a few more that may be useful.\n\n\nAlsudais, Abdulkareem. 2021. “Incorrect Data in the Widely Used Inside Airbnb Dataset.” Decision Support Systems 141:113453. https://doi.org/10.1016/j.dss.2020.113453.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/10.1177%2F0263276419851846.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 0 (0):e12403. https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching Introductory GIS Programming to Geographers Using an Open Source Python Approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are Statisticians Cold-Blooded Bosses? A New Perspective on the ‘Old’concept of Statistical Population.” Teaching Statistics 35 (1). Wiley Online Library:66–71. https://doi.org/10.1111/j.1467-9639.2012.00524.x.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nO’Sullivan, David, and Steven M Manson. 2015. “Do Physicists Have Geography Envy? And What Can Geographers Learn from It?” Annals of the Association of American Geographers 105 (4). Taylor & Francis:704–22.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nRose, Gillian. 1997. “Situating Knowledges: Positionality, Reflexivities and Other Tactics.” Progress in Human Geography 21 (3):305–20. https://doi.org/10.1191/030913297673302122.\n\n\nScheider, Simon, Enkhbold Nyamsuren, Han Kruiger, and Haiqi Xu. 2020. “Why Geographic Data Science Is Not a Science.” Geography Compass 14 (11). Wiley Online Library:e12537.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2P) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500."
  },
  {
    "objectID": "sessions/reading_week.html",
    "href": "sessions/reading_week.html",
    "title": "Reading Week",
    "section": "",
    "text": "Past student performance strongly suggests that you should engage with the assigned readings in order to do well on the Data Audit.\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any missed reading(s).\nStart to think about the research question(s) for your Policy Briefing.\nStart to think about how you might ask your question(s) of the data.\n\n\n\nLooking ahead to the Group Work, I’d also strongly suggest that you browse the Full Reference List for ideas. The bibliography is a working document and I will add more items as and when I come across them or new works are published, but this is a good time to start reading about the ethical and practical issues arising from Airbnb’s operations and the data to which we have access."
  },
  {
    "objectID": "sessions/reading_week.html#overview",
    "href": "sessions/reading_week.html#overview",
    "title": "Reading Week",
    "section": "",
    "text": "Past student performance strongly suggests that you should engage with the assigned readings in order to do well on the Data Audit.\n\n\n\n\n\n\nImportant\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any missed reading(s).\nStart to think about the research question(s) for your Policy Briefing.\nStart to think about how you might ask your question(s) of the data.\n\n\n\nLooking ahead to the Group Work, I’d also strongly suggest that you browse the Full Reference List for ideas. The bibliography is a working document and I will add more items as and when I come across them or new works are published, but this is a good time to start reading about the ethical and practical issues arising from Airbnb’s operations and the data to which we have access."
  },
  {
    "objectID": "sessions/reading_week.html#readings",
    "href": "sessions/reading_week.html#readings",
    "title": "Reading Week",
    "section": "Readings",
    "text": "Readings\nFor the group work you will also want to consider:\n\nElwood and Wilson (2017) &lt;URL&gt;\nElwood and Leszczynski (2018) &lt;URL&gt;\nBemt et al. (2018) &lt;URL&gt;\nAmoore (2019) &lt;URL&gt;\nCrawford and Finn (2015) &lt;URL&gt;\nMattern (2020) &lt;URL&gt;\nMattern (2015) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Setting Up",
    "section": "",
    "text": "In the first week we will be focussing on the supporting infrastructure for ‘doing data science’. That is to say, we’ll be dealing with the installation and configuration of tools such as GitHub and Docker which support replicable, shareable, and document-able data science. As a (free) bonus, the use of these tools also protects you against catastrophic (or the merely irritating) data loss thanks to over-zealous editing of code or content. You should see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career.\n\n\n\n\n\n\nLearning objectives\n\n\n\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nYou should also see this session as connecting to Quantitative Methods Week 1 content on ‘setting quantitative research questions’ since the main assessment will require you to develop a data-led policy briefing. In other words, you’ll need to map current policy on to one or more research questions that can be quantitatively examined using the tools and techniques acquired over the course of the term! While you don’t need to start work on this yet, you should keep it in the back of your mind for when you come across readings/results that you’d like to explore in more detail."
  },
  {
    "objectID": "sessions/week1.html#overview",
    "href": "sessions/week1.html#overview",
    "title": "Setting Up",
    "section": "",
    "text": "In the first week we will be focussing on the supporting infrastructure for ‘doing data science’. That is to say, we’ll be dealing with the installation and configuration of tools such as GitHub and Docker which support replicable, shareable, and document-able data science. As a (free) bonus, the use of these tools also protects you against catastrophic (or the merely irritating) data loss thanks to over-zealous editing of code or content. You should see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career.\n\n\n\n\n\n\nLearning objectives\n\n\n\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nYou should also see this session as connecting to Quantitative Methods Week 1 content on ‘setting quantitative research questions’ since the main assessment will require you to develop a data-led policy briefing. In other words, you’ll need to map current policy on to one or more research questions that can be quantitatively examined using the tools and techniques acquired over the course of the term! While you don’t need to start work on this yet, you should keep it in the back of your mind for when you come across readings/results that you’d like to explore in more detail."
  },
  {
    "objectID": "sessions/week1.html#preparation",
    "href": "sessions/week1.html#preparation",
    "title": "Setting Up",
    "section": "Preparation",
    "text": "Preparation\nAlthough none of these activities are compulsory in advance of the first session, getting your computer set up to code does take time and most of these preparatory activites are fairly straightforward… with a few exceptions noted below. If you are able to get these tools installed in advance then you can focus on the taught content in the first two practicals rather than also wrestling with an installation process. This will also give us more time to help you if you discover that you’re one of the unlucky few for whom getting set up is a lot more work!\n\n\n\n\n\n\nTip\n\n\n\nComplete as many of these activities as you can:\n\nGo through the computer health check.\nTry to install the Base Utilities.\nHave a go at installing the programming environment.\n\n\n\nThe last of these is the stage where you’re most likely to encounter problems that will need our assistance, so knowing that you need our help in Week 1 means that you can ask for it much sooner in the practical!"
  },
  {
    "objectID": "sessions/week1.html#class",
    "href": "sessions/week1.html#class",
    "title": "Setting Up",
    "section": "Class",
    "text": "Class\nIn this week’s workshop we will review the module aims, learning outcomes, and expectations with a general introduction to the course.\n\n\n\nSession\nPresentation\n\n\n\n\nGetting Oriented\nSlides\n\n\nTools of the Trade\nSlides\n\n\nWriting Code\nSlides\n\n\nGroup Working\nSlides"
  },
  {
    "objectID": "sessions/week1.html#practical",
    "href": "sessions/week1.html#practical",
    "title": "Setting Up",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nNotebook\n\nChange “notice the \\!” to notice the “\\”!\nAdd back videos for Docker, etc.?\nTypo “IDEA” for “IDE”\nTask 4 should be renamed to something like “Creating your first (remote) file”\nMove Task 5 ahead of Task 4 and add parquet, GPKG, and feather extensions\nUpdate information on the type of token to create in 6.2\nUpdate 6.5 instructions so sequencing makes sense and add ‘tip’\n\n\n\n\nThis week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2, and familiarising you with ‘how people do data science’. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: get out of the way of doing ‘stuff’; support teams in negotating conflicts in code; make it easy to share results; and make it easy to ensure that everyone is ‘on the same page’.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nGetting you up and running with the coding and collaboration tools.\nProviding you with hands-on experience of using these tools.\nConfiguring your programming environment for the rest of the programme.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Foundations (Pt. 2)",
    "section": "",
    "text": "This week we will dig into data (lists and dictionaries) in greater detail so that you understand how we design structures to store and organise data to simplify our analysis. We will also look at how frequently-used code can be packaged up in functions and libraries. This is the point at which we begin to engage with code in a more abstract way because we are increasingly interested in reusability and flexibility.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTo see how ‘simple’ concepts can be (re)combined to tackle complex problems.\nTo develop an understanding of code re-use through functions.\nTo develop an appreciation of the utility of packages and namespaces.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nDictionaries\nVideo\nSlides\n\n\nLOLs\nVideo\nNotes\n\n\nDOLs to Data\nVideo\nSlides\n\n\nFunctions\nVideo\nSlides\n\n\nPackages\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nEtherington (2016) &lt;URL&gt;\nDonoho (2017) &lt;URL&gt;\nUnwin (1980) &lt;URL&gt;\n\nComplete the short Moodle quiz asociated with the week’s activities."
  },
  {
    "objectID": "sessions/week3.html#overview",
    "href": "sessions/week3.html#overview",
    "title": "Foundations (Pt. 2)",
    "section": "",
    "text": "This week we will dig into data (lists and dictionaries) in greater detail so that you understand how we design structures to store and organise data to simplify our analysis. We will also look at how frequently-used code can be packaged up in functions and libraries. This is the point at which we begin to engage with code in a more abstract way because we are increasingly interested in reusability and flexibility.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTo see how ‘simple’ concepts can be (re)combined to tackle complex problems.\nTo develop an understanding of code re-use through functions.\nTo develop an appreciation of the utility of packages and namespaces.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nDictionaries\nVideo\nSlides\n\n\nLOLs\nVideo\nNotes\n\n\nDOLs to Data\nVideo\nSlides\n\n\nFunctions\nVideo\nSlides\n\n\nPackages\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nEtherington (2016) &lt;URL&gt;\nDonoho (2017) &lt;URL&gt;\nUnwin (1980) &lt;URL&gt;\n\nComplete the short Moodle quiz asociated with the week’s activities."
  },
  {
    "objectID": "sessions/week3.html#practical",
    "href": "sessions/week3.html#practical",
    "title": "Foundations (Pt. 2)",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\nThis week’s practical will take you through the use of dictionaries and introduce the concept of ‘nested’ data structures. We’ll also be looking at how functions (and variables) can be collected into resuable packages that we can either make ourselves or draw on a worldwide bank of experts – I know who I’d rather depend on when the opportunity arises!\nHowever, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling Notebooks 6, 8, 9, and 10. You can either:\n\nFollow the instructions for running these in Google’s Collaboratory; or\nDownload the Raw files from the Code Camp Repository on GitHub.\n\nNote: there is an issue with the GeoJSON tasks in Notebooks 8 and 9. We can discuss in the Class.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nComparing the use of Python lists and dictionaries to store tabular data.\nExtending lists and dictionaries into nested data structures.\nBeginning to make use of packages to access/interact with data.\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Numeric Data",
    "section": "",
    "text": "This week we will be introducing the use of the pandas library for data analysis and management through a focus on numeric data and its distribution(s). This marks a major shift from working with concepts (lists, dictionaries, functions, etc.) largely in isolation to encountering all of them together ‘in the wild’ as part of a full data science workflow. So we are moving from the acquisition of concepts to their integration in the same way that we will — over the course of these three sessions — be coming from data acquisition to data integration.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn appreciation of how and why this module differs from (QM) CASA0007.\nThe beginnings of a more integrative understanding of foundational computer science concepts and the practice(s) of data science.\nA basic understanding of data acquisition and manipulation in Python.\n\n\n\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLogic\nVideo\nSlides\n\n\nRandomness\nVideo\nSlides\n\n\nData\nVideo\nSlides\n\n\nPandas\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nD’Ignazio and Klein (2020), chap. 4, What Gets Counted Counts\n&lt;URL&gt;\nWachsmuth and Weisler (2018)] &lt;URL&gt;\nHarris (2018) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week5.html#overview",
    "href": "sessions/week5.html#overview",
    "title": "Numeric Data",
    "section": "",
    "text": "This week we will be introducing the use of the pandas library for data analysis and management through a focus on numeric data and its distribution(s). This marks a major shift from working with concepts (lists, dictionaries, functions, etc.) largely in isolation to encountering all of them together ‘in the wild’ as part of a full data science workflow. So we are moving from the acquisition of concepts to their integration in the same way that we will — over the course of these three sessions — be coming from data acquisition to data integration.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn appreciation of how and why this module differs from (QM) CASA0007.\nThe beginnings of a more integrative understanding of foundational computer science concepts and the practice(s) of data science.\nA basic understanding of data acquisition and manipulation in Python.\n\n\n\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLogic\nVideo\nSlides\n\n\nRandomness\nVideo\nSlides\n\n\nData\nVideo\nSlides\n\n\nPandas\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nD’Ignazio and Klein (2020), chap. 4, What Gets Counted Counts\n&lt;URL&gt;\nWachsmuth and Weisler (2018)] &lt;URL&gt;\nHarris (2018) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week5.html#practical",
    "href": "sessions/week5.html#practical",
    "title": "Numeric Data",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\nIn this practical we will begin working with the InsideAirbnb data, which you will have briefly examined in CASA0005. This week we focus on the first ‘class’ of data in the data set: simple numeric columns. We will see how to use Pandas for (simple) visualisation and (the beginnings of) analysis. It is hoped that you will see how Pandas combines and builds on techniques that we’ve already seen: while Pandas is incredibly sophisticated, the underlying concepts have been covered in the preceding three weeks! At this point we will also begin to make use of Pandas functionality to subset and explore the data.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nSeeing how Pandas is ‘just’ a sophisticated extension of what we’ve already done.\nFamiliarising yourself with Pandas functionality.\nPerforming basic data cleaning and exploration tasks (including visualisation).\nSelecting and aggregating data in pandas.\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week7.html",
    "href": "sessions/week7.html",
    "title": "Textual Data",
    "section": "",
    "text": "Although the direct use of textual (both structured and unstructured) data is still relatively rare in spatial analyses, the growth of crowd-sourced and user-generated content points to the growing importance of this area. he tools and approaches in this area are also evolving quickly and changing rapidly, so this week is intended primarily to familiarise you with the basic landscape in preparation for you developing your skills further in your own time!\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nAn awareness of the benefits of separating content from presentation.\nA basic understanding of pattern-matching in Python (you will have been exposed to this Week 2 of CASA0005)\nA basic understanding of how text can be ‘cleaned’ to make it more amenable for analysis\nAn appreciation of parallelisation in the context of text processing.\nAn appreciation of how text can be analysed.\n\n\n\nThe manipulation of text requires a high level of abstraction – of thinking about words as data in ways that are deeply counter-intuitive – but the ability to do forms a critical bridge between this block and the subsequent one, while also reinforcing the idea that numerical, spatial, and textual data analyses provide alternative (and often complementary) views into the data.\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nNotebooks as Documents\nVideo\nSlides\n\n\nPatterns in Text\nVideo\nSlides\n\n\nCleaning Text\nVideo\nSlides\n\n\nAnalysing Text\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nLadd (2020) &lt;URL&gt;\nLavin (2019) &lt;URL&gt;\n\nComplete the short Moodle quiz associated with this week’s activities."
  },
  {
    "objectID": "sessions/week7.html#overview",
    "href": "sessions/week7.html#overview",
    "title": "Textual Data",
    "section": "",
    "text": "Although the direct use of textual (both structured and unstructured) data is still relatively rare in spatial analyses, the growth of crowd-sourced and user-generated content points to the growing importance of this area. he tools and approaches in this area are also evolving quickly and changing rapidly, so this week is intended primarily to familiarise you with the basic landscape in preparation for you developing your skills further in your own time!\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nAn awareness of the benefits of separating content from presentation.\nA basic understanding of pattern-matching in Python (you will have been exposed to this Week 2 of CASA0005)\nA basic understanding of how text can be ‘cleaned’ to make it more amenable for analysis\nAn appreciation of parallelisation in the context of text processing.\nAn appreciation of how text can be analysed.\n\n\n\nThe manipulation of text requires a high level of abstraction – of thinking about words as data in ways that are deeply counter-intuitive – but the ability to do forms a critical bridge between this block and the subsequent one, while also reinforcing the idea that numerical, spatial, and textual data analyses provide alternative (and often complementary) views into the data.\n\n\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nNotebooks as Documents\nVideo\nSlides\n\n\nPatterns in Text\nVideo\nSlides\n\n\nCleaning Text\nVideo\nSlides\n\n\nAnalysing Text\nVideo\nSlides\n\n\n\n\n\n\n\nCome to class prepared to present/discuss:\n\nLadd (2020) &lt;URL&gt;\nLavin (2019) &lt;URL&gt;\n\nComplete the short Moodle quiz associated with this week’s activities."
  },
  {
    "objectID": "sessions/week7.html#practical",
    "href": "sessions/week7.html#practical",
    "title": "Textual Data",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\nIn the practical we will continue to work with the InsideAirbnb data, here focussing on the third ‘class’ of data in the data set: text. We will see how working with text is more complex than working with numeric or spatial data and, consequently, why the computational costs rise accordingly. This practical should suggest some new lines of inquiry for Data+Policy Briefing.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nApplying simple regular expressions to find patterns in text.\nHow to clean text in preparation for further analysis.\nSimple transformations that allow you to analyse text (e.g. TF/IDF)\nWays of exploring groups/similarity in textual data.\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Grouping Data",
    "section": "",
    "text": "This week we will be looking at various ways of grouping data, whether it is by variable or by algorithm. So we begin by covering how data can be aggregated in Python using Pandas before turning to the practical challenges of classification (labeled data) and clustering (unlabeled data). This ‘completes’ the pipeline begun in Week 4 using the tools introduced in Weeks 1 and 2, but if you remember your ‘epicycles of analysis’ illustration then you’ll realise that this is, at best, a first pass through the data science process and there are multiple places where insights derived from the practicals (on outliers/problematic records, on data quality issues, on data selection, etc.) could be fed back through the pipeline to adjust and improve the analytical outputs.\nWe will also be shifting our focus in the live session to the final parts of the group submission, but you should also be looking at how this module connects and integrates ideas covered in CASA0001 (UST), CASA0005 (GIS), and CASA0007 (QM). So there will be only a minimal live-coding session in order to leave as much time as possible for the groups to meet and start working on their final projects.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn understanding of the differences between aggregation, classification, and clustering.\nAn appreciation of the utility of deriving grouped variables and proxies from raw data.\nAn appreciation of how clustering as part of an analytical pipeline differs from the material covered in CASA0007 and so enhances our understanding of ‘paradigms’ in CASA0001.\nA general appreciation of how different clustering algorithms work and how this differs from classifcation.\n\n\n\n\n\nYou should, by now, be familiar with the concept of how to cluster data from the QM module (CASA0007), so this week is actually focussed on how to move beyond k-means. The point is to contextualise these approaches as part of a data science ‘pipeline’ and to contrast to them with the more theoretical aspects covered elsewhere. We are less interested in the mathematical and technical aspects, and more interested in how one might go about selecting the appropriate algorithm for a particular problem.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nGrouping Data\nVideo\nSlides\n\n\nClassification\nVideo\nSlides\n\n\nClustering\nVideo\nSlides\n\n\nClustering and Geography\nVideo\nSlides\n\n\n\n\n\n\n\nYou should come to class prepared to present/discuss:\n\nD’Ignazio and Klein (2020), chap. 3, On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints &lt;URL&gt;\nBadger, Bui, and Gebeloff (2019) &lt;URL&gt;\nMassey (1996) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week9.html#overview",
    "href": "sessions/week9.html#overview",
    "title": "Grouping Data",
    "section": "",
    "text": "This week we will be looking at various ways of grouping data, whether it is by variable or by algorithm. So we begin by covering how data can be aggregated in Python using Pandas before turning to the practical challenges of classification (labeled data) and clustering (unlabeled data). This ‘completes’ the pipeline begun in Week 4 using the tools introduced in Weeks 1 and 2, but if you remember your ‘epicycles of analysis’ illustration then you’ll realise that this is, at best, a first pass through the data science process and there are multiple places where insights derived from the practicals (on outliers/problematic records, on data quality issues, on data selection, etc.) could be fed back through the pipeline to adjust and improve the analytical outputs.\nWe will also be shifting our focus in the live session to the final parts of the group submission, but you should also be looking at how this module connects and integrates ideas covered in CASA0001 (UST), CASA0005 (GIS), and CASA0007 (QM). So there will be only a minimal live-coding session in order to leave as much time as possible for the groups to meet and start working on their final projects.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn understanding of the differences between aggregation, classification, and clustering.\nAn appreciation of the utility of deriving grouped variables and proxies from raw data.\nAn appreciation of how clustering as part of an analytical pipeline differs from the material covered in CASA0007 and so enhances our understanding of ‘paradigms’ in CASA0001.\nA general appreciation of how different clustering algorithms work and how this differs from classifcation.\n\n\n\n\n\nYou should, by now, be familiar with the concept of how to cluster data from the QM module (CASA0007), so this week is actually focussed on how to move beyond k-means. The point is to contextualise these approaches as part of a data science ‘pipeline’ and to contrast to them with the more theoretical aspects covered elsewhere. We are less interested in the mathematical and technical aspects, and more interested in how one might go about selecting the appropriate algorithm for a particular problem.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nGrouping Data\nVideo\nSlides\n\n\nClassification\nVideo\nSlides\n\n\nClustering\nVideo\nSlides\n\n\nClustering and Geography\nVideo\nSlides\n\n\n\n\n\n\n\nYou should come to class prepared to present/discuss:\n\nD’Ignazio and Klein (2020), chap. 3, On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints &lt;URL&gt;\nBadger, Bui, and Gebeloff (2019) &lt;URL&gt;\nMassey (1996) &lt;URL&gt;"
  },
  {
    "objectID": "sessions/week9.html#practical",
    "href": "sessions/week9.html#practical",
    "title": "Grouping Data",
    "section": "Practical",
    "text": "Practical\nThe previous week has set up nicely for approaching aggregation, classification, and clustering as functions of the (transformed and reduced) data space. With this, you have essentially covered a full data science analytical pipeline from start (setting up) to finish (cluster/classification analysis) and can hopefully see how these pieces fit together to support one another, and how there is no such thing as a ‘right’ way to approach an analysis… just better and worse.\n\n\n\n\n\n\nChanges before 22/23\n\n\n\n\nPoint to cross-module content/recaps.\nMake link between videos+readings and practical explicit.\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nHow to group and aggregate data.\nThe connections between classification and clustering.\nThe use of classification as a predictive process with labeled data.\nThe choice of k in k-means and extraction of representative centroids.\nThe use of alternative clustering algorithms (DBSCAN, OPTICS, Self-Organising Maps, and ADBSCAN).\n\n\n\nTo access the practical:\n\nPreview on GitHub\nDownload the Notebook"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-1",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-1",
    "title": "Computers in Urban Studies",
    "section": "Wave 1",
    "text": "Wave 1\n“A computer in every institution”:\n\nRoughly the 1950s–70s\nComputers as ‘super-human’ calculators for data\nData + models as theory-testing tool\n\nRetrospectively: the 1st quantitative revolution.\n\nMany see this as incorrect and focus on the theoretical aspect."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-2",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-2",
    "title": "Computers in Urban Studies",
    "section": "Wave 2",
    "text": "Wave 2\n“A computer in every office”:\n\nRoughly the 1980s–2000s\nComputers as tools for thinking about spatial relationships\nExplicit modelling of local spatial effects\n\nRetrospectively: the GIS revolution.\n\nI personally see this as incorrect because GIS is Wave 1."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-3",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#wave-3",
    "title": "Computers in Urban Studies",
    "section": "Wave 3",
    "text": "Wave 3\n“A computer in every thing”:\n\nRoughly the mid-2000s–?\nComputers as tools for generating data (pace ABM researchers)\nGeodata being continuously produced as byproduct of other activities\nShift from researching attributes to behaviours (pace Hägerstrand)\n\nRetrospectively: the big data revolution or 2nd quantitative revolution.\n\nShift from computers as processors of data to integrated, pervasive systems that spew out data on everything."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#all-waves-still-going",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#all-waves-still-going",
    "title": "Computers in Urban Studies",
    "section": "All Waves Still Going!",
    "text": "All Waves Still Going!\nWave 1: Computers help me do it faster\n\nGIS is ‘just’ the industrial revolution impacting cartography.\n\nWave 2: Computers help me to think\n\nGeocomputation & local stats are qualitatively & meaningfully different.\n\nWave 3: Computers help me to learn\n\nNot ‘just’ about the ‘bigness’ of data, though that is important.\n\n\nWave 2 is about implementing ideas such as recursion and iteration – these could, in theory, have been tackled in Wave 1, but in practice that’s not what people were doing.\nWave 3 is about more explicitly allowing computers to learn about data so that we can extract insight from these models – these could also, in theory, have been tackled in Wave 2 but in practice that’s not what people were doing.\nI’m not totally happy about my description of Wave 3 and will try to dig into this in a little more detail but suggestions welcome!"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#anticipated-by-hägerstrand-1967",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#anticipated-by-hägerstrand-1967",
    "title": "Computers in Urban Studies",
    "section": "Anticipated by Hägerstrand (1967)",
    "text": "Anticipated by Hägerstrand (1967)\n\nI think that the computer can do three different and useful things for us. The first and simplest operation is… descriptive mapping the second… is the analytical one The third kind of service is… to run process models by which we might try to reproduce observed or create hypothetical chains of events of a geographical nature.\nT. Hägerstrand (1967), ‘The Computer and the Geographer’, TiBG"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#but-persistent-critiques",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#but-persistent-critiques",
    "title": "Computers in Urban Studies",
    "section": "But Persistent Critiques",
    "text": "But Persistent Critiques\n\nThere is a clear disparity between the sophisticated theoretical and methodological framework which we are using and our ability to say anything really meaningful about events as they unfold around us. There are too many anomalies… There is an ecological problem, an urban problem… and yet we seem incapable of saying anything of any depth or profundity about any of them. When we do say something it appears trite and rather ludicrous.\nHarvey (1972, p.6)"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#data-science",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#data-science",
    "title": "Computers in Urban Studies",
    "section": "Data science?",
    "text": "Data science?\n\n\n\n\n\nIndustry-led\nSpatially ignorant (often)\nDisciplinarily greedy (often)"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#what-is-different",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#what-is-different",
    "title": "Computers in Urban Studies",
    "section": "What is Different?",
    "text": "What is Different?\nAccording to Donoho (2017) ‘data science’ differs from plain old ‘statistics’ through an interest in:\n\nData gathering, preparation, and exploration;\nData representation and transformation;\nComputing with data;\nData visualisation and presentation;\nData modelling; and\nA reflexive ‘science of data science’."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#in-practice",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#in-practice",
    "title": "Computers in Urban Studies",
    "section": "In Practice…",
    "text": "In Practice…\nI think there are several distinguishing features that I encounter in day-to-day (geography) work:\n\nData-driven methods development & deployment\nExplicit tuning/meta-parameterisation\nExplicit feature optimisation/engineering\nExplicit training/testing from ‘one shot’ data\n‘Black boxes’ feature prominently & ‘online learning’ emerging quickly\n\nData science as process and pipeline, not just input to research."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#do-we-need-it",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#do-we-need-it",
    "title": "Computers in Urban Studies",
    "section": "Do We Need It?",
    "text": "Do We Need It?\n\n\n\nSource: xkcd\n\nMore readings:\n\nO’Sullivan, D. and Manson, S.M. (2015) ‘Do physicists have geography envy? And what can geographers learn from it?’ Annals of the Association of American Geographers 105(4), DOI.\nScheider, S. and Nyamsuren, E. and Kruiger, H. and Xu, H. (2020) ‘Why georgaphic data science is not a science’, Geography Compass, DOI."
  },
  {
    "objectID": "assessments/exam.html",
    "href": "assessments/exam.html",
    "title": "Timed Open Book Exam",
    "section": "",
    "text": "This ‘exam’ (due by Thursdsay, 23 November 2023 @ 7:30pm and worth 30%) will be administered as a quiz through Moodle: you will be asked to download a small data set and perform a series of specified analytical tasks using that data. For each question you will be asked both to type in an answer (which may be numerical or textual) and to paste in the code that you have used to obtain that answer."
  },
  {
    "objectID": "assessments/exam.html#expectations",
    "href": "assessments/exam.html#expectations",
    "title": "Timed Open Book Exam",
    "section": "Expectations",
    "text": "Expectations\nThis is the first year that we have attempted to run a timed, open-book examination as part of this module so we may make adjustments to the process based on the capabilities of the class (taken in the round) and the challenges presented by the data set selected.\nHowever, as a general set of expectations:\n\nAny content covered prior to Week 7 may be encountered in the exam (so we will not test you on material covered in, or after, the Textual Data session.\nThe principal library upon which the exam will rely is pandas.\nThe exam is ‘open book’, but there are three important limitations:\n\nYou may not obtain direct assisstance from another person (either in-person or online);\nYou may not ask questions relating to the exam on StackOverflow or other ‘online help boards’.\nYou may use ChatGPT or another LLM.\n\nHowever, if you make use of an external resource (e.g. you search for and find relevant code on StackOverflow or use ChatGPT) you must include a citation in the code submission area of the form: # Source: &lt;link to resource used&gt;.\nAnyone found to have cheated on this submission will receive a mark of 0 and the resit assessment will involve producing an analysis in a timed context under direct supervision of module staff: in other words, you will be required to be present in-person for the resit (this is likely to be during the summer period) and will have to complete a set of coding tasks specified by the module leader.\n\nIn other words: don’t do it."
  },
  {
    "objectID": "assessments/peer.html",
    "href": "assessments/peer.html",
    "title": "Group Self-Evaluation",
    "section": "",
    "text": "This individual reflection and peer mark assessment (10%) due Wednesday, 20 December 2023 @ 6pm asks you to reflect on the process of ‘doing data science’ as part of a team, since this format is typical of real-world projects. Many companies (e.g. Apple, Google, etc.) employ an Agile project format in which teams undertake a ‘Retrospective’ at the end of a project in order to identify ways to improve how they work in the future. We are not asking you to do this as a group (indeed, it’s an individual reflection), but we hope that this will help you to develop as a budding analyst or data scientist."
  },
  {
    "objectID": "assessments/peer.html#format",
    "href": "assessments/peer.html#format",
    "title": "Group Self-Evaluation",
    "section": "Format",
    "text": "Format\nYou must answer these 3 questions for formative purposes as part of the submission process and prior to submitting the peer marks:\n\nWhat factors do you think help to explain what went well/not well in the project?\nHow do you think the other students in your group would evaluate your contribution to each of these outcomes?\nBriefly describe one event or experience from the group assessment that gave you new insight into ‘doing’ data science, and explain how and why this will be useful to you in the future."
  },
  {
    "objectID": "assessments/peer.html#restrictions",
    "href": "assessments/peer.html#restrictions",
    "title": "Group Self-Evaluation",
    "section": "Restrictions",
    "text": "Restrictions\nThere is no word limit for the formative parts of the assessment, but as an indication ca. 125 words should be enough for a brief, thoughtful engagement. The formative component is optional, but it is the sole evidence we will take into account in the event of discrepancies in the group self-evaluation (aka. peer mark)."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#large-language-models-llms",
    "href": "lectures/1.2-Tools_of_the_Trade.html#large-language-models-llms",
    "title": "Tools of the Trade",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\n\nChatGPT from OpenAI (an increasingly ‘ironic’ name) is simply the most famous of a growing number of Large Language Models that draw on information found on the web and in open texts to perform sophisticated summarisation tasks."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-4",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-4",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nMany programmers are starting to use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-4",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-4",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose of the code.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#oh-my-git",
    "href": "lectures/1.3-Writing_Code.html#oh-my-git",
    "title": "Writing Code",
    "section": "Oh My Git!",
    "text": "Oh My Git!"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#vs-code",
    "href": "lectures/1.3-Writing_Code.html#vs-code",
    "title": "Writing Code",
    "section": "VS Code",
    "text": "VS Code\nYou"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#more-thing",
    "href": "lectures/1.3-Writing_Code.html#more-thing",
    "title": "Writing Code",
    "section": "1 More Thing…",
    "text": "1 More Thing…\nYou can also use Jupyter notebooks in Microsoft’s flagship Visual Studio Code IDE. Also free. Together with ‘Dev Containers’ (which run Docker images) you have a second way to complete the module while using all the same sets of tools.\n\n\nThat is the power of open source software and standards!"
  },
  {
    "objectID": "lectures/2.5-Iteration.html#a-debugging-manifesto",
    "href": "lectures/2.5-Iteration.html#a-debugging-manifesto",
    "title": "Iteration",
    "section": "A Debugging Manifesto!1",
    "text": "A Debugging Manifesto!1\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#danger-will-robinson",
    "href": "lectures/1.2-Tools_of_the_Trade.html#danger-will-robinson",
    "title": "Tools of the Trade",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nHere’s what we’ve noticed about ChatGPT’s output so far:\n\nOver-use of flowery language (e.g. ‘intricate methodologies’, ‘exhaustive reviews’, …).\nLack of overall structure/coherence (i.e. individual paragraphs seem reasonable but the whole doesn’t ‘work’ as a single output).\nPoor referencing, including the use of non-existent or irrelevant references (i.e. papers that don’t actually exist, or that seem superficially useful but are not relevant in practice).\nThe more specific the information, the less likely it is to be correct (e.g. Who is Jon Reades? vs. What are the main areas where NLP is useful in built environment research?)"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#over-confidence-in-action",
    "href": "lectures/1.2-Tools_of_the_Trade.html#over-confidence-in-action",
    "title": "Tools of the Trade",
    "section": "Over-Confidence in Action",
    "text": "Over-Confidence in Action\nWho is Jon Reades?1\n\nJon Reades is a Senior Lecturer in the Department of Geography at University College London (UCL). He is also the director of the MSc in Smart Cities and Urban Analytics program at UCL.\nReades’ research focuses on the intersection of cities, data, and technology, with a particular emphasis on the use of spatial data and analytics to understand urban processes and support decision-making in cities. He has published widely on topics such as spatial analysis, urban data science, and the use of smart technologies in urban planning and governance.\nIn addition to his academic work, Reades is actively involved in a number of professional organizations and initiatives related to urban analytics and smart cities. He is a co-founder of the Urban Dynamics Institute at UCL and has served on the editorial board of several academic journals in the fields of geography and urban studies.\n\nAnd, by the way, there’s only one of me!"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#read-the-fine-print",
    "href": "lectures/1.2-Tools_of_the_Trade.html#read-the-fine-print",
    "title": "Tools of the Trade",
    "section": "Read the Fine Print",
    "text": "Read the Fine Print"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#why",
    "href": "lectures/1.4-Group_Working.html#why",
    "title": "Group Working",
    "section": "Why?",
    "text": "Why?\n\nThe majority of work that you will do over the rest of your life is likely to be team- and project-based: you will be assigned to a small team to complete a project. This is true whether you are in academia or industry, government or NGO."
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#challenges",
    "href": "lectures/1.4-Group_Working.html#challenges",
    "title": "Group Working",
    "section": "Challenges",
    "text": "Challenges\n\nWe make superficial assessments of the flaws/strengths of others.\nWe make a range of assumptions about the motivations of others.\nWe make a range of assumptions about the situations of others.\nWe respond differently to stimuli and stresses.\nWe have a hard time talking about any of this.\n\n\nThese are just some of the challenges to setting up your group.\nIn particular, when we meet a new group, it’s often easy to be impressed by someone who is self-confident and well-spoken: they must know what they’re doing! Group dynamics can be like a popularity contest in which the person who is easy-going or ‘fun’ is making a wonderful contribution, while the one who asks difficult questions is ‘not constructive’ and ‘not contributing’. Someone who is ‘quiet’ doesn’t seem to have much to say…"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#resources",
    "href": "lectures/1.4-Group_Working.html#resources",
    "title": "Group Working",
    "section": "Resources",
    "text": "Resources\n\nThe Five Dysfunctions of a Team\nThe Mythical Man Month"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#many-models",
    "href": "lectures/1.4-Group_Working.html#many-models",
    "title": "Group Working",
    "section": "Many Models…",
    "text": "Many Models…\n\nArtist Collectives — shared responsibilities for ideas and outputs; a lot of freedom and fuzziness in roles, but propensity for struggles over direction and power.\nTheatre — every member has a specific role; there is freedom within the role and clear lines of responsibility for delivery, but can blow up spectacularly.\nCo-Creation — emphasis on participation and recognition of diverse strengths; problem- and communication-focused; lots of effort and uncertainty, but results can be much more meaningful and durable.\n…\n\n\nThe arts world is particularly innovative when it comes to project work because teams are always being formed and reformed around individual outputs (a play, an artistic collaboration, a theatre production)…\nFields such as consultancy and software development have their own norms, but they are for the most part less experimental in their structure.\nHowever, the point is that there is no one-size-fits-all pattern for a successful piece of group work."
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#identify-your-strengths",
    "href": "lectures/1.4-Group_Working.html#identify-your-strengths",
    "title": "Group Working",
    "section": "Identify Your Strengths",
    "text": "Identify Your Strengths\n\n\nDominance\nInfluence\n\nSteadiness\nConscientiousness"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#identifying-your-strengths",
    "href": "lectures/1.4-Group_Working.html#identifying-your-strengths",
    "title": "Group Working",
    "section": "Identifying Your Strengths",
    "text": "Identifying Your Strengths\n\n\nDominance\n\nResults-oriented\nInnovative\nCompetitive\nDirect\n\nInfluence\n\nPeople-oriented\nEnthusiastic\nOptimistic\nCreative\n\n\nSteadiness\n\nSincere\nDependable\nPatient\nModest\n\nConscientiousness\n\nAccurate\nCautious\nPrecise\nAnalytical\n\n\n\n\nHere is one way of moving beyond the stereotype that the person doing the talking is also doing the thinking. The DISC model is apparently connected back to Wonder Woman by her creator W.M Marston. Like Wonder Woman, this is a model that reflects its cultural context: it was created for American teams and if you aren’t American then you might find that these four personality types don’t ‘fit’ very well. But it’s still a good place to start talking about preferences, behaviours, and the way they impact others."
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#too-much-of-a-good-thing",
    "href": "lectures/1.4-Group_Working.html#too-much-of-a-good-thing",
    "title": "Group Working",
    "section": "Too Much of a Good Thing",
    "text": "Too Much of a Good Thing\n\n\nDominance\n\nFails to involve others\nImpatient\nOffensive\n\nInfluence\n\nToo social\nEasily distracted\nOverly optimistic\n\n\nSteadiness\n\nIndirect\nAvoids conflict\nDelays difficult decisions\n\nConscientiousness\n\nPerfectionist\nAvoids unsystematic people\nDelays decisions over risks\n\n\n\n\nAnything that is a strength in one context can become a weakness in another.\nFor instance, a dominant personality might assume that the fact no one else is objecting means that there is support for their proposal; however, it could just be that the rest of the group wants to avoid conflict even though they think there are significant risks.\nOr someone who is motivated by the social component might spend so long bringing people together to talk about directions (and constantly adding new ideas to the mix) that very little progress is made on the actual project!"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#consequences",
    "href": "lectures/1.4-Group_Working.html#consequences",
    "title": "Group Working",
    "section": "Consequences1",
    "text": "Consequences1\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#software-dev-models",
    "href": "lectures/1.4-Group_Working.html#software-dev-models",
    "title": "Group Working",
    "section": "Software Dev Models",
    "text": "Software Dev Models\nFrom relational to practical… nothing ever goes to plan:\n\nAgile principles1: iterative delivery of successful projects focussed on individuals and interactions; working software, customer collaboration; and responding to change.\nScrum methodology2: organised around stand ups, sprints, sprint review, and retrospectives.\nKanban methodology: use of ‘sticky notes’ in columns to organise and prioritise visually.\nXP methdology: one person writes/codes while a partner researches/solves/feeds back.\n\n\nOne challenge that teams face in the world of software is failure due to complexity and change.\nYou may have seen ‘Waterfall charts’ of the sort produced by project management software where one task cascades into the next until the project is delivered at the end. This is recipe for failure. The Mythical Man Month comprehensively debunked this approach in 1975.\nThe general conclusion was that “Adding more people to a project that is late makes it later.” The reason this happens is that you cannot break a complex task into discrete parts, send someone away to work on it in isolation, and then have them report back at the end with a finished product. I have seen this happen in previous years’ group work too: one person did a lit review while someone else was writing code but they didn’t line up at the end!\nI saw this in graphic design too: a client writes a brief for exactly what they want. You deliver something exactly like what they asked for. And they say that’s not what they meant.\nThe Agile philosophy emerged in response to these kinds of failures, with the approach that the product should be finished as quickly as possible, but as provisionally as possible.\n\nSource: SimpleLearnSource: Northeastern University"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#every-good-plan",
    "href": "lectures/1.4-Group_Working.html#every-good-plan",
    "title": "Group Working",
    "section": "Every Good Plan1…",
    "text": "Every Good Plan1…\n\n\n\n\n\nSource: Zentao"
  },
  {
    "objectID": "lectures/1.4-Group_Working.html#nothing-ever-goes-to-plan",
    "href": "lectures/1.4-Group_Working.html#nothing-ever-goes-to-plan",
    "title": "Group Working",
    "section": "Nothing Ever Goes to Plan",
    "text": "Nothing Ever Goes to Plan\nFrom relational to practical…\n\nAgile principles1: iterative delivery of successful projects focussed on individuals and interactions; working software, customer collaboration; and responding to change.\nScrum methodology2: organised around stand ups, sprints, sprint review, and retrospectives.\nKanban methodology: use of ‘sticky notes’ in columns to organise and prioritise visually.\nXP methdology: one person writes/codes while a partner researches/solves/feeds back.\n\n\nOne challenge that teams face in the world of software is failure due to complexity and change.\nYou may have seen ‘Waterfall charts’ of the sort produced by project management software where one task cascades into the next until the project is delivered at the end. This is recipe for failure. The Mythical Man Month comprehensively debunked this approach in 1975.\nThe general conclusion was that “Adding more people to a project that is late makes it later.” The reason this happens is that you cannot break a complex task into discrete parts, send someone away to work on it in isolation, and then have them report back at the end with a finished product. I have seen this happen in previous years’ group work too: one person did a lit review while someone else was writing code but they didn’t line up at the end!\nI saw this in graphic design too: a client writes a brief for exactly what they want. You deliver something exactly like what they asked for. And they say that’s not what they meant.\nThe Agile philosophy emerged in response to these kinds of failures, with the approach that the product should be finished as quickly as possible, but as provisionally as possible. So you always have a working application, even if it only does a fraction of what you intend for it to do in the long run.\nThis has been elevated by companies like Facebook to slogans along the lines of ‘Fail Fast’, but the point is to recognise as quickly as possible when something isn’t working or isn’t headed in the direction you planned.\nFor the group work at the heart of this module, the point of the iterative process that begins in Week 6 is to help you develop competency in using the tools required to make your submission, to start you thinking along the lines that are required for the more complex questions, and to tease out points of failure in the group’s ability to work together long before you are under the tight deadline at the end of term!\n\nSource: SimpleLearnSource: Northeastern University"
  },
  {
    "objectID": "setup/index.html#sec-requirements",
    "href": "setup/index.html#sec-requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Before trying to do anything else please complete the basic health check, which also includes our recommendations if you are considering buying a new computer when you start your studies. Once you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment. We also provide information about Code Camp which is a self-paced introduction to the fundamentals of programming in Python."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#week-to-week",
    "href": "lectures/1.1-Getting_Oriented.html#week-to-week",
    "title": "Getting Oriented",
    "section": "Week-to-Week",
    "text": "Week-to-Week\nThe specific activities for each week can be found on the microsite: jreades.github.io/fsds/. These include:\n\nPreparation: readings, pre-recorded lectures, quizzes/feedback.\nIn-Class: discussing readings and lectures; responding to assessment requirements; discussing issues arising from the previous week’s practical, and ‘live coding’.\nPracticals: working through a weekly ‘programming notebook’ in a small group with support from your PGTAs.\n\n\n\n\n\n\n\nBring Your Computer\n\n\nPlease remember to bring your own computer to the practical sessions! The tools we use are not installed on cluster systems."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#assessments",
    "href": "lectures/1.1-Getting_Oriented.html#assessments",
    "title": "Getting Oriented",
    "section": "Assessments",
    "text": "Assessments\n\nTimed, Open Book Exam (30% of module grade): A quiz requiring a mix of numeric and textual answers to short data analysis questions for which you must write the code.\nGroup Report (60% of module grade; 2,500 words max): A structured, small-group submission in which students respond to set questions and develop an exploratory analysis of the assigned data set.\nSelf-Evaluation (10% of module grade): A structured individual reflection combined with numerical scoring of peers on their contribution to the group’s outcomes.\n\n\nAssessment logic:\n\nTeach and test the most challenging aspects of data science ‘work’ without mastery of Python.\nDiscover transferrability of skills and tools across projects, disciplines, and industries.\nBuild on content from QM (e.g. setting quantitative research questions) and GIS (e.g. spatial statistics).\nDevelop experience with non-academic research formats and writing.\n\nSo,\n\nIs a Moodule quiz due Thursdsay, 23 November 2023 @ 7:30pm (after Reading Week) and it will focus on the effective use of the pandas library.\nIs a Quarto document due Tuesday, 19 December @ 6pm (immediately after the end of term) that combines the analysis and outputs in one document with a set of specified questions upon which randomly-selected groups will receive feedback throughout the term.\nIs a reflection and ranking exercise due ?var:assess.peerdate (the day after the Quarto submission).\n\nI will talk more about these over the course of the term."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#resources",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#resources",
    "title": "Computers in Urban Studies",
    "section": "Resources",
    "text": "Resources\n\nThe Data of Urban Spaces — a nice collection of pieces on Towards Data Science"
  },
  {
    "objectID": "lectures/5.3-Files.html#the",
    "href": "lectures/5.3-Files.html#the",
    "title": "File Formats",
    "section": "The",
    "text": "The"
  },
  {
    "objectID": "lectures/5.3-Files.html#solution-we-need-a-duck-an-arrow-and-a-parquet",
    "href": "lectures/5.3-Files.html#solution-we-need-a-duck-an-arrow-and-a-parquet",
    "title": "File Formats",
    "section": "> Solution: We Need a Duck, an Arrow, and a Parquet",
    "text": "&gt; Solution: We Need a Duck, an Arrow, and a Parquet"
  },
  {
    "objectID": "lectures/5.3-Files.html#for-later-reference",
    "href": "lectures/5.3-Files.html#for-later-reference",
    "title": "File Formats",
    "section": "For (Later) Reference",
    "text": "For (Later) Reference\n# Notice the engine and dtype_backend options\ndf = pandas.read_csv(fname, engine='pyarrow', \n                dtype_backend='pyarrow')\n\n# And for parquet files\ndf = pandas.read_parquet(fname, columns=[...])\n\n# And for DuckDB we can actually joing two\n# files before they even get to Python!\nq = f'''\n  SELECT * FROM \n    read_parquet('epc-ppd-2022-*.parquet') as ppd, \n    read_parquet('epc-ldd-2022-*.parquet') as ldd,\n  WHERE ppd.uid=ldd.uid\n'''\ndf = duckdb.query(q).df()\nP.S. There’s also a command-line tool for DuckDB so you don’t even need Python."
  },
  {
    "objectID": "lectures/5.3-Files.html#solution-we-need-an-arrow-a-duck-and-a-parquet",
    "href": "lectures/5.3-Files.html#solution-we-need-an-arrow-a-duck-and-a-parquet",
    "title": "File Formats",
    "section": "> Solution: We Need an Arrow, a Duck, and a Parquet",
    "text": "&gt; Solution: We Need an Arrow, a Duck, and a Parquet"
  },
  {
    "objectID": "lectures/5.3-Files.html#arrow-and-parquet",
    "href": "lectures/5.3-Files.html#arrow-and-parquet",
    "title": "File Formats",
    "section": "Arrow and Parquet",
    "text": "Arrow and Parquet\n\nArrow is an in-memory columnar format for data. Data is stored in a structured way in RAM making it blazingly fast for operations.\nParquet is a highly-compressed columnar file format for data. Data is stored in a structured way on your hard drive.\nFeather is a raw storage format for Arrow.\n\nTL;DR: for most applications Parquet will give nice, small files on disk and the benefits of columnar file storage; for computationally intensive applications where disk space and interoperability with other systems isn’t an issue then Feather might work."
  },
  {
    "objectID": "lectures/5.3-Files.html#what-about-the-duck",
    "href": "lectures/5.3-Files.html#what-about-the-duck",
    "title": "File Formats",
    "section": "What About the Duck?",
    "text": "What About the Duck?\n\n\n\n\n\nServerless SQL queries against Parquet files\nQueries returned as Pandas data frames\nSelect and filter before loading\nFast conversion between CSV and Parquet via Arrow"
  },
  {
    "objectID": "lectures/9.4-Visualising_Data.html#dont-underestimate-text",
    "href": "lectures/9.4-Visualising_Data.html#dont-underestimate-text",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!1",
    "text": "Don’t Underestimate Text!1\n\n\n\n\n\n\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/2.4-Lists.html#resources",
    "href": "lectures/2.4-Lists.html#resources",
    "title": "Lists",
    "section": "Resources",
    "text": "Resources\n\nLists in Python\nTuples in Python\nRange and lists\nSequence types\nThe Complete Guide to Lists (by a CASA alum!)"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#resources",
    "href": "lectures/3.3-DOLs_to_Data.html#resources",
    "title": "Data Structures",
    "section": "Resources",
    "text": "Resources\n\n8 Data Structures Every Data Scientist Should Know (by a CASA alum)"
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#space-adds-complexity",
    "href": "lectures/9.4-Clustering_and_Geography.html#space-adds-complexity",
    "title": "Clustering and Geography",
    "section": "Space Adds Complexity",
    "text": "Space Adds Complexity\nWe now have to consider two more types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#trade-offs-again",
    "href": "lectures/9.4-Clustering_and_Geography.html#trade-offs-again",
    "title": "Clustering and Geography",
    "section": "Trade-offs (Again)…",
    "text": "Trade-offs (Again)…\nConsider:\n\nClustering algorithms are inherently spatial.\nClustering algorithms do not take space geography into account.\n\nDoes this matter?\n\nAll clustering algorithms are about inter-observation and intra-cluster distances so they have some conceptualisation of ‘space’.\nSpatially-aware clustering algorithms exist but are generally much more computationally-intensive than ‘regular ones’."
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#different-approaches",
    "href": "lectures/9.4-Clustering_and_Geography.html#different-approaches",
    "title": "Clustering and Geography",
    "section": "Different Approaches",
    "text": "Different Approaches\n\n\n\nAlgorithm\nPros\nCons\nGeographically Aware?\n\n\n\n\nk-Means\nFast. Deterministic.\nEvery observation to a cluster.\nN.\n\n\nDBSCAN\nAllows for clusters and outliers.\nSlower. Choice of \\(\\epsilon\\) critical. Can end up with all outliers.\nN, but implicit in \\(\\epsilon\\).\n\n\nOPTICS\nFewer parameters than DBSCAN.\nEven slower.\nN, but implicit in \\(\\epsilon\\).\n\n\nHierarchical/ HDBSCAN\nCan cut at any number of clusters.\nNo ‘ideal’ solution.\nY, with connectivity parameter.\n\n\nADBSCAN\nScales. Confidence levels.\nMay need large data set to be useful. Choice of \\(\\epsilon\\) critical.\nY.\n\n\nMax-p\nCoherent regions returned.\nVery slow if model poorly specified.\nY."
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "href": "lectures/9.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "title": "Clustering and Geography",
    "section": "Setting the Relevant Distance",
    "text": "Setting the Relevant Distance\nMany clustering algorithms rely on a distance specification (usually \\(\\epsilon\\)). So to set this threshold:\n\nIn high-dimensional spaces this threshold will need to be large.\nIn high-dimensional spaces the scale will be meaningless (i.e. not have a real-world meaning, only an abstract one).\nIn 2- or 3-dimensional (geographical) space this threshold could be meaningful (i.e. a value in metres could work)."
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "href": "lectures/9.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "title": "Clustering and Geography",
    "section": "Choosing a Distance Metric",
    "text": "Choosing a Distance Metric\n\n\n\n\n\n\n\n\nn Dimensions\nHow to Set\nExamples\n\n\n\n\n2 or 3\nTheory/Empirical Data\nWalking speed; Commute distance\n\n\n2 or 3\nK/L Measures\nPlot with Simulation for CIs to identify significant ‘knees’.\n\n\n3\nMarked Point Pattern?\n\n\n\n&gt; 3\nkNN\nCalculate average kNN distance based on some expectation of connectivity.\n\n\n\n\nRemember: inter-observation distance increases with dimensionality!"
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#experian",
    "href": "lectures/9.4-Clustering_and_Geography.html#experian",
    "title": "Clustering and Geography",
    "section": "Experian",
    "text": "Experian\nSpecialist in consumer segmentation and geodemographics (bit.ly/2jMRhAW).\n\nMarket cap: £14.3 billion.\nMosaic: “synthesises of 850 million pieces of information… to create a segmentation that allocates 49 million individuals and 26 million households into one of 15 Groups and 66 detailed Types.””\nMore than 450 variables used.\n\nMost retail companies will have their own segmentation scheme. Competitors: CACI, Nielsen, etc."
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#experian-groups",
    "href": "lectures/9.4-Clustering_and_Geography.html#experian-groups",
    "title": "Clustering and Geography",
    "section": "Experian Groups",
    "text": "Experian Groups"
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#experian-mapping",
    "href": "lectures/9.4-Clustering_and_Geography.html#experian-mapping",
    "title": "Clustering and Geography",
    "section": "Experian Mapping",
    "text": "Experian Mapping"
  },
  {
    "objectID": "lectures/9.4-Clustering_and_Geography.html#output-area-classification",
    "href": "lectures/9.4-Clustering_and_Geography.html#output-area-classification",
    "title": "Clustering and Geography",
    "section": "Output Area Classification",
    "text": "Output Area Classification\nOAC set up as ‘open source’ alternative to Mosaic:\n\nWell documented (UCL Geography a major contributor)\nDoesn’t require a license or payment\nCan be tweaked/extended/reweighted by users as needed"
  },
  {
    "objectID": "lectures/9.2-Classification.html#spot-the-difference",
    "href": "lectures/9.2-Classification.html#spot-the-difference",
    "title": "Classification",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nOn Maps\n\nGroup observations by ‘class’.\nTypically based on 1-D distribution.\nClasses are assigned by user choice.\n\n\nOn Labels\n\nLabel observations by ‘class’.\nTypically based on model outputs.\nLabels are assigned by user feedback.\n\n\n\n\nIn this session we are primarily concerned with the first column — classification as a modelling process is better considered a data science/modelling problem that is beyond the scope of this module."
  },
  {
    "objectID": "lectures/9.2-Classification.html#map-classification-choices",
    "href": "lectures/9.2-Classification.html#map-classification-choices",
    "title": "Classification",
    "section": "Map Classification Choices",
    "text": "Map Classification Choices\n\nAssign classes manually.\nSplit range evenly.\nSplit data evenly\nSplit data according to distribution\nSplit data according to their similarity to each other.\n\n\n\nAccording to some logic/theory/regulatory or policy fact or objective.\nEqual intervals for cases without heavy skew\nQuantiles or HeadTailBreaks for cases with heavy skew\nSD for cases with normal distribution; BoxPlot for others.\nNatural breaks/FIsher Jenks for cases where distribution is discontinuous"
  },
  {
    "objectID": "lectures/9.2-Classification.html#mapclassify",
    "href": "lectures/9.2-Classification.html#mapclassify",
    "title": "Classification",
    "section": "Mapclassify",
    "text": "Mapclassify\nMapclassify (part of PySAL) provides a wide range of classifiers:\n\n\n\nNo Parameters\nk Parameter\n\n\n\n\nBoxPlot\nUserDefined\n\n\nStdMean\nPercentiles\n\n\nMaxP\nQuantiles\n\n\nHeadTailBreaks\nNatural Breaks\n\n\nEqualInterval\nMaximum Breaks\n\n\n\nJenksCaspall/Sampled/Forced\n\n\n\nFisherJenks/Sampled\n\n\n\nk will a user-specified number of classes or binning criterion."
  },
  {
    "objectID": "lectures/9.2-Classification.html#raw",
    "href": "lectures/9.2-Classification.html#raw",
    "title": "Classification",
    "section": "Raw",
    "text": "Raw"
  },
  {
    "objectID": "lectures/9.2-Classification.html#user-defined",
    "href": "lectures/9.2-Classification.html#user-defined",
    "title": "Classification",
    "section": "User Defined",
    "text": "User Defined\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, 125000.00]\n0\n\n\n( 125000.00, 250000.00]\n4\n\n\n( 250000.00, 925000.00]\n865\n\n\n( 925000.00, 1500000.00]\n85\n\n\n(1500000.00, 4500000.00]\n29"
  },
  {
    "objectID": "lectures/9.2-Classification.html#box-plot",
    "href": "lectures/9.2-Classification.html#box-plot",
    "title": "Classification",
    "section": "Box Plot",
    "text": "Box Plot\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -31429.25]\n0\n\n\n( -31429.25, 391267.00]\n246\n\n\n( 391267.00, 495010.00]\n246\n\n\n( 495010.00, 673064.50]\n245\n\n\n( 673064.50, 1095760.75]\n175\n\n\n(1095760.75, 4416659.00]\n70"
  },
  {
    "objectID": "lectures/9.2-Classification.html#standard-deviations",
    "href": "lectures/9.2-Classification.html#standard-deviations",
    "title": "Classification",
    "section": "Standard Deviations",
    "text": "Standard Deviations\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -171366.63]\n0\n\n\n(-171366.63, 216174.43]\n0\n\n\n( 216174.43, 991256.55]\n892\n\n\n( 991256.55, 1378797.61]\n53\n\n\n(1378797.61, 4416659.00]\n38"
  },
  {
    "objectID": "lectures/9.2-Classification.html#max-p",
    "href": "lectures/9.2-Classification.html#max-p",
    "title": "Classification",
    "section": "Max P",
    "text": "Max P\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346594.00]\n142\n\n\n( 346594.00, 461577.00]\n279\n\n\n( 461577.00, 529197.00]\n140\n\n\n( 529197.00, 530662.00]\n3\n\n\n( 530662.00, 613465.00]\n115\n\n\n( 613465.00, 842387.00]\n167\n\n\n( 842387.00, 4416659.00]\n137"
  },
  {
    "objectID": "lectures/9.2-Classification.html#head-tail-breaks",
    "href": "lectures/9.2-Classification.html#head-tail-breaks",
    "title": "Classification",
    "section": "Head Tail Breaks",
    "text": "Head Tail Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 603715.49]\n670\n\n\n( 603715.49, 976290.79]\n218\n\n\n( 976290.79, 1508985.73]\n66\n\n\n(1508985.73, 2257581.55]\n16\n\n\n(2257581.55, 2826007.08]\n9\n\n\n(2826007.08, 3553496.25]\n3\n\n\n(3553496.25, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/9.2-Classification.html#equal-interval",
    "href": "lectures/9.2-Classification.html#equal-interval",
    "title": "Classification",
    "section": "Equal Interval",
    "text": "Equal Interval\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 825125.00]\n842\n\n\n( 825125.00, 1423714.00]\n108\n\n\n(1423714.00, 2022303.00]\n17\n\n\n(2022303.00, 2620892.00]\n10\n\n\n(2620892.00, 3219481.00]\n4\n\n\n(3219481.00, 3818070.00]\n1\n\n\n(3818070.00, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/9.2-Classification.html#quantiles",
    "href": "lectures/9.2-Classification.html#quantiles",
    "title": "Classification",
    "section": "Quantiles",
    "text": "Quantiles\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346009.00]\n140\n\n\n( 346009.00, 405677.86]\n140\n\n\n( 405677.86, 461959.29]\n140\n\n\n( 461959.29, 529612.86]\n141\n\n\n( 529612.86, 639488.86]\n140\n\n\n( 639488.86, 827691.43]\n140\n\n\n( 827691.43, 4416659.00]\n141"
  },
  {
    "objectID": "lectures/9.2-Classification.html#natural-breaks",
    "href": "lectures/9.2-Classification.html#natural-breaks",
    "title": "Classification",
    "section": "Natural Breaks",
    "text": "Natural Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 433543.00]\n356\n\n\n( 433543.00, 605879.00]\n316\n\n\n( 605879.00, 842387.00]\n174\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/9.2-Classification.html#maximum-breaks",
    "href": "lectures/9.2-Classification.html#maximum-breaks",
    "title": "Classification",
    "section": "Maximum Breaks",
    "text": "Maximum Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 1688895.00]\n961\n\n\n(1688895.00, 1926265.50]\n4\n\n\n(1926265.50, 2278155.50]\n5\n\n\n(2278155.50, 2929865.50]\n9\n\n\n(2929865.50, 3349991.00]\n2\n\n\n(3349991.00, 3959682.50]\n1\n\n\n(3959682.50, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/9.2-Classification.html#fisher-jenks",
    "href": "lectures/9.2-Classification.html#fisher-jenks",
    "title": "Classification",
    "section": "Fisher Jenks",
    "text": "Fisher Jenks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 435961.00]\n363\n\n\n( 435961.00, 607480.00]\n310\n\n\n( 607480.00, 842387.00]\n173\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/9.2-Classification.html#jenks-caspall",
    "href": "lectures/9.2-Classification.html#jenks-caspall",
    "title": "Classification",
    "section": "Jenks Caspall",
    "text": "Jenks Caspall\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 365741.00]\n188\n\n\n( 365741.00, 441979.00]\n187\n\n\n( 441979.00, 520791.00]\n167\n\n\n( 520791.00, 638474.00]\n160\n\n\n( 638474.00, 890055.00]\n156\n\n\n( 890055.00, 1626454.00]\n103\n\n\n(1626454.00, 4416659.00]\n22"
  },
  {
    "objectID": "lectures/9.2-Classification.html#summary",
    "href": "lectures/9.2-Classification.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\n\nThe choice of classification scheme should be data- and distribution-led. This is simply a demonstration of how different schemes can shape your understanding of the data."
  },
  {
    "objectID": "lectures/9.2-Classification.html#code-useful-tips",
    "href": "lectures/9.2-Classification.html#code-useful-tips",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the classes:\nkl = 7\ncls = [mapclassify.BoxPlot, ...,  mapclassify.JenksCaspall]\nSetting up the loop:\nfor cl in cls:\n    try: \n        m = cl(ppd.Value, k=kl)\n    except TypeError:\n        m = cl(ppd.Value)\n    \n    f = plt.figure()\n    gs = f.add_gridspec(nrows=2, ncols=1, height_ratios=[1,4])\n\n    ax1 = f.add_subplot(gs[0,0])\n    ...\n\n    ax2 = f.add_subplot(gs[1,0])\n    ..."
  },
  {
    "objectID": "lectures/9.2-Classification.html#code-useful-tips-1",
    "href": "lectures/9.2-Classification.html#code-useful-tips-1",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the distribution:\n    ax1 = f.add_subplot(gs[0,0])\n    sns.kdeplot(ppd.Value, ax=ax1, color='r')\n    ax1.ticklabel_format(style='plain', axis='x') \n\n    y = ax1.get_ylim()[1]\n    for b in m.bins:\n        ax1.vlines(b, 0, y, linestyles='dotted')"
  },
  {
    "objectID": "lectures/9.2-Classification.html#code-useful-tips-2",
    "href": "lectures/9.2-Classification.html#code-useful-tips-2",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nAdjusting the legend text:\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k,v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\nSetting up the map:\n    ax2 = f.add_subplot(gs[1,0])\n    ppd.assign(cl=m.yb).plot(column='cl', k=len(m.bins), categorical=True, legend=True, ax=ax2)\n    \n    mapping = dict([(i,s) for i,s in enumerate(m.get_legend_classes())])\n    ax2.set_axis_off()\n    replace_legend_items(ax2.get_legend(), mapping)"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#section",
    "href": "lectures/10.2-Linking_Spatial_Data.html#section",
    "title": "Linking Spatial Data",
    "section": "",
    "text": "Data Set 1\n\n\n\nSensorID\nLatitude\nLongitude\n\n\n\n\n1\n51.5070\n-0.1347\n\n\n2\n51.5071\n-0.0042\n\n\n3\n51.5074\n-0.1223\n\n\n4\n51.5073\n-0.1122\n\n\n5\n51.5072\n0.1589\n\n\n\ndf = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Latitude': [51.5070, 51.5071, 51.5074, 51.5073, 51.5073],\n  'Longitude': [-0.1347, -0.0042, -0.1223, -0.1122, 0.1589]\n})\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1\nTemperature\n5ºC\n\n\n1\nHumidity\n15%\n\n\n3\nTemperature\n7ºC\n\n\n4\nTemperature\n7ºC\n\n\n6\nHumidity\n18%\n\n\n\ndf1 = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Parameter': ['Temperature','Humidity','Temperature','Temperature','Humidity'], \n  'Value': ['5ºC', '15%', '7ºC', '7ºC', '18%']\n})\n\n\n\n\nObviously, we can use non-spatial operations on spatial data sets.\n\n\nThis wouldn’t be a particularly good design for a data structure… why?"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/10.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking Spatial Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/10.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking Spatial Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/10.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking Spatial Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/10.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking Spatial Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/10.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking Spatial Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/10.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking Spatial Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/10.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking Spatial Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/10.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking Spatial Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\nA. Here’s how I’d do this:\n\nCity = New York (probably a key)\n\nCuisine = Italian (probably a key)\n\nStyle = Family (probably an enumeration/free text)\n\nLocation = Within Distance of X from Request (probably a buffered spatial query)"
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/10.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking Spatial Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/10.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking Spatial Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server.\n\n\n\n\n\nSee also Carto and competitors."
  },
  {
    "objectID": "lectures/10.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/10.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking Spatial Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich, but £££ for commercial use.\nMySQL: free, unless you want dedicated support; was feature-poor (though this looks to have changed with MySQL8); heavyweight.\nPostreSQL: free, unless you want dedicated support; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/10.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\npd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#inner-join",
    "href": "lectures/10.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart\n\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#inner-join-result",
    "href": "lectures/10.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\npd.merge(df1, df2, \n         how = 'inner',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#but-what-if",
    "href": "lectures/10.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then:\npd.merge(df1, df2, \n         how      = 'inner',\n         left_on  = 'SensorID',\n         right_on = 'SensorKey')\n \nWe will get an ‘extra’ field:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#outer-join",
    "href": "lectures/10.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\npd.merge(df1, df2,\n         how = 'outer',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#left-join",
    "href": "lectures/10.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\npd.merge(df1, df2, \n        how = 'left',\n        on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#append-concat",
    "href": "lectures/10.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#concat",
    "href": "lectures/10.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#append",
    "href": "lectures/10.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 0, 'Parameter': 5,  'Humidity', 'Value': 0.45},\n    {'SensorID': 1, 'Parameter': 5,  'Humidity', 'Value': 0.31},\n    {'SensorID': 2, 'Parameter': 4, 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 3, 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/10.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/10.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/10.1-Linking_Data.html#resources",
    "href": "lectures/10.1-Linking_Data.html#resources",
    "title": "Linking Data",
    "section": "Resources",
    "text": "Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#choices-choices",
    "href": "lectures/10.3-Visualising_Data.html#choices-choices",
    "title": "Visualising Data",
    "section": "Choices, Choices…",
    "text": "Choices, Choices…\n\nmatplotlib: the ‘big beast’ of visualisation in Python. Similar to MATLAB. Highly customisable. Very complex.\nseaborn: a layer that sits over top of matplotlib and makes it easier to produce good-quality graphics.\nbokeh: web-based visualisation tool that can integrate with Jupyter or output to static HTML files.\nplotly: another web-based visualisation tool that can integrate with Jupyter.\n\nMore emerging all the time: Vega/Altair, HoloViews, etc.\n\n\nIf you’re really wedded to ggplot, plotnine is a clone of ggplot’s interface (Grammer of Graphics) in Python. A brief overview of visualisation libraries could be helpful."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#seaborn",
    "href": "lectures/10.3-Visualising_Data.html#seaborn",
    "title": "Visualising Data",
    "section": "Seaborn",
    "text": "Seaborn\nDesigned to provide ggplot-like quality output using matplotlib:\n\nImprove on default colourmaps and colour defaults.\nIntegration with pandas data frames (Note: not geopandas!).\nOffers more plot types out of the box.\nStill offers access to matplotlib’s back-end."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#plot-types",
    "href": "lectures/10.3-Visualising_Data.html#plot-types",
    "title": "Visualising Data",
    "section": "Plot Types",
    "text": "Plot Types\n\n\n\nPartial Overview of Seaborn Plots\n\n\n\n\nFor the fuller overview see Overview of seaborn plotting functions and the full API reference."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#in-practice",
    "href": "lectures/10.3-Visualising_Data.html#in-practice",
    "title": "Visualising Data",
    "section": "In Practice",
    "text": "In Practice\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\",\n             hue=\"region\", style=\"event\",\n             data=fmri)"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#in-practice-2",
    "href": "lectures/10.3-Visualising_Data.html#in-practice-2",
    "title": "Visualising Data",
    "section": "In Practice 2",
    "text": "In Practice 2\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\ndf = sns.load_dataset(\"penguins\")\n\nax = sns.swarmplot(data=df, x=\"body_mass_g\", y=\"sex\", hue=\"species\")\nax.set(ylabel=\"\")"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#configuring-seaborn",
    "href": "lectures/10.3-Visualising_Data.html#configuring-seaborn",
    "title": "Visualising Data",
    "section": "Configuring Seaborn",
    "text": "Configuring Seaborn\nSeaborn ‘themes’ act as shortcuts for setting multiple matplotlib parameters:\n\n\n\nSeaborn Command\nAccomplishes\n\n\n\n\nset_theme(...)\nSet multiple theme parameters in one step.\n\n\naxes_style(...)\nReturn a parameter dict for the aesthetic style of the plots.\n\n\nset_style(...)\nSet the aesthetic style of the plots.\n\n\nplotting_context(...)\nReturn a parameter dict to scale elements of the figure.\n\n\nset_context(...)\nSet the plotting context parameters.\n\n\n\nYou can also access:\n\nPalettes: colormaps can be generated using sns.color_palette(...) and set using sns.set_palette(...).\nAxes Styles: includes darkgrid, whitegrid, dark, white, ticks."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#anatomy-of-a-figure",
    "href": "lectures/10.3-Visualising_Data.html#anatomy-of-a-figure",
    "title": "Visualising Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\nSource."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#writing-a-figure",
    "href": "lectures/10.3-Visualising_Data.html#writing-a-figure",
    "title": "Visualising Data",
    "section": "Writing a Figure",
    "text": "Writing a Figure\nThere are multiple ways to access/write elements of a plot:\n\nFigure: high-level features (e.g. title, padding, etc.). Can be accessed via plt.gcf() (get current figure) or upon creation (e.g. f, ax = plt.subplots(1,1) or f = plt.figure()).\nAxes: axis-level features (e.g. labels, tics, spines, limits, etc.). Can be accessed via plt.gca() (get current axes) or upon creation (e.g. f, ax = plt.subplots(1,1) or ax = f.add_subplot(1,1,1)).\n\nAnnotations, artists, and other features are typically written into the axes using the coordinate space of the figure (e.g. decimal degrees for lat/long, metres for BNG, etc.)."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#adding-a-3rd-dimension",
    "href": "lectures/10.3-Visualising_Data.html#adding-a-3rd-dimension",
    "title": "Visualising Data",
    "section": "Adding a 3rd Dimension",
    "text": "Adding a 3rd Dimension\nThis ‘feature’ is less well-developed but does work:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax  = plt.axes(projection='3d')\n# OR\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\n# THEN\nax.contour3D(X, Y, Z, ...)\nax.plot_surface(x, y, z, ...)\nax.plot3D(xline, yline, zline, ...)\nax.scatter3D(x, y, z, ...)\n# ax.plot_surface and ax.plot_wire also give you 3D renderings\nYou can then set the elevation and azimuth using: ax.view_init(&lt;elevation&gt;, &lt;azimuth&gt;)."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#saving-outputs",
    "href": "lectures/10.3-Visualising_Data.html#saving-outputs",
    "title": "Visualising Data",
    "section": "Saving Outputs",
    "text": "Saving Outputs\nStraightforward via save figure function, but lots of options!\nplt.savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n    orientation='portrait', papertype=None, format=None,\n    transparent=False, bbox_inches=None, pad_inches=0.1,\n    frameon=None, metadata=None)\nThe format can be largely determined by the file extension in the fname (file name) and the supported formats depends on what you’ve installed! You can find out what’s available to you using: plt.gcf().canvas.get_supported_filetypes()."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#jupyter",
    "href": "lectures/10.3-Visualising_Data.html#jupyter",
    "title": "Visualising Data",
    "section": "Jupyter",
    "text": "Jupyter\nBy default, Jupyter’s output is static matplotlib, but we can extend this in three ways:\n\nMake the static plot zoomable and pannable using %matplotlib widget (declare this at the top of your notebook).\nMake the plot more directly interactive using ipywidgets (import interact and related libs as needed).\nUse a browser-based visualisation tool such as bokeh, plotly, altair/vega, holoviews, or even d3 (format may be very, very different from what you are ‘used to’ in Python)."
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#widgets",
    "href": "lectures/10.3-Visualising_Data.html#widgets",
    "title": "Visualising Data",
    "section": "Widgets",
    "text": "Widgets\n%matplotlib widget\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = hackney.plot(edgecolor='k', facecolor='none')\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#interact",
    "href": "lectures/10.3-Visualising_Data.html#interact",
    "title": "Visualising Data",
    "section": "Interact()",
    "text": "Interact()\nTaking an example from Dani’s work:\nfrom ipywidgets import interact\n# Alternatives: interactive, fixed, interact_manual\ninteract(\n    &lt;function&gt;, # Function to make interactive\n    &lt;param0&gt;,   # e.g. Data to use\n    &lt;param1&gt;,   # e.g. Range start/end/step\n    &lt;param2&gt;    # e.g. Fixed value\n);"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#bokeh",
    "href": "lectures/10.3-Visualising_Data.html#bokeh",
    "title": "Visualising Data",
    "section": "Bokeh",
    "text": "Bokeh"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#automation",
    "href": "lectures/10.3-Visualising_Data.html#automation",
    "title": "Visualising Data",
    "section": "Automation",
    "text": "Automation\nPlots built on top of matploblib can, to some extent, be automated using functions. For example, to draw circles and place text:\ndef circle(ax, x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, \n                    linewidth=1, edgecolor='black', \n                    facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, \n                                  foreground='w')])\n    ax.add_artist(circle)\n\ndef text(ax, x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n         ha='center', va='top', weight='bold', color='blue')"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#dont-underestimate-text",
    "href": "lectures/10.3-Visualising_Data.html#dont-underestimate-text",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!1",
    "text": "Don’t Underestimate Text!1\n\n\n\n\n\n\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.3-Visualising_Data.html#resources",
    "href": "lectures/10.3-Visualising_Data.html#resources",
    "title": "Visualising Data",
    "section": "Resources",
    "text": "Resources\n\n\n\nMatplotlib Cheatsheets\nIntroduction to PyPlot (includes lots of parameter information)\nVisualisation with Seaborn\nSeaborn Tutorial\nElite Data Science Seaborn Tutorial\nDatacamp Seaborn Tutorial\nThree-Dimensional Plotting in Matplotlib\nAn easy introduction to 3D plotting with Matplotlib\n\n\n\nUsing text effectively in data viz\nChoosing fonts for charts and tables\nBokeh Gallery\nBokeh User Guide\nProgramming Historian: Visualizing Data with Bokeh and Pandas\nReal Python: Data Viz with Bokeh\nData Viz with Bokeh (Pt. 1)\nUsing Interact\nText in Data Visualizations"
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#useful-but-limited",
    "href": "lectures/9.1-Grouping_Data.html#useful-but-limited",
    "title": "Grouping Data",
    "section": "Useful, But Limited?",
    "text": "Useful, But Limited?\n\n\n\nMethod\nAchieves\n\n\n\n\ncount()\nTotal number of items\n\n\nfirst(), last()\nFirst and last item\n\n\nmean(), median()\nMean and median\n\n\nmin(), max()\nMinimum and maximum\n\n\nstd(), var()\nStandard deviation and variance\n\n\nmad()\nMean absolute deviation\n\n\nprod()\nProduct of all items\n\n\nsum()\nSum of all items\n\n\n\n\nHere are a bunch of pandas functions that have to do with aggregating data in some way. Some of these you’ll have seen before, some you may not. However, up to this point if you wanted to to know the median price of each type of Airbnb listing, or the sum of each type of vehicle sold, you’d have had to select out one listing type or vehicle type, call median or sum, and then remember the result. Let’s change that."
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#grouping-operations",
    "href": "lectures/9.1-Grouping_Data.html#grouping-operations",
    "title": "Grouping Data",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#in-practice",
    "href": "lectures/9.1-Grouping_Data.html#in-practice",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(&lt;fields&gt;).&lt;function&gt;\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "href": "lectures/9.1-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "title": "Grouping Data",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#pivot-tables",
    "href": "lectures/9.1-Grouping_Data.html#pivot-tables",
    "title": "Grouping Data",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(&lt;series&gt;, &lt;bins&gt;) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#in-practice-1",
    "href": "lectures/9.1-Grouping_Data.html#in-practice-1",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#deriving-measures-of-diversity",
    "href": "lectures/9.1-Grouping_Data.html#deriving-measures-of-diversity",
    "title": "Grouping Data",
    "section": "Deriving Measures of Diversity",
    "text": "Deriving Measures of Diversity\n\nOne of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few… Location Quotient (LQ), Herfindah-Hirschman Index (HHI), Shanon Entropy.\n\n\nI like easy measures."
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#location-quotient",
    "href": "lectures/9.1-Grouping_Data.html#location-quotient",
    "title": "Grouping Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R. \\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[&lt; 1\\]\n\n\nLow Regional Share\n\\[&gt; 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#herfindahl-hirschman-index",
    "href": "lectures/9.1-Grouping_Data.html#herfindahl-hirschman-index",
    "title": "Grouping Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry: \\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[&lt; 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#shannon-entropy",
    "href": "lectures/9.1-Grouping_Data.html#shannon-entropy",
    "title": "Grouping Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure: \\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#spot-the-difference",
    "href": "lectures/9.3-Clustering.html#spot-the-difference",
    "title": "Clustering",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nClassification\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms (A) present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in 1–2 dimensions\n\n\nClustering\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms A present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in &lt; 9 dimensions\n\n\n\n\nClustering algorithms can suffer from the ‘curse of dimensionality’ such that high-dimensional spaces cluster poorly without either dimensionality reduction or the use of specialist algorithms such as Spherical k-Means."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#the-first-geodemographic-classification",
    "href": "lectures/9.3-Clustering.html#the-first-geodemographic-classification",
    "title": "Clustering",
    "section": "The First Geodemographic Classification?",
    "text": "The First Geodemographic Classification?\n\nSource: booth.lse.ac.uk/map/"
  },
  {
    "objectID": "lectures/9.3-Clustering.html#more-than-100-years-later",
    "href": "lectures/9.3-Clustering.html#more-than-100-years-later",
    "title": "Clustering",
    "section": "More than 100 Years Later",
    "text": "More than 100 Years Later\n\nSource: vis.oobrien.com/booth/"
  },
  {
    "objectID": "lectures/9.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "href": "lectures/9.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "title": "Clustering",
    "section": "Intimately Linked to Rise of The State",
    "text": "Intimately Linked to Rise of The State\n\nGeodemographics only possible in context of a State – without a Census it simply wouldn’t work… until now?\nClearly tied to social and economic ‘control’ and intervention: regeneration, poverty & exclusion, crime, etc.\nPresumes that areas are the relevant unit of analysis; in geodemographics these are usually called neighbourhoods… which should ring a few bells.\nIn practice, we are in the realm of ‘homophily’, a.k.a. Tobler’s First Law of Geography"
  },
  {
    "objectID": "lectures/9.3-Clustering.html#where-is-it-used",
    "href": "lectures/9.3-Clustering.html#where-is-it-used",
    "title": "Clustering",
    "section": "Where is it used?",
    "text": "Where is it used?\nAnything involving grouping individuals, households, or areas into larger ‘groups’…\n\nStrategic marketing (above the line, targeted, etc.)\nRetail analysis (store location, demand modelling, etc.)\nPublic sector planning (resource allocation, service development, etc.)\n\nCould see it as a subset of customer segmentation."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#problem-domains",
    "href": "lectures/9.3-Clustering.html#problem-domains",
    "title": "Clustering",
    "section": "Problem Domains",
    "text": "Problem Domains\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nSupervised\nRegression\nClassification\n\n\nUnsupervised\nDimensionality Reduction\nClustering\n\n\n\n\n\nIn classification we ‘know’ the answer (we test against labels).\nIn clustering we don’t ‘know’ the answer (we look for clusters)."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#measuring-fit",
    "href": "lectures/9.3-Clustering.html#measuring-fit",
    "title": "Clustering",
    "section": "Measuring ‘Fit’",
    "text": "Measuring ‘Fit’\n\nUsually working towards an ‘objective criterion’ for quality… these are known as cohesion and separation measures."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#how-your-data-looks",
    "href": "lectures/9.3-Clustering.html#how-your-data-looks",
    "title": "Clustering",
    "section": "How Your Data Looks…",
    "text": "How Your Data Looks…\nClustering is one area where standardisation (and, frequently, normalisation) are essential:\n\nYou don’t (normally) want scale in any one dimension to matter more than scale in another.\nYou don’t want differences between values in one dimension to matter more than differences in another.\nYou don’t want skew in one dimension to matter more than skew in another.\n\nYou also want uncorrelated variables… why?"
  },
  {
    "objectID": "lectures/9.3-Clustering.html#first-steps",
    "href": "lectures/9.3-Clustering.html#first-steps",
    "title": "Clustering",
    "section": "First Steps",
    "text": "First Steps\nYou will normally want a continuous variable… so these types of data are especially problematic:\n\nDummies / One-Hot Encoded\nCategorical / Ordinal\nPossible solutions: k-modes, CCA, etc."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#performance",
    "href": "lectures/9.3-Clustering.html#performance",
    "title": "Clustering",
    "section": "Performance",
    "text": "Performance\nTypically about trade-offs between:\n\n\n\nAccuracy\nGeneralisation"
  },
  {
    "objectID": "lectures/9.3-Clustering.html#trade-offs",
    "href": "lectures/9.3-Clustering.html#trade-offs",
    "title": "Clustering",
    "section": "Trade-Offs",
    "text": "Trade-Offs\nNeed to balance:\n\nAbility to cluster at speed.\nAbility to replicate results.\nAbility to cope with fuzzy/indeterminate boundaries.\nAbility to cope with curse of dimensionality.\nUnderlying representation of group membership…"
  },
  {
    "objectID": "lectures/9.3-Clustering.html#visualising-the-trade-offs",
    "href": "lectures/9.3-Clustering.html#visualising-the-trade-offs",
    "title": "Clustering",
    "section": "Visualising the Trade-Offs",
    "text": "Visualising the Trade-Offs\n\n\n\nNotice the limitations to k-means: it may be fast but it’s got problems if your data is non-linear/non-Gaussian.\nAnd this doesn’t even include options like HDBSCAN, HAC/Hierarchical Clustering, and many more!\n\n\nDetails on scikit-learn.org."
  },
  {
    "objectID": "lectures/9.3-Clustering.html#putting-it-all-into-context",
    "href": "lectures/9.3-Clustering.html#putting-it-all-into-context",
    "title": "Clustering",
    "section": "Putting it All into Context",
    "text": "Putting it All into Context"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#and-more-still",
    "href": "lectures/7.4-Analysing_Text.html#and-more-still",
    "title": "Analysing Text",
    "section": "And More Still!",
    "text": "And More Still!"
  },
  {
    "objectID": "lectures/7.4-Analysing_Text.html#and-this-too",
    "href": "lectures/7.4-Analysing_Text.html#and-this-too",
    "title": "Analysing Text",
    "section": "And This too!",
    "text": "And This too!"
  },
  {
    "objectID": "sessions/reading_week.html#additional-context",
    "href": "sessions/reading_week.html#additional-context",
    "title": "Reading Week",
    "section": "Additional Context",
    "text": "Additional Context\nYou might also find the following content interesting in terms of the practical limitations of ‘AI’ tools and the ways in which they reproduce errors in our own thinking rather that offering a neutral insight into processes:\n\nMission Control: A History of the Urban Dashboard\nAI translation jeopardises Afghan asylum claims\nAn Iowa school district is using ChatGPT to decide which books to ban\nSingapore’s tech-utopia dream is turning into a surveillance state nightmare\n\nAnd here’s a nice example of why it’s not about the algorithm:\n\n\n\nAlgorithmic Perfection\n\n\nSource: Zemanek (1983)\n\nHow I’m fighting bias in algorithms\n\n\n\nEthics, Politics & Data-driven Research and Technology\n\n\n\nWhat constitutes dataset bias? (and what can we do about it?)\n\n\n\nDoes debiasing word embeddings actually work? (+ explanation of GN-GloVe, Hard-debiasing)"
  },
  {
    "objectID": "lectures/9.1-Grouping_Data.html#resources",
    "href": "lectures/9.1-Grouping_Data.html#resources",
    "title": "Grouping Data",
    "section": "Resources",
    "text": "Resources\n\nStop aggregating away the signal in your data"
  },
  {
    "objectID": "lectures/2.7-Git.html#a-rock-climbing-analogy",
    "href": "lectures/2.7-Git.html#a-rock-climbing-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Rock-Climbing Analogy",
    "text": "A Rock-Climbing Analogy"
  }
]