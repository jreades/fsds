[
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "In order to get you started on your data science ‘journey’ you will need to follow the guidance provided on the pages we’ve linked to below from the CASA Computing Platform web site."
  },
  {
    "objectID": "setup/index.html#sec-requirements",
    "href": "setup/index.html#sec-requirements",
    "title": "Getting Started",
    "section": "Steps to Setting Up",
    "text": "Steps to Setting Up\nThere are three steps for getting set up with the CASA Computing Platform:\n\nComplete the health check, which also includes our recommendations if you are considering buying a new computer when you start your studies.\nInstalling and configuring the basic utilities that will enable you to install the programming platform.\nInstalling and configuring the required software.\n\nWe also provide information about Code Camp, which is a reasonably short, self-paced introduction to the fundamentals of programming in Python.\n\n\n\n\n\n\nWarningWhen Installation Fails\n\n\n\nIn an emergency, some of the earlier material in the module will work in Google Colab, but this will make your life significantly harder. We are unable to provide support for this."
  },
  {
    "objectID": "setup/index.html#soft-skills",
    "href": "setup/index.html#soft-skills",
    "title": "Getting Started",
    "section": "Soft Skills",
    "text": "Soft Skills\nOver the years, based on student experience and feedback we have collected a range of advice that is not purely technical in nature. This section covers managing distractions, an introduction to how to read (in the academic sense of reading journal articles and books for meaning and relevance) as well as how to think (in the sense of why the modules are the way they are and the importance of reflection), and how to ask for help (because that’s what we’re to do, but first you need to help yourself!)."
  },
  {
    "objectID": "sessions/week8.html",
    "href": "sessions/week8.html",
    "title": "Textual Data",
    "section": "",
    "text": "Although the direct use of textual (both structured and unstructured) data is still relatively rare in spatial analyses, the growth of crowd-sourced and user-generated content points to the growing importance of this area. he tools and approaches in this area are also evolving quickly and changing rapidly, so this week is intended primarily to familiarise you with the basic landscape in preparation for you developing your skills further in your own time!\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nAn awareness of the benefits of separating content from presentation.\nA basic understanding of pattern-matching in Python (you will have been exposed to this Week 2 of CASA0005)\nA basic understanding of how text can be ‘cleaned’ to make it more amenable for analysis\nAn appreciation of parallelisation in the context of text processing.\nAn appreciation of how text can be analysed.\n\n\n\n\nThe manipulation of text requires a high level of abstraction – of thinking about words as data in ways that are deeply counter-intuitive – but the ability to do forms a critical bridge between this block and the subsequent one, while also reinforcing the idea that numerical, spatial, and textual data analyses provide alternative (and often complementary) views into the data.",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#overview",
    "href": "sessions/week8.html#overview",
    "title": "Textual Data",
    "section": "",
    "text": "Although the direct use of textual (both structured and unstructured) data is still relatively rare in spatial analyses, the growth of crowd-sourced and user-generated content points to the growing importance of this area. he tools and approaches in this area are also evolving quickly and changing rapidly, so this week is intended primarily to familiarise you with the basic landscape in preparation for you developing your skills further in your own time!\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nAn awareness of the benefits of separating content from presentation.\nA basic understanding of pattern-matching in Python (you will have been exposed to this Week 2 of CASA0005)\nA basic understanding of how text can be ‘cleaned’ to make it more amenable for analysis\nAn appreciation of parallelisation in the context of text processing.\nAn appreciation of how text can be analysed.\n\n\n\n\nThe manipulation of text requires a high level of abstraction – of thinking about words as data in ways that are deeply counter-intuitive – but the ability to do forms a critical bridge between this block and the subsequent one, while also reinforcing the idea that numerical, spatial, and textual data analyses provide alternative (and often complementary) views into the data.",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#readings",
    "href": "sessions/week8.html#readings",
    "title": "Textual Data",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nMiller and Goodchild (2015)\nURL\n\n\nDelmelle and Nilsson (2021)\nURL\n\n\nReades et al. (2025)\nURL\n\n\n\n\nStudy Guide\nReading Miller and Goodchild (2015):\n\nHow does “data-driven geography” differ from traditional geographic research?\nHow can “data-driven approaches” be incorporated into geographic research, and what are their potential benefits and limitations?\n\nReflecting on Reades et al. (2025):\n\nWhy has text become increasingly interesting to computational social scientists?\nWhat are the specific advantages of textual data for understanding cities?\nWhat are some of the key challenges and limitations of using textual data in urban research, and how can researchers address these challenges?\n\nConnecting this to Delmelle and Nilsson (2021):\n\nWhat is the framework that Delmelle and Nilsson developed for understanding the language used to advertise properties, and how does it connect to the racial and income profiles of neighborhoods?\nWhat are the implications for understanding neighborhood change and (potential) discrimination in the housing market?\n\nCollecitvely:\n\nHow do these readings connect to the broader themes of the course, and what are the implications for your own research?\n\n\n\n\n\n\n\nTipConnections\n\n\n\nConceptually, this is by far the hardest week of the entire term: there is very little upon which to draw from other modules, and the processing of text with computers rarely makes it beyond simple regular expressions; however, the growth in data that is ‘accidental, open, and everywhere’ (Arribas-Bel 2014) means that a lot more of it is unstructured and contains free-text written by humans as well as numerical and coordinate data generated by sensors or transactions.\n\n\nIf you’re feeling ambitious then you can use the tutorial from the Programming Historian to look at the foundations of text processing and how we can extract important terms from a document as well as, ultimately, the foundations upon which modern Large Language Models are built.",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#pre-recorded-lectures",
    "href": "sessions/week8.html#pre-recorded-lectures",
    "title": "Textual Data",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nNotebooks as Documents\nVideo\nSlides\n\n\nPatterns in Text\nVideo\nSlides\n\n\nCleaning Text\nVideo\nSlides\n\n\nAnalysing Text\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#practical",
    "href": "sessions/week8.html#practical",
    "title": "Textual Data",
    "section": "Practical",
    "text": "Practical\nIn the practical we will continue to work with the InsideAirbnb data, here focussing on the third ‘class’ of data in the data set: text. We will see how working with text is more complex than working with numeric or spatial data and, consequently, why the computational costs rise accordingly. This practical should suggest some new lines of inquiry for Group Project.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nApplying simple regular expressions to find patterns in text.\nHow to clean text in preparation for further analysis.\nSimple transformations that allow you to analyse text (e.g. TF/IDF)\nWays of exploring groups/similarity in textual data.\n\n\n\nTo access the practical:\n\nPreview\nDownload\n\nBonus material (not necessary for the assessment, just ‘nice to know’ if you’re interested in the topic) containing material related to Natural Language Processing (NLP):\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week6.html",
    "href": "sessions/week6.html",
    "title": "Numeric Data",
    "section": "",
    "text": "This week we will be introducing the use of the pandas library for data analysis and management through a focus on numeric data and its distribution(s). This marks a major shift from working with concepts (lists, dictionaries, functions, etc.) largely in isolation to encountering all of them together ‘in the wild’ as part of a full data science workflow. So we are moving from the acquisition of concepts to their integration in the same way that we will — over the course of these three sessions — be coming from data acquisition to data integration.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nAn appreciation of how and why this module differs from (QM) CASA0007.\nThe beginnings of a more integrative understanding of foundational computer science concepts and the practice(s) of data science.\nA basic understanding of data acquisition and manipulation in Python.",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#overview",
    "href": "sessions/week6.html#overview",
    "title": "Numeric Data",
    "section": "",
    "text": "This week we will be introducing the use of the pandas library for data analysis and management through a focus on numeric data and its distribution(s). This marks a major shift from working with concepts (lists, dictionaries, functions, etc.) largely in isolation to encountering all of them together ‘in the wild’ as part of a full data science workflow. So we are moving from the acquisition of concepts to their integration in the same way that we will — over the course of these three sessions — be coming from data acquisition to data integration.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nAn appreciation of how and why this module differs from (QM) CASA0007.\nThe beginnings of a more integrative understanding of foundational computer science concepts and the practice(s) of data science.\nA basic understanding of data acquisition and manipulation in Python.",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#readings",
    "href": "sessions/week6.html#readings",
    "title": "Numeric Data",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nAnderson (2008)\nURL\n\n\nD’Ignazio and Klein (2020a) Ch.4\nURL\n\n\nCox and Slee (2016)\nURL\n\n\n\n\nStudy Guide\nLooking at Anderson (2008):\n\nHow does the idea of “more is different” relate to the concept of the Petabyte Age?\nWhat does Anderson mean by “Correlation is enough”?\nWhat examples does Anderson provide to show that the scientific method is becoming obsolete?\n\nReading D’Ignazio and Klein (2020a, Ch.4):\n\nHow do their ideas about the power dynamics of data collection challenge Anderson’s claim that “the numbers speak for themselves”?\nWhat are the potential consequences of relying solely on correlation, as Anderson suggests, without considering the social and political contexts of data?\nHow might the concept of the “privilege hazard” inform an analysis of Anderson’s argument?\n\nConsidering Cox and Slee (2016):\n\nHow does the case study presented in Cox and Slee illustrate the issues explored in the other readings?\nAccording to Cox and Slee, how did the cleaning of the New York City listings reflect the company’s power and privilege?\nHow does this cleaning potentially harm less-privileged stakeholders in the company’s platform?\n\n\n\n\n\n\n\nTipConnections\n\n\n\nRead D’Ignazio and Klein (2020b) to highlight the importance of thinking about what a data set captures… and what it excludes. Cox and Slee (2016) is also a first introduction to the underlying data that we’ll be working with over the rest of term. D’Ignazio and Klein (2020b) should be getting you thinking about how ‘cleaning’ is not just about hygiene but has real implications for what you can and will find in your data: data is often a lot less ‘tidy’ than tidyr might lead you think! You should almost never be claiming that your (social) data represents the ‘universe’ of behaviours or is somehow ‘complete’.",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#pre-recorded-lectures",
    "href": "sessions/week6.html#pre-recorded-lectures",
    "title": "Numeric Data",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLogic\nVideo\nSlides\n\n\nRandomness\nVideo\nSlides\n\n\nData\nVideo\nSlides\n\n\nPandas\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#practical",
    "href": "sessions/week6.html#practical",
    "title": "Numeric Data",
    "section": "Practical",
    "text": "Practical\nIn this practical we will begin working with the InsideAirbnb data, which you will have briefly examined in CASA0005. This week we focus on the first ‘class’ of data in the data set: simple numeric columns. We will see how to use Pandas for (simple) visualisation and (the beginnings of) analysis. It is hoped that you will see how Pandas combines and builds on techniques that we’ve already seen: while Pandas is incredibly sophisticated, the underlying concepts have been covered in the preceding three weeks! At this point we will also begin to make use of Pandas functionality to subset and explore the data.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nSeeing how Pandas is ‘just’ a sophisticated extension of what we’ve already done.\nFamiliarising yourself with Pandas functionality.\nPerforming basic data cleaning and exploration tasks (including visualisation).\nSelecting and aggregating data in pandas.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Reduce, Reuse, Recycle",
    "section": "",
    "text": "This week we will look at how frequently-used code can be packaged up in functions and libraries. This is the point at which we begin to engage with code in a more abstract way because we are increasingly interested in reusability and flexibility.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nTo develop an understanding of code re-use through functions.\nTo develop an appreciation of the utility of packages and namespaces.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#overview",
    "href": "sessions/week4.html#overview",
    "title": "Reduce, Reuse, Recycle",
    "section": "",
    "text": "This week we will look at how frequently-used code can be packaged up in functions and libraries. This is the point at which we begin to engage with code in a more abstract way because we are increasingly interested in reusability and flexibility.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nTo develop an understanding of code re-use through functions.\nTo develop an appreciation of the utility of packages and namespaces.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#readings",
    "href": "sessions/week4.html#readings",
    "title": "Reduce, Reuse, Recycle",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nMassey (1996)\nURL\n\n\nD’Ignazio and Klein (2020) Ch.3\nURL\n\n\n\n\nStudy Guide\n\nReading Massey’s seminal “Politicising space and place”:\n\n\nWhat are the two conflicting geographical imaginations at play in contemporary society? Explain their contradictory nature.\nIn the case study of ‘high-tech’ scientists, how do the ‘power geometries’ of work and home reflect gendered power relations?\n\n\nLooking at D’Ignazio and Klein’s “On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints”:\n\n\nWhat does the “view from nowhere” concept mean in the context of data visualization, and why do they find it problematic?\nHow do they propose to address the limitations of data visualization? Describe their concept of “data visceralization” and its significance.\nWhy do they suggest that you shouldn’t never use the ‘god trick’?\n\n\n\n\n\n\n\nTipConnections\n\n\n\nThese are two of the more challenging readings this term, but they are critical to understanding what we are trying to teach you: it’s not just about learning to code, it’s about learning how to deploy code/quantitative methods to support your argument, while maintaining a keen eye on how bias – in both the technical and the ethical senses – can creep into your thinking and, consequently, your results!",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#pre-recorded-lectures",
    "href": "sessions/week4.html#pre-recorded-lectures",
    "title": "Reduce, Reuse, Recycle",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nFunctions\nVideo\nSlides\n\n\nDecorators\nVideo\nSlides\n\n\nPackages\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#in-person-lectures",
    "href": "sessions/week4.html#in-person-lectures",
    "title": "Reduce, Reuse, Recycle",
    "section": "In-Person Lectures",
    "text": "In-Person Lectures\nIn this week’s session will provide additional detail on the assessments.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMore on the Assessments\nIn class\nSlides\n\n\nGroup Working\nIn class\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#practical",
    "href": "sessions/week4.html#practical",
    "title": "Reduce, Reuse, Recycle",
    "section": "Practical",
    "text": "Practical\nThis week’s practical will be looking at how functions (and variables) can be collected into resuable packages that we can either make ourselves or draw on a worldwide bank of experts – I know who I’d rather depend on when the opportunity arises! However, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling the following sessions:\n\nFunctions\nPackages\n\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nSeeing how functions and decorators can help us to reuse code efficiently.\nBeginning to make use of packages to access/interact with data.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Foundations (Pt. 1)",
    "section": "",
    "text": "This week we will be quickly covering the fundamentals of Python programming, while developing a critical appreciation of data science as an ongoing ‘process’ that calls for iterative improvement and deeper reflection. We will be contextualising computers within a wider landscape of geographical/spatial research. And we will be (briefly) reviewing the basics of Python with a focus on simple data structures. We’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nA review of basic Python syntax and operators.\nAn understanding of how none of this all that new.\n\n\n\nSo we will be contextualising all of this within the longer history of the study of geography (or planning!) through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#overview",
    "href": "sessions/week2.html#overview",
    "title": "Foundations (Pt. 1)",
    "section": "",
    "text": "This week we will be quickly covering the fundamentals of Python programming, while developing a critical appreciation of data science as an ongoing ‘process’ that calls for iterative improvement and deeper reflection. We will be contextualising computers within a wider landscape of geographical/spatial research. And we will be (briefly) reviewing the basics of Python with a focus on simple data structures. We’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nA review of basic Python syntax and operators.\nAn understanding of how none of this all that new.\n\n\n\nSo we will be contextualising all of this within the longer history of the study of geography (or planning!) through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#readings",
    "href": "sessions/week2.html#readings",
    "title": "Foundations (Pt. 1)",
    "section": "Readings",
    "text": "Readings\nCome to class having read:\n\n\n\nCitation\nArticle\n\n\n\n\nBurton (1963)\nURL\n\n\nDonoho (2017)\nURL\n\n\nFranklin (2024)\nURL\n\n\n\n\nStudy Guide\nThe following questions will help guide your reading and prepare you for class discussions:\n\nThinking about Burton (1963), evaluate the impact of the “quantitative revolution” on the field of Geography.\n\n\nHow do Burton’s views on the quantitative revolution differ from the perspectives presented in the more recent sources?\nHow has the relationship between geography and computing evolved over time?\nWas the quantitative revolution a singular event or part of a more continuous process?\n\n\nThinking about Donoho (2017)’s definition of data science…\n\n\nWhat is the “Big Data” meme and why does Donoho find it misleading?\nExplain Breiman’s concept of the “two cultures” in data analysis.\nWhat does Tukey argue are the three essential constituents of a science, and how does he apply them to the field of data analysis?\nWhat are CTFs and how do they relate to the “predictive culture” of data analysis?\nWhat are the six divisions of the Greater Data Science (GDS) framework?\n\n\nAccording to Franklin (2024)…\n\n\nWhat are three recent developments that distinguish the current landscape of open software in quantitative methods from previous eras?\nWhat are some of the emerging subfields within quantitative methods, and what common goal do they share?\nThe author argues that the rebranding of quantitative methods as “data science” or “analytics” can be beneficial in certain contexts. Explain these benefits.\nWhat concerns are raised about the fragmentation of the quantitative methods community identity? \n\n\n\n\n\n\n\nTipConnections\n\n\n\nFranklin (2024) offers another perspective on ‘the discipline’ of quantitative human geography and its heterogeneity, while Donoho (2017) will give you context on how data science might differ from what’s covered in Quantitative Methods. You might also find Unwin (1980) useful for understanding why the practicals are set up the way they are and why we don’t post ‘answers’ until a few days after the last practical group has completed its session. You should read Burton (1963) while reflecting on Arribas-Bel and Reades (2018) with a view to seeing that ‘there is nothing new under the sun’: we tend to think that the challenges we face now in terms of data volumes and complexity are novel, but they are not. Indeed, here’s John Graham-Cumming keynoting a 2012 conference talking about the Lyons Tea Company and how its programmers invented Dykstra’s shortest path algorithm more than 20 years before Dykstra did!",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#pre-recorded-lectures",
    "href": "sessions/week2.html#pre-recorded-lectures",
    "title": "Foundations (Pt. 1)",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nThis week is very busy because we need to cover off the basics for those of you who were unable to engage with Code Camp, while recapping only the crucial bits for those of you who were able to do so.\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nPython: the Basics (Part 1)\nVideo\nSlides\n\n\nPython: the Basics (Part 2)\nVideo\nSlides\n\n\nLists\nVideo\nSlides\n\n\nIteration\nVideo\nSlides\n\n\nThe Command Line\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#in-person-lectures",
    "href": "sessions/week2.html#in-person-lectures",
    "title": "Foundations (Pt. 1)",
    "section": "In-Person Lectures",
    "text": "In-Person Lectures\nIn this week’s session we will look at the practices of data scientists in the wider context of the arrival of LLMs.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nWhat We Do\nIn Class\nSlides\n\n\nOn Writing\nIn Class\nSlides\n\n\nOn Coding\nIn Class\nSlides\n\n\nCLI+Podman+PostGIS\nIn Class\nDemo & Discussion",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#practical",
    "href": "sessions/week2.html#practical",
    "title": "Foundations (Pt. 1)",
    "section": "Practical",
    "text": "Practical\nThis week’s practical requires you to have completed installation of the programming environment. Make sure you have completed setup of the environment.\nWe will take you through the fundamentals of Python, including the use of simple1 Boolean logic and lists. However, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling the following sessions:\n\nThe Basics\nBoolean Logic\nLists\n\nTo run the code for these sessions you can:\n\nFollow the instructions for running these in Google’s Collaboratory; or\nCreate a new Notebook in Podman’s JupyterLab (File &gt; New &gt; Notebook) and copy+paste the code into new Code cells.\n\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nEnsuring that you are set up with Git/GitHub\nReviewing Python basics\nReviewing Python lists and logic\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#footnotes",
    "href": "sessions/week2.html#footnotes",
    "title": "Foundations (Pt. 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: simple does not mean ‘easy’! Just because we say something is ‘basic’ or ‘simple’ does not mean that we think it is straightforward for someone learning to code for the first time!↩︎",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week11.html",
    "href": "sessions/week11.html",
    "title": "Dimensions in Data",
    "section": "",
    "text": "This session is ‘supplemental’, meaning that it is here to help you integrate ideas seen across Term 1 (and which will be encountered again in Term 2) in a way that sup ports long-term learning. It is not essential to passing the course and there are no ‘bonus points’ for using methods found in this session.",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#overview",
    "href": "sessions/week11.html#overview",
    "title": "Dimensions in Data",
    "section": "Overview",
    "text": "Overview\nThis is the most profoundly abstract aspect of data analysis: how to conceive of your data as a multi-dimensional space that can be reshaped and transformed to support your analytical objectives. This foregrounds the importance of judgement since, as the economist Ronald Coase is reputed to have said:\n\n“If you torture the data long enough, it will confess.”\n\nBy which you should understand that transformation is a form of ‘torture’1: it can force the data to reveal relationships that were previously hidden from the data scientist. However, taken too far the data will confess to whatever you want, which isn’t the purpose of critical, reproducible, sound data science!\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nA deeper understanding of the issues surrounding clustering that were covered in Week 6 of CASA0005 (GIS) and CASA0007 (QM).\nAn understanding of how data transformation works and the reasons for choosing one transform over another.\nAn appreciation of the pros and cons of at least two dimensionality reduction techniques.",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#preparatory-lectures",
    "href": "sessions/week11.html#preparatory-lectures",
    "title": "Dimensions in Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nThe Data Space\nVideo\nSlides\n\n\nTransformation\nVideo\nSlides\n\n\nDimensionality\nVideo\nSlides",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#other-preparation",
    "href": "sessions/week11.html#other-preparation",
    "title": "Dimensions in Data",
    "section": "Other Preparation",
    "text": "Other Preparation\n\nThe following readings may be useful for reflecting on the topics covered in this session:\n\nBunday (n.d.) &lt;URL&gt;\nHarris (n.d.) &lt;URL&gt;\nCima (n.d.) &lt;URL, PDF with Figures&gt;\n\n\n\n\n\n\n\n\nTipConnections\n\n\n\nThese readings provide very practical insights into the ways that data transformation can ‘torture the data until it confesses’ as well as how we can use data transformation to generate new ways of seeing our data and, consequently, new ways of understanding it. You should be coming out of these readings with a clearer understanding of why there’s rarely a ‘right’ or ‘wrong’ approach to a real-world data set, but there are ‘better’ and ‘worse’ approaches. These readings are predominantly non-academic so they should (I hope) be fairly accessible and quick to read despite the potential dryness of the topics.",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#practical",
    "href": "sessions/week11.html#practical",
    "title": "Dimensions in Data",
    "section": "Practical",
    "text": "Practical\nThis practical will show you how data transformation is an essential, but often overlooked, aspect of data analysis: depending on the choices we make here, we can reduce (or increase) the dimensionality of the data and make it more (or less) tractable for subsequent analysis. This approach to the pipeline relies on you being able to see your data as existing in an abstract ‘space’ that can be manipulated in order to foreground, compress, or even mask attributes.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nWorking with a more complex data structure to create new ‘grouped’ variables (as the simplest form of transformation)\nUsing sklearn to fit and transform data in a flexible manner.\nDoing two types of dimensionality reduction to demonstrate how different linear and non-linear dimensionality reduction are.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#footnotes",
    "href": "sessions/week11.html#footnotes",
    "title": "Dimensions in Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this is a metaphor only!↩︎",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Setting Up",
    "section": "",
    "text": "In the first week we will be focussing on the supporting infrastructure for ‘doing data science’. That is to say, we’ll be dealing with the installation and configuration of tools such as GitHub and Docker which support replicable, shareable, and document-able data science. As a (free) bonus, the use of these tools also protects you against catastrophic (or the merely irritating) data loss thanks to over-zealous editing of code or content. You should see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career.\n\n\n\n\n\n\nImportantLearning objectives\n\n\n\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nIf you missed the Induction Week ‘install fest’, please now complete as many of these activities as you can:\n\nGo through the computer health check.\nInstall the base utilities.\nInstall the programming environment.\n\nThe last of these is the stage where you’re most likely to encounter problems that will need our assistance, so knowing that you need our help in Week 1 means that you can ask for it much sooner in the practical!",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#overview",
    "href": "sessions/week1.html#overview",
    "title": "Setting Up",
    "section": "",
    "text": "In the first week we will be focussing on the supporting infrastructure for ‘doing data science’. That is to say, we’ll be dealing with the installation and configuration of tools such as GitHub and Docker which support replicable, shareable, and document-able data science. As a (free) bonus, the use of these tools also protects you against catastrophic (or the merely irritating) data loss thanks to over-zealous editing of code or content. You should see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career.\n\n\n\n\n\n\nImportantLearning objectives\n\n\n\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nIf you missed the Induction Week ‘install fest’, please now complete as many of these activities as you can:\n\nGo through the computer health check.\nInstall the base utilities.\nInstall the programming environment.\n\nThe last of these is the stage where you’re most likely to encounter problems that will need our assistance, so knowing that you need our help in Week 1 means that you can ask for it much sooner in the practical!",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#readings",
    "href": "sessions/week1.html#readings",
    "title": "Setting Up",
    "section": "Readings",
    "text": "Readings\nPlease make time to read:\n\n\n\nCitation\nArticle\n\n\n\n\nArribas-Bel and Reades (2018)\nURL\n\n\n\n\nStudy Guide\nThe following questions will help guide your reading and prepare you for class discussions:\n\nDrawing on Arribas-Bel and Reades (2018), compare and contrast GIS, Geocomputation, and Geographical Data Science (GDS):\n\n\nWhat are their core focuses and methodological approaches?\nHow do they differ in their relationship to technological change?\nWhat are the unique contributions of GDS in the context of “big data” and the rise of data science?\n\n\nStill drawing on Arribas-Bel and Reades (2018), consider the role of technological determinism in the evolution of geographical thought:\n\n\nDo technological advancements determine the direction of geographical inquiry?\nHow do the authors characterize the relationship between technological change and the development of geographical thought?\nWhat evidence do they provide to support their view?",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#in-person-lectures",
    "href": "sessions/week1.html#in-person-lectures",
    "title": "Setting Up",
    "section": "In-Person Lectures",
    "text": "In-Person Lectures\nIn this week’s workshop we will review the module aims, learning outcomes, and expectations with a general introduction to the course.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nGetting Started\nIn Class\nSlides\n\n\nComputers in Urban Studies\nIn Class\nSlides\n\n\nPrinciples\nIn Class\nSlides\n\n\nTools of the Trade\nIn Class\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#practical",
    "href": "sessions/week1.html#practical",
    "title": "Setting Up",
    "section": "Practical",
    "text": "Practical\nThis week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2, and familiarising you with ‘how people do data science’. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: get out of the way of doing ‘stuff’; support teams in negotating conflicts in code; make it easy to share results; and make it easy to ensure that everyone is ‘on the same page’.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nGetting you up and running with the coding and collaboration tools.\nProviding you with hands-on experience of using these tools.\nConfiguring your programming environment for the rest of the programme.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Overview",
    "section": "",
    "text": "We have ‘flipped’ the classroom for this module so we expect you to come to ‘lecture’ (except in Week 1!) having already watched the assigned videos and completed the assigned readings. From Week 3 you may be called upon to answer questions about the key points in, and relevance of, an assigned video or reading to the rest of the class.\nThis means that there is a mix of ‘asynchronous’ (work that you do in your own time) and ‘synchronous’ (work that we do during scheduled hours) interaction. Synchronous activities will normally be recorded for review afterwards, but you should bear in the mind the following: 1) we cannot be responsible for equipment failure; 2) we are unable to record practicals and other small-group activities; and 3) a 2-hour video of the in-person session will be rather less educational and informative than actually being there.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#preparation",
    "href": "sessions/index.html#preparation",
    "title": "Overview",
    "section": "Preparation",
    "text": "Preparation\nThe nature and amount of preparation will vary from week to week, but may include:\n\nReadings from both academic and non-academic sources.\nPre-recorded lectures from CASA staff.\nPre-ecorded videos from non-CASA staff.\nPreparing contributions to set tasks (e.g. summaries, Q&A, etc.)\n\nTo get the most value from the module you must do the readings. We have raised the stakes for not doing so since, in previous years, students who did not do the readings often struggled with the final assessment and went on to have even more significant struggles with their dissertation.\nIf you don’t do the readings you are leaving a lot of easy marks on the table. More importantly, we believe that the single most important skill that you can acquire from FSDS is not the ability to code, it’s the ability to critically interrogate data and recognise the strengths and limitations that are relevant to the problem at-hand. You will learn the technical aspects of data analysis in the practicals. You will learn the critical dimension from doing the readings.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#in-person-session",
    "href": "sessions/index.html#in-person-session",
    "title": "Overview",
    "section": "In-Person Session",
    "text": "In-Person Session\nNote that this is not your usual lecture, it’s going to be a very interactive in-person session involving responses to assigned questions, lecturer-led discussions, and some amount of ‘live coding’. We will assume you have completed the preparatory activities, which will include both ‘Preparatory Lectures’ (a series of short videos for which the slides are also available for note-taking) and ‘Other Preparation’ (primarily readings).\nFor the readings you will have a Template to complete. Download the raw QMD file and fill one in for each of the week’s assigned readings.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#practicals",
    "href": "sessions/index.html#practicals",
    "title": "Overview",
    "section": "Practicals",
    "text": "Practicals\nIn order to make use of these materials you will need to install the Spatial Data Science programming environment.\nPracticals are run in groups to maximise your ability to ask questions and interact with other students. You will be notified of your group by the Professional Services team; there may be limited opportunities to switch, and the best way would be to swap with another student and then notify us of the arrangment. You may wish to download the week’s Jupyter notebook before the start of class in order to familiarise yourself with the material.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#improvements",
    "href": "sessions/index.html#improvements",
    "title": "Overview",
    "section": "Improvements",
    "text": "Improvements\nWe are always making improvements to FSDS and try to keep track of student-feedback here.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "ref/Template.html",
    "href": "ref/Template.html",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "ref/Template.html#what-kind-of-reading-is-it",
    "href": "ref/Template.html#what-kind-of-reading-is-it",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "ref/Template.html#who-is-the-intended-audience",
    "href": "ref/Template.html#who-is-the-intended-audience",
    "title": "Summary of Reading",
    "section": "2 Who is the intended audience?",
    "text": "2 Who is the intended audience?\n\nIs it intended for academics, field experts, the general public? etc.? For example, if you think this is intended for someone who needs to learn new skills or someone who is in a policy-making position how do you know this?"
  },
  {
    "objectID": "ref/Template.html#how-is-the-piece-structured",
    "href": "ref/Template.html#how-is-the-piece-structured",
    "title": "Summary of Reading",
    "section": "3 How is the piece structured?",
    "text": "3 How is the piece structured?\n\nBriefly, how is it organised in terms of headings, sub-headings, sections, etc.? Can you explain this structure responds to the kind of reading and type of audience?"
  },
  {
    "objectID": "ref/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "href": "ref/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "title": "Summary of Reading",
    "section": "4 What are the key ideas, concepts, or theories discussed?",
    "text": "4 What are the key ideas, concepts, or theories discussed?\n\nBriefly identify the specific areas that the contribution engages with. How do you know this?"
  },
  {
    "objectID": "ref/Template.html#what-is-the-overall-contribution",
    "href": "ref/Template.html#what-is-the-overall-contribution",
    "title": "Summary of Reading",
    "section": "5 What is the overall contribution?",
    "text": "5 What is the overall contribution?\n\nWhat does it build on or what gap does it respond to? What are the key findings or conclusions?"
  },
  {
    "objectID": "ref/Template.html#what-issues-or-gaps-remain",
    "href": "ref/Template.html#what-issues-or-gaps-remain",
    "title": "Summary of Reading",
    "section": "6 What issues or gaps remain?",
    "text": "6 What issues or gaps remain?\n\nIf relevant, are there assumptions that might not hold in other contexts? Can you think of other case studies or contexts where the reading would apply and explain why? If you think the contribution is generally valid can you explain why? Are there areas for future work identified?"
  },
  {
    "objectID": "ref/Bibliography.html",
    "href": "ref/Bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Bibliography\n\n\nAbhinav. 2025. “Docker’s Gone — Here’s Why It’s Time to Move on | by Abhinav | Medium.” Online; Medium.\n\n\nAlsudais, Abdulkareem. 2021. “Incorrect Data in the Widely Used Inside Airbnb Dataset.” Decision Support Systems 141:113453. https://doi.org/10.1016/j.dss.2020.113453.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/10.1177/0263276419851846.\n\n\nAnalyst Uttam. 2025. “‘Data Science Is a Dead Career’ — the Truth Behind the Trend No One Wants to Say Out Loud.” https://medium.com/ai-analytics-diaries/https-analystuttam-substack-com-p-data-science-is-a-dead-career-87ee2d8bd338.\n\n\nAnderson, C. 2008. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete.” Wired. https://www.wired.com/2008/06/pb-theory/.\n\n\nArribas-Bel, Daniel. 2014. “Accidental, Open and Everywhere: Emerging Data Sources for the Understanding of Cities.” Applied Geography 49. Elsevier:45–53. https://doi.org/10.1016/j.apgeog.2013.09.012.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBergstrom, C. T., and J. D. West. 2025. “Modern-Day Oracles or Bullshit Machines.” https://thebullshitmachines.com.\n\n\nBivens, J. 2019. “The Economic Costs and Benefits of Airbnb.” Economic Policy Institute. The economic costs and benefits of Airbnb.\n\n\nBrockes, E. 2023. “Airbnb was wild, disruptive and cheap: we loved it. But it wasn’t a love strong enough to last.” The Guardian. https://www.theguardian.com/commentisfree/2023/mar/08/airbnb-wild-disruptive-cheap-lettings-agency.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nClark, J. 2023. “Bidding wars: inside the super-charged fight for rental properties.” The Guardian. https://www.theguardian.com/money/2023/apr/08/bidding-wars-inside-the-super-charged-fight-for-rental-properties.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020d. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020c. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020e. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020b. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020a. Data Feminism. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\nDark Matter Labs. 2019. “A Smart Commons: A New Model for INvesting in the Commons.” Medium. https://provocations.darkmatterlabs.org/a-smart-commons-528f4e53cec2.\n\n\nDavenport, T. H., and D. J. Patil. 2012. “Data Scientist: The Sexiest Job of the 21st Century.” Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\nDelmelle, Elizabeth C, and Isabelle Nilsson. 2021. “The Language of Neighborhoods: A Predictive-Analytical Framework Based on Property Advertisement Text and Mortgage Lending Data.” Computers, Environment and Urban Systems 88. Elsevier:101658. https://doi.org/10.1016/j.compenvurbsys.2021.101658.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics‘.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching introductory GIS programming to geographers using an open source Python approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30. https://doi.org/10.1080/03098265.2015.1086981.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nFitzpatrick, B., and B. Collins-Sussman. n.d. “The Myth of the ’Genius Programmer’.” Google. https://www.youtube.com/watch?v=0SARbwvhupQ.\n\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGoel, N. 2025. “New Junior Developers Can’t Actually Code.” https://nmn.gl/blog/ai-and-learning.\n\n\nGraham, P. 2022. “Putting Ideas into Words.” 2022. https://paulgraham.com/words.html.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHägerstrand, Torsten. 1967. “The Computer and the Geographer.” Transactions of the Institute of British Geographers. JSTOR, 1–19.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHarvey, D. 2008 [1972]. “Revolutionary and Counter-Revolutionary Theory in Geography and the Problem of Ghetto Formation.” In Geographic Thought, edited by G. Henderson and M. Waterstone, 1st Edition. Routledge. https://doi.org/10.4324/9780203893074.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nHuck, G. 2025. “The Developer Skill You Might Be Neglecting.” https://stackoverflow.blog/2025/01/17/the-developer-skill-you-might-be-neglecting/.\n\n\nIqbal, N., and A. Chakrabortty. 2023. “Why are London’s inner-city schools disappearing?” Edited by A. Bransbury. The Guardian. 2023. https://www.theguardian.com/news/audio/2023/apr/26/why-are-london-schools-disappearing-podcast.\n\n\nJolly, J. 2023. “Owners of 100,000 properties held by foreign shell companies unknown despite new UK laws.” The Guardian. https://www.theguardian.com/business/2023/sep/03/owners-of-100000-properties-held-by-foreign-shell-companies-unknown-despite-new-uk-laws.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nKlaas, B. 2025. “The Death of the Student Essay–and the Future of Cognition.” https://www.forkingpaths.co/p/the-death-of-the-student-essayand.\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2). Oxford University Press:97–111.\n\n\n———. 1996. Selected Papers on Computer Science. Cambridge University Press.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLansley, Guy. 2016. “Cars and Socio-Economics: Understanding Neighbourhood Variations in Car Characteristics from Administrative Data.” Regional Studies, Regional Science 3 (1). Taylor & Francis:264–85. https://doi.org/10.1080/21681376.2016.1177466.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are statisticians cold-blooded bosses? a new perspective on the ’old’ concept of statistical population.” Teaching Statistics 35 (1). Wiley Online Library:66–71. https://doi.org/10.1111/j.1467-9639.2012.00524.x.\n\n\nLund, C. 2025. “The Incuriosity Engine.” https://cooperlund.medium.com/the-incuriosity-engine-16bdf41e229d.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458.\n\n\nMattern, Shannon. 2015. “Mission control: A history of the urban dashboard.” Places Journal. https://doi.org/10.22269/150309.\n\n\n———. 2017. “A City Is Not a Computer.” Places Journal. https://doi.org/10.22269/170207.\n\n\nMcCullough, D. 2002. “Interview with NEH chairman Bruce Cole.” Humanities Magazine.\n\n\nMiller, Harvey J, and Michael F Goodchild. 2015. “Data-Driven Geography.” GeoJournal 80. Springer:449–61. https://doi.org/10.1007/s10708-014-9602-6.\n\n\nMinton, A. 2023. “New York is breaking free of Airbnb’s clutches. This is how the rest of the world can follow suit.” The Guardian. https://www.theguardian.com/commentisfree/2023/sep/27/new-york-airbnb-renters-cities-law-ban-properties.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nNeate, R. 2023. “‘This is where people with staggering wealth end up’: who will buy Britain’s most expensive house?” The Guardian. https://www.theguardian.com/money/2023/apr/08/britain-most-expensive-house-rutland-gate-mansion-london-super-rich-buyer.\n\n\nO’Sullivan, David, and Steven M Manson. 2015. “Do physicists have geography envy? And what can geographers learn from it?” Annals of the Association of American Geographers 105 (4). Taylor & Francis:704–22. https://doi.org/10.1080/00045608.2015.1039105.\n\n\nOpen Data Institute. n.d. “SOD0009 - Evidence on Statistics and Open Data.” https://committees.parliament.uk/writtenevidence/45220/pdf/.\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nReades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian. https://doi.org/10.46430/phen0111.\n\n\nReades, J., H. Yingjie, Emmanouil Tranos, and E. Delmelle. 2025. “The City as Text.” Nature Cities. https://doi.org/https://doi.org/10.1038/s44284-025-00314-x.\n\n\nRichmond, T. 2025. “EducAItion, educAItion, educAItion: Could Generative Artificial Intelligence pose a risk to educational standards?” Social Market Foundation. 2025. https://www.smf.co.uk/publications/ai-and-learning/.\n\n\nRose, Gillian. 1997. “Situating Knowledges: Positionality, Reflexivities and Other Tactics.” Progress in Human Geography 21 (3):305–20. https://doi.org/10.1191/030913297673302122.\n\n\nScheider, Simon, Enkhbold Nyamsuren, Han Kruiger, and Haiqi Xu. 2020. “Why Geographic Data Science Is Not a Science.” Geography Compass 14 (11). Wiley Online Library:e12537.\n\n\nSchneider, A. 2013. “How to Become a Data Scientist Before You Graduate.” Berkeley Science Review. http://berkeleysciencereview.com/how-to-become-a-data-scientist-before-you-graduate/.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2P) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSmith, D. 2010. “Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London.” GLA Economics. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nStrauß, Stefan. 2015. “Datafication and the Seductive Power of Uncertainty–a Critical Exploration of Big Data Enthusiasm.” Information 6 (4). MDPI:836–47.\n\n\nSusnjara, S., and I. Smalley. 2025. “What Is Virtualization?” 2025. https://www.ibm.com/think/topics/virtualization.\n\n\nTravers, Tony, Sam Sims, and Nicolas Bosetti. 2016. “Housing and Inequality in London.” Centre for London. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nTufte, Edward R, and Peter R Graves-Morris. 1983. The Visual Display of Quantitative Information. Vol. 2. 9. Graphics press Cheshire, CT.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nVanderPlas, Jake. 2014. “Is Seattle Really Seeing an Uptick in Cycling?” http://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1):160018. https://doi.org/10.1038/sdata.2016.18.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608. https://doi.org/10.1177/0309132520924722.\n\n\nXiao, Ningchuan. 2016. GIS Algorithms: Theory and Applications for Geographic Information Science & Technology. Research Methods. SAGE. https://doi.org/https://dx.doi.org/10.4135/9781473921498.\n\n\nXie, Tessa. 2024a. “How to Better Communicate as a Data Scientist.” Towards Data Science. https://www.divingintodata.com/p/how-to-better-communicate-as-a-data-scientist-6fc5428d3143.\n\n\n———. 2024b. “One Mindset Shift That Will Make You a Better Data Scientist.” Diving Into Data. https://www.divingintodata.com/p/one-mindset-shift-that-will-make-you-a-better-data-scientist-a015f8000ad7.\n\n\n———. 2024c. “The Most Undervalued Skill for Data Scientists.” Towards Data Science. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321.\n\n\nZemanek, H. 1983. “Algorithmic Perfection.” Annals of the History of Computing. AMER FED INFORM PROCESSING SOC.\n\n\nZervas, Georgios, Davide Proserpio, and John W Byers. 2021. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” Marketing Letters 32. Springer:1–16.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html",
    "href": "practicals/Practical-11-Dimensions_in_Data.html",
    "title": "Practical 11: Dimensions in Data",
    "section": "",
    "text": "In this session the focus is on MSOA-level Census data from 2011. We’re going to explore this as a possible complement to the InsideAirbnb data. Although it’s not ideal to use 2011 data with scraped from Airbnb this year, we:\nUltimately, however, you don’t need to use this for your analysis, this practical is intended as a demonstration of how transformation and dimensionality reduction work in practice and the kinds of issues that come up."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#load-msoa-excel-file",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#load-msoa-excel-file",
    "title": "Practical 11: Dimensions in Data",
    "section": "Load MSOA Excel File",
    "text": "Load MSOA Excel File\n\n\n\n\n\n\nTipDifficulty level: Low.\n\n\n\n\n\n\n\n\n\nYou might like to load the cached copy of the file into Excel so that you can see how the next bit works. You can find the rest of the MSOA Atlas here.\n\nsrc_url   = 'https://data.london.gov.uk/download/msoa-atlas/39fdd8eb-e977-4d32-85a4-f65b92f29dcb/msoa-data.xls'\ndest_path = Path('data/msoa')\n\nYou will want to manually download the data to have a look in Excel while you sort out the answer below. We’re mainly doing this so that you use and make sense of documentation for handling more complex data structures in Excel.\n\nQuestion\n\nexcel_atlas = pd.read_excel(\n    cache_data(src_url, dest_path), \n    ???, # Which sheet is the data in?\n    header=[0,1,2])            # Where are the column names... there's three of them!\n\nNotice the format of the output and notice that all of the empty cells in the Excel sheet have come through as Unnamed: &lt;col_no&gt;_level_&lt;level_no&gt;:\n\nexcel_atlas.head(1)\n\n\nprint(f\"Shape of the MSOA Atlas data frame is: {excel_atlas.shape[0]:,} x {excel_atlas.shape[1]:,}\")\n\nYou should get: Shape of the MSOA Atlas data frame is: 984 x 207, but how on earth are you going to access the data?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#accessing-multiindexes",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#accessing-multiindexes",
    "title": "Practical 11: Dimensions in Data",
    "section": "Accessing MultiIndexes",
    "text": "Accessing MultiIndexes\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\nThe difficulty is conceptual, not technical.\n\n\n\nUntil now we have understood the pandas index as a single column-like ‘thing’ in a data frame, but pandas also supports hierarchical and grouped indexes that allow us to interact with data in more complex ways… should we need it. Generally:\n\nMultiIndex == hierarchical index on columns\nDataFrameGroupBy == iterable pseudo-hierarchical index on rows\n\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nWe’ll be looking at Grouping Data in much more detail in next week, so the main thing to remember is that grouping is for rows, multi-indexing is about columns.\n\n\n\nDirect Access\nOf course, one way to get at the data is to use .iloc[...] since that refers to columns by position and ignores the complexity of the index. Try printing out the the first five rows of the first column using iloc:\n\nexcel_atlas.iloc[???]\n\nYou should get:\n0    E02000001\n1    E02000002\n2    E02000003\n3    E02000004\n4    E02000005\nName: (Unnamed: 0_level_0, Unnamed: 0_level_1, MSOA Code), dtype: object\n\n\nNamed Access\nBut to do it by name is a little trickier:\n\nexcel_atlas.columns.tolist()[:5]\n\nNotice how asking for the first five columns has given us a list of… what exactly?\n\nQuestion\nSo to get the same output by column name what do you need to copy from above:\n\nexcel_atlas.loc[0:5, ???]\n\nThe answer is really awkward, so we’re going to look for a better way…\n\n\n\nGrouped Access\nDespite this, one way that MultiIndexes can be useful is for accessing column-slices from a ‘wide’ dataframe. We can, for instance, select all of the Age Structure columns in one go and it will be simpler than what we did above.\n\nexcel_atlas.loc[0:5, ('Age Structure (2011 Census)')]\n\n\n\nUnderstanding Levels\nThis works because the MultiIndex tracks the columns using levels, with level 0 at the ‘top’ and level 2 (in our case) at the bottom. These are the unique values for the top level (‘row 0’):\n\nexcel_atlas.columns.levels[0]\n\nThese are the values for those levels across the actual columns in the data frame, notice the repeated ‘Age Structure (2011 Census)’:\n\nexcel_atlas.columns.get_level_values(0)[:10]\n\nAnd here are the values for the second level of the index (‘row 1’ in the Excel file):\n\nexcel_atlas.columns.get_level_values(1)[:10]\n\nBy extension, if we drop a level 0 index then all of the columns that it supports at levels 1 and 2 are also dropped: so when we drop Mid-year Estimate totals from level 0 then all 11 of the ‘Mid-year Estimate totals (2002…2012)’ columns are dropped in one go.\n\nexcel_atlas[['Mid-year Estimate totals']].head(3)\n\n\ntest = excel_atlas.drop(columns=['Mid-year Estimate totals'], axis=1, level=0)\n\nprint(f\"Excel source had {excel_atlas.shape[1]} columns.\")\nprint(f\"Test now has {test.shape[1]} columns.\")\n\n\n# Tidy up if the variable exists\nif 'test' in locals():\n    del(test)\n\n\n\nQuestions\n\nWhat data type is used for storing/accessing MultiIndexes?\nWhy is this is the appropriate data type?\nHow (conceptually) are the header rows in Excel are mapped on to levels in pandas?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#tidying-up",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#tidying-up",
    "title": "Practical 11: Dimensions in Data",
    "section": "Tidying Up",
    "text": "Tidying Up\n\n\n\n\n\n\nTipDifficulty level: Low\n\n\n\n\n\nAlthough there’s a lot of dealing with column names.\n\n\n\n\nDropping Named Levels\nThere’s a lot of data in the data frame that we don’t need for our Airbnb work, so let’s go a bit further with the dropping of column-groups using the MultiIndex.\n\nto_drop = ['Mid-year Estimate totals','Mid-year Estimates 2012, by age','Religion (2011)',\n           'Land Area','Lone Parents (2011 Census)','Central Heating (2011 Census)','Health (2011 Census)',\n           'Low Birth Weight Births (2007-2011)','Obesity','Incidence of Cancer','Life Expectancy',\n           'Road Casualties']\ntidy = excel_atlas.drop(to_drop, axis=1, level=0)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n\nShape of the MSOA Atlas data frame is now: 984 x 111\n\n\nThis should drop you down to 984 x 111. Notice below that the multi-level index has not changed but the multi-level values remaining have!\n\nprint(f\"There are {len(tidy.columns.levels[0].unique())} categories.\") # The categories\nprint(f\"But only {len(tidy.columns.get_level_values(0).unique())} values.\") # The actual values\n\n\n\nSelecting Columns using a List Comprehension\nNow we need to drop all of the percentages from the data set. These can be found at level 1, though they are specified in a number of different ways so you’ll need to come up with a way to find them in the level 1 values using a list comprehension…\nI’d suggest looking for: “(%)”, “%”, and “Percentages”. You may need to check both start and end of the string. You could also use a regular expression here instead of multiple tests. Either way works, but have a think about the tradeoffs between intelligibility, speed, and what you understand…\n\nQuestion\nSelection using multiple logical tests:\n\nto_drop = [x for x in tidy.columns.get_level_values(1) if (???)]\nprint(to_drop)\n\nSelection using a regular expression:\n\nprint([x for x in tidy.columns.get_level_values(1) if re.search(???, x)])\n\nEither way you should get a set of columns that ‘look’ like duplicates because we’re unpacking the top level of the hierarchical index: (only the first 10 are shown here):\n\n1: Percentages\n2: Percentages\n3: Percentages\n4: Percentages\n5: Percentages\n6: White (%)\n7: Mixed/multiple ethnic groups (%)\n8: Asian/Asian British (%)\n9: Black/African/Caribbean/Black British (%)\n10: Other ethnic group (%)\n\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nSee how regular expressions keep coming baaaaaaaaack? That said, you can also often make use of simple string functions like startswith and endswith for this problem.\n\n\n\n\n\nDrop by Level\nYou now need to drop these columns using the level keyword as part of your drop command. You have plenty of examples of how to drop values in place, but I’d suggest first getting the command correct (maybe duplicate the cell below and change the code so that the result is saved to a dataframe called test before overwriting tidy?) and then saving the change.\n\nQuestion\n\ntidy = tidy.drop(to_drop, axis=1, level=???)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n\nThe data frame should now be 984 x 76. This is a bit more manageable though still a lot of data columns. Depending on what you decide to do for your final project you might want to revisit some of the columns that we dropped above…\n\n\n\nFlattening the Index\nAlthough this ia big improvement, you’ll have trouble saving or linking this data to other inputs. The problem is that Level 2 of the multi-index is mainly composed of ‘Unnamed’ values and so we need to merge it with Level 1 to simplify our data frame, and then merge that with level 0…\n\ntidy.columns.values[:3]\n\nLet’s use code to sort this out!\n\nnew_cols = []\nfor c in tidy.columns.values:\n    \n    #print(f\"Column label: {c}\")\n    l1 = f\"{c[0]}\"\n    l2 = f\"{c[1]}\"\n    l3 = f\"{c[2]}\"\n    \n    # The new column label\n    clabel = ''\n    \n    # Assemble new label from the levels\n    if not l1.startswith(\"Unnamed\"):\n        l1 = l1.replace(\" (2011 Census)\",'').replace(\" (2011)\",'').replace(\"Household \",'').replace(\"House Prices\",'').replace(\"Car or van availability\",'Vehicles').replace(' (2011/12)','')\n        l1 = l1.replace('Age Structure','Age').replace(\"Ethnic Group\",'').replace('Dwelling type','').replace('Income Estimates','')\n        clabel += l1\n    if not l2.startswith(\"Unnamed\"):\n        l2 = l2.replace(\"Numbers\",'').replace(\" House Price (£)\",'').replace(\"Highest level of qualification: \",'').replace(\"Annual Household Income (£)\",'hh Income').replace('Whole house or bungalow: ','').replace(' qualifications','')\n        l2 = l2.replace('At least one person aged 16 and over in household has English as a main language',\"1+ English as a main language\").replace(\"No people in household have English as a main language\",\"None have English as main language\")\n        clabel += ('-' if clabel != '' else '') + l2\n    if not l3.startswith(\"Unnamed\"):\n        clabel += ('-' if clabel != '' else '') + l3\n    \n    # Replace other commonly-occuring verbiage that inflates column name width\n    clabel = clabel.replace('--','-').replace(\" household\",' hh').replace('Owned: ','')\n    \n    #clabel = clabel.replace(' (2011 Census)','').replace(' (2011)','').replace('Sales - 2011.1','Sales - 2012')\n    #clabel = clabel.replace('Numbers - ','').replace(' (£)','').replace('Car or van availability','Vehicles')\n    #clabel = clabel.replace('Household Income Estimates (2011/12) - ','').replace('Age Structure','Age')\n    \n    new_cols.append(clabel)\n\nThe new columns should be:\nMSOA Code, MSOA Name, Age-All Ages, Age-0-15, Age-16-29, Age-30-44, Age-45-64, Age-65+, Age-Working-age, Households-All Households, Composition-Couple hh with dependent children, Composition-Couple hh without dependent children, Composition-Lone parent hh, Composition-One person hh, Composition-Other hh Types, White, Mixed/multiple ethnic groups, Asian/Asian British, Black/African/Caribbean/Black British, Other ethnic group, BAME, Country of Birth-United Kingdom, Country of Birth-Not United Kingdom, Language-1+ English as a main language, Language-None have English as main language, Tenure-Owned outright, Tenure-Owned with a mortgage or loan, Tenure-Social rented, Tenure-Private rented, Household spaces with at least one usual resident, Household spaces with no usual residents, Detached, Semi-detached, Terraced (including end-terrace), Flat, maisonette or apartment, Population Density-Persons per hectare (2012), Median-2005, Median-2006, Median-2007, Median-2008, Median-2009, Median-2010, Median-2011, Median-2012, Median-2013 (p), Sales-2005, Sales-2006, Sales-2007, Sales-2008, Sales-2009, Sales-2010, Sales-2011, Sales-2011.1, Sales-2013(p), Qualifications-No, Qualifications-Level 1, Qualifications-Level 2, Qualifications-Apprenticeship, Qualifications-Level 3, Qualifications-Level 4 and above, Qualifications-Other, Qualifications-Schoolchildren and full-time students: Age 18 and over, Economic Activity-Economically active: Total, Economic Activity-Economically active: Unemployed, Economic Activity-Economically inactive: Total, Economic Activity-Unemployment Rate, Adults in Employment-No adults in employment in hh: With dependent children, Total Mean hh Income, Total Median hh Income, Vehicles-No cars or vans in hh, Vehicles-1 car or van in hh, Vehicles-2 cars or vans in hh, Vehicles-3 cars or vans in hh, Vehicles-4 or more cars or vans in hh, Vehicles-Sum of all cars or vans in the area, Vehicles-Cars per hh\n\n\n\n\n\n\nCautionStop\n\n\n\nMake sure you understand what is happening here before just moving on to the next thing. Try adding print() statements if it will help it to make sense. This sort of code comes up a lot in the real world.\n\n\n\ntidy.columns = new_cols  # &lt;- Blow away complex index, replace with simple\ntidy.head()\n\nYou might want to have a look at what the code below drops first before just running it… remember that you can pull apart any complex code into pieces:\n\ntidy['MSOA Code'].isna()\ntidy[tidy['MSOA Code'].isna()].index\n\n\ntidy.drop(index=tidy[tidy['MSOA Code'].isna()].index, inplace=True)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#add-innerouter-london-mapping",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#add-innerouter-london-mapping",
    "title": "Practical 11: Dimensions in Data",
    "section": "Add Inner/Outer London Mapping",
    "text": "Add Inner/Outer London Mapping\n\n\n\n\n\n\nWarningDifficulty: Moderate, since I’m not giving you many clues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nWe touched on lambda functions last week; it’s a ‘trivial’ function that we don’t even want to bother defining with def. We also used the lambda function in the context of apply so this is just another chance to remind yourself how this works. This is quite advanced Python, so don’t panic if you don’t get it right away and have to do some Googling…\n\n\nWe want to add the borough name and a ‘subregion’ name. We already have the borough name buried in a separate column, so step 1 is to extract that from the MSOA Name. Step 2 is to use the borough name as a lookup to the subregion name using a lambda function. The format for a lambda function is usually lambda x: &lt;code that does something with x and returns a value&gt;. Hint: you’ve got a dictionary and you know how to use it!\n\nAdd Boroughs\nWe first need to extract the borough names from one of the existing fields in the data frame… a regex that does replacement would be fastest and easiest: focus on what you don’t need from the MSOA Name string and replacing that using a regex…\n\nQuestion\n\ntidy['Borough'] = tidy['MSOA Name'].???\ntidy.Borough.unique()\n\nYou should get:\narray(['City of London', 'Barking and Dagenham', 'Barnet', 'Bexley',\n       'Brent', 'Bromley', 'Camden', 'Croydon', 'Ealing', 'Enfield',\n       'Greenwich', 'Hackney', 'Hammersmith and Fulham', 'Haringey',\n       'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Islington',\n       'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth',\n       'Lewisham', 'Merton', 'Newham', 'Redbridge',\n       'Richmond upon Thames', 'Southwark', 'Sutton', 'Tower Hamlets',\n       'Waltham Forest', 'Wandsworth', 'Westminster'], dtype=object)\n\n\n\nMap Boroughs to Subregions\nAnd now you need to understand how to apply the mapping ot the Borough field using a lambda function. It’s fairly straightforward once you know the syntax: just a dictionary lookup. But as usual, you might want to first create a new cell and experiment with the output from the apply function before using it to write the Subregion field of the data frame…\n\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden','City of London']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet']:\n    mapping[b]='Outer West and North West'\n\n\nQuestion\n\ntidy['Subregion'] = tidy.Borough.apply(???)\n\n\n\n\nAnd Save\nThere’s a little snipped of useful code to work out here: we need to check if the clean directory exists in the data directory; if we don’t then the tidy.to_parquet() call will fail.\n\nmsoas = Path('data/clean/MSOA_Atlas.parquet')\nif not msoas.parent.exists():\n    msoas.parent.mkdir()\ntidy.to_parquet(msoas.resolve())\nprint(\"Done.\")\n\nDone.\n\n\n\n\nQuestions\n\nWhat are the advantages to apply and lambda functions over looping and named functions?\nWhen might you choose a named function over a lambda function?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#merge-data-geography",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#merge-data-geography",
    "title": "Practical 11: Dimensions in Data",
    "section": "Merge Data & Geography",
    "text": "Merge Data & Geography\n\n\n\n\n\n\nTipDifficulty: Low, except for plotting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nWe’ll cover joins (of which a merge is just one type) in the final week’s lectures, but between what you’d done in GIS and what we have here there should be enough here for you to start being able to make sense of how they work so that you don’t have to wait until Week 10 to think about how this could help you with your Group Assessment.\n\n\nFirst, we need to download the MSOA source file, which is a zipped archive of a Shapefile:\n\n# Oh look, we can read a Shapefile without needing to unzip it!\nmsoas = gpd.read_file(\n    cache_data('https://github.com/jreades/fsds/blob/master/data/src/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip?raw=true', \n               msoas.parent), driver='ESRI Shapefile')\n\n+ data/clean/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip found locally!\n\n\n/opt/conda/lib/python3.12/site-packages/pyogrio/raw.py:198: RuntimeWarning:\n\ndriver ESRI Shapefile does not support open option DRIVER\n\n\n\n\nIdentifying Matching Columns\nLooking at the first few columns of each data frame, which one might allow us to link the two files together? You’ve done this in GIS. Remember: the column names don’t need to match for us to use them in a join, it’s the values that matter.\n\nprint(f\"Column names: {', '.join(tidy.columns.tolist()[:5])}\")\ntidy.iloc[:3,:5]\n\nColumn names: MSOA Code, MSOA Name, Age-All Ages, Age-0-15, Age-16-29\n\n\n\n\n\n\n\n\n\nMSOA Code\nMSOA Name\nAge-All Ages\nAge-0-15\nAge-16-29\n\n\n\n\n0\nE02000001\nCity of London 001\n7375.0\n620.0\n1665.0\n\n\n1\nE02000002\nBarking and Dagenham 001\n6775.0\n1751.0\n1277.0\n\n\n2\nE02000003\nBarking and Dagenham 002\n10045.0\n2247.0\n1959.0\n\n\n\n\n\n\n\n\n\nMerge\nOne more thing: if you’ve got more than one choice I’d always go with a code over a name because one is intended for matching and other is not…\n\nQuestion\n\ngdf = pd.merge(msoas, tidy, left_on=???, right_on=???, how='inner')\ngdf = gdf.drop(columns=['MSOA11CD','MSOA11NM','OBJECTID'])\n\nprint(f\"Final MSOA Atlas data frame has shape {gdf.shape[0]:,} x {gdf.shape[1]}\")\n\nYou should get Final data frame has shape 983 x 86.\n\n\n\nPlot Choropleth\nLet’s plot the median income in 2011 column using the plasma colour ramp… The rest is to show you how to customise a legend.\n\ncol = 'Median-2011'\nfig = gdf.plot(column=???, cmap='???', \n         scheme='FisherJenks', k=7, edgecolor='None', \n         legend=True, legend_kwds={'frameon':False, 'fontsize':8},\n         figsize=(8,7));\nplt.title(col.replace('-',' '));\n\n# Now to modify the legend: googling \"geopandas format legend\"\n# brings me to: https://stackoverflow.com/a/56591102/4041902\nleg = fig.get_legend()\nleg._loc = 3\n\nfor lbl in leg.get_texts():\n    label_text = lbl.get_text()\n    [low, hi] = label_text.split(', ')\n    new_text = f'£{float(low):,.0f} - £{float(hi):,.0f}'\n    lbl.set_text(new_text)\n\nplt.show();\n\n\n\nSave\n\ngdf.to_geoparquet(msoa.parent / 'MSOA_Atlas.geoparquet'))\n\n\n\nQuestions\n\nTry changing the colour scheme, classification scheme, and number of classes to see if you feel there’s a better opeion than the one shown above… Copy the cell (click on anywhere outside the code and then hit C to copy. Then click on this cell once, and hit V to paste."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#reload",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#reload",
    "title": "Practical 11: Dimensions in Data",
    "section": "Reload",
    "text": "Reload\n\n\n\n\n\n\nTip\n\n\n\nIn future 'runs' of this notebook you can now just pick up here and skip all of Task 1.\n\n\nOn subsequent runs of this notebook you might just want to start here!\n\n# Notice this handy code: we check if the data is already\n# in memory. And notice this is just a list comprehension\n# to see what is locally loaded.\nif 'gdf' not in locals():\n    gdf = gpd.read_parquet(msoa.parent / 'MSOA_Atlas.geoparquet')\nprint(gdf.shape)\n\n\ncategoricals = ['Borough','Subregion']\nfor c in categoricals:\n    gdf[c] = gdf[c].astype('category')"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#split",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#split",
    "title": "Practical 11: Dimensions in Data",
    "section": "Split",
    "text": "Split\n\n\n\n\n\n\nTipDifficulty: Low!\n\n\n\n\n\n\n\n\n\nFor our purposes this is a little bit overkill as you could also use pandas’ sample(frac=0.2) and the indexes, but it’s useful to see how this works. You use test/train split to get four data sets out: the training data gives you two (predictors + target as separate data sets) and the testing data gives you two as well (predictors + target as separate data sets). These are sized accoridng to the test_size specfied in the test_train_split parameters.\n\nfrom sklearn.model_selection import train_test_split \n\npdf = gdf['Median-2011'].copy() # pdf for Median *P*rice b/c we need *something*\n\n# df == *data* frame (a.k.a. predictors or independent variables)\n# pr == *predicted* value (a.k.a. dependent variable)\n# Notice we don't want the median price included in our training data\ndf_train, df_test, pr_train, pr_test = train_test_split(\n                gdf.drop(columns=['Median-2011']), pdf, test_size=0.2, random_state=44)\n\nBelow you should see that the data has been split roughly on the basis of the test_size parameter.\n\nprint(f\"Original data size: {gdf.shape[0]:,} x {gdf.shape[1]}\")\nprint(f\"  Training data size: {df_train.shape[0]:,} x {df_train.shape[1]} ({(df_train.shape[0]/gdf.shape[0])*100:.0f}%)\")\nprint(f\"  Testing data size:  {df_test.shape[0]:,} x {df_test.shape[1]} ({(df_test.shape[0]/gdf.shape[0])*100:.0f}%)\")\n\nAlso notice the indexes of each pair of data sets match:\n\nprint(\", \".join([str(x) for x in df_train.index[:10]]))\nprint(\", \".join([str(x) for x in pr_train.index[:10]]))"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#plot-testtrain-data",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#plot-testtrain-data",
    "title": "Practical 11: Dimensions in Data",
    "section": "Plot Test/Train Data",
    "text": "Plot Test/Train Data\n\n\n\n\n\n\nTipDifficulty: Low, but important!\n\n\n\n\n\n\n\n\n\n\nboros = gpd.read_file(os.path.join('data','geo','Boroughs.gpkg'))\n\n\nf,axes = plt.subplots(1,2, figsize=(12,5))\ndf_train.plot(ax=???)\ndf_test.plot(ax=???)\nboros.plot(ax=???, facecolor='none', edgecolor='r', linewidth=.5, alpha=0.4)\nboros.plot(ax=???, facecolor='none', edgecolor='r', linewidth=.5, alpha=0.4)\naxes[0].set_title('Training Data')\naxes[1].set_title('Testing Data');\naxes[0].set_ylim(150000,210000)\naxes[1].set_ylim(150000,210000)\naxes[0].set_xlim(500000,565000)\naxes[1].set_xlim(500000,565000)\naxes[1].set_yticks([]);\n\n\nQuestions\n\nWhy might it be useful to produce a map of a test/train split?\nWhy might it matter more if you were dealing with user locations or behaviours?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#select-columns",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#select-columns",
    "title": "Practical 11: Dimensions in Data",
    "section": "Select Columns",
    "text": "Select Columns\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nOne thing you’ll need to explain is why I keep writing df[cols+['Subregion'] and why I don’t just add it to the cols variable at the start? Don’t try to answer this now, get through the rest of Tasks 3 and 4 and see what you think.\n\ncols = ['Tenure-Owned outright', 'Tenure-Owned with a mortgage or loan',\n        'Tenure-Social rented', 'Tenure-Private rented']\n\nAnswer: one part of the answer is that it makes it easy to change the columns we select without having to remember to keep Subregion, but the more important reason is that it allows us to re-use this ‘definition’ of cols elsewhere throughout the rest of this practical without needing to remember to remove Subregion.\n\ntr_raw  = df_train[cols+['Subregion']].copy() # train raw\ntst_raw = df_test[cols+['Subregion']].copy()  # test raw"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#fit-to-data",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#fit-to-data",
    "title": "Practical 11: Dimensions in Data",
    "section": "Fit to Data",
    "text": "Fit to Data\n\n\n\n\n\n\nWarningDifficulty: Moderate if you want to understand what reshape is doing.\n\n\n\n\n\n\n\n\n\nFit the training data:\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Notice what this is doing! See if you can explain it clearly.\nscalers = [???.fit(???[x].values.reshape(-1,1)) for x in cols]"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#apply-transformations",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#apply-transformations",
    "title": "Practical 11: Dimensions in Data",
    "section": "Apply Transformations",
    "text": "Apply Transformations\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nTrain:\n\ntr_normed = tr_raw.copy()\n\nfor i, sc in enumerate(scalers):\n    # Ditto this -- can you explain what this code is doing\n    tr_normed[cols[i]] = sc.transform(df_???[cols[i]].values.reshape(-1,1))\n\nTest:\n\ntst_normed = tst_raw.copy()\n\nfor i, sc in enumerate(scalers):\n    tst_normed[cols[i]] = sc.transform(df_???[cols[i]].values.reshape(-1,1))\n\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: You don't _have_ to fully understand the next section, but if you are able to get your head around it then this will seriously help you to prepare for the use of more advanced techniques in modelling and programming.\n\n\nCheck out the properties of tst_normed below. If you’ve understood what the MinMaxScaler is doing then you should be able to spot something unexpected in the transformed test outputs. If you’ve really understood this, you’ll see why this result is problematic for models. Hint: one way to think of it is an issue of extrapolation.\n\nfor c in cols:\n    print(f\"  Minimum: {tst_normed[c].min():.4f}\")\n    ???"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions",
    "title": "Practical 11: Dimensions in Data",
    "section": "Plot Distributions",
    "text": "Plot Distributions\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\n\ntr_raw.columns     = [re.sub('(-|/)',\"\\n\",x) for x in tr_raw.columns.values]\ntst_raw.columns    = [re.sub('(-|/)',\"\\n\",x) for x in tst_raw.columns.values]\ntr_normed.columns  = [re.sub('(-|/)',\"\\n\",x) for x in tr_normed.columns.values]\ntst_normed.columns = [re.sub('(-|/)',\"\\n\",x) for x in tst_normed.columns.values]\n\n\nsns.pairplot(data=tr_raw, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);\n\n\nsns.pairplot(data=tr_normed, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);\n\n\nQuestions\n\nWhy do I keep writing df[cols+['Subregion']? Why I don’t just add Subregions to the cols variable at the start?\nWhat has changed between the two plots (of tr_raw and tr_normed)?\nWhat is the potential problem that the use of the transformer fitted on tr_normed to data from tst_normed might cause? Hint: this is why I asked you to investigate the data in the empty code cell above.\nCan you explain what this is doing: [MinMaxScaler().fit(df_train[x].values.reshape(-1,1)) for x in cols]?\nCan you explain what this is doing: sc.transform(df_test[cols[i]].values.reshape(-1,1))?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#z-score-standardisation",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#z-score-standardisation",
    "title": "Practical 11: Dimensions in Data",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nstsc = StandardScaler().fit(tr[col].values.reshape(-1,1))\n\ntr[f\"Z. {col}\"]  = stsc.transform(???)\ntst[f\"Z. {col}\"] = stsc.transform(???)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#inter-quartile-standardisation",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#inter-quartile-standardisation",
    "title": "Practical 11: Dimensions in Data",
    "section": "Inter-Quartile Standardisation",
    "text": "Inter-Quartile Standardisation\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nrs = ???(quantile_range=(25.0, 75.0)).fit(???)\n\ntr[f\"IQR. {col}\"] = rs.transform(???)\ntst[f\"IQR. {col}\"] = rs.transform(???)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions-1",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions-1",
    "title": "Practical 11: Dimensions in Data",
    "section": "Plot Distributions",
    "text": "Plot Distributions\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: The point of these next plots is simply to show that *linear* transformations (which are 'reversible') is about changing things like the magnitude/scale of our data but doesn't fundamentally change relationships *between* observations.\n\n\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nsns.jointplot(data=tr, x=f\"{col}\", y=f\"Z. {col}\", kind='kde'); # hex probably not the best choice\n\n\nsns.jointplot(data=tr, x=f\"{col}\", y=f\"IQR. {col}\", kind='kde'); # hex probably not the best choice\n\n\nsns.jointplot(data=tr, x=f\"Z. {col}\", y=f\"IQR. {col}\", kind='hex'); # hex probably not the best choice\n\nPerhaps a little more useful…\n\nax = sns.kdeplot(tr[f\"Z. {col}\"])\nsns.kdeplot(tr[f\"IQR. {col}\"], color='r', ax=ax)\nplt.legend(loc='upper right', labels=['Standard', 'Robust']) # title='Foo'\nax.ticklabel_format(useOffset=False, style='plain')\nax.set_xlabel(\"Standardised Value for No cars or vans in hh\")\n\n\nQuestions?\n\nCan you see the differences between these two rescalers?\nCan you explain why you might want to choose one over the other?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#the-normal-distribution",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#the-normal-distribution",
    "title": "Practical 11: Dimensions in Data",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nZ-scores are often associated with the normal distribution because their interpretation implicitly assumes a normal distribution. Or to put it another way… You can always calculate z-scores for your data (it’s just a formula applied to data points), but their intuitive meaning is lost if your data don’t have something like a normal distribution (or follow the 68-95-99.7 rule).\nBut… what if our data are non-normal? Well, Just because data are non-normal doesn’t mean z-scores can’t be calculated (we already did that above); we just have to be careful what we do with them… and sometimes we should just avoid them entirely.\n\nCreating a Normal Distribution\nBelow is a function to create that theoretical normal distribution. See if you can understand what’s going and add comments to the code to explain what each line does.\n\ndef normal_from_dist(series): \n    mu = ???          # mean of our data\n    sd = ???          # standard deviation of our data\n    n  = ???          # count how many observations are in our data\n    s = np.random.normal(???, ???, ???)   #use the parameters of the data just calculated to generate n random numbers, drawn from a normal distributions \n    return s                   #return this set of random numbers\n\nTo make it easier to understand what the function above is doing, let’s use it! We’ll use the function to plot both a distribution plot with both histogram and KDE for our data, and then add a second overplot distplot to the same fig showing the theoretical normal distribution (in red). We’ll do this in a loop for each of the three variables we want to examine.\n\n\nVisual Comparisons\nLooking at the output, which of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation most appropriate as measures of centrality and spread?\nAlso, how would you determine the meaning of some of the departures from the normal distribution?\n\nselection = [x for x in df_train.columns.values if x.startswith('Composition')]\n\nfor c in selection:\n    ax = sns.kdeplot(df_train[c])\n    sns.kdeplot(normal_from_dist(df_train[c]), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title='Foo'\n    ax.ticklabel_format(useOffset=False, style='plain')\n    if ax.get_xlim()[1] &gt; 999999:\n        plt.xticks(rotation=45)\n    plt.show()\n\n\n\nQuestions\n\nWhich, if any, of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation most appropriate as measures of centrality and spread?\nHow might you determine the significance of some of the departures from the normal distribution?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#logarithmic-transformations",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#logarithmic-transformations",
    "title": "Practical 11: Dimensions in Data",
    "section": "Logarithmic Transformations",
    "text": "Logarithmic Transformations\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nTo create a new series in the data frame containing the natural log of the original value it’s a similar process to what we’ve done before, but since pandas doesn’t provide a log-transform operator (i.e. you can’t call df['MedianIncome'].log() ) we need to use the numpy package since pandas data series are just numpy arrays with some fancy window dressing that makes them even more useful.\nLet’s perform the transform then compare to the un-transformed data. Comment the code below to ensure that you understand what it is doing.\n\nApply and Plot\n\ncols = ['Median-2012','Total Mean hh Income']\n\nfor m in cols:\n    s  = df_train[m] # s == series\n    ts = ???.???(s)   # ts == transformed series\n    \n    ax = sns.kdeplot(s)\n    sns.kdeplot(normal_from_dist(s), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title also an option\n    plt.title(\"Original Data\")\n    \n    ### USEFUL FORMATTING TRICKS ###\n    # This turns off scientific notation in the ticklabels\n    ax.ticklabel_format(useOffset=False, style='plain')\n    # Notice this snippet of code\n    ax.set_xlabel(ax.get_xlabel() + \" (Raw Distribution)\")\n    # Notice this little code snippet too\n    if ax.get_xlim()[1] &gt; 999999:\n        plt.xticks(rotation=45)\n    \n    plt.show()\n    \n    ax = sns.kdeplot(ts)\n    sns.kdeplot(normal_from_dist(ts), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n    ax.ticklabel_format(useOffset=False, style='plain')\n    ax.set_xlabel(ax.get_xlabel() + \" (Logged Distribution)\")\n    if ax.get_xlim()[1] &gt; 999999:\n        plt.xticks(rotation=45)\n    plt.title(\"Log-Transformed Data\")\n    plt.show()\n\nHopefully, you can see that the transformed data do indeed look ‘more normal’; the peak of the red and blue lines are closer together and the blue line at the lower extreme is also closer to the red line, but we can check this by seeing what has happened to the z-scores."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#power-transformations",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#power-transformations",
    "title": "Practical 11: Dimensions in Data",
    "section": "Power Transformations",
    "text": "Power Transformations\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\n\ncols = ['Median-2012','Total Mean hh Income']\npt = ???(method='yeo-johnson')\n\nfor m in cols:\n    s  = df_train[m] # s == series\n    ts = pt.fit_transform(s.values.reshape(-1,1))\n    print(f\"Using lambda (transform 'exponent') of {pt.lambdas_[0]:0.5f}\")\n    \n    ax = sns.kdeplot(ts.reshape(-1,))\n    \n    sns.kdeplot(normal_from_dist(???), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n    ax.ticklabel_format(useOffset=False, style='plain')\n    ax.set_xlabel(m + \" (Transformed Distribution)\")\n    if ax.get_xlim()[1] &gt; 999999: # &lt;-- What does this do?\n        plt.xticks(rotation=45)\n    plt.title(\"Power-Transformed Data\")\n    plt.show();"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#calculating-shares",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#calculating-shares",
    "title": "Practical 11: Dimensions in Data",
    "section": "Calculating Shares",
    "text": "Calculating Shares\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nSadly, there’s no transformer to work this out for you automatically, but let’s start by converting the raw population and household figures to shares so that our later dimensionality reduction steps aren’t impacted by the size of the MSOA.\n\ngdf[['Age-All Ages','Households-All Households']].head(5)\n\n\nSpecify Totals Columns\n\ntotal_pop = gdf['Age-All Ages']\ntotal_hh  = gdf['Households-All Households']\ntotal_vec = gdf['Vehicles-Sum of all cars or vans in the area']\n\n\n\nSpecify Columns for Pop or HH Normalisation\n\npop_cols  = ['Age-', 'Composition-', 'Qualifications-', 'Economic Activity-', 'White', 'Mixed/multiple',\n             'Asian/Asian British', 'Black/African', 'BAME', 'Other ethnic',\n             'Country of Birth-']\nhh_cols   = [???, ???, ???, 'Detached', 'Semi-detached', 'Terraced', 'Flat, ']\n\n\npopre = re.compile(r'^(?:' + \"|\".join(pop_cols) + r')')\nhhre  = re.compile(r'^(?:' + \"|\".join(???) + r')')\n\n\n\nApply to Columns\n\ntr_gdf = gdf.copy()\ntr_gdf['Mean hh size'] = tr_gdf['Age-All Ages']/tr_gdf['Households-All Households']\n\nfor c in gdf.columns:\n    print(c)\n    if popre.match(c):\n        print(\"  Normalising by total population.\")\n        tr_gdf[c] = gdf[c]/???\n    elif ???.match(???):\n        print(\"  Normalising by total households.\")\n        tr_gdf[c] = gdf[c]/???\n    elif c.startswith('Vehicles-') and not c.startswith('Vehicles-Cars per hh'):\n        print(\"  Normalising by total vehicles.\")\n        tr_gdf[c] = gdf[c]/???\n    else:\n        print(\"  Passing through.\")"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#removing-columns",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#removing-columns",
    "title": "Practical 11: Dimensions in Data",
    "section": "Removing Columns",
    "text": "Removing Columns\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nTo perform dimensionality we can only have numeric data. In theory, categorical data can be converted to numeric and retained, but there are two issues:\n\nNominal data has no innate order so we can’t convert &gt; 2 categories to numbers and have to convert them to One-Hot Encoded values.\nA binary (i.e. One-Hot Encoded) variable will account for a lot of variance in the data because it’s only two values they are 0 and 1!\n\nSo in practice, it’s probably a good idea to drop categorical data if you’re planning to use PCA.\n\nDrop Totals Columns\n\npcadf = tr_gdf.drop(columns=['Age-All Ages', 'Households-All Households',\n                             'Vehicles-Sum of all cars or vans in the area'])\npcadf = pcadf.set_index('MSOA Code')\n\n\n\nDrop Non-Numeric Columns\n\npcadf.select_dtypes(['category','object']).columns\n\n\npcadf.drop(columns=pcadf.select_dtypes(['category','object']).columns.to_list(), inplace=True)\npcadf.drop(columns=['BNG_E','BNG_N','geometry', 'LONG', 'LAT','Shape__Are', 'Shape__Len'], inplace=True)\n\n\npcadf.columns"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#rescale-reduce",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#rescale-reduce",
    "title": "Practical 11: Dimensions in Data",
    "section": "Rescale & Reduce",
    "text": "Rescale & Reduce\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nIn order to ensure that our results aren’t dominated by a single scale (e.g. House Prices!) we need to rescale all of our data. You could easily try different scalers as well as a different parameters to see what effect this has on your results.\n\nRobustly Rescale\nSet up the Robust Rescaler for inter-decile standardisation: 10th and 90th quantiles.\n\nrs = ???\n\nfor c in pcadf.columns.values:\n    pcadf[c] = rs.fit_transform(pcadf[c].values.reshape(-1, 1))\n\n\n\nPCA Reduce\n\nfrom sklearn.decomposition import PCA \n\npca = PCA(n_components=50, whiten=True) \n\npca.fit(pcadf)\n\nexplained_variance = pca.explained_variance_ratio_\nsingular_values = pca.singular_values_\n\n\n\nExamine Explained Variance\n\nx = np.arange(1,???)\nplt.plot(x, explained_variance)\nplt.ylabel('Share of Variance Explained')\nplt.show()\n\n\nfor i in range(0, 20):\n    print(f\"Component {i:&gt;2} accounts for {explained_variance[i]*100:&gt;2.2f}% of variance\")\n\nYou should get that Component 0 accounts for 31.35% of the variance and Component 19 accounts for 0.37%.\n###: How Many Components?\nThere are a number of ways that we could set a threshold for dimensionality reduction: - The most common is to look for the ‘knee’ in the Explained Variance plot above. That would put us at about 5 retained components. - Another is to just keep all components contributing more than 1% of the variance. That would put us at about 10 components. - You can also (I discovered) look to shuffle the data and repeatedly perform PCA to build confidence intervals. I have not implemented this (yet).\nIn order to do anything with these components we need to somehow reattach them to the MSOAs. So that entails taking the transformed results (X_train and X_test)\n\nkn = knee_locator.KneeLocator(x, explained_variance, \n                              curve='convex', direction='decreasing', \n                              interp_method='interp1d')\nprint(f\"Knee detected at: {kn.knee}\")\nkn.plot_knee()\n\n\nkeep_n_components = 7\n\n# If we weren't changing the number of components we\n# could re-use the pca object created above. \npca = PCA(n_components=keep_n_components, whiten=True)\n\nX_train = pca.fit_transform(???)\n\n# Notice that we get the _same_ values out,\n# so this is a *deterministic* process that\n# is fully replicable (allowing for algorithmic\n# and programming language differences).\nprint(f\"Total explained variance: {pca.explained_variance_ratio_.sum()*100:2.2f}%\")\nfor i in range(0, keep_n_components):\n    print(f\"  Component {i:&gt;2} accounts for {pca.explained_variance_ratio_[i]*100:&gt;5.2f}% of variance\")\n\n# Notice...\nprint(f\"X-train shape: {len(X_train)}\")\nprint(f\"PCA df shape: {pcadf.shape[0]}\")\n# So each observation has a row in X_train and there is \n# 1 column for each component. This defines the mapping\n# of the original data space into the reduced one\nprint(f\"Row 0 of X-train contains {len(X_train[0])} elements.\")"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#components-to-columns",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#components-to-columns",
    "title": "Practical 11: Dimensions in Data",
    "section": "Components to Columns",
    "text": "Components to Columns\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nYou could actually do this more quickly (but less clearly) using X_train.T to transpose the matrix!\n\nfor i in range(0,keep_n_components):\n    s = pd.Series(X_train[:,???], index=pcadf.???)\n    pcadf[f\"Component {i+1}\"] = s\n\n\npcadf.sample(3).iloc[:,-10:-4]"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#reattaching-geodata",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#reattaching-geodata",
    "title": "Practical 11: Dimensions in Data",
    "section": "(Re)Attaching GeoData",
    "text": "(Re)Attaching GeoData\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\n\nmsoas = gpd.read_file(os.path.join('data','geo','Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip'), driver='ESRI Shapefile')\nmsoas = msoas.set_index('MSOA11CD')\nprint(msoas.columns)\n\n\nmsoas.head(1)\n\n\npcadf.head(1)\n\n\ngpcadf = pd.merge(msoas.set_index(['MSOA11CD'], drop=True), pcadf, left_index=True, right_index=True, how='inner')\nprint(f\"Geo-PCA df has shape {gpcadf.shape[0]} x {gpcadf.shape[1]}\")\n\nYou should get PCA df has shape 983 x 89.\n\ngpcadf['Borough'] = gpcadf.MSOA11NM.apply(???)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#map-the-first-n-components",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#map-the-first-n-components",
    "title": "Practical 11: Dimensions in Data",
    "section": "Map the First n Components",
    "text": "Map the First n Components\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nHow would you automate this so that the loop creates one plot for each of the first 3 components? How do you interpret these?\n\nfor comp in [f\"Component {x}\" for x in range(1,3)]:\n    ax = gpcadf.plot(column=???, cmap='plasma', \n         scheme='FisherJenks', k=7, edgecolor='None', legend=True, figsize=(9,7));\n    boros.plot(ax=ax, edgecolor='w', facecolor='none', linewidth=1, alpha=0.7)\n    ax.set_title(f'PCA {comp}')\n\nYour first component map should look something like this:\n\n\n\nPCA Component 1"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#umap-on-raw-data",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#umap-on-raw-data",
    "title": "Practical 11: Dimensions in Data",
    "section": "UMAP on Raw Data",
    "text": "UMAP on Raw Data\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\n\nfrom umap import UMAP\n\n# You might want to experiment with all\n# 3 of these values -- it may make sense \n# to package a lot of this up into a function!\nkeep_dims=2\nrs=42\n\nu = UMAP(\n    n_neighbors=25,\n    min_dist=0.01,\n    n_components=keep_dims,\n    random_state=rs)\n\nX_embedded = u.fit_transform(???)\nprint(X_embedded.shape)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#write-to-data-frame",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#write-to-data-frame",
    "title": "Practical 11: Dimensions in Data",
    "section": "Write to Data Frame",
    "text": "Write to Data Frame\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nCan probably also be solved using X_embedded.T.\n\nfor ix in range(0,X_embedded.shape[1]):\n    print(ix)\n    s = pd.Series(X_embedded[:,???], index=pcadf.???)\n    gpcadf[f\"Dimension {ix+1}\"] = s"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#visualise",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#visualise",
    "title": "Practical 11: Dimensions in Data",
    "section": "Visualise!",
    "text": "Visualise!\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nrddf = gpcadf.copy() # Reduced Dimension Data Frame\n\n\nSimple Scatter\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nsns.scatterplot(x=rddf[???], y=rddf[???], hue=rddf['Borough'], legend=False, ax=ax)\n\n\n\nSeaborn Jointplot\nThat is suggestive of there being struccture in the data, but with 983 data points and 33 colours it’s hard to make sense of what the structure might imply. Let’s try this again using the Subregion instead and taking advantage of the Seaborn visualisation library’s jointplot (joint distribution plot):\n\nrddf['Subregion'] = rddf.Borough.apply(lambda x: mapping[x])\n\n\n# Sets some handy 'keywords' to tweak the Seaborn plot\nkwds = dict(s=7,alpha=0.95,edgecolor=\"none\")\n# Set the *hue order* so that all plots have some colouring by Subregion\nho = ['Inner East','Inner West','Outer West and North West','Outer South','Outer East and North East']\n\n\ng = sns.jointplot(data=rddf, x=???, y=???, height=8, \n                  hue=???, hue_order=ho, joint_kws=kwds)\ng.ax_joint.legend(loc='upper right', prop={'size': 8});\n\nYour jointplot should look like this:\n\n\n\nUMAP Jointplot\n\n\nWhat do you make of this?\nMaybe let’s give this one last go splitting the plot out by subregion so that we can see how these vary:\n\nfor r in rddf.Subregion.unique():\n    g = sns.jointplot(data=rddf[rddf.Subregion==r], x='Dimension 1', y='Dimension 2', \n                  hue='Borough', joint_kws=kwds)\n    g.ax_joint.legend(loc='upper right', prop={'size': 8});\n    g.ax_joint.set_ylim(0,15)\n    g.ax_joint.set_xlim(0,15)\n    plt.suptitle(r)\n\nWe can’t unfortunately do any clustering at this point to create groups from the data (that’s next week!) so for now note that there are several large-ish groups (in terms of membership) and few small ones picked up by t-SNE. Alos note that there is strong evidence of some incipient structure: Inner East and West largely clump together, while Outher East and Outer South also seem to group together, with Outer West being more distinctive. If you look back at the PCA Components (especially #1) you might be able to speculate about some reasons for this! Please note: this is only speculation at this time!\nNext week we’ll also add the listings data back in as part of the picture!"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#map-the-n-dimensions",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#map-the-n-dimensions",
    "title": "Practical 11: Dimensions in Data",
    "section": "Map the n Dimensions",
    "text": "Map the n Dimensions\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nfor comp in [f\"Dimension {x}\" for x in range(1,3)]:\n    f, ax = plt.subplots(1,1,figsize=(12,8))\n    rddf.plot(???);\n    boros.plot(edgecolor='w', facecolor='none', linewidth=1, alpha=0.7, ax=ax)\n    ax.set_title(f'UMAP {comp}')\n\nYour first dimension map should look something like this:\n\n\n\nUMAP Dimension 1"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#and-save-1",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#and-save-1",
    "title": "Practical 11: Dimensions in Data",
    "section": "And Save",
    "text": "And Save\n\nrddf.to_parquet(os.path.join('data','clean','Reduced_Dimension_Data.geoparquet'))\n\n\nQuestions\n\nHow would you compare/contrast PCA components with UMAP dimensions? Why do they not seem to show the same thing even though both seem to show something?\nWhat might you do with the output of either the PCA or UMAP processes?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#credits",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#credits",
    "title": "Practical 11: Dimensions in Data",
    "section": "Credits!",
    "text": "Credits!\n\nContributors:\nThe following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk).\n\n\nLicense\nThese teaching materials are licensed under a mix of The MIT License and the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license.\n\n\nPotential Dependencies:\nThis notebook may depend on the following libraries: pandas, geopandas, sklearn, matplotlib, seaborn"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html",
    "href": "practicals/Practical-09-Selecting_Data.html",
    "title": "Practical 9: Selecting Data",
    "section": "",
    "text": "WarningImportant\n\n\n\nThis practical focusses on data linkage! You will have seen quite a bit of this of these across the preceding three to four weeks, but they were picked up in an ad-hoc way, here we try to systematise things a bit. We’re also going to look at alternatives to loading the entire data set into memory in Python: using the DuckDB driver we can perform a lot of operations ‘outside’ of Python’s memory and processing limits while still benefiting from reproducibility (the commands are stored in the Python script) and access (we can bring the filtered, linked data into Python when we need to do so)."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#a-simple-query",
    "href": "practicals/Practical-09-Selecting_Data.html#a-simple-query",
    "title": "Practical 9: Selecting Data",
    "section": "A simple query",
    "text": "A simple query\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        pd.read_parquet(f'{pqt}').head(3)\n\nreps = 10\nsecs = load(reps)\nprint(f\"Average time per call: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.43687"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#selecting-columns",
    "href": "practicals/Practical-09-Selecting_Data.html#selecting-columns",
    "title": "Practical 9: Selecting Data",
    "section": "Selecting columns",
    "text": "Selecting columns\nTo load a columnar subset of the data we have two options:\n\nLoad all the data and then subset (which always happens with CSV files but is optional with other formats)\nLoad only the columns we care about (which is possible with parquet files)\n\nAnd in code these are implemented as:\n\nLoad then filter\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        pd.read_parquet(f'{pqt}')[['listing_url', 'price', 'number_of_reviews', 'property_type', 'host_name']].head(5)\n\nreps = 10\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.41828\nNot bad, not bad…\n\n\nFilter then load\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        pd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'property_type', 'host_name']).head(5)\n\nreps = 10\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.32897\nWhoa! Notice the difference in the time needed to complete the operation! We just accelerated this step by 0.79 times.!"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#adding-a-constraint",
    "href": "practicals/Practical-09-Selecting_Data.html#adding-a-constraint",
    "title": "Practical 9: Selecting Data",
    "section": "Adding a constraint",
    "text": "Adding a constraint\nSo that is performance for loading data, now let’s see what happens when we start subsetting the data by row as well as by column.\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        df = pd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'property_type', 'host_name'])\n        df[(df.price &lt; 250) & (df.number_of_reviews &gt; 0) & (df.property_type=='Entire home/apt')].head(5)\n\nreps = 10\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.33295\nFor improved legibility you can also write this as:\n\ndf = pd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'last_review', 'host_name'])\ndf[\n    (df.price &lt; 250) & \n    (df.number_of_reviews &gt; 0) & \n    (df.property_type=='Entire home/apt')\n].head(5)\n\nNotice here that we are using three conditions to filter the data as well as a column filter on loading to minimise the amount of data loaded into memory. Applying the filters simultaneously will also make it easy to see what you’ve done (you aren’t applying each one separately) and to adjust the overall cleaning process.\nThis filter is fairly straightforward and performance should have been only slightly slower than doing the column subset in the first place, but things get a bit more complicated when you want to aggregate the return…"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#aggregating-the-return",
    "href": "practicals/Practical-09-Selecting_Data.html#aggregating-the-return",
    "title": "Practical 9: Selecting Data",
    "section": "Aggregating the return",
    "text": "Aggregating the return\nThere is a lot to unpack here, and notice that it takes three steps to achieve our goal of selecting, grouping, aggregating, sorting, and printing out the ten most frequent combinations of room and property type, although we could reduce it to two if we really wanted to do so (at the cost of some legibility).\nAverage time per call of about: 0.32253\nI get an average time per call of about: 0.32207\nHopefully the first two steps are fairly clear, so let’s focus on the final one:\n\nGroup By\nThis is a reasonably intelligible step in which we group the data loaded by room and property:\n\ndfg = df.groupby(\n        by=['room_type','property_type'],\n        observed=True\n    )\ndfg\n\nThe order here matters: groupby(by=[&lt;A&gt;,&lt;B&gt;]) does not return the same result as groupby(by=[&lt;B&gt;,&lt;A&gt;]). Try it:\n\ndf.groupby(\n        by=['property_type','room_type'],\n        observed=True\n    )\n\nThe other thing to note here is the observed=True. This is a nice bit of additional functionality that, if you set it to False will return a number for all possible combinations, inserting a zero if that combintaion is not observed in the data. That’s useful if non-observations (i.e. there was no data for this group) is relevant to your data presentation or subsequent modelling/statistical tests1.\n\n\nAgg\nThe agg step aggregates the data specified in the functions:\n\ndfg.agg(\n        freq = (\"property_type\", \"count\"),\n        median_price = (\"price\", \"median\"),\n)\n\nPandas offers a lot of different ways to do this, but the above approach is perhaps the most flexible since we are telling Pandas to apply the count function to the property_type field and assign it to a column called freq, and to apply the median function to the price field and assign that to a column called median_price.\n\n\n‘Degroup’\nIn order to do anything further with the aggregated data you will almost always want to convert your GroupedDataFrame back to a regular DataFrame and that means resetting the index reset_index() – this is just one of those things to learn about grouped data in Pandas.\n\n\nSort\nFinally, to sort the data you need to sort_values, where by specifies the fields you want to sort on and ascending is a matching (optional) list that specifies the sort order for each sort column. If you just want to sort everything in ascending order then you don’t need to specify the ascending values, and if you wanted to sort everything in descending order then it’s just ascending=False."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#a-first-query",
    "href": "practicals/Practical-09-Selecting_Data.html#a-first-query",
    "title": "Practical 9: Selecting Data",
    "section": "A first query",
    "text": "A first query\nLet’s see a quick demonstration:\n\nquery = f'''\nSELECT *\nFROM read_parquet('{pqt}') \nLIMIT 3;\n'''\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        db.sql(query)\n\nreps = 25\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.00371\nThe timing code should already show a significant improvement over pandas-only code: on my laptop it is 47.10 times faster. Now let’s unpack what’s happening:\n\nWe import the duckdb library as db.\nWe set up a SQL query using a multi-line f-string\nWe use DuckDb to execute the query and return a pandas dataframe (df)\n\nWhat’s particularly elegant here (and quite different from trying to talk to a Postres or MySQL database) is that there’s no connect-execute-collect pattern; we just build the query and execute it!\n\n\n\n\n\n\nNoteI do declare…\n\n\n\nNow let’s take a look at the SQL query… SQL is what’s called a declarative language, meaning that it is about the logic we want the program to follow rather than the ‘flow’ of execution. Python supports some declarative elements but is more commonly seen as an imperative language supporting procedural or functional approaches. This is a long way of saying: SQL won’t look like Python even though we’re executing SQL from within Python.\n\n\nSo our query (with added line numbers for clarity) looked liked this:\nSELECT *\nFROM read_parquet('{pqt}') \nLIMIT 3\nLine-by-line this means:\n\nSelect all columns (* == everything&gt;`)\nFrom the parquet file (&lt;table location&gt;)\nLimit the return to 3 rows (LIMIT &lt;row count&gt;)\n\nLet’s look at some variations…"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#selecting-columns-1",
    "href": "practicals/Practical-09-Selecting_Data.html#selecting-columns-1",
    "title": "Practical 9: Selecting Data",
    "section": "Selecting columns",
    "text": "Selecting columns\n\nquery = f'''\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nLIMIT 5;\n'''\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        db.sql(query)\n\nreps = 25\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.00012\nEven though it looked like pandas performed massively better when we updated our query to filter then load, DuckDB is even faster! I get that DuckDB is 1084.16 faster than pandas.\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nLIMIT 5;\nIt should be fairly easy to see how the query has changed from last time, but line-by-line this means:\n\nSelect a set of columns from the table in the order specified (SELECT &lt;column 1&gt;, &lt;column 30&gt;, &lt;column 5&gt;...)\nFrom the parquet file (FROM &lt;table location&gt;)\nLimit the return to 5 rows (LIMIT &lt;row count&gt;)"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#adding-a-constraint-1",
    "href": "practicals/Practical-09-Selecting_Data.html#adding-a-constraint-1",
    "title": "Practical 9: Selecting Data",
    "section": "Adding a constraint",
    "text": "Adding a constraint\n\nquery = f'''\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nWHERE price &lt; 250 \nAND number_of_reviews &gt; 0\nAND property_type='Entire home/apt'\nLIMIT 5;\n'''\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        db.sql(query)\n\nreps = 25\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.00015\nPerformance should still be noteable improvement over pandas, though in my simple tests it’s ‘only’ about 30% faster. IF we wanted to be more rigorous about this we’d want to repeat the same operation many times (e.g. hundreds or thousands of time) to account for the fact that during any given query the computer might be doing other things that impact performance.\nAnyway, in this query we’ve added three constraints using a WHERE, which is asking DuckDB to find all of the rows where the following things are true:\n\nThe price must be less than ($)250/night\nThe number_of_reviews must be more than 0\nThe property_type must be Entire home/apt"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#aggregating-the-return-1",
    "href": "practicals/Practical-09-Selecting_Data.html#aggregating-the-return-1",
    "title": "Practical 9: Selecting Data",
    "section": "Aggregating the return",
    "text": "Aggregating the return\nThat’s hopefully enough to get you started on selecting data, but databases ‘excel’ at aggregating data in various ways. We aren’t going to get into things like windowing functions or stored procedures here, but even simple aggregates done in DuckDB can vastly improve on the performance of pandas.\n\n\n\n\n\n\nTip\n\n\n\nWhen you aggregate data you need to retrieve every column in the SELECT portion that you GROUP BY in the WHERE portion of the query. This will make sense when you see the examples below… and should also make sense based on the Pandas equivalent above!\n\n\n\nquery = f'''\nSELECT property_type, room_type, \n    COUNT(*) AS frequency, MEDIAN(price) AS median_price\nFROM read_parquet('{pqt}') \nWHERE price &lt; 1000 \nAND number_of_reviews &gt; 0\nGROUP BY room_type, property_type\nORDER BY frequency DESC, room_type, property_type\nLIMIT 5;\n'''\n\n@timing\ndef load(n:int):\n    for i in range(n):\n        db.sql(query)\n\nreps = 25\nsecs = load(reps)\nprint(f\"Average time per call of about: {secs/reps:.5f}\")\n\nI get an average time per call of about: 0.00016\nPerformance is comparable to pandas in my very basic test, but I find the operation more clearly expressed in SQL than pandas. There are quite a few changes to the query here so it’s worth reviewing them in more detail:\nSELECT property_type, room_type, \n    COUNT(*) AS freq, MEDIAN(price) AS median_price\nFROM read_parquet('{pqt}') \nWHERE price &lt; 1000 \nAND number_of_reviews &gt; 0\nGROUP BY room_type, property_type\nORDER BY frequency DESC, room_type, property_type\nLIMIT 10;\nKey things to note:\n\nWe have two new aggregate functions:\n\nCOUNT(*) returns a count of the number of rows in each group specified in the GROUP BY clause. We don’t need to specify a column here because * means ‘all rows’.\nMEDIAN(price) returns, unsurprisingly, the median value of the price column for each group specified in the GROUP BY clause.\nNote also the AS frequency which ‘renames’ the column returned by the query; it’s the same concept as the import x as y in Python.\n\nGROUP BY is where the aggregation happens, and here we’re asking DuckDB to take all of the rows selected (WHERE price &lt; 1000 AND number_of_reviews &gt; 0) and group them using the room_type and property_type fields.\nORDER BY orders the returned records by the columns we specify, and they can be either ASCending (the default) or DESCending (descending).\n\nWhat you should also be noting here is that:\n\nThis query returns very quickly compared to the pandas equivalent.\nWe have been able to express our selection, grouping, and organising criteria very succinctly.\n\nIn terms of both speed and intelligibility, there can be quite substantial advantages to moving some of your workflow into a database or a database-like format such as Parquet and then querying that from Python. Databases are designed for the kind of application that Pandas struggles with, and if you get to windowing functions and stored procedures you’ll see how there are situations where something is far easier to express in Python/Pandas than in SQL.\nSo the trick here is to recognise when you are facing a problem that: a) will benefit from being expressed/tackled in a different language; and b) won’t create undue overhead on your technology ‘stack’. In working with environmental and built environment data I was able to cut the processing time by 80% when I moved the bulk of the data linkage work from Pandas into Parquet+DuckDB. But, by the same token, what’s the point of using Postgres and managing a spatial database to perform a single step in a much longer workflow unless the performance considerations are so massive they outweigh any other issue."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#additional-data",
    "href": "practicals/Practical-09-Selecting_Data.html#additional-data",
    "title": "Practical 9: Selecting Data",
    "section": "Additional Data",
    "text": "Additional Data\nFor the rest of this notebook we need three data sets about MSOAs (which are statistical neighbourhoods that we’ll treat as a proxy for ‘real’ neighbourhoods). The first two are non-spatial:\n\nmsoa_names_url = 'https://houseofcommonslibrary.github.io/msoanames/MSOA-Names-1.20.csv'\nmsoa_popst_url = 'https://orca.casa.ucl.ac.uk/~jreades/data/sapemsoaquinaryagetablefinal.xlsx'\n\nmsoa_nms = pd.read_csv( cache_data(msoa_names_url, 'data') )\n\n# This one can't be cached because we need to access\n# an Excel sheet name and the cache_data function isn't\n# smart enough to handle this use case.\nmsoa_pq = Path('data/MSOA_population_estimates.parquet')\nif msoa_pq.exists():\n    msoa_df = pd.read_parquet(msoa_pq)\nelse:\n    msoa_df = pd.read_excel(msoa_popst_url, sheet_name=\"Mid-2022 MSOA 2021\", header=3)\n    msoa_df.to_parquet(msoa_pq)\n\nprint(f\"msoa_df  has {msoa_df.shape[0]:,} rows and {msoa_df.shape[1]:,} columns.\")\nprint(f\"msoa_nms has {msoa_nms.shape[0]:,} rows and {msoa_nms.shape[1]:,} columns.\")\n\n+ data/MSOA-Names-1.20.csv found locally!\nmsoa_df  has 7,264 rows and 43 columns.\nmsoa_nms has 7,201 rows and 6 columns.\n\n\nWe also need some geodata:\n\nmsoa_gpkg = gpd.read_file( cache_data(f'{host}/~jreades/data/MSOA-2011.gpkg', ddir) ).to_crs('epsg:27700')\n\n+ data/geo/MSOA-2011.gpkg found locally!\n\n\n\n\n\n\n\n\nTipThe preferred solution\n\n\n\nTo keep it simple: you should assume that non-spatial joins are always going to be faster than spatial ones, even in a performant spatial database. Asking if one number is less than another, or if a piece of text is found in another piece of text, is much simpler than asking if one object falls within the boundaries of another. Spatial databases are fast and very cool, but if you can express your problem non-spatially it will be faster to solve it that way too."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#joining-by-attribute",
    "href": "practicals/Practical-09-Selecting_Data.html#joining-by-attribute",
    "title": "Practical 9: Selecting Data",
    "section": "Joining by attribute",
    "text": "Joining by attribute\nSo in our case, to join the two MSOA data sets we’re going to need to match the MSOA codes which have (slightly) different names in the two datasets:\n\n%%time\n\nrs = pd.merge(msoa_df, msoa_nms[['msoa11cd','msoa11hclnm','Laname']], left_on='MSOA 2021 Code', right_on='msoa11cd', how='left')\nprint(f\"Result set has {rs.shape[0]:,} rows and {rs.shape[1]:,} columns.\")\nrs.head(3)\n\nResult set has 7,264 rows and 46 columns.\nCPU times: user 3 ms, sys: 267 μs, total: 3.27 ms\nWall time: 3.26 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM60 to 64\nM65 to 69\nM70 to 74\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11hclnm\nLaname\n\n\n\n\n0\nE06000001\nHartlepool\nE02002483\nHartlepool 001\n10323\n265\n296\n356\n302\n238\n...\n281\n254\n210\n180\n93\n82\n28\nE02002483\nClavering\nHartlepool\n\n\n1\nE06000001\nHartlepool\nE02002484\nHartlepool 002\n10460\n325\n349\n295\n340\n283\n...\n363\n276\n248\n175\n86\n49\n28\nE02002484\nHeadland & West View\nHartlepool\n\n\n2\nE06000001\nHartlepool\nE02002485\nHartlepool 003\n8040\n238\n287\n295\n262\n225\n...\n272\n198\n159\n143\n61\n31\n12\nE02002485\nJesmond\nHartlepool\n\n\n\n\n3 rows × 46 columns\n\n\n\nBut wait! There’s an issue lurking in the data!\n\nprint(f\"There are {rs.msoa11hclnm.isna().sum()} missing MSOA Names!\")\n\nThere are 184 missing MSOA Names!\n\n\nCan you work out why this has happened? There is a clue in the column names!\nThere’s no way to solve this problem except by changing the code to use this URL instead for the MSOA Names."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#constraining-the-join",
    "href": "practicals/Practical-09-Selecting_Data.html#constraining-the-join",
    "title": "Practical 9: Selecting Data",
    "section": "Constraining the Join",
    "text": "Constraining the Join\nWe can also try to constrain the result set to one LA thanks to data in the MSOA Names database:\n\n%%time \n\nla_nm = 'Waltham Forest'\nsdf   = msoa_nms[msoa_nms.Laname==la_nm][['msoa11cd','msoa11hclnm','Laname']].copy()\n\nrs = pd.merge(msoa_df, sdf, left_on='MSOA 2021 Code', right_on='msoa11cd', how='inner')\nprint(f\"Result set has {rs.shape[0]:,} rows and {rs.shape[1]:,} columns.\")\nrs.head(3)\n\nResult set has 28 rows and 46 columns.\nCPU times: user 1.01 ms, sys: 953 μs, total: 1.97 ms\nWall time: 1.93 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM60 to 64\nM65 to 69\nM70 to 74\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11hclnm\nLaname\n\n\n\n\n0\nE09000031\nWaltham Forest\nE02000895\nWaltham Forest 001\n8363\n208\n233\n250\n228\n215\n...\n242\n209\n153\n194\n137\n93\n45\nE02000895\nChingford Green West\nWaltham Forest\n\n\n1\nE09000031\nWaltham Forest\nE02000896\nWaltham Forest 002\n9322\n256\n278\n264\n230\n241\n...\n257\n218\n216\n190\n111\n111\n54\nE02000896\nChingford Green East\nWaltham Forest\n\n\n2\nE09000031\nWaltham Forest\nE02000897\nWaltham Forest 003\n8438\n233\n262\n276\n212\n209\n...\n205\n162\n136\n98\n104\n87\n24\nE02000897\nFriday Hill\nWaltham Forest\n\n\n\n\n3 rows × 46 columns\n\n\n\nWithout the how=inner, the result set would still have all of the rows but some of the columns would be nearly completely empty."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#constraining-the-join-1",
    "href": "practicals/Practical-09-Selecting_Data.html#constraining-the-join-1",
    "title": "Practical 9: Selecting Data",
    "section": "Constraining the Join",
    "text": "Constraining the Join\nNow, running the same query to get the Waltham Forest data can be done two ways:\n\n%%time\n\nboro = 'Waltham Forest'\nquery = f'''\nSELECT * \nFROM \n    read_parquet('data/MSOA_population_estimates.parquet') as n \nINNER JOIN \n    read_csv('{cache_data(msoa_names_url, 'data')}', header=true) as m\nON \n    n.\"MSOA 2021 Code\"=m.msoa11cd\nWHERE \n    m.Laname='{boro}';\n'''\n\ndb.sql(query).to_df().head(3)\n\n+ data/MSOA-Names-1.20.csv found locally!\nCPU times: user 29.7 ms, sys: 5.17 ms, total: 34.8 ms\nWall time: 32 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11nm\nmsoa11nmw\nmsoa11hclnm\nmsoa11hclnmw\nLaname\n\n\n\n\n0\nE09000031\nWaltham Forest\nE02000895\nWaltham Forest 001\n8363\n208\n233\n250\n228\n215\n...\n194\n137\n93\n45\nE02000895\nWaltham Forest 001\nWaltham Forest 001\nChingford Green West\nNone\nWaltham Forest\n\n\n1\nE09000031\nWaltham Forest\nE02000896\nWaltham Forest 002\n9322\n256\n278\n264\n230\n241\n...\n190\n111\n111\n54\nE02000896\nWaltham Forest 002\nWaltham Forest 002\nChingford Green East\nNone\nWaltham Forest\n\n\n2\nE09000031\nWaltham Forest\nE02000897\nWaltham Forest 003\n8438\n233\n262\n276\n212\n209\n...\n98\n104\n87\n24\nE02000897\nWaltham Forest 003\nWaltham Forest 003\nFriday Hill\nNone\nWaltham Forest\n\n\n\n\n3 rows × 49 columns\n\n\n\nEverything here is basically the same except for:\n\nWe changed the LEFT JOIN to an INNER JOIN – this should make sense to you if you’ve watched the lectures.\nWe added a WHERE m.Laname=&lt;borough name&gt; which restricts the match to only those rows where the Local Authority name is Waltham Forest.\n\nHowever, note that this query can also be written this way:\n\n%%time\n\nboro = 'Waltham Forest'\nquery = f'''\nSELECT * \nFROM \n    read_parquet('data/MSOA_population_estimates.parquet') as n, \n    read_csv('{cache_data(msoa_names_url, 'data')}', header=true) as m\nWHERE m.Laname='{boro}'\nAND n.\"MSOA 2021 Code\"=m.msoa11cd;\n'''\n\ndb.sql(query).to_df().head(3)\n\n+ data/MSOA-Names-1.20.csv found locally!\nCPU times: user 27.2 ms, sys: 2.65 ms, total: 29.8 ms\nWall time: 30.4 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11nm\nmsoa11nmw\nmsoa11hclnm\nmsoa11hclnmw\nLaname\n\n\n\n\n0\nE09000031\nWaltham Forest\nE02000895\nWaltham Forest 001\n8363\n208\n233\n250\n228\n215\n...\n194\n137\n93\n45\nE02000895\nWaltham Forest 001\nWaltham Forest 001\nChingford Green West\nNone\nWaltham Forest\n\n\n1\nE09000031\nWaltham Forest\nE02000896\nWaltham Forest 002\n9322\n256\n278\n264\n230\n241\n...\n190\n111\n111\n54\nE02000896\nWaltham Forest 002\nWaltham Forest 002\nChingford Green East\nNone\nWaltham Forest\n\n\n2\nE09000031\nWaltham Forest\nE02000897\nWaltham Forest 003\n8438\n233\n262\n276\n212\n209\n...\n98\n104\n87\n24\nE02000897\nWaltham Forest 003\nWaltham Forest 003\nFriday Hill\nNone\nWaltham Forest\n\n\n\n\n3 rows × 49 columns\n\n\n\nThe second way is a little easier to read, but it only allows you to do inner joins where attributes need to match in both tables for a row to be kept. This situation is such a common ‘use case’ that it makes sense to have this simpler syntax, but the previous code will work for inner, left, right, and outer joins."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#load-geodata",
    "href": "practicals/Practical-09-Selecting_Data.html#load-geodata",
    "title": "Practical 9: Selecting Data",
    "section": "Load Geodata",
    "text": "Load Geodata\nA lot of additional geo-data can be accessed from the GeoPortal. And see also my discussion on lookup tables. For demonstration purposes we’re going to work the same set of files ‘as usual’:\n\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\nmsoas = gpd.read_file( cache_data(f'{host}/~jreades/data/MSOA-2011.gpkg', ddir) ).to_crs('epsg:27700')\n\n+ data/geo/Water.gpkg found locally!\n+ data/geo/Boroughs.gpkg found locally!\n+ data/geo/Greenspace.gpkg found locally!\n+ data/geo/MSOA-2011.gpkg found locally!"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#select-london-msoas",
    "href": "practicals/Practical-09-Selecting_Data.html#select-london-msoas",
    "title": "Practical 9: Selecting Data",
    "section": "Select London MSOAs",
    "text": "Select London MSOAs\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nOne thing to remember here is that computers are exact. So if you say that the selection should only be of MSOAs within London then you actually need to think about whether a shared border qualifies as ‘within’. Watch the lectures again if you’re unsure, but that’s why here we take this slightly clunky approach of buffering the London boundary before doing the selection.\n\n\n\nUnion\nAs we don’t have a boundary file for London, we can generate use using the union operator (as we do here) or using the dissolve() approach. Consider the pros and cons of each approach in terms of performance, output format, and legibility.\nSo here’s approach 1, which is a method call returning a GeoDataFrame (which is why we can call plot):\n\nboros.dissolve().plot();\n\n\n\n\n\n\n\n\nAnd here’s approach 2, which is an attribute and returns a raw polygon (so no reason to call plot, but it’s come back without the rest of the data frame!):\n\nboros.union_all()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nNotice how we’re also demonstrating some additional ways of plotting ‘on the fly’ (without generating a data frame) as well as (below) showing you how to zoom in/out."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#selecting-london-msoas",
    "href": "practicals/Practical-09-Selecting_Data.html#selecting-london-msoas",
    "title": "Practical 9: Selecting Data",
    "section": "Selecting London MSOAs",
    "text": "Selecting London MSOAs\n\nldn = gpd.GeoDataFrame(gpd.GeoSeries(data=boros.union_all())).rename(columns={0:'geometry'}).set_geometry(\"geometry\")\nldn = ldn.set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .7))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.9, .3, .3), linewidth=0.75)\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n\n\n\n\n\n\n\n\n\nA (Bad) First Join\n\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='within', how='inner')\nax = ldn.plot(facecolor=(.5, .5, .9, .7))\nldn_msoas.plot(ax=ax, facecolor='none', edgecolor=(.9, .3, .3), linewidth=0.75)\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionWhat has gone wrong???\n\n\n\nBefore you move on to the solution, stop and actually think about why this hasn’t worked as you might (sensibly) have expected? This is another reason that you need to pay close attention to the differences between spatial and non-spatial joins.\n\n\n\n\nBuffer and Join\nIn order to ensure that we get all the MSOAs within London we need to buffer the boundary by some amount to ensure that within returns what we want. If cover were easier to use then that option might be preferable.\n\nQuestion\n\nldn['buffered'] = ldn.geometry.???(???)\nldn = ldn.set_geometry('buffered').set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n\nBy default we want do an inner join because we want to drop everything that doesn’t line up between the two data sets (i.e. don’t keep the thousands of other non-London MSOAs).\n\n\nQuestion\n\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='???', how='inner')\nldn_msoas.plot()\n\nYour result should be:\n\n\n\n\n\n\n\n\n\n\n\n\nImportant Note\nIf your plot above looks like the output from pandas and not geopandas then the list of columns and the documentation for set_geometry might help you to understand what is going wrong:\n\nprint(\", \".join(ldn_msoas.columns.to_list()))\n\nMSOA11CD, MSOA11NM, LAD11CD, LAD11NM, RGN11CD, RGN11NM, USUALRES, HHOLDRES, COMESTRES, POPDEN, HHOLDS, AVHHOLDSZ, geometry, index_right, geometry_right\n\n\nIt’s important to recognise that join and sjoin are not the same even though they may effectively perform the same function. An issue can arise if we join two geodata frames using the join function from pandas. The latter doesn’t know anything about spatial data and we can therefore ‘lose track’ of the geometry column. Worse, there are actually two geometry columns now, so we need to tell Geopandas which one to use!\nThe easiest way to do this is to simply rename the geometry we want and then set is as the active geometry. Here’s the code to use if you have a geometry_left column and aren’t able to show a map:\n\nldn_msoas = ldn_msoas.rename(columns={'geometry_left':'geometry'}).set_geometry('geometry')\nldn_msoas.drop(columns='geometry_right', inplace=True)\n\nWe also no longer really need to keep the full MSOA data set hanging about.\n\ntry:\n    del(msoas)\nexcept NameError:\n    print(\"msoas already deleted.\")\n\n\nQuestion\n\nCan you explain why the outputs of the dissolve and union_all look differnet? And use that as the basis for explaining why they are different?\n\n\nAnswer 1\n\n\nHow do you know that the units for the buffering operation are metres? 250 could be anything right?\n\n\nAnswer 2\n\n\nWhy do we need to buffer the London geometry before performing the within spatial join?\n\n\nAnswer 3"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#create-borough-and-regional-mappings",
    "href": "practicals/Practical-09-Selecting_Data.html#create-borough-and-regional-mappings",
    "title": "Practical 9: Selecting Data",
    "section": "Create Borough and Regional Mappings",
    "text": "Create Borough and Regional Mappings\nWe don’t actually make use of these in this session, but both operations could be relevant to your final reports:\n\nThe Borough-to-Subregion mapping could help you to group your data into larger sets so that your resulst become more reobust. it also connects us to long-run patterns of socio-economic development in London.\nThe MSOA Names data set (which you used above) gives you something that you could use to label one or more ‘neighbourhoods’ on a map with names that are relevant. So rather than talking about “As you can see, Sutton 003, is…”, you can write “The Wrythe area of Sutton is significantly different from the surrounding areas…”\n\nThey also usefully test your understanding of regular expressions and a few other aspects covered in previous weeks.\n\nReplace\nYou’ve done this before: notice that the MSOA Name contains the Borough name with a space and some digits at the end. Use a regex (in str.replace()) to extract the LA name from the MSOA name. See if you do this without having to find your previous answer!\n\nQuestion\n\nldn_msoas['Borough'] = ldn_msoas.MSOA11NM.str.replace(r'???','',regex=True)\n\n# Just check results look plausible; you should have:\n# - 33 boroughs\n# - A df shape of 983 x 13\nprint(ldn_msoas.Borough.unique())\nprint(f\"There are {len(ldn_msoas.Borough.unique())} boroughs.\")\nprint(f\"Overall shape of data frame is {' x '.join([str(x) for x in ldn_msoas.shape])}\")\n\n\n\n\nMap\nNow that we’ve got the borough names we can set up a mapping dict here so that we can apply it as part of the groupby operation below (you should have 33 keys when done):\n\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet','City of London']:\n    mapping[b]='Outer West and North West'\nprint(len(mapping.keys()))\n\n33\n\n\n\nQuestion\n\nldn_msoas['Subregion'] = ldn_msoas.Borough.map(???)\n\n\n\n\nAnd Save\n\nldn_msoas.to_parquet(ddir / 'London_MSOA_Names.geoparquet')"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#spatial-join",
    "href": "practicals/Practical-09-Selecting_Data.html#spatial-join",
    "title": "Practical 9: Selecting Data",
    "section": "Spatial Join",
    "text": "Spatial Join\nAssociate LA (Local Authority) names to the listings using a spatial join, but notice the how here:\n\nQuestion\n\ngdf_la = gpd.sjoin(listings, ???, predicate='???', how='left')\nprint(gdf_la.columns.to_list())\n\n\n\nTidy Up\n\ngdf_la.drop(columns=['index_right','HECTARES','NONLD_AREA','ONS_INNER'], inplace=True)\n\nYou’ll need to look closely to check that the value_counts output squares with your expectations. If you don’t get 33 then there’s an issue and you’ll need to run the code in Section 9.5.2:\n\nif len(gdf_la.NAME.unique()) == 33:\n    print(\"All good...\")\nelse:\n    print(\"Need to run the next section of code...\")\n    print(f\"Now there are... {len(gdf_la.NAME.unique())} boroughs?\")\n    gdf_la.NAME.value_counts(dropna=False)\n\nAll good...\n\n\n\n\nFind Problematic Listings\nIf you were told that you need to run the next section of code then see if you can work out what happened…\n\ntry:\n    print(gdf_la[gdf_la.NAME.isna()].sample(2)[['name', 'NAME']])\n    ax = gdf_la[gdf_la.NAME.isna()].plot(figsize=(9,6), markersize=5, alpha=0.5)\n    boros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\nexcept ValueError as e:\n   pass\n\nIn short: in some cases there may be records that fall outside of London because of Airbnb’s shuffling approach:\n\ngdf_la.drop(index=gdf_la[gdf_la.NAME.isna()].index, axis=1, inplace=True)\nprint(f\"Data frame is {gdf_la.shape[0]:,} x {gdf_la.shape[1]}\")\n\n\n\nCheck and Save\n\nax = gdf_la.plot(column='NAME', markersize=0.5, alpha=0.5, figsize=(9,7))\nboros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\n\nYou should get the following:\n\n\n\n\n\n\n\n\n\n\ngdf_la.to_parquet(ddir / 'Listings_with_LA.geoparquet')\n\n\nQuestion\n\nDo you understand the difference between how='inner' and how='left'?"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#create-la-data",
    "href": "practicals/Practical-09-Selecting_Data.html#create-la-data",
    "title": "Practical 9: Selecting Data",
    "section": "Create LA Data",
    "text": "Create LA Data\nNow that we’ve assigned every listing to a borough, we can derive aggregate values for different groups of zones.\n\nSelect LA\nSelect a LA that is relevant to you to explore further…\n\nLA = 'Waltham Forest'\n\n\n\nSpatial Join\nThe first thing we want to do is join MSOA identifiers to each listing. In both cases we want to constrain the data to only be for ‘our’ LA of interest since that will speed up the process substantially:\n\ngdf_msoa = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            ldn_msoas[ldn_msoas.Borough==LA][['MSOA11CD','MSOA11NM','USUALRES','HHOLDS','Subregion','geometry']], predicate='within')\ngdf_msoa.head(2)\n\n\n\n\n\n\n\n\nindex\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\n...\nreviews_per_month\ngeometry\nNAME\nGSS_CODE\nindex_right\nMSOA11CD\nMSOA11NM\nUSUALRES\nHHOLDS\nSubregion\n\n\n\n\n0\n4\n809481748064671744\nhttps://www.airbnb.com/rooms/809481748064671711\n2024-06-15\n4 min walk from the station 1 bedroom apartment\nThis indipendent apartment has a style. Well ...\n30949469\nElisa\n2015-04-10\nLondon, United Kingdom\n...\n2.68\nPOINT (536184.011 187333.002)\nWaltham Forest\nE09000031\n883\nE02000916\nWaltham Forest 022\n9485\n3506\nOuter East and North East\n\n\n1\n6\n738113134161127040\nhttps://www.airbnb.com/rooms/738113134161127067\n2024-06-14\nWarm contemporary B&B in stylish Victorian house.\nYou’ll love the stylish decor of this charming...\n483663228\nDonald\n2022-10-15\nNone\n...\n1.03\nPOINT (538477.558 191735.825)\nWaltham Forest\nE09000031\n868\nE02000901\nWaltham Forest 007\n9371\n3553\nOuter East and North East\n\n\n\n\n2 rows × 42 columns\n\n\n\n\n\nAggregate\nNow aggregate the data by MSOA, deriving median price and a count of the listings:\n\ngrdf_msoa = gdf_msoa.groupby('MSOA11NM').agg(\n    listing_count = ('price','count'),\n    median_price = ('price','median')\n).reset_index()\nprint(f\"Have {grdf_msoa.shape[0]:,} rows and {grdf_msoa.shape[1]:,} columns\")\ngrdf_msoa.head(2)\n\nHave 28 rows and 3 columns\n\n\n\n\n\n\n\n\n\nMSOA11NM\nlisting_count\nmedian_price\n\n\n\n\n0\nWaltham Forest 001\n11\n110.0\n\n\n1\nWaltham Forest 002\n11\n90.0\n\n\n\n\n\n\n\n\n\nJoin (Again)\nHere we see the difference between merge and join. You’ll notice that join operates by taking one data frame as the implicit ‘left’ table (the one which calls join) while the one that is passed to the join function is, implicitly, the ‘right’ table. Join operates only using indexes, so you’ll need to insert the code to specify the same index on both data frames, but this can be done on-the-fly as part of the joining operation:\n\nmsoa_gdf = grdf_msoa.set_index('MSOA11NM').join(\n                ldn_msoas[ldn_msoas.Borough==LA].set_index('MSOA11NM'), \n                rsuffix='_r').set_geometry('geometry')\nmsoa_gdf.head(3)\n\n\n\n\n\n\n\n\nlisting_count\nmedian_price\nMSOA11CD\nLAD11CD\nLAD11NM\nRGN11CD\nRGN11NM\nUSUALRES\nHHOLDRES\nCOMESTRES\nPOPDEN\nHHOLDS\nAVHHOLDSZ\ngeometry\nindex_right\ngeometry_right\nBorough\nSubregion\n\n\nMSOA11NM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWaltham Forest 001\n11\n110.0\nE02000895\nE09000031\nWaltham Forest\nE12000007\nLondon\n7979\n7962\n17\n36.4\n3271\n2.4\nMULTIPOLYGON (((537919.442 195742.428, 538051....\n0\nPOLYGON ((528150.2 159979.2, 528100.9 160037.3...\nWaltham Forest\nOuter East and North East\n\n\nWaltham Forest 002\n11\n90.0\nE02000896\nE09000031\nWaltham Forest\nE12000007\nLondon\n8814\n8719\n95\n31.3\n3758\n2.3\nMULTIPOLYGON (((539172.688 195540, 539696.813 ...\n0\nPOLYGON ((528150.2 159979.2, 528100.9 160037.3...\nWaltham Forest\nOuter East and North East\n\n\nWaltham Forest 003\n6\n96.5\nE02000897\nE09000031\nWaltham Forest\nE12000007\nLondon\n8077\n7991\n86\n42.9\n3345\n2.4\nMULTIPOLYGON (((538862.624 194017.438, 539001....\n0\nPOLYGON ((528150.2 159979.2, 528100.9 160037.3...\nWaltham Forest\nOuter East and North East\n\n\n\n\n\n\n\n\nmsoa_gdf.plot(column='median_price', legend=True, figsize=(8,8));\n\nYou should get something like:\n\n\n\n\n\n\n\n\n\n\n\nSave\nJust so that we can pick up here without having to re-run all the preceding cells.\n\nmsoa_gdf.to_parquet(ddir / f'{LA}-MSOA_data.geoparquet')\n\n\nQuestion\n\nDo you understand the differences between pd.merge and df.join? and gpd.sjoin?\n\n\n\n\n\nDo you understand why it may be necessary to set_geometry in some cases?"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#footnotes",
    "href": "practicals/Practical-09-Selecting_Data.html#footnotes",
    "title": "Practical 9: Selecting Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne could imagine, for instance, that you want to present the data in a table in which case you want the zero because there needs to be a value for each cell in the table.↩︎"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "",
    "text": "Part 2 of Practical 8 is optional and should only be attempted if Part 1 made sense to you.\nThe later parts are largely complete and ready to run; however, that doesn’t mean you should just skip over them and think you’ve grasped what’s happening and it will be easy to apply in your own analyses. I would not pay as much attention to LDA topic mining since I don’t think it’s results are that good, but I’ve included it here as it’s still commonly-used in the Digital Humanities and by Marketing folks. Word2Vec is much more powerful and forms the basis of the kinds of advances seen in ChatGPT and other LLMs."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#required-modules",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#required-modules",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Required Modules",
    "text": "Required Modules\n\n\n\n\n\n\nNote\n\n\n\nNotice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: sklearn and nltk.\n\n\nStandard libraries we’ve seen before.\n\nimport pandas as pd\nimport geopandas as gpd\nimport re\nimport matplotlib.pyplot as plt\n\n# New, but fairly straightforward (for simple things)\nfrom bs4 import BeautifulSoup\n\nVectorisers we will use from the ‘big beast’ of Python machine learning: Sci-Kit Learn.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# We don't use this but I point out where you *could*\nfrom sklearn.preprocessing import OneHotEncoder \n\nNLP-specific libraries that we will use for tokenisation, lemmatisation, and frequency analysis.\n\nimport nltk\n\ntry:\n    from nltk.corpus import wordnet as wn\n    from nltk.stem.wordnet import WordNetLemmatizer\n    from nltk.corpus import stopwords\n    stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n    tokenizer = ToktokTokenizer()\nexcept:\n    nltk.download(\"stopwords\")\n    nltk.download(\"punkt_tab\")\n    nltk.download(\"averaged_perceptron_tagger_eng\")\n    from nltk.corpus import stopwords\n    from nltk.corpus import wordnet as wn\n    from nltk.stem.wordnet import WordNetLemmatizer\n    stopwords.words('english')\n\nfrom nltk.corpus import stopwords\nstopword_list = set(stopwords.words('english'))\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom nltk import ngrams, FreqDist\n\nlemmatizer = WordNetLemmatizer()\ntokenizer = ToktokTokenizer()\n\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\nRemaining libraries that we’ll use for processing and display text data. Most of this relates to dealing with the various ways that text data cleaning is hard because of the myriad formats it comes in.\n\ntry: \n    from wordcloud import WordCloud\nexcept:\n    ! pip install wordcloud\nimport string\nimport unicodedata\nfrom wordcloud import WordCloud, STOPWORDS\n\nThis next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:\n\nfrom IPython.display import display_markdown\n\ndef as_markdown(head='', body='Some body text'):\n    if head != '':\n        display_markdown(f\"##### {head}\\n\\n&gt;{body}\\n\", raw=True)\n    else:\n        display_markdown(f\"&gt;{body}\\n\", raw=True)\n\nas_markdown('Result!', \"Here's my output...\")\n\nResult!\n\nHere’s my output…"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#loading-data",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#loading-data",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Loading Data",
    "text": "Loading Data\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nBecause I generally want each practical to stand on its own (unless I’m trying to make a point), I’ve not moved this to a separate Python file (e.g. utils.py, but in line with what we covered back in the lectures on Functions and Packages, this sort of thing is a good candidate for being split out to a separate file to simplify re-use.\n\n\nRemember this function from last week? We use it to save downloading files that we already have stored locally. But notice I’ve made some small changes… what do these do to help the user?\n\nfrom pathlib import Path\nfrom requests import get\nfrom functools import wraps\n\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(src:str, dst:str, min_size=100) -&gt; Path:\n        if src.find('?') == -1:\n            url = Path(src)\n        else:\n            url = Path(src[:src.find('?')])\n        fn  = url.name  # Extract the filename\n        dsn = Path(f\"{dst}/{fn}\") # Destination filename\n        if dsn.is_file() and dsn.stat().st_size &gt; min_size:\n            print(f\"+ {dsn} found locally!\")\n            return(dsn)\n        else:\n            print(f\"+ {dsn} not found, downloading!\")\n            return(f(src, dsn))\n    return wrapper\n\n@check_cache\ndef cache_data(src:Path, dst:Path) -&gt; str:\n    \"\"\"Downloads a remote file.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dst : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n      \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how\n    # to use the 'splat' operator! os.makedirs creates missing\n    # directories in a path automatically.\n    if not dst.parent.exists():\n        dst.parent.mkdir(parents=True, exist_ok=True)\n        \n    # Download and write the file\n    with dst.open(mode='wb') as file:\n        response = get(src)\n        file.write(response.content)\n        \n    print(' + Done downloading...')\n\n    return dst.resolve()\n\n\n\n\n\n\n\nTip\n\n\n\nFor very large non-geographic data sets, remember that you can use_cols (or columns depending on the file type) to specify a subset of columns to load.\n\n\nLoad the main data set:\n\n# Load the data sets created in the previous practical\nlux    = gpd.read_parquet(Path('data/clean/luxury.geoparquet'))\naff    = gpd.read_parquet(Path('data/clean/affordable.geoparquet'))\nbluesp = gpd.read_parquet(Path('data/clean/bluespace.geoparquet'))"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#removing-html",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#removing-html",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Removing HTML",
    "text": "Removing HTML\n\n\n\n\n\n\nWarningDifficulty level: Moderate\n\n\n\n\n\nYou’ll have seen that tehre are some things that are not text in the sample, most notably this stuff: &lt;br&gt; and &lt;br /&gt;. That is raw HTML, the markup language for web pages. To go any further we will need to strip this out. Depending on what you needed to extract from a web page you can select different elements by name, class, or id, and do a whole bunch of other things, but here we’re just going to crudely strip the tags out.\nYou’re also going to see a powerful new method: apply. Apply simply runs some function against every row or column in the data set depending on how it’s called. In this case, we use a lambda function (a function without a formal definition) to create a ‘Beautiful Soup’ HTML parser and then strip the tags from each row.\n\n\n\nHint: you need to need to get the text out of the each returned &lt;p&gt; and &lt;div&gt; element! I’d suggest also commenting this up since there is a lot going on on some of these lines of code!\n\nQuestion\n\ncleaned = sample.apply(lambda x: BeautifulSoup(??, 'html.parser').??(\" \"))\ncleaned\n\n\n\n10390    Stunning & Spacious Air Conditioned 3 bedroom 3BR Apartment on the Ground floor with its own terrace in a prime Location Chelsea . The Perfect Place for a Lovely and comfortable Stay in the heart of Chelsea, just few minutes walk to River Thames ...\n34253    his charming house boasts a prime location just a 5-minute stroll from iconic landmarks such as Big Ben and the London Eye, as well as being conveniently close to Waterloo station. You'll find bus stops and supermarkets nearby for added convenien...\n49617    Key features: Three double bedrooms Two bathrooms Close to Waterloo Excellent transport links Beautiful finish Wood flooring throughout Full description: An impressive 1115sqft (104sqm) contemporary three double bedroom two bathroom first floor c...\n56882    Private double ensuite bedroom in a shared home. 8 min walk to Surbiton station (20 min direct to Waterloo), 3 min walk to River Thames, 20 min walk to Kingston town centre, easy bus or train to Wimbledon. The home is shared with at least two peo...\n21289           Cosy two bedroom apartment on the ninth floor with an amazing river view, just a few minutes from the Pimlico station in the heart of the city (zone 1). BigBen, Buckingham Palace, Hyde Park and Chelsea are just 15/20 minutes walking distance.\nName: description, dtype: object"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#lower-case",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#lower-case",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Lower Case",
    "text": "Lower Case\n\n\n\n\n\n\nTipDifficulty Level: Low.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nlower = cleaned.??\nlower\n\nYou should get something like the following:\n\n\n10390    stunning & spacious air conditioned 3 bedroom 3br apartment on the ground floor with its own terrace in a prime location chelsea . the perfect place for a lovely and comfortable stay in the heart of chelsea, just few minutes walk to river thames ...\n34253    his charming house boasts a prime location just a 5-minute stroll from iconic landmarks such as big ben and the london eye, as well as being conveniently close to waterloo station. you'll find bus stops and supermarkets nearby for added convenien...\n49617    key features: three double bedrooms two bathrooms close to waterloo excellent transport links beautiful finish wood flooring throughout full description: an impressive 1115sqft (104sqm) contemporary three double bedroom two bathroom first floor c...\n56882    private double ensuite bedroom in a shared home. 8 min walk to surbiton station (20 min direct to waterloo), 3 min walk to river thames, 20 min walk to kingston town centre, easy bus or train to wimbledon. the home is shared with at least two peo...\n21289           cosy two bedroom apartment on the ninth floor with an amazing river view, just a few minutes from the pimlico station in the heart of the city (zone 1). bigben, buckingham palace, hyde park and chelsea are just 15/20 minutes walking distance.\nName: description, dtype: object"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#stripping-punctuation",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#stripping-punctuation",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Stripping ‘Punctuation’",
    "text": "Stripping ‘Punctuation’\n\n\n\n\n\n\nCautionDifficulty level: Hard\n\n\n\n\n\nThis is because you need to understand: 1) why we’re compiling the regular expression and how to use character classes; and 2) how the NLTK tokenizer differs in its approach to the regex.\n\n\n\n\nRegular Expression Approach\nWe want to clear out punctuation using a regex that takes advantage of the [...] (character class) syntax. The really tricky part is remembering how to specify the ‘punctuation’ when some of that punctuation has ‘special’ meanings in a regular expression context. For instance, . means ‘any character’, while [ and ] mean ‘character class’. So this is another escaping problem and it works the same way it did when we were dealing with the Terminal…\nHints: some other factors…\n\nYou will want to match more than one piece of punctuation at a time, so I’d suggest add a + to your pattern.\nYou will need to look into metacharacters for creating a kind of ‘any of the characters in this class’ bag of possible matches.\n\n\nQuestion\n\npat = re.compile(r'[???]+')\nsubbed = lower.apply(lambda x: pat.sub('',x))\n\nDepending on how thorough you are, you should get something like this:\n\n\n10390    stunning  spacious air conditioned 3 bedroom 3br apartment on the ground floor with its own terrace in a prime location chelsea  the perfect place for a lovely and comfortable stay in the heart of chelsea just few minutes walk to river thames and...\n34253    his charming house boasts a prime location just a 5minute stroll from iconic landmarks such as big ben and the london eye as well as being conveniently close to waterloo station youll find bus stops and supermarkets nearby for added convenience i...\n49617    key features: three double bedrooms two bathrooms close to waterloo excellent transport links beautiful finish wood flooring throughout full description: an impressive 1115sqft 104sqm contemporary three double bedroom two bathroom first floor con...\n56882    private double ensuite bedroom in a shared home 8 min walk to surbiton station 20 min direct to waterloo 3 min walk to river thames 20 min walk to kingston town centre easy bus or train to wimbledon the home is shared with at least two people at ...\n21289                   cosy two bedroom apartment on the ninth floor with an amazing river view just a few minutes from the pimlico station in the heart of the city zone 1 bigben buckingham palace hyde park and chelsea are just 1520 minutes walking distance\nName: description, dtype: object\n\n\n\n\n\nTokenizer\nThe other way to do this, which is probably easier, is to draw on the tokenizers already provided by NLTK. For our purposes word_tokenize is probably fine, but depending on your needs there are other options and you can also write your own.\n\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\ntokens = lower.apply(word_tokenize)\ntokens\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n10390    [stunning, &, spacious, air, conditioned, 3, bedroom, 3br, apartment, on, the, ground, floor, with, its, own, terrace, in, a, prime, location, chelsea, ., the, perfect, place, for, a, lovely, and, comfortable, stay, in, the, heart, of, chelsea, ,...\n34253    [his, charming, house, boasts, a, prime, location, just, a, 5-minute, stroll, from, iconic, landmarks, such, as, big, ben, and, the, london, eye, ,, as, well, as, being, conveniently, close, to, waterloo, station, ., you, 'll, find, bus, stops, a...\n49617    [key, features, :, three, double, bedrooms, two, bathrooms, close, to, waterloo, excellent, transport, links, beautiful, finish, wood, flooring, throughout, full, description, :, an, impressive, 1115sqft, (, 104sqm, ), contemporary, three, double...\n56882    [private, double, ensuite, bedroom, in, a, shared, home, ., 8, min, walk, to, surbiton, station, (, 20, min, direct, to, waterloo, ), ,, 3, min, walk, to, river, thames, ,, 20, min, walk, to, kingston, town, centre, ,, easy, bus, or, train, to, w...\n21289    [cosy, two, bedroom, apartment, on, the, ninth, floor, with, an, amazing, river, view, ,, just, a, few, minutes, from, the, pimlico, station, in, the, heart, of, the, city, (, zone, 1, ), ., bigben, ,, buckingham, palace, ,, hyde, park, and, chel...\nName: description, dtype: object\n\n\n\n\n\n\n\n\nWarningNot Equivalent!\n\n\n\nNotice that these aren’t giving you the same result!\nFor our purposes it is easier in the subsequent stages to work with the list output from tokenize rather than the string output from re.sub, but if you needed to convert tokenise back to a string you could do it this way:\ntokens = lower.apply(word_tokenize).apply(lambda x: ' '.join(x))\nThe other way around would be:\nlower.apply(lambda x: pat.sub('',x)).str.split(' ')"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#stopword-removal",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#stopword-removal",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Stopword Removal",
    "text": "Stopword Removal\n\n\n\n\n\n\nWarningDifficulty Level: Moderate\n\n\n\n\n\nYou need to remember how list comprehensions work to use the stopword_list.\n\n\n\n\nstopword_list = set(stopwords.words('english'))\nprint(stopword_list)\n\n{'and', 'was', \"we'll\", 'itself', 'to', \"shan't\", \"you'll\", 'nor', 'by', 'having', 'more', 'over', 'has', 'who', 'under', 'mustn', 'had', 'ma', 'whom', 'or', 'our', 'it', 'once', 'above', 'during', 'each', \"needn't\", 'such', 'her', 'm', 'these', \"i've\", 'that', \"they'll\", 've', 'its', \"we're\", 'y', 'him', 'd', 'out', \"we've\", 'where', 'will', 'doing', 'wasn', 'off', 'hadn', 'don', \"didn't\", 'while', 'because', \"they've\", 'what', 'is', 'should', 'below', \"haven't\", 'hers', 'won', 'which', 'yourself', 'himself', 'shouldn', 'my', \"hasn't\", 'down', 't', 'they', 'll', 'no', 'theirs', 'between', 's', \"they're\", \"should've\", \"we'd\", \"weren't\", 'this', 'as', 'themselves', 'doesn', \"you'd\", 'further', 'yours', 'you', 'wouldn', 'with', 'he', 'on', 'yourselves', 'i', \"he's\", 'other', 'be', 'most', 'few', \"mightn't\", 'now', 'couldn', 'needn', 'were', 'but', \"i'll\", 'his', \"that'll\", 'there', 'here', \"it'll\", \"he'll\", 'ourselves', 'the', \"i'm\", 'weren', \"don't\", 'isn', 'for', \"mustn't\", 'how', 'in', 'those', 'at', 'into', \"he'd\", 'me', 'do', \"it'd\", 'your', 'we', \"couldn't\", 'own', 'myself', 'again', 'just', \"aren't\", 'about', 'aren', 'too', \"wouldn't\", 'o', 'shan', 'are', 'if', 'very', 'a', 'their', 'am', \"she'd\", 'through', \"she'll\", 'ain', 'only', 'any', \"doesn't\", \"it's\", 'she', \"you've\", 'an', \"you're\", 'when', 'being', 'hasn', 'than', 'after', 'have', 'not', 'same', 'from', \"hadn't\", 'haven', 'against', \"i'd\", 'does', 'until', 'ours', 'before', \"won't\", 'been', \"she's\", 'of', 'didn', 'did', 'herself', \"shouldn't\", 'so', 'some', 'then', 'why', 'up', 're', \"they'd\", 'them', \"wasn't\", 'both', 'all', 'can', 'mightn', \"isn't\"}\n\n\n\nQuestion\n\npassthru = []\nfor t in tokens[2:4]:\n    passthru.append([x for x in t if ?? and len(x) &gt; 1])\n\nfor p in passthru:\n    as_markdown(\"Stopwords removed:\", p)"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#lemmatisation-vs-stemming",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#lemmatisation-vs-stemming",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Lemmatisation vs Stemming",
    "text": "Lemmatisation vs Stemming\n\n\n\n\n\n\nTipDifficulty level: Low.\n\n\n\n\n\n\n\n\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer \n\n\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('monkeys'))\nprint(lemmatizer.lemmatize('cities'))\nprint(lemmatizer.lemmatize('complexity'))\nprint(lemmatizer.lemmatize('Reades'))\n\nmonkey\ncity\ncomplexity\nReades\n\n\n\nstemmer = SnowballStemmer(language='english')\nprint(stemmer.stem('monkeys'))\nprint(stemmer.stem('cities'))\nprint(stemmer.stem('complexity'))\nprint(stemmer.stem('Reades'))\n\nmonkey\nciti\ncomplex\nread\n\n\n\nlemmatizer = WordNetLemmatizer()\nstemmer    = SnowballStemmer(language='english')\n\n# This would be better if we passed in a PoS (Part of Speech) tag as well,\n# but processing text for parts of speech is *expensive* and for the purposes\n# of this tutorial, not necessary.\nlemmas  = passthru.apply(lambda x: [lemmatizer.lemmatize(t) for t in x])\nstemmed = passthru.apply(lambda x: [stemmer.stem(t) for t in x])\n\nas_markdown(\"Lemmatised\", '\\n\\n&gt;'.join(lemmas.str.join(' ').to_list()))\nprint(\"\\n\\n\")\nas_markdown(\"Stemmed\", '\\n\\n&gt;'.join(stemmed.str.join(' ').to_list()))\n\nLemmatised\n\nstunning spacious air conditioned bedroom 3br apartment ground floor terrace prime location chelsea perfect place lovely comfortable stay heart chelsea minute walk river thames stamford bridge short journey bus underground south kensington earl court knightsbridge hyde park easy access london attraction perfect choice family group friend\n\n\ncharming house boast prime location 5-minute stroll iconic landmark big ben london eye well conveniently close waterloo station ’ll find bus stop supermarket nearby added convenience inside house comprises three cozy bedroom one luxurious queen-sized bed two two comfortable single bed kitchen equipped basic cooking one toilet sink well full bathroom\n\n\nkey feature three double bedroom two bathroom close waterloo excellent transport link beautiful finish wood flooring throughout full description impressive 1115sqft 104sqm contemporary three double bedroom two bathroom first floor conversion kennington road short walk waterloo lambeth north station cut old vic ’s bar restaurant\n\n\nprivate double ensuite bedroom shared home min walk surbiton station 20 min direct waterloo min walk river thames 20 min walk kingston town centre easy bus train wimbledon home shared least two people time sharing kitchen space room bathroom attached room house listed separately 1-2 guest time able see previous review listing one new\n\n\ncosy two bedroom apartment ninth floor amazing river view minute pimlico station heart city zone bigben buckingham palace hyde park chelsea 15/20 minute walking distance\n\n\n\n\n\n\n\n\nStemmed\n\nstun spacious air condit bedroom 3br apart ground floor terrac prime locat chelsea perfect place love comfort stay heart chelsea minut walk river thame stamford bridg short journey bus underground south kensington earl court knightsbridg hyde park easi access london attract perfect choic famili group friend\n\n\ncharm hous boast prime locat 5-minut stroll icon landmark big ben london eye well conveni close waterloo station ll find bus stop supermarket nearbi ad conveni insid hous compris three cozi bedroom one luxuri queen-siz bed two two comfort singl bed kitchen equip basic cook one toilet sink well full bathroom\n\n\nkey featur three doubl bedroom two bathroom close waterloo excel transport link beauti finish wood floor throughout full descript impress 1115sqft 104sqm contemporari three doubl bedroom two bathroom first floor convers kennington road short walk waterloo lambeth north station cut old vic ’s bar restaur\n\n\nprivat doubl ensuit bedroom share home min walk surbiton station 20 min direct waterloo min walk river thame 20 min walk kingston town centr easi bus train wimbledon home share least two peopl time share kitchen space room bathroom attach room hous list separ 1-2 guest time abl see previous review list one new\n\n\ncosi two bedroom apart ninth floor amaz river view minut pimlico station heart citi zone bigben buckingham palac hyde park chelsea 15/20 minut walk distanc"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#downloading-the-custom-module",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#downloading-the-custom-module",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Downloading the Custom Module",
    "text": "Downloading the Custom Module\nIn a Jupyter notebook, this code allows us to edit and reload the library dynamically:\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n\n\nTipDifficulty level: Low.\n\n\n\n\n\n\n\n\n\nThis custom module is not perfect, but it gets the job done… mostly and has some additional features that you could play around with for a final project (e.g. detect_entities and detect_acronyms).\n\ntry: \n    from textual import *\nexcept:\n    try:\n        from unidecode import unidecode\n    except:\n        ! pip install unidecode\n    import urllib.request\n    host  = 'https://orca.casa.ucl.ac.uk'\n    turl  = f'{host}/~jreades/__textual__.py'\n    tdirs = Path('textual')\n    tpath = Path(tdirs,'__init__.py')\n\n    if not tpath.exists():\n        tdirs.mkdirs(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(turl, tpath)\n    from textual import *\n\nAll NLTK libraries installed..."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#importing-the-custom-module",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#importing-the-custom-module",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Importing the Custom Module",
    "text": "Importing the Custom Module\n\n\n\n\n\n\nTipDifficulty Level: Low.\n\n\n\n\n\nBut only because you didn’t have to write the module! However, the questions could be hard…\n\n\n\n\nnormalised = sample.apply(normalise_document, remove_digits=True)\nas_markdown('Normalised', '\\n\\n&gt;'.join(normalised.to_list()))\n\nNormalised\n\nstunning spacious conditioned bedroom apartment ground floor terrace prime location chelsea . perfect place lovely comfortable stay heart chelsea minute walk river thames stamford bridge short journey underground south kensington earls court knightsbridge hyde park easy access london attractions . perfect choice families group friends\n\n\ncharm house boast prime location minute stroll iconic landmark london well conveniently close waterloo station . find stop supermarket nearby added convenience . inside house comprise three cozy bedrooms luxurious queensized comfortable single . kitchen equip basic cooking toilet sink well full bathroom .\n\n\nfeature three double bedroom bathroom close waterloo excellent transport link beautiful finish wood flooring throughout full description impressive sqft . contemporary three double bedroom bathroom first floor conversion kennington road short walk waterloo lambeth north station restaurant .\n\n\nprivate double ensuite bedroom share home . walk surbiton station . direct waterloo . walk river thames walk kingston town centre easy train wimbledon . home share least people time share kitchen space . room bathroom attach . room house list separately guest time . able previous review listing\n\n\ncosy bedroom apartment ninth floor amazing river view minute pimlico station heart city . zone . bigben buckingham palace hyde park chelsea minute walk distance .\n\n\n\nhelp(normalise_document)\n\n\n\n\n\n\nCautionStop!\n\n\n\nBeyond this point, we are moving into Natural Language Processing. If you are already struggling with regular expressions, I would recommend stopping here. You can come back to revisit the NLP components and creation of word clouds later."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#select-and-tokenise",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#select-and-tokenise",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Select and Tokenise",
    "text": "Select and Tokenise\n\n\n\n\n\n\nTipDifficulty level: Low, except for the double list-comprehension.\n\n\n\n\n\n\n\n\n\n\nSelect and Extract Corpus\nSee useful tutorial here. Although we shouldn’t have any empty descriptions, by the time we’ve finished normalising the textual data we may have created some empty values and we need to ensure that we don’t accidentally pass a NaN to the vectorisers and frequency distribution functions.\n\n# This makes it easy to go back and re-run\n# the analysis with other data subsets.\nsrcdf = bluesp \n\n\n\n\n\n\n\nTipCoding Tip\n\n\n\nNotice how you only need to change the value of the variable here to try any of the different selections we did above? This is a simple kind of parameterisation somewhere between a function and hard-coding everything.\n\n\n\ncorpus = srcdf.description_norm.fillna(' ').values\nprint(corpus[0:3])\n\n['well decorate sunny garden flat fulham park gardens close putney bridge tube .this comfortable double bedroom open plan flat sleep people . situated quiet residential street close shop restaurant kings road . close river thames bishops park tennis courts childre play area . broadband desk area . cleaning professional team adhere covid requirement sanitation .'\n 'enjoy warm cosy modern apartment . river view gorgeous view city . walk barking station  easy transportation link city within minsmins . drive distance canary wharf district line   overground easy link northern line jubilee line .'\n 'make memory unique familyfriendly place . near nice bridge minute away harrods london . cycling area  jungle park  river view playground available walkable distance . minute walk uxbridge underground station . minute walk shop mall . private parking bike . security service available . security deposit pound . refund hour checkout .']\n\n\n\n\nTokenise\nThere are different forms of tokenisation and different algorithms will expect differing inputs. Here are two:\n\nsentences = [nltk.sent_tokenize(text) for text in corpus]\nwords     = [[nltk.tokenize.word_tokenize(sentence) \n                  for sentence in nltk.sent_tokenize(text)] \n                  for text in corpus]\n\nNotice how this has turned every sentence into an array and each document into an array of arrays:\n\nprint(f\"Sentences 0: {sentences[0]}\")\nprint()\nprint(f\"Words 0: {words[0]}\")\n\nSentences 0: ['well decorate sunny garden flat fulham park gardens close putney bridge tube .this comfortable double bedroom open plan flat sleep people .', 'situated quiet residential street close shop restaurant kings road .', 'close river thames bishops park tennis courts childre play area .', 'broadband desk area .', 'cleaning professional team adhere covid requirement sanitation .']\n\nWords 0: [['well', 'decorate', 'sunny', 'garden', 'flat', 'fulham', 'park', 'gardens', 'close', 'putney', 'bridge', 'tube', '.this', 'comfortable', 'double', 'bedroom', 'open', 'plan', 'flat', 'sleep', 'people', '.'], ['situated', 'quiet', 'residential', 'street', 'close', 'shop', 'restaurant', 'kings', 'road', '.'], ['close', 'river', 'thames', 'bishops', 'park', 'tennis', 'courts', 'childre', 'play', 'area', '.'], ['broadband', 'desk', 'area', '.'], ['cleaning', 'professional', 'team', 'adhere', 'covid', 'requirement', 'sanitation', '.']]"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#fit-the-vectoriser",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#fit-the-vectoriser",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Fit the Vectoriser",
    "text": "Fit the Vectoriser\n\ncvectorizer = CountVectorizer(ngram_range=(1,3))\ncvectorizer.fit(corpus)\n\nCountVectorizer(ngram_range=(1, 3))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizer?Documentation for CountVectorizeriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ninput \n'content'\n\n\n\nencoding \n'utf-8'\n\n\n\ndecode_error \n'strict'\n\n\n\nstrip_accents \nNone\n\n\n\nlowercase \nTrue\n\n\n\npreprocessor \nNone\n\n\n\ntokenizer \nNone\n\n\n\nstop_words \nNone\n\n\n\ntoken_pattern \n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range \n(1, ...)\n\n\n\nanalyzer \n'word'\n\n\n\nmax_df \n1.0\n\n\n\nmin_df \n1\n\n\n\nmax_features \nNone\n\n\n\nvocabulary \nNone\n\n\n\nbinary \nFalse\n\n\n\ndtype \n&lt;class 'numpy.int64'&gt;"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#brief-demonstration",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#brief-demonstration",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Brief Demonstration",
    "text": "Brief Demonstration\nOnce we’ve fit the vectoriser, we can find the integer associated with any word in the corpus and can also ask how many times it occurs:\n\npd.options.display.max_colwidth=750\n\n# Note that the term can be a unigram, a bigram, or a trigram!\nterm = 'minute walk'\n# Find the vocabulary mapping for the term\nprint(f\"Vocabulary mapping for '{term}' is {cvectorizer.vocabulary_[term]}\")\n\n# How many times is it in the data\nprint(f\"Found {srcdf.description_norm.str.contains(term).sum():,} rows containing '{term}'\")\n\n# Print the descriptions containing the term\nfor i, x in enumerate(srcdf[srcdf.description_norm.str.contains(term)].description_norm.to_list()):\n    as_markdown(f\"{term}: {i}\",x)\n    if i &gt; 4: break\n\nVocabulary mapping for 'minute walk' is 10349\nFound 115 rows containing 'minute walk'\n\n\nminute walk: 0\n\nmake memory unique familyfriendly place . near nice bridge minute away harrods london . cycling area jungle park river view playground available walkable distance . minute walk uxbridge underground station . minute walk shop mall . private parking bike . security service available . security deposit pound . refund hour checkout .\n\n\n\nminute walk: 1\n\nnice cozy private room pimlico central london . prime location visit city close river thames minute walk pimlico station walk victoria station . stop local shop within minute walk . apartment basic clean proper picture apartment soon .\n\n\n\nminute walk: 2\n\nprivate quiet bright large double room ensuite bathroom . location stun next thames minute walk stamford brook tube . close riverside great restaurant place historic interest .\n\n\n\nminute walk: 3\n\nbeautiful modern onebedroom flat heart london . covent garden literally minute walk apartment . charring cross embankment covent garden underground station minute walk . plenty restaurant around minute walk thames river . london .\n\n\n\nminute walk: 4\n\npeople stay love place quirt private cosy serene culture easy transport link whole london . bedroom double brand suit . hammock relax front room . minute walk canal cafe restaurant near local crown meter walk flat\n\n\n\nminute walk: 5\n\nsuits couple front room maybe minute elephant castle tube bath shower minikitchen small garden small dining room . minute walk thames minute london bridge"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#transform-the-corpus",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#transform-the-corpus",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Transform the Corpus",
    "text": "Transform the Corpus\nYou can only tranform the entire corpus after the vectoriser has been fitted. There is an option to fit_transform in one go, but I wanted to demonstrate a few things here and some vectorisers are don’t support the one-shot fit-and-transform approach. Note the type of the transformed corpus:\n\ncvtcorpus = cvectorizer.transform(corpus)\ncvtcorpus # cvtcorpus for count-vectorised transformed corpus\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 38238 stored elements and shape (347, 19396)&gt;\n\n\n\nSingle Document\nHere is the first document from the corpus:\n\ndoc_df = pd.DataFrame(cvtcorpus[0].T.todense(), \n                      index=cvectorizer.get_feature_names_out(), columns=[\"Counts\"]\n                     ).sort_values('Counts', ascending=False)\ndoc_df.head(10)\n\n\n\n\n\n\n\n\nCounts\n\n\n\n\nclose\n3\n\n\npark\n2\n\n\nflat\n2\n\n\narea\n2\n\n\nbedroom open plan\n1\n\n\nroad\n1\n\n\npeople\n1\n\n\nsituated quiet\n1\n\n\nsituated quiet residential\n1\n\n\nputney bridge tube\n1\n\n\n\n\n\n\n\n\n\nTransformed Corpus\n\ncvdf = pd.DataFrame(data=cvtcorpus.toarray(),\n                        columns=cvectorizer.get_feature_names_out())\nprint(f\"Raw count vectorised data frame has {cvdf.shape[0]:,} rows and {cvdf.shape[1]:,} columns.\")\ncvdf.iloc[0:3,0:7]\n\nRaw count vectorised data frame has 347 rows and 19,396 columns.\n\n\n\n\n\n\n\n\n\naback\naback spacious\naback spacious apartment\nabba\nabba voyage\nabba voyage walk\nabba walk\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ncvdf.iloc[-3:,-7:]\n\n\n\n\n\n\n\n\nzone public transportation\nzone recently\nzone recently refurbish\nzone tube\nzone tube station\nzone victoria\nzone victoria line\n\n\n\n\n344\n0\n0\n0\n0\n0\n0\n0\n\n\n345\n0\n0\n0\n0\n0\n0\n0\n\n\n346\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nQuestions\n\nWhy is the single document view a list of terms and counts?\nWhy is the corpus view a table of terms (columns) and integers (rows)?\n\n\n\n\nFilter Low-Frequency Words\nThese are likely to be artefacts of text-cleaning or human input error. As well, if we’re trying to look across an entire corpus then we might not want to retain words that only appear in a couple of documents.\nLet’s start by getting the column sums:\n\nsums = cvdf.sum(axis=0)\nprint(f\"There are {len(sums):,} terms in the data set.\")\nsums.head()\n\nThere are 19,396 terms in the data set.\n\n\naback                       1\naback spacious              1\naback spacious apartment    1\nabba                        2\nabba voyage                 1\ndtype: int64\n\n\nRemove columns (i.e. terms) appearing in less than 1% of documents. You can do this by thinking about what the shape of the data frame means (rows and/or columns) and how you’d get 1% of that!\n\nQuestion\n\nfilter_terms = sums &gt;= cvdf.shape[0] * ???\n\nNow see how we can use this to strip out the columns corresponding to low-frequency terms:\n\nfcvdf = cvdf.drop(columns=cvdf.columns[~filter_terms].values)\nprint(f\"Filtered count vectorised data frame has {fcvdf.shape[0]:,} rows and {fcvdf.shape[1]:,} columns.\")\nfcvdf.iloc[0:3,0:7]\n\nFiltered count vectorised data frame has 347 rows and 1,686 columns.\n\n\n\n\n\n\n\n\n\naccess\naccess buses\naccess buses west\naccess central\naccess central london\naccess everything\naccess everything centrally\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nfcvdf.sum(axis=0)\n\naccess                     68\naccess buses                4\naccess buses west           4\naccess central              5\naccess central london       5\n                           ..\nzone comfortable cosy       7\nzone london                 6\nzone london warm            4\nzone recently               8\nzone recently refurbish     8\nLength: 1686, dtype: int64\n\n\nWe’re going to pick this up again in Task 7.\n\n\nQuestions\n\nCan you explain what doc_df contains?\nWhat does cvdf contain? Explain the rows and columns.\nWhat is the function of filter_terms?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#fit-and-transform",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#fit-and-transform",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Fit and Transform",
    "text": "Fit and Transform\n\ntfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), \n                               max_df=0.75, min_df=0.01) # &lt;-- these matter!\ntftcorpus    = tfvectorizer.fit_transform(corpus) # TF-transformed corpus"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#single-document-1",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#single-document-1",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Single Document",
    "text": "Single Document\n\ndoc_df = pd.DataFrame(tftcorpus[0].T.todense(), index=tfvectorizer.get_feature_names_out(), columns=[\"Weights\"])\ndoc_df.sort_values('Weights', ascending=False).head(10)\n\n\n\n\n\n\n\n\nWeights\n\n\n\n\nclose\n0.243962\n\n\narea\n0.207529\n\n\nquiet residential\n0.203155\n\n\nputney\n0.203155\n\n\nstreet close\n0.203155\n\n\nkings road\n0.196091\n\n\nresidential\n0.184943\n\n\ndecorate\n0.184943\n\n\ntennis\n0.184943\n\n\nsunny\n0.184943"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#transformed-corpus-1",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#transformed-corpus-1",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Transformed Corpus",
    "text": "Transformed Corpus\n\ntfidf = pd.DataFrame(data=tftcorpus.toarray(),\n                        columns=tfvectorizer.get_feature_names_out())\nprint(f\"TF/IDF data frame has {tfidf.shape[0]:,} rows and {tfidf.shape[1]:,} columns.\")\ntfidf.head()\n\nTF/IDF data frame has 347 rows and 1,660 columns.\n\n\n\n\n\n\n\n\n\naccess\naccess buses\naccess buses west\naccess central\naccess central london\naccess everything\naccess everything centrally\naccessible\naccommodate\naccommodation\n...\nyear\nzone\nzone central\nzone central city\nzone comfortable\nzone comfortable cosy\nzone london\nzone london warm\nzone recently\nzone recently refurbish\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 1660 columns\n\n\n\n\nQuestions\n\nWhat does the TF/IDF score represent?\nWhat is the role of max_df and min_df?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#for-counts",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#for-counts",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "For Counts",
    "text": "For Counts\n\n\n\n\n\n\nTipDifficulty level: Easy!\n\n\n\n\n\n\n\n\n\n\nfcvdf.sum().sort_values(ascending=False)\n\nwalk                   501\nlondon                 327\nminute                 282\nriver                  276\nstation                275\n                      ... \ncapital stay london      4\nsmall garden             4\ncapital stay             4\nsmart bedroom            4\nhighgate                 4\nLength: 1686, dtype: int64\n\n\n\nff = Path('RobotoMono-VariableFont_wght.ttf')\ndp = Path('/home/jovyan/fonts/')\ntp = Path(Path.home(),'Library','Fonts')\nif tp.exists():\n    fp = tp / ff\nelse:\n    fp = dp / ff\n\n\nf,ax = plt.subplots(1,1,figsize=(5,5))\nplt.gcf().set_dpi(300)\nCloud = WordCloud(\n    background_color=\"white\", \n    max_words=75,\n    font_path=fp\n).generate_from_frequencies(fcvdf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n#plt.savefig(\"Wordcloud 1.png\")"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#for-tfidf-weighting",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#for-tfidf-weighting",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "For TF/IDF Weighting",
    "text": "For TF/IDF Weighting\n\n\n\n\n\n\nTipDifficulty level: Low, but you’ll need to be patient!\n\n\n\n\n\n\n\n\n\n\ntfidf.sum().sort_values(ascending=False)\n\nwalk                       23.946637\nminute                     17.656652\nlondon                     17.114220\nriver                      15.518695\napartment                  14.149614\n                             ...    \nsmart bedroom               0.332017\nsmart bedroom battersea     0.332017\nsmart netflixs close        0.332017\nterrace overlooking         0.332017\nnetflixs close              0.332017\nLength: 1660, dtype: float64\n\n\n\nf,ax = plt.subplots(1,1,figsize=(5,5))\nplt.gcf().set_dpi(300)\nCloud = WordCloud(\n    background_color=\"white\", \n    max_words=100,\n    font_path=fp\n).generate_from_frequencies(tfidf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n#plt.savefig(\"Wordcloud 2.png\")\n\n\n\n\n\n\n\n\n\nQuestions\n\nWhat does the sum represent for the count vectoriser?\nWhat does the sum represent for the TF/IDF vectoriser?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#calculate-topics",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#calculate-topics",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Calculate Topics",
    "text": "Calculate Topics\n\nvectorizer.fit(corpus) \ntcorpus = vectorizer.transform(corpus) # tcorpus for transformed corpus\n\nLDA = LatentDirichletAllocation(n_components=3, random_state=42) # Might want to experiment with n_components too\nLDA.fit(tcorpus)\n\nLatentDirichletAllocation(n_components=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LatentDirichletAllocation?Documentation for LatentDirichletAllocationiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \n3\n\n\n\ndoc_topic_prior \nNone\n\n\n\ntopic_word_prior \nNone\n\n\n\nlearning_method \n'batch'\n\n\n\nlearning_decay \n0.7\n\n\n\nlearning_offset \n10.0\n\n\n\nmax_iter \n10\n\n\n\nbatch_size \n128\n\n\n\nevaluate_every \n-1\n\n\n\ntotal_samples \n1000000.0\n\n\n\nperp_tol \n0.1\n\n\n\nmean_change_tol \n0.001\n\n\n\nmax_doc_update_iter \n100\n\n\n\nn_jobs \nNone\n\n\n\nverbose \n0\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    \n\n\n\nfirst_topic = LDA.components_[0]\ntop_words = first_topic.argsort()[-25:]\n\nfor i in top_words:\n    print(vectorizer.get_feature_names_out()[i])\n\nwaterloo\nquiet\nfloor\ncity\npark\nkitchen\nhouse\neasy\nspace\naccess\ncanal\nminute\nstreet\nlocate\nriver view\nclose\napartment\nroom\nbedroom\nflat\nstation\nriver\nview\nlondon\nwalk\n\n\n\nfor i,topic in enumerate(LDA.components_):\n    as_markdown(f'Top 10 words for topic #{i}', ', '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]]))\n\nTop 10 words for topic #0\n\nwaterloo, quiet, floor, city, park, kitchen, house, easy, space, access, canal, minute, street, locate, river view, close, apartment, room, bedroom, flat, station, river, view, london, walk\n\n\n\nTop 10 words for topic #1\n\nshop, double, restaurant, location, great, bathroom, stay, minute walk, city, thames, waterloo, river view, kitchen, central, flat, close, bedroom, room, view, minute, station, river, london, apartment, walk\n\n\n\nTop 10 words for topic #2\n\nhouse, central london, locate, restaurant, distance, wharf, shop, location, river thames, private, bedroom, flat, room, central, park, close, view, apartment, thames, minute walk, river, station, london, minute, walk"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#maximum-likelihood-topic",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#maximum-likelihood-topic",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Maximum Likelihood Topic",
    "text": "Maximum Likelihood Topic\n\ntopic_values = LDA.transform(tcorpus)\ntopic_values.shape\n\n(347, 3)\n\n\n\npd.options.display.max_colwidth=20\nsrcdf['Topic'] = topic_values.argmax(axis=1)\nsrcdf.head()\n\n\n\n\n\n\n\n\ngeometry\nlisting_url\nname\ndescription\namenities\nprice\ndescription_norm\nTopic\n\n\n\n\n5\nPOINT (524555.52...\nhttps://www.airb...\nFulham Garden Flat\nWell decorated s...\n[\"Dining table\",...\n180.0\nwell decorate su...\n2\n\n\n426\nPOINT (543953.65...\nhttps://www.airb...\nCosy London Flat...\nEnjoy my warm an...\n[\"Bed linens\", \"...\n96.0\nenjoy warm cosy ...\n1\n\n\n516\nPOINT (506207.28...\nhttps://www.airb...\nFurnished Apartment\nMake some memori...\n[\"Kitchen\", \"Pai...\n250.0\nmake memory uniq...\n2\n\n\n725\nPOINT (536247.17...\nhttps://www.airb...\nComfortable room...\nMy place is clos...\n[\"Hangers\", \"Bre...\nNaN\nplace close mile...\n1\n\n\n1492\nPOINT (528407.92...\nhttps://www.airb...\nModern, bright, ...\nLuxury canal sid...\n[\"Dining table\",...\n239.0\nluxury canal sid...\n0\n\n\n\n\n\n\n\n\npd.options.display.max_colwidth=75\nsrcdf[srcdf.Topic==1].description_norm.head(10)\n\n426     enjoy warm cosy modern apartment . river view gorgeous view city . walk...\n725     place close mile tube station  brick lane shoreditch  queen mary univer...\n1957    beautifully refurbish fully equip flat pimlico . lovely zone neighborho...\n3536    private quiet bright  large double room ensuite bathroom . location stu...\n4149    spacious bed flat city center river thames view . walk waterloo  blackf...\n5414    fabulous riverside flat  balcony great view thames canary wharf . next ...\n5868    whole family stylish place . river viewings bedroom apartment hour conc...\n7024    elegant master bedroom king size ensuite bathroom charming  quiet road ...\n7581    bedroom manhattan apartment stylish chelsea creek development  locate l...\n7646    luxury retreat breathtaking canal views welcome enchant canal view flat...\nName: description_norm, dtype: object\n\n\n\nvectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', analyzer='word', max_df=0.7, min_df=0.05)\ntopic_corpus = vectorizer.fit_transform(srcdf[srcdf.Topic==1].description.values) # tcorpus for transformed corpus\n\n\ntopicdf = pd.DataFrame(data=topic_corpus.toarray(),\n                        columns=vectorizer.get_feature_names_out())\n\n\nf,ax = plt.subplots(1,1,figsize=(5,5))\nplt.gcf().set_dpi(300)\nCloud = WordCloud(\n            background_color=\"white\", \n            max_words=75).generate_from_frequencies(topicdf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n# plt.savefig('Wordcloud 3.png')"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#configure",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#configure",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Configure",
    "text": "Configure\nfrom gensim.models.word2vec import Word2Vec\ndims = 100\nprint(f\"You've chosen {dims} dimensions.\")\n\nwindow = 4\nprint(f\"You've chosen a window of size {window}.\")\n\nmin_v_freq  = 0.01 # Don't keep words appearing less than 1% frequency\nmin_v_count = math.ceil(min_v_freq * srcdf.shape[0])\nprint(f\"With a minimum frequency of {min_v_freq} and {srcdf.shape[0]:,} documents, minimum vocab frequency is {min_v_count:,}.\")"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#train",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#train",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Train",
    "text": "Train\n%%time \n\ncorpus      = srcdf.description_norm.fillna(' ').values\n#corpus_sent = [nltk.sent_tokenize(text) for text in corpus] # &lt;-- with more formal writing this would work well\ncorpus_sent = [d.replace('.',' ').split(' ') for d in corpus] # &lt;-- deals better with many short sentences though context may end up... weird\nmodel       = Word2Vec(sentences=corpus_sent, vector_size=dims, window=window, epochs=200, \n                 min_count=min_v_count, seed=42, workers=1)\n\n#model.save(f\"word2vec-d{dims}-w{window}.model\") # &lt;-- You can then Word2Vec.load(...) which is useful with large corpora"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#explore-similarities",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#explore-similarities",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Explore Similarities",
    "text": "Explore Similarities\nThis next bit of code only runs if you have calculated the frequencies above in the Frequencies and Ngrams section.\npd.set_option('display.max_colwidth',150)\n\ndf = fcounts[1] # &lt;-- copy out only the unigrams as we haven't trained anything else\n\nn     = 14 # number of words\ntopn  = 7  # number of most similar words\n\nselected_words = df[df['Ngram Size 1'] &gt; 5].reset_index().level_0.sample(n, random_state=42).tolist()\n\nwords = []\nv1    = []\nv2    = []\nv3    = []\nsims  = []\n\nfor w in selected_words:\n    try: \n        vector = model.wv[w]  # get numpy vector of a word\n        #print(f\"Word vector for '{w}' starts: {vector[:5]}...\")\n    \n        sim = model.wv.most_similar(w, topn=topn)\n        #print(f\"Similar words to '{w}' include: {sim}.\")\n    \n        words.append(w)\n        v1.append(vector[0])\n        v2.append(vector[1])\n        v3.append(vector[2])\n        sims.append(\", \".join([x[0] for x in sim]))\n    except KeyError:\n        print(f\"Didn't find {w} in model. Can happen with low-frequency terms.\")\n    \nvecs = pd.DataFrame({\n    'Term':words,\n    'V1':v1, \n    'V2':v2, \n    'V3':v3,\n    f'Top {topn} Similar':sims\n})\n\nvecs\n#print(model.wv.index_to_key) # &lt;-- the full vocabulary that has been trained"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#apply",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#apply",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Apply",
    "text": "Apply\nWe’re going to make use of this further next week…\n\nQuestions\n\nWhat happens when dims is very small (e.g. 25) or very large (e.g. 300)?\nWhat happens when window is very small (e.g. 2) or very large (e.g. 8)?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#applications",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#applications",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Applications",
    "text": "Applications\nThe above is still only the results for the one of the subsets of apartments alone. At this point, you would probably want to think about how your results might change if you changed any of the following:\n\nUsing one of the other data sets that we created, or even the entire data set!\nApplying the CountVectorizer or TfidfVectorizer before selecting out any of our ‘sub’ data sets.\nUsing the visualisation of information to improve our regex selection process.\nReducing, increasing, or constraining (i.e. ngrams=(2,2)) the size of the ngrams while bearing in mind the impact on processing time and interpretability.\nFiltering by type of listing or host instead of keywords found in the description (for instance, what if you applied TF/IDF to the entire data set and then selected out ‘Whole Properties’ before splitting into those advertised by hosts with only one listing vs. those with multiple listings?).\nLinking this back to the geography.\n\nOver the next few weeks we’ll also consider alternative means of visualising the data!"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#resources",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#resources",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Resources",
    "text": "Resources\nThere is a lot more information out there, including a whole book and your standard O’Reilly text.\nAnd some more useful links:\n\nPandas String Contains Method\nUsing Regular Expressions with Pandas\nSummarising Chapters from Frankenstein using TF/IDF"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html",
    "href": "practicals/Practical-06-Numeric_Data.html",
    "title": "Practical 6: Numeric Data",
    "section": "",
    "text": "This session is a tour-de-pandas; since this is Python’s equivalent of the tidyverse meets data.tables it is fundamental to the data science ecosystem and is probably one of the most-widely used libraries in the language as a whole. I get more than 286,000 questions tagged with pandas on StackOverflow.\nThis week we are also going to start looking more closely at the InsideAirbnb data which forms the core of the work that we do over the rest of the term. The focus of this notebook is simple numeric data: no mapping or text data… yet… and direct manipulation of data types, derivation of summary statistics, and simple plotting.\nWe hope that you will be able to draw on the past few practical sessions to develop a more intuitive understanding of how to interact with pandas since it supports both a ‘dictionary-of-lists’ style of interaction and a methods-based style of interaction with the ‘Data Frame’."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#reading-remote-data",
    "href": "practicals/Practical-06-Numeric_Data.html#reading-remote-data",
    "title": "Practical 6: Numeric Data",
    "section": "Reading Remote Data",
    "text": "Reading Remote Data\n\n\n\n\n\n\nTipDifficulty: Low (this time around).\n\n\n\n\n\n\n\n\n\nYou will need to do several things here to read the remote, compressed CSV file specified by url into a data frame called df. Setting low_memory=False ensures that pandas will try to load the entire data set before guessing the data type for each column! Obviously, with very large files this is probably a bad idea and it’s possible to force a particular column type while readng in the data as well.\n\nFor larger data sets there is Polars, DuckDB, and platforms like Dask (see, eg, this) and beyond.\n\n\nParameterising Files\nAnyway, here’s where to find the data:\n\n# Set download URL\nymd  = '20250615'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'\n\n\n\nReading CSV with Pandas\nNow, what goes into the read_csv function below?\n\nQuestion\n\n# your code here\ndf = pd.read_csv(??, compression='gzip', low_memory=False)\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nYou should get a data frame containing 75 columns and 93,486 rows of data."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#file-names",
    "href": "practicals/Practical-06-Numeric_Data.html#file-names",
    "title": "Practical 6: Numeric Data",
    "section": "File Names",
    "text": "File Names\nYou should always be looking for ways to avoid hard-coding values that might change over time, especially those linked to the date of the data file.\nIn this case you might try to work out how to make it easy to update the code to download the latest file. For instance, if the file looks like 20250910-listings.csv.gz then I might well specify the url as {date}-listings.csv.gz or {year}{month}{day}-listings.csv.gz and set up the variables that I need beforehand, possibly in a separate ‘configuration’ file that I read in at the start.\nUsing parameters makes it easier to write robust code that doesn’t have unwanted side-effects. Here’s a common one: you write code to download and process a file named 20251111-data.csv.gz. You save the outputs to clean-data.csv.gz.\n\nQuestion\nWhat happens when your boss asks you to process 20251211-data.csv.gz?"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#file-saving",
    "href": "practicals/Practical-06-Numeric_Data.html#file-saving",
    "title": "Practical 6: Numeric Data",
    "section": "File Saving",
    "text": "File Saving\n\n\n\n\n\n\nTipDifficulty: Low\n\n\n\n\n\n\n\n\n\nNow save the file somewhere local so that you don’t have to keep downloading 40MB of compressed data every time you want to start the practical. We’ll be using this data for the rest of term, so you might as well save yourself some time and bandwidth! We’ll talk more about data processing pipelines over the course of the term, but I’d suggest putting this data set into a data/raw folder because then you can have directories like data/clean and data/analytical as you move through the process of cleaning and prepping your data for analysis.\n\npath = Path(f'data/raw/{Path(url).name}') # What does this do?\nprint(f\"Writing to: {path}\")\n\nWriting to: data/raw/20250615-London-listings.csv.gz\n\n\n\nif not path.parent.exists(): # And what does *this* do?\n    print(f\"Creating {path.parent}\")\n    path.parent.mkdir(parents=True, exist_ok=True)\n\nif not path.exists():  \n    df.to_csv(path, index=False)\n    print(\"Done.\")"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#file-loading",
    "href": "practicals/Practical-06-Numeric_Data.html#file-loading",
    "title": "Practical 6: Numeric Data",
    "section": "File Loading",
    "text": "File Loading\nNow let’s write something that will allow us to more quickly write our code and validate the results in exploratory phase. For simplicity I’ve called this ‘testing’, but you could also think of it as ‘dev’ mode. What we want is to be able to easily swap between testing and operational contexts using a ‘switch’ (typically, a Boolean value) and limit the data load in testing mode.\nTo achieve this you could set pandas to:\n\nLoad only the first 10,000 rows using nrows if we are testing\nUse the columns specified in cols\nAllow pandas to load the entire data set before deciding on the column type by setting low_memory appropriately.\n\n\nRow Subsetting\nLet’s tackle the rows problem first:\n\nQuestion\n\ntesting = True\n\nif testing:\n    df = pd.read_csv(path, \n                low_memory=??, ??)\nelse:\n    df = pd.read_csv(path, \n                low_memory=??)\n\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nSo notice how this code deliberately works the same for either testing or operational execution – we just flip between the option by changing the testing variable from True to False!\nTo make this more robust and useful we could use this testing variable throughout our code if we wanted to change other behaviours based on development/deployment context. The state of the switch could then be set globally using an external configuration file (usually just called a ‘conf file’). The easiest way to do this is to have a conf.py which contains your global parameters and then every script or notebook file reads in the configuration and sets these variables.\nSomething like:\n\ntesting = False\n\nAnd:\n\nfrom conf import *\n\n\n\n\nColumn Subsetting\nNow let’s tackle the column problem… In order to avoid having to load lots of data that we aren’t sure we need yet, we can restrict the columns that we load. We got cols below by copying the output of (df.columns.to_list() and then removing the fields that we thought we weren’t interested in.\n\ncols = ['id', 'listing_url', 'last_scraped', 'name', \n    'description', 'host_id', 'host_name', 'host_since', \n    'host_location', 'host_about', 'host_is_superhost', \n    'host_listings_count', 'host_total_listings_count', \n    'host_verifications', 'latitude', 'longitude', \n    'property_type', 'room_type', 'accommodates', \n    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', \n    'amenities', 'price', 'minimum_nights', 'maximum_nights', \n    'availability_365', 'number_of_reviews', \n    'first_review', 'last_review', 'review_scores_rating', \n    'license', 'reviews_per_month']\nprint(f\"Cols contains {len(cols)} columns.\")\n\nCols contains 34 columns.\n\n\nSo let’s extend our previous answer\n\nQuestion\n\ntesting = True\n\nif testing:\n    df = pd.read_csv(opath, \n                low_memory=False, nrows=10000, ??)\nelse:\n    df = pd.read_csv(path, \n                low_memory=False, ??)\n\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#releasing-memory",
    "href": "practicals/Practical-06-Numeric_Data.html#releasing-memory",
    "title": "Practical 6: Numeric Data",
    "section": "Releasing Memory",
    "text": "Releasing Memory\nTwo risks when working with Jupyter notebooks are:\n\nYou have run code in an order other than the order shown in the notebook; or\nYou have made edits to code but not re-run the changed code.\n\nSo you’re still working from code that is no longer visible or where the a step (or five) has been missed/changed since you ran it! When that happens you can get very confusing issues because what you see doesn’t square with what the computer has executed. To resolve this without having to re-run the entire notebook (though that can also be a good choice!) you might want to ‘delete’ the current object and re-load or re-run the relevant data or code.\n\ndel(df)\n\nSo we use del(df) to ensure that we aren’t accidentally using the ‘old’ data frame. But another good reason to delete data you’re no longer using is to free up memory."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#numeric-indexing",
    "href": "practicals/Practical-06-Numeric_Data.html#numeric-indexing",
    "title": "Practical 6: Numeric Data",
    "section": "Numeric Indexing",
    "text": "Numeric Indexing\nYou can always access rows and columns by their integer location as if it is a 2D list and you want to access the 3rd to 5th columns and the 9th to 425th rows. So if you remember that iloc means you’re using the index location that will help you to work out what’s going on.\nSo can you read this:\n\ndf.iloc[\n    4552:4557, # &lt;- rows\n    14:19      # &lt;- columns\n]\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nproperty_type\nroom_type\naccommodates\n\n\n\n\n4552\n51.59074\n-0.07300\nEntire home\nEntire home/apt\n2.0\n\n\n4553\n51.53624\n-0.10740\nEntire rental unit\nEntire home/apt\n4.0\n\n\n4554\n51.55775\n0.00173\nEntire guest suite\nEntire home/apt\n2.0\n\n\n4555\n51.49577\n-0.18463\nEntire rental unit\nEntire home/apt\n3.0\n\n\n4556\n51.50581\n-0.03495\nPrivate room in home\nPrivate room\n1.0\n\n\n\n\n\n\n\nWe’re accessing the 4552nd through 4556th rows, and the 14th through 18th columns."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#label-indexing",
    "href": "practicals/Practical-06-Numeric_Data.html#label-indexing",
    "title": "Practical 6: Numeric Data",
    "section": "Label Indexing",
    "text": "Label Indexing\nWhere it can get confusing is when you see this:\n\ndf.loc[\n    4552:4557,\n    ['latitude','longitude','property_type','room_type','price']\n]\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nproperty_type\nroom_type\nprice\n\n\n\n\n4552\n51.59074\n-0.07300\nEntire home\nEntire home/apt\nNaN\n\n\n4553\n51.53624\n-0.10740\nEntire rental unit\nEntire home/apt\nNaN\n\n\n4554\n51.55775\n0.00173\nEntire guest suite\nEntire home/apt\nNaN\n\n\n4555\n51.49577\n-0.18463\nEntire rental unit\nEntire home/apt\nNaN\n\n\n4556\n51.50581\n-0.03495\nPrivate room in home\nPrivate room\nNaN\n\n\n4557\n51.53649\n-0.04638\nEntire rental unit\nEntire home/apt\nNaN\n\n\n\n\n\n\n\nThis code seems similar, but why does this use loc and not iloc, and why doesn’t it actually return the same data??? In this case, the index (the numbers down the left-hand side in bold) is numeric, so we can treat it as either a label (which allows us to use df.loc) or a list-type index (which allows us to use df.iloc). So with loc the value 4557 is treated as a ‘key’ so it’s retrieved, whereas with .iloc it’s treated like a list index accessed with range and so isn’t returned. It’s a bit weird, but hopefully makes some sense now."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#non-numeric-indexes",
    "href": "practicals/Practical-06-Numeric_Data.html#non-numeric-indexes",
    "title": "Practical 6: Numeric Data",
    "section": "Non-numeric Indexes",
    "text": "Non-numeric Indexes\nNotice the change in indexing because ‘listing_url’ is no longer a column, it’s the index now!\n\ndf.set_index('listing_url').iloc[0:3,13:18] \n\n\n\n\n\n\n\n\nlatitude\nlongitude\nproperty_type\nroom_type\naccommodates\n\n\nlisting_url\n\n\n\n\n\n\n\n\n\nhttps://www.airbnb.com/rooms/1126718007114818431\n51.535030\n-0.394000\nPrivate room in home\nPrivate room\n2.0\n\n\nhttps://www.airbnb.com/rooms/702589406864297985\n51.554232\n-0.037135\nPrivate room in casa particular\nPrivate room\n2.0\n\n\nhttps://www.airbnb.com/rooms/1122535727514526769\n51.412310\n0.026380\nEntire rental unit\nEntire home/apt\n4.0\n\n\n\n\n\n\n\nNotice how this works differently if we specify a non-numeric index:\n\ndf.set_index('listing_url').loc[\n    :, # &lt;- Special syntax that means 'all rows' (or all columns)\n    ['latitude','longitude','property_type','room_type','accommodates']\n].head(3)\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nproperty_type\nroom_type\naccommodates\n\n\nlisting_url\n\n\n\n\n\n\n\n\n\nhttps://www.airbnb.com/rooms/1126718007114818431\n51.535030\n-0.394000\nPrivate room in home\nPrivate room\n2.0\n\n\nhttps://www.airbnb.com/rooms/702589406864297985\n51.554232\n-0.037135\nPrivate room in casa particular\nPrivate room\n2.0\n\n\nhttps://www.airbnb.com/rooms/1122535727514526769\n51.412310\n0.026380\nEntire rental unit\nEntire home/apt\n4.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt’s vital that you understand how this code works. By which I mean why it does something at all, not exactly how to use loc and iloc (though that is also useful).\ndf.set_index(...) changes the index from the default row number to another field in the data frame. This operation returns a new data frame with listing_url as its index. Because set index returned a data frame, we can simply add another method call (iloc or loc) on to the end of that line and it returns a new data frame in turn!\nThe fact that each operation returns a new data frame (or data series) is why you can even do this:\n\n    df.set_index('listing_url').iloc[0:3].latitude.mean()\n\nnp.float64(51.500523858482154)"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-rows",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-rows",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting Rows",
    "text": "Selecting Rows\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nYou will want to refer to the Randomness lecture to understand how we can select the same random sample each time and to the session on Logic lecture to cover NaNs and NAs.\n\n\n\n\n\n\n\n\nTipDifficulty: Low\n\n\n\n\n\n\n\n\n\nI often like to start my EDA by simply printing out randomly-selected rows to get a feel for what’s in the data. Does what I see square with what I read in the documentation? What does the name look like? What do I see in last_scraped and is it a sensible? What’s the id field for?\n\ndf.sample(3)\n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_about\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nlicense\nreviews_per_month\n\n\n\n\n51587\n5.658688e+17\nhttps://www.airbnb.com/rooms/565868803547613397\n2024-06-15\nBright treetop flat overlooking Clapham Common\nWelcome to our flat above the trees, looking o...\n332998152.0\nClare\n2020-02-02\nEngland, United Kingdom\nI love to travel and share my flat with others...\n...\n$293.00\n2\n90.0\n15.0\n15.0\n2022-08-08\n2023-09-25\n5.0\nNaN\n0.66\n\n\n80110\n6.750005e+17\nhttps://www.airbnb.com/rooms/675000522740608810\n2024-06-14\nCharming room in the Heart of London\nCharming, contemporary studio/guesthouse in th...\n432912179.0\nIfedayo\n2021-11-21\nLondon, United Kingdom\nMy name is Ifedayo, based in London. I like to...\n...\n$75.00\n1\n365.0\n365.0\n10.0\n2022-07-24\n2023-06-18\n4.8\nNaN\n0.43\n\n\n64089\n6.583323e+17\nhttps://www.airbnb.com/rooms/658332309864126164\n2024-06-16\nMarigold Mews by Veeve\nMarigold Mews\n33889201.0\nVeeve\n2015-05-21\nLondon, United Kingdom\nGreetings from the Veeve team!\\n\\n\\nVeeve mana...\n...\n$270.00\n3\n1125.0\n285.0\n1.0\n2023-01-02\n2023-01-02\n5.0\nNaN\n0.06\n\n\n\n\n3 rows × 34 columns\n\n\n\nSee if you can work out from the documentation (Google search time!) how to get the same ‘random’ sample every time you re-run this code block:\n\nQuestion\n\ndf.sample(3, ??)"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dealing-with-nans-and-nulls",
    "href": "practicals/Practical-06-Numeric_Data.html#dealing-with-nans-and-nulls",
    "title": "Practical 6: Numeric Data",
    "section": "Dealing with NaNs and Nulls",
    "text": "Dealing with NaNs and Nulls\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\nThere is a lot going on here and you should be paying close attention.\n\n\n\nIf you really dig into the data you will see that a number of data types that aren’t ‘appropriate’ for their contents: the id columns are floats; the dates aren’t dates; there’s a boolean that’s not a boolean… It would be nice to fix these!\n\n\n\n\n\n\nNote\n\n\n\nI had intended to ask you to fix these by combining code from previous weeks with information provided in the lecture, but it turns out that the InsideAirbnb data set is dirty. There are a lot of NaN values and some of these are deeply problematic for some of the column types in pandas. There are also a number of challenges with other columns so, instead, I’ve opted to show you how I would clean this data as a first pass to get it into a format where it’s tractable for further cleaning.\n\n\n\nIdentifying Problem Rows\nThe reason I’m not asking you to do this part yourselves is that it took me nearly an hour just to work out why I couldn’t convert some of the columns to the right data types; then I started finding rows like these:\n\ndf[df.price.isna()][['id','name','price','room_type']].head(4)\n\n\n\n\n\n\n\n\nid\nname\nprice\nroom_type\n\n\n\n\n236\n20256005.0\nSuitable for travel and evection.\nNaN\nEntire home/apt\n\n\n238\n7377571.0\ndouble room, near brixton,london\nNaN\nPrivate room\n\n\n239\n38519592.0\nHeart of the east London. Room 3 with Cable TV\nNaN\nPrivate room\n\n\n240\n6761407.0\nPrivate room in Shoreditch\nNaN\nPrivate room\n\n\n\n\n\n\n\n\ndf[df.room_type.isna()][['id','name','price','room_type']].head(4)\n\n\n\n\n\n\n\n\nid\nname\nprice\nroom_type\n\n\n\n\n50162\n1.359431e+07\nBeautiful & bright in Queen's Park\nNaN\nNaN\n\n\n52260\n8.446589e+17\nDesigner Notting Hill Flat\nNaN\nNaN\n\n\n56508\n1.066257e+18\nHaggerston Artists condo x2 bed\nNaN\nNaN\n\n\n77222\n5.342282e+07\nCentral 1-bed flat in Trendy Zone 2 East London\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf[~(df.price.str.startswith('$', na=False))][['id','name','price','room_type']].head(4)\n\n\n\n\n\n\n\n\nid\nname\nprice\nroom_type\n\n\n\n\n236\n20256005.0\nSuitable for travel and evection.\nNaN\nEntire home/apt\n\n\n238\n7377571.0\ndouble room, near brixton,london\nNaN\nPrivate room\n\n\n239\n38519592.0\nHeart of the east London. Room 3 with Cable TV\nNaN\nPrivate room\n\n\n240\n6761407.0\nPrivate room in Shoreditch\nNaN\nPrivate room\n\n\n\n\n\n\n\nIf I had to guess, I’d say that it’s the result some kind of partial extract/write process because there are elements in some of the problem row(s) that look right but they are in the wrong columns. So we can probably drop some of these rows, but one thing to do is look at the frequency of NaNs across the data frame first. So we need to look for NaNs and Nulls, but it’s quite obvious that a NaN in the listing id is a basic problem and we should drop these.\n\ndf[df.id.isna()][['id','name','price','room_type']]\n\n\n\n\n\n\n\n\nid\nname\nprice\nroom_type\n\n\n\n\n50163\nNaN\nhttps://a0.muscache.com/im/pictures/user/User-...\n0\n3.0\n\n\n52261\nNaN\nhttps://a0.muscache.com/im/pictures/user/7d57d...\n364\n5.0\n\n\n56509\nNaN\nhttps://a0.muscache.com/im/pictures/user/652bf...\n4\n2.0\n\n\n77223\nNaN\nhttps://a0.muscache.com/im/pictures/user/3a51c...\n174\n2.0\n\n\n78241\nNaN\nhttps://a0.muscache.com/im/users/11520835/prof...\n19\n4.5\n\n\n\n\n\n\n\nAs always, if you don’t know that’s going on, break it down:\n\nYou have seen how column works ([[&lt;column names&gt;]]), so that’s just selecting the columns that we want to show;\nYou know how row selection works (df[&lt;selection criteria&gt;]), so that isn’t anything really new either;\nSo the only really new part is df.id.isna(): df.id is the id column (we could have written this df['id'] if we wanted) and isna() is a test for whether or not a value is NaN.\n\nSo this shows that only one row in the 10,000 row sub-sample has a NaN for its id.\nIf you’re not sure what the next line does, try breaking it down by running the inner bits before you run the drop command; and also try looking online for examples of how to use df.drop (e.g. just up above):\n\nprint(f\"Data frame contains {df.shape[0]:,} rows.\")\ndf.drop(df[df.id.isna()].index.array, axis=0, inplace=True)\nprint(f\"Data frame contains {df.shape[0]:,} rows.\")\n\nData frame contains 93,486 rows.\nData frame contains 93,481 rows.\n\n\nWith that really troublesome data out of the way, you can now turn to counting NaNs or Nulls in the remaining data with a view to identifying other rows that can probably be dropped.\n\n\nCounting Nulls by Column\nAs a starting point I would look to drop the columns that contain only NaNs. Remember that we’ve dropped a row from the data frame so our maximum is now \\(n-1\\))! Notice how this next command works:\n\n# returns a data frame with all values set to True/False according to Null status\ndf.isnull() \n# counts these values by column (we'll see another option in a moment)\ndf.isnull().sum(axis=0) \n# Sort results in descending order\ndf.isnull().sum(axis=0).sort_values(ascending=False) \n\n\ndf.isnull().sum(axis=0).sort_values(ascending=False)[:12]\n\nlicense                 93481\nhost_about              45684\nbeds                    32197\nbathrooms               32126\nprice                   32063\nfirst_review            24746\nreviews_per_month       24746\nlast_review             24746\nreview_scores_rating    24746\nhost_location           21192\nbedrooms                11686\ndescription              3189\ndtype: int64\n\n\nThe most obvious ones here are: license, host_about, beds, bathrooms.\n\ndf.drop(columns=['license','host_about'], inplace=True)\n\nBecause we have dropped everything inplace the code simply runs and doesn’t return anything.\n\n\nCounting Nulls by Row\nWe now know that there are still quite a few problems, but we do still need a way to identify the rows that are causing most of the problems.\nNotice here that the change from axis=0 to axis=1 changes the ‘direction’ of the sum from columns to rows. And we are getting back a data series because the summing operation reduces it to just one column.\n\ndf.isnull().sum(axis=1).sort_values(ascending=False).head(10)\n\n78240    23\n56508    23\n52260    23\n50162    23\n77222    23\n4801     14\n8322     14\n45832    14\n13539    14\n4684     14\ndtype: int64\n\n\nSo that Series shows how many NaN values there are by index value (i.e. the row number). The first column is the row id, the second is the number of NaNs in that row.\nIf we save the results to a variable called probs (i.e. problems) then we can decide what to do next.\n\nprobs = df.isnull().sum(axis=1)\nprint(type(probs))       # Note that this has returned a series!\nprobs.plot.hist(bins=30) # Oooooooh, check out what we can do with a series!\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\nLooking at this histogram, I would think about dropping rows missing more than about 5 values on the basis that they are the ones that are most likely be problematic. We can use the index from probs to select out the rows we want to inspect from the main data frame.\nHere’s another bit of code that bears unpacking:\n\nprint(f\"df contains {df.shape[0]:,} rows.\")\ncutoff = 5\ndf.drop(probs[probs &gt; cutoff].index, inplace=True)\nprint(f\"df contains {df.shape[0]:,} rows.\")\n\ndf contains 93,481 rows.\ndf contains 82,856 rows.\n\n\n\nprobs &gt; 5: this selects only those rows in the ‘probs’ series whose value is greater than 5\nprobs[...].index returns the index values from the Series, which we will then pass to the drop command.\ndf.drop(..., inplace=True) will then drop the rows selected by probs[probs&gt;5].index."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#profiling-not-supported",
    "href": "practicals/Practical-06-Numeric_Data.html#profiling-not-supported",
    "title": "Practical 6: Numeric Data",
    "section": "Profiling (Not Supported)",
    "text": "Profiling (Not Supported)\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nThe Pandas Profiling tool (rebranded a year or so back as ydata-profiling) offers an alternative way of understanding what’s going on in your data. The output looks rather nice and you might be tempted to ask why we didn’t use this straight away on the full data set – well, if you really want to know, see what happens when you profile all 70,000-odd rows and 70-odd columns in the raw data frame… in effect: while it’s ‘nice to have’, the likelihood of crashing your computer increases significantly and it’s a bit of a tangent, so that’s why it’s no longer included in the Podman image.\nIf you do want to explore this then you’ll need to install the library, and this is a good chance to look at how to install software on another machine:\n\nfrom ydata_profiling import ProfileReport"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#managing-memory",
    "href": "practicals/Practical-06-Numeric_Data.html#managing-memory",
    "title": "Practical 6: Numeric Data",
    "section": "Managing Memory",
    "text": "Managing Memory\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nSo as to why you’d want to fix your data types, there are two reasons: 1) to ensure that you can make the most of your data; 2) to ensure that it takes up as little space as possible in memory. Some simple examples:\n\nA column containing only the strings 'True' (4 bytes) and 'False' (5 bytes) will take up vastly more space than a column containing only True and False (1 bit each).\nA column containing only 'Red', 'Green', and 'Blue' (3, 5, and 4 bytes each respectively) will take up much more space that a column where we use the numbers 1, 2, 3 to represent these values and have a map that tells us 1==Red, 2==Blue, and 3==Green.\n\nLet’s test this idea out before looking more closely at how to convert each type of data:\n\n# String type memory usage\nrtm = df.room_type.memory_usage(deep=True) \n# Categorical type memory usage\nctm = df.room_type.astype('category').memory_usage(deep=True) \n\nprint(f\"The raw memory usage of `room_type` is {rtm/1024:,.0f} Kb.\")\nprint(f\"The categorical memory usage of `room_type` is {ctm/1024:,.0f} Kb.\")\nprint(f\"That's {(ctm/rtm)*100:.0f}% of the original!\")\n\nThe raw memory usage of `room_type` is 5,741 Kb.\nThe categorical memory usage of `room_type` is 729 Kb.\nThat's 13% of the original!\n\n\n\n# String type memory usage\nshm = df.host_is_superhost.memory_usage(deep=True) \n# Boolean type memory usage\nbhm = df.host_is_superhost.replace({'f':False, 't':True}).astype('bool').memory_usage(deep=True) \n\nprint(f\"The raw memory usage of `host_is_superhost` is {shm/1024:,.0f} Kb.\")\nprint(f\"The boolean memory usage of `host_is_superhost` is {bhm/1024:,.0f} Kb.\")\nprint(f\"That's {(bhm/shm)*100:.0f}% of the original!\")\n\nThe raw memory usage of `host_is_superhost` is 4,686 Kb.\nThe boolean memory usage of `host_is_superhost` is 728 Kb.\nThat's 16% of the original!"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#boolean-values",
    "href": "practicals/Practical-06-Numeric_Data.html#boolean-values",
    "title": "Practical 6: Numeric Data",
    "section": "Boolean Values",
    "text": "Boolean Values\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nLet’s start with columns that are likely to be boolean:\n\nbools = ['host_is_superhost']\ndf.sample(5, random_state=43)[bools]\n\n\n\n\n\n\n\n\nhost_is_superhost\n\n\n\n\n45788\nf\n\n\n36397\nf\n\n\n37218\nf\n\n\n13743\nf\n\n\n7179\nf\n\n\n\n\n\n\n\nHere we have to map ‘t’ to True and ‘f’ to False before converting the column to a boolean type. If you simply tried to replace them with the strings ‘True’ and ‘False’, then any string that is not None would convert to a True boolean.\n\n# This approach requires us to map 't' \n# and 'f' to True and False\nfor b in bools:\n    print(f\"Converting {b}\")\n    df[b] = df[b].replace({'f':False, 't':True}).astype('bool')\n\nConverting host_is_superhost\n\n\n\ndf.sample(5, random_state=43)[bools]\n\n\n\n\n\n\n\n\nhost_is_superhost\n\n\n\n\n45788\nFalse\n\n\n36397\nFalse\n\n\n37218\nFalse\n\n\n13743\nFalse\n\n\n7179\nFalse"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dates",
    "href": "practicals/Practical-06-Numeric_Data.html#dates",
    "title": "Practical 6: Numeric Data",
    "section": "Dates",
    "text": "Dates\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nI’ve found dates to be particularly challenging, though pandas has tried to make this process less painful than it was a few years ago. What can be particularly frustrating is if one row has a non-sensical date value (e.g. a t, as happened in 2019/20) then the entire type conversion will fail. When that happens, pandas is not great about communicating where the problem occurred and I had to work it out by trying to convert parts of each series (using .iloc) to the datetime type until I had a block that failed. I then knew that I could narrow this down further using integer location indexing.\n\ndates = ['last_scraped','host_since','first_review','last_review']\n\nprint(f\"Currently {dates[1]} is of type '{df[dates[1]].dtype}'\", \"\\n\")\ndf.sample(5, random_state=43)[dates]\n\nCurrently host_since is of type 'object' \n\n\n\n\n\n\n\n\n\n\nlast_scraped\nhost_since\nfirst_review\nlast_review\n\n\n\n\n45788\n2024-06-16\n2011-10-03\n2014-12-30\n2017-08-28\n\n\n36397\n2024-06-16\n2014-12-31\n2022-10-10\n2022-10-10\n\n\n37218\n2024-06-16\n2021-12-30\n2024-06-02\n2024-06-02\n\n\n13743\n2024-06-16\n2016-06-09\n2016-07-24\n2016-07-24\n\n\n7179\n2024-06-16\n2019-07-19\n2023-01-13\n2024-01-05\n\n\n\n\n\n\n\n\nfor d in dates:\n    print(\"Converting \" + d)\n    df[d] = pd.to_datetime(df[d])\n\nConverting last_scraped\nConverting host_since\nConverting first_review\nConverting last_review\n\n\n\ndf.sample(5, random_state=43)[dates]\n\n\n\n\n\n\n\n\nlast_scraped\nhost_since\nfirst_review\nlast_review\n\n\n\n\n45788\n2024-06-16\n2011-10-03\n2014-12-30\n2017-08-28\n\n\n36397\n2024-06-16\n2014-12-31\n2022-10-10\n2022-10-10\n\n\n37218\n2024-06-16\n2021-12-30\n2024-06-02\n2024-06-02\n\n\n13743\n2024-06-16\n2016-06-09\n2016-07-24\n2016-07-24\n\n\n7179\n2024-06-16\n2019-07-19\n2023-01-13\n2024-01-05\n\n\n\n\n\n\n\nOf course, it’s not actually clear there what has changed! But if you dig a little more deeply:\n\nprint(f\"Now {dates[1]} is of type '{df[dates[1]].dtype}'\", \"\\n\")\ndf.sample(5, random_state=45)[dates[1]].dt.strftime('%A %B %d, %Y')\n# Try some other formats!\n\nNow host_since is of type 'datetime64[ns]' \n\n\n\n90719      Tuesday August 25, 2015\n86895      Thursday April 04, 2024\n23856    Tuesday February 06, 2024\n11538      Monday October 14, 2019\n21073    Tuesday December 03, 2019\nName: host_since, dtype: object\n\n\nIn that line of code we:\n\nTook a random sample (setting the state to 45),\nTook the second column from the dates list (dates[1]),\nUsed the date ‘accessor method’ (.dt),\nAnd called string format time with the format %A %B %d, %Y (Full Day of Week, Month Name, Date, 4-digit Year)"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#categories",
    "href": "practicals/Practical-06-Numeric_Data.html#categories",
    "title": "Practical 6: Numeric Data",
    "section": "Categories",
    "text": "Categories\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nWe know that these are likely to be categories because there’d be no other way to allow users to effectively search Airbnb.\n\ncats = ['property_type','room_type']\n\nprint(f\"Currently {cats[1]} is of type '{df[cats[1]].dtype}'\", \"\\n\")\ndf.sample(5, random_state=42)[cats]\n\nCurrently room_type is of type 'object' \n\n\n\n\n\n\n\n\n\n\nproperty_type\nroom_type\n\n\n\n\n63675\nEntire rental unit\nEntire home/apt\n\n\n29219\nEntire rental unit\nEntire home/apt\n\n\n15274\nPrivate room in condo\nPrivate room\n\n\n82638\nEntire rental unit\nEntire home/apt\n\n\n27097\nEntire rental unit\nEntire home/apt\n\n\n\n\n\n\n\nThis next piece of code is quite useful for grouping and counting operations: we are counting the occurences of each unique value in part particular column or combination of columns:\n\n\ndf[cats[0]].value_counts()\n\nproperty_type\nEntire rental unit              34289\nPrivate room in rental unit     11681\nPrivate room in home             9833\nEntire condo                     8412\nEntire home                      7437\n                                ...  \nReligious building                  1\nShared room in villa                1\nMinsu                               1\nPrivate room in nature lodge        1\nPrivate room in floor               1\nName: count, Length: 94, dtype: int64\n\n\n\n\n\ndf[cats[1]].value_counts()\n\nroom_type\nEntire home/apt    54157\nPrivate room       28197\nShared room          327\nHotel room           175\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne column has many different values (including Campers/RVs and Yurts!), the other has just four. If I were looking to conduct research I’d probably start with the room_type column since I may not care about hotels and therefore never even need to decide whether I care about boutique ones!\n\n\n\nfor c in cats:\n    print(f\"Converting {c}\")\n    df[c] = df[c].astype('category')\n\nConverting property_type\nConverting room_type\n\n\n\nprint(f\"Now {cats[1]} is of type '{df[cats[1]].dtype}'\", \"\\n\")\nprint(df[cats[1]].cat.categories.values)\n\nNow room_type is of type 'category' \n\n['Entire home/apt' 'Hotel room' 'Private room' 'Shared room']\n\n\n\ndf.sample(5, random_state=42)[cats]\n\n\n\n\n\n\n\n\nproperty_type\nroom_type\n\n\n\n\n63675\nEntire rental unit\nEntire home/apt\n\n\n29219\nEntire rental unit\nEntire home/apt\n\n\n15274\nPrivate room in condo\nPrivate room\n\n\n82638\nEntire rental unit\nEntire home/apt\n\n\n27097\nEntire rental unit\nEntire home/apt"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dealing-with-strings",
    "href": "practicals/Practical-06-Numeric_Data.html#dealing-with-strings",
    "title": "Practical 6: Numeric Data",
    "section": "Dealing with Strings",
    "text": "Dealing with Strings\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe’ll have to put some more work into dealing with the description and other ‘free-from’ text fields later in the term, but for now let’s just deal with a straightforward one: price!\n\nmoney = ['price']\ndf.sample(5, random_state=42)[money]\n\n\n\n\n\n\n\n\nprice\n\n\n\n\n63675\n$1,047.00\n\n\n29219\n$249.00\n\n\n15274\n$61.00\n\n\n82638\nNaN\n\n\n27097\n$150.00\n\n\n\n\n\n\n\nYou will get an error when you run the next code block, that’s because I want you to do a little thinking about how to extend the code to fix the data. You’ve already got the code you need to fix it, you just need to do a bit of thinking about ‘method chaining’!\n\nfor m in money:\n    print(f\"Converting {m}\")\n    try:\n        df[m] = df[m].str.replace('$','', regex=False).astype('float')\n    except ValueError as e:\n        print(f\"    xxxx Unable to convert {m} to float xxxx\")\n        print(e)\n\nConverting price\n    xxxx Unable to convert price to float xxxx\ncould not convert string to float: '1,000.00'\n\n\nLook closely at the error and then think about what you need to add to the code below:\n\n\n\n\n\n\nNote\n\n\n\nFor now don’t worry about what regex=False means. It will all make sense when we get to dealing with text.\n\n\n\nQuestion\n\nfor m in money:\n    print(f\"Converting {m}\")\n    df[m] = df[m].str.replace('$','', regex=False).str.replace(??).astype('float')\n\n\ndf.sample(5, random_state=42)[money]\n\n\n\n\n\n\n\n\nprice\n\n\n\n\n63675\n1047.0\n\n\n29219\n249.0\n\n\n15274\n61.0\n\n\n82638\nNaN\n\n\n27097\n150.0\n\n\n\n\n\n\n\nAnd here’s a final thing to note that looks… a little odd:\n\ndf.sort_values(by='price', ascending=False).head(5)[['id','name','price','minimum_nights']]\n\n\n\n\n\n\n\n\nid\nname\nprice\nminimum_nights\n\n\n\n\n2220\n1.069627e+18\nClose To London Bridge\n80000.0\n2\n\n\n39737\n9.364490e+17\nClose To Waterloo & London Eye (CHA)\n80000.0\n2\n\n\n42956\n1.040471e+18\nRoom In Zone 1 (TOB)\n80000.0\n2\n\n\n53426\n2.261002e+07\nCLOSE TO LONDON EYE AND TUBE (PANA)\n78679.0\n2\n\n\n54526\n4.155799e+07\nClose To London Eye (HED)\n75000.0\n2"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dealing-with-integers",
    "href": "practicals/Practical-06-Numeric_Data.html#dealing-with-integers",
    "title": "Practical 6: Numeric Data",
    "section": "Dealing with Integers",
    "text": "Dealing with Integers\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nThis is the issue that made me abandon the idea of making you clean the data yourselves. Although floats have no issues with np.nan in the Series, by default there are no numpy integer arrays that can cope with NaNs. This was such a major issue for Pandas that they’ve actually created their own data type that does support NaN values in integer columns. There are a lot of integer columns, but only one of them seems to be a problem.\n\nints  = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',\n         'beds','minimum_nights','maximum_nights','availability_365']\nfor i in ints:\n    print(f\"Converting {i}\")\n    try:\n        df[i] = df[i].astype('float').astype('int')\n    except ValueError as e:\n        print(\"  - !!!Converting to unsigned 16-bit integer!!!\")\n        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())\n\nConverting id\nConverting host_id\nConverting host_listings_count\nConverting host_total_listings_count\nConverting accommodates\nConverting beds\n  - !!!Converting to unsigned 16-bit integer!!!\nConverting minimum_nights\nConverting maximum_nights\nConverting availability_365\n\n\nSo we convert the column but using a try / except approach that allows to trap ValueError exceptions triggered by the presence of NaNs in the column. The following code tells us that there are just eight of these in the 10k sample, but they’re enough to cause the code to fail if you don’t trap them. The alternatives would be to: a) drop those rows; or b) leave the data as floats. For some reason the latter offends my sense of order, and the former feels like avoiding the problem rather than dealing with it.\n\ndf.beds.isna().value_counts()\n\nbeds\nFalse    60769\nTrue     22087\nName: count, dtype: int64"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#validation",
    "href": "practicals/Practical-06-Numeric_Data.html#validation",
    "title": "Practical 6: Numeric Data",
    "section": "Validation",
    "text": "Validation\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nOrdinarily, at this point I would then output information to confirm that all of the opeations I think I’ve undertaken were correctly applied.\n\ndf.info()"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#saving",
    "href": "practicals/Practical-06-Numeric_Data.html#saving",
    "title": "Practical 6: Numeric Data",
    "section": "Saving",
    "text": "Saving\nAlso at this point I would save a copy of the cleaned data, though I would only consider this data partially cleaned since we’ve not made it any further than just ensuring that each column is in an appropriate format and that some particularly problematic rows have been dropped!\n\ncsv_out = Path(f'data/clean/{path.name}')\npq_out  = Path(f'data/clean/{path.name.replace('.csv.gz','.parquet')}')\n\nif not csv_out.parent.exists():\n    print(f\"Creating {csv_out.parent}\")\n    csv_out.parent.mkdir(parents=True, exist_ok=True)\n    \ndf.to_csv(csv_out, index=False)\ndf.to_parquet(pq_out, index=False)\nprint(f\"Saved {df.shape[0]:,} rows of {df.shape[1]:,} columns to {csv_out.resolve()}\")\nprint(\"Done.\")\n\nSaved 82,856 rows of 32 columns to /home/jovyan/work/practicals/data/clean/20250615-London-listings.csv.gz\nDone.\n\n\nWe’ll shortly begin to look at the parquet file format because it’s fast, it preserves data types, it’s compressed, and it will avoid the kinds of the problems that come up when you move to/from CSV as a default; however, for now let’s keep working with what we understand."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-using-data-types",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-using-data-types",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting using Data Types",
    "text": "Selecting using Data Types\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nIf we wanted to filter in/out certain columns pandas can do that! Let’s try for floats and ints (hint: these are 64-bit data types).\n\nQuestion\n\ndf.select_dtypes(include=[??])"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-using-conditions",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-using-conditions",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting using Conditions",
    "text": "Selecting using Conditions\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nConditional selection is usally done as a combination of the selection approaches above in combination with conditionals. So to try to select only the Entire home/apt room type we are testing for cases where the room_type equals our target term (Entire home/apt):\n\nQuestion\n\ndf[df.??=='??']['property_type'].value_counts().head(10)\n\nYour output should be:\n\nproperty_type\nEntire rental unit           34289\nEntire condo                  8412\nEntire home                   7437\nEntire serviced apartment     1653\nEntire townhouse              1041\nEntire loft                    352\nEntire guesthouse              217\nName: count, dtype: int64"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#arbitrary-selection-criteria",
    "href": "practicals/Practical-06-Numeric_Data.html#arbitrary-selection-criteria",
    "title": "Practical 6: Numeric Data",
    "section": "Arbitrary Selection Criteria",
    "text": "Arbitrary Selection Criteria\n\n\n\n\n\n\nWarningDifficulty: Moderate, if the previous section made sense to you.\n\n\n\n\n\n\n\n\n\nOK, now let’s look for the Entire home/apt listings that cost more than the average price of all listings… to do that let’s get a sense of where the mean and median value fall:\n\nQuestion\n\nprint(f\"The mean price is ${df.price.??():0.2f}\")\nprint(f\"The median price is ${df.price.??():0.2f}\")\n\nYou should get:\n\nThe mean price is $209.56\nThe median price is $137.00\n\nYou should see that the mean is higher than the median price but both are very roughly plausible values. Given your understanding of distributions from, say, Quantitative Methods, what can you say about the pricing distribution of Airbnb units?\nYou might want to have a look at the documentation: it’s rather a long list, but most of your descriptive stats are on that page in the Cumulative / Descriptive Stats section, and there’s also lots of information about methods for strings and categorical data.\n\n\nFiltering: it’s ‘logical’\nSo we want to take Entire home/apt and filter the data set together with the price per night from the price column. For that, let’s use the mean price/night of $209.56. Note: this is totally arbitrary.\n\nQuestion\nSo here we want to filter on two values in the data set using &:\n\npricey = df[\n    (??) & \n    (df.price&gt;df.price.??)\n]\nprint(f\"Selected {pricey.shape[0]:,} rows\")\n\nYou should get 16,257 rows.\nIn the code above we see two things:\n\nThe use of the bitwise & (it’s not the same as and and you should recall our work with the bitarray earlier in the term).\nThe fact that you need parentheses around the selection in order to make the the & work."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selection-with-an-aggregate",
    "href": "practicals/Practical-06-Numeric_Data.html#selection-with-an-aggregate",
    "title": "Practical 6: Numeric Data",
    "section": "Selection with an Aggregate",
    "text": "Selection with an Aggregate\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nLet’s find the cheapest and most expensive listings using min and max methods:\n\nQuestion\nLeast expensive:\n\ndf[df.price==df.price.??()][['price','id','listing_url','room_type','description']]\n\nMost expensive:\n\ndf[df.price==df.price.??()][['price','id','listing_url','room_type','description']]\n\nYou should see one or more units priced at exceedingly high levels… and here’s a way to see a few more of these budget-busting options.\n\ndf.sort_values(by='price', ascending=False).head(3)[\n    ['price','listing_url','room_type','description']\n]\n\n\n\n\n\n\n\n\nprice\nlisting_url\nroom_type\ndescription\n\n\n\n\n2220\n80000.0\nhttps://www.airbnb.com/rooms/1069626776402758037\nPrivate room\n7 minutes walk to London Bridge and Borough Ma...\n\n\n39737\n80000.0\nhttps://www.airbnb.com/rooms/936449020154835053\nPrivate room\n10 minutes walk to London Eye and Westminster&lt;...\n\n\n42956\n80000.0\nhttps://www.airbnb.com/rooms/1040471243366946776\nPrivate room\n* ZONE 1 &lt;br /&gt;* PRIVATE ROOM &lt;br /&gt;* CAN WAL...\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionStop: Ask yourself if the result is plausible.\n\n\n\n\n\n\n\n\nQuestion\nWhat do you make of this result?"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selection-with-a-range",
    "href": "practicals/Practical-06-Numeric_Data.html#selection-with-a-range",
    "title": "Practical 6: Numeric Data",
    "section": "Selection with a Range",
    "text": "Selection with a Range\n\n\n\n\n\n\nWarningDifficulty: Moderate\n\n\n\n\n\n\n\n\n\nPerhaps we aren’t just looking for extremes… how about all of the properties falling within the middle of the distribution? We can ask for any abitrary quantile we like, so let’s go with the 25th and 75th percentile to get the middle 50% of the data. Google how to get percentiles from pandas.\n\nQuestion\n\ndfr = df[ \n            (df.price &gt; df.price.quantile(??)) & \n            (df.price &lt; df.price.quantile(??)) ]\n\nprint(f\"Lower Quartile: {df.price.quantile(??):&gt;6.2f}\")\nprint(f\"Upper Quartile: {df.price.quantile(??):&gt;6.2f}\")\nprint()\nprint(f\"Range selected contains {dfr.shape[0]:,} rows.\")\nprint(f\"Minimum price: {dfr.price.??():&gt;6.2f}\")\nprint(f\"Maximum price: {dfr.price.??():&gt;6.2f}\")\n\nThat example contains a few things to which you need to pay attention:\n\nAgain you can see that, with mutiple selections, we had to put parentheses around each one – this forces Python to…\nProcess the & (bit-wise AND) that asks pandas to “Find all the rows where condition 1 AND condition 2 are both True”. So it calculates the True/False for the left side and the True/False for the right side of the &, and then combines them.\n\nI find this parentheses business annoying and frequently get an error when I forget to add them, but I’m guessing it’s tied to operator precedence and how the various operations are interpreted by Python."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#footnotes",
    "href": "practicals/Practical-06-Numeric_Data.html#footnotes",
    "title": "Practical 6: Numeric Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is also how [polars](https://pola.rs/) does things↩︎"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html",
    "href": "practicals/Practical-04-Reuse.html",
    "title": "Practical 4: Efficient Code",
    "section": "",
    "text": "In this notebook we are going to look in more detail at how we can reduce, reuse, and recycle our code to make our lives easier and our code more efficient."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#what-do-we-do-break-it-down",
    "href": "practicals/Practical-04-Reuse.html#what-do-we-do-break-it-down",
    "title": "Practical 4: Efficient Code",
    "section": "What Do We Do? Break It Down!",
    "text": "What Do We Do? Break It Down!\n\nStep 1. Analyse the Problem\nThe first step to writing a program is thinking about your goal and the steps required to achieve that. We don’t write programs like we write essays: all at once by writing a whole lot of code and then hoping for the best when we hit ‘submit’. \nWhen you’re tackling a programming problem you break it down into separate, simpler steps, and then tick them off one by one. Doing this gets easier as you become more familiar with programming, but it remains crucial and, in many cases, good programmers in large companies spend more time on design than they do on actual coding.\n\n\nStep 2. Functions & Packages\nWe have discussed how functions are a useful programming tool to enable us to re-use chunks of code. Basically, a function is a way to do something to something in a portable, easy-to-use little bundle of code.\nSome steps in a program are done so many times by so many people that, eventually, someone writes a package that bundles up those operations into something easy to use that saves you having to figure out the gory details. Reading a file (even one on a computer halfway round the world) is one of those things. Making sense of the data in that file for you is probably not.\n\n\n\nxkcd: Easy vs. Hard\n\n\nTo a computer, reading data from a remote location (e.g. a web site halfway around the world) is not really any different from reading one that’s sitting on your your local hard drive (e.g. on your desktop). To simplify things a great deal: the computer really just needs to know the location of the file and an appropriate protocol for accessing that file (e.g. http, https, ftp, local…) and then a clever programming language like Python will typically have packages that can kind of take of the rest.\nIn all cases – local and remote – you use the package to handle the hard bit of knowing how to actually ‘read’ data (because all files are just 1s and 0s of data) at the device level and then Python gives you back a ‘file handle’ that helps you to achieve things like ‘read a line’ or ‘close an open file’. You can think of a filehandle as something that gives you a ‘grip’ on a file-like object no matter where or what it is, and the package is the way that this magic is achieved.\n\n\nStep 3. Look for Ways to Recycle\nAlways look for ways to avoid reinventing the wheel. This is where Python’s packages (or R’s for that matter) come into their own. If it’s something that programmers often need to do, then chances are that someone has written a package to do it!\nThe point of packages is that they can help us to achieve quite a lot very quickly since we can just make use of someone else’s code. In the same way that we won’t mark you down for Googling the answer to a coding question, we also won’t mark you down for using someone else’s package to help you get going with your programming. That’s the whole point!\nOften, if you’re not sure where to start, Google (or StackOverflow) is the place to go:\nhow to read text file on web server python\nBoom!\n\n\nStep 4. Make a Plan\nOK, so we need to break this hard problem down into something simpler. We can do this by thinking about it as three separate steps:\n\nWe want to read a remote file (i.e. a text file somewhere the planet),\nWe want to turn it into a local data structure (i.e a list or a dictionary),\nWe want to perform some calculations on the data (e.g. calculate the mean, find the easternmost city, etc.).\n\nWe can tackle each of those in turn, getting the first bit working, then adding the second bit, etc. It’s just like using lego to build something: you take the same pieces and assemble them in different ways to produce different things."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#text-into-data",
    "href": "practicals/Practical-04-Reuse.html#text-into-data",
    "title": "Practical 4: Efficient Code",
    "section": "Text into Data",
    "text": "Text into Data\nWe now need to work on turning the response into useful data. We got partway there by splitting on line-breaks (splitlines()), but now we need to get columns for each line. You’ll notice that we are dealing with a CSV (Comma-Separated Value) file and that the format looks quite simple… So, in theory, to turn this into data we ‘just’ need to split each row into separate fields using the commas.\nThere’s a handy function associated with strings called split:\n\nprint('abcdefgh'.split('d'))\n\nYou can also investigate further how the split function works using:\n\nhelp('abcdefgh'.split)\n\nSo this seems like a good solution to turn our text into data:\n\ntest = rows[-1].split(',')\nprint(test)\nprint(f\"The price of {test[0]} is {test[-1]}\")\n\nI’d say that we’re now getting quite close to something that looks like ‘real data’: I know how to convert a raw response from a web server into a string, to split that string into rows, and can even access individual elements from a row!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#insight",
    "href": "practicals/Practical-04-Reuse.html#insight",
    "title": "Practical 4: Efficient Code",
    "section": "Insight!",
    "text": "Insight!\nSo, although the code was basically the same for both of these files (good), we would need to change quite a bit in order to print out the same information from different versions of the same data. So our code is rather brittle.\nOne of the issues is that our instincts about how to manage data doesn’t align with how the computer can most efficiently manage it. We make the mistake of thinking that the computer needs to do things that same way that we do when reading text and so assume that we need to:\n\nRepresent the rows as a list.\nRepresent the columns as a list for each row.\n\nThis thinking suggests that the ‘right’ data structure would clearly be a list-of-lists (LoLs!), but if you understand what happened here then the next section will make a lot more sense!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#understanding-whats-an-appropriate-data-structure",
    "href": "practicals/Practical-04-Reuse.html#understanding-whats-an-appropriate-data-structure",
    "title": "Practical 4: Efficient Code",
    "section": "Understanding What’s an ‘Appropriate’ Data Structure",
    "text": "Understanding What’s an ‘Appropriate’ Data Structure\nIf you stop to think about it, then our list-of-lists approach to the data isn’t very easy to navigate. Notice that if the position or name of a column changes then we need to change our program every time we re-run it! It’s not very easy to read either since we don’t really know what u[5] is supposed to be. That way lies all kinds of potential errors!\nAlso consider that, in order to calculate out even a simple aggregate such as the sum of a field for all rows we need to step through a lot of irrelevant data as well: we have to write a for loop and then step through each row with an ‘accumulator’ (somewhere to store the total). That’s slow.\nThat doesn’t make much sense since this should all be easier and faster in Python than in Excel, but right now it’s harder, and quite possibly slower as well! So how does the experienced programmer get around this? ‘Simple’ (i.e. neither simple, nor obvious, until you know the answer): she realises that the data is organised the wrong way! We humans tend to think in rows of data: this apartment has the following attributes (price, location, etc.), or that city has the following attributes (population, location). We read across the row because that’s the easiest way for us to think about it. But, in short, a list-of-lists does not seem to be the right way to store this data!\nCrucially, a computer doesn’t have to work that way. For a computer, it’s as easy to read down a column as it is to read across a row. In fact, it’s easier, because each column has the same type of data: one column contains names (strings), another column contains prices (integers or floats), and other columns contain other types of data (floats, etc.). Better still, the order of the columns often doesn’t matter as long as we know what the columns are called: it’s easier to ask for the ‘description column’ than it is to ask for the 6th column since, for all we know, the description column might be in a different place for different files but they are all (relatively) likely to use the ‘description’ label for the column itself."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#a-dictionary-of-lists-to-the-rescue",
    "href": "practicals/Practical-04-Reuse.html#a-dictionary-of-lists-to-the-rescue",
    "title": "Practical 4: Efficient Code",
    "section": "A Dictionary of Lists to the Rescue",
    "text": "A Dictionary of Lists to the Rescue\nSo, if we don’t care about column order, only row order, then a dictionary of lists would be a nice way to handle things. And why should we care about column order? With our CSV files above we already saw what a pain it was to fix things when the layout of the columns changed from one data set to the next. If, instead, we can just reference the ‘description’ column then it doesn’t matter where that column actually is. Why is that?\nWell, here are the first four rows of data from the CSV file:\nname, property_type, room_type, price, bedrooms, beds, host_name, latitude, longitude\nGorgeous 2 bed flat w easy access to Earlsfield St, Entire serviced apartment, Entire home/apt, , 2.0, , Katie, 51.44243, -0.18903\nWelcome to London!, Private room in bed and breakfast, Private room, $54.00, 1.0, 1.0, Sabahat, 51.59339662817043, -0.0934109485544038\n2 bedroom 8th floor serviced apartment., Entire serviced apartment, Entire home/apt, $187.00, 2.0, 2.0, Cubo Apartments Canary Wharf, 51.49926, -0.02224\nPrime city studio apartment, Entire rental unit, Entire home/apt, $82.00, 1.0, 1.0, Alan & Wanlin, 51.52548766243926, -0.0783275711639452\nCozy Room near Canary Wharf, Private room in home, Private room, , , , Sujata, 51.51468, -0.0299\nHere’s how it would look as a dictionary of lists organised by column, and not by row, though note that (for now) I’ve changed the NaN (Not a Number) values to something that will be easier to work with:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4],\n    'Name'       : ['Gorgeous 2 bed flat w easy access to Earlsfiel...', 'Welcome to London!', '2 bedroom 8th floor serviced apartment.','Prime city studio apartment','Cozy Room near Canary Wharf'],\n    'Longitude'  : [-0.189030, -0.093411, -0.022240, -0.078328, -0.029900],\n    'Latitude'   : [51.442430, 51.593397, 51.499260, 51.525488, 51.514680],\n    'Bedrooms'   : [2, 1, 2, 1, 3],\n}\n\nprint(myData['Name'])\nprint(myData['Bedrooms'])\n\n['Gorgeous 2 bed flat w easy access to Earlsfiel...', 'Welcome to London!', '2 bedroom 8th floor serviced apartment.', 'Prime city studio apartment', 'Cozy Room near Canary Wharf']\n[2, 1, 2, 1, 3]\n\n\nWhat does this do better? Well, for starters, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already, but the real benefit is coming up…"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#behold-the-power-of-the-dol",
    "href": "practicals/Practical-04-Reuse.html#behold-the-power-of-the-dol",
    "title": "Practical 4: Efficient Code",
    "section": "Behold the Power of the DoL",
    "text": "Behold the Power of the DoL\nNow let’s look at what you can do with this data structure. I’d encourage you to try to answer the questions without tips or help, but because this is quite a big shift in complexity, for each of the questions there’s also substantial guidance in the ‘Unpacking’ section.\n\n\n\n\n\n\nTipWe’ll step through most of these in detail below.\n\n\n\n\n\n\n\n\n\nWe can find the latitude of ‘Prime city studio apartment’ by putting together what we know about searching for a value in a list with what we know about dictionaries:\n\nloc = 'Prime city studio apartment'\nlat = myData['Latitude'][ myData['Name'].index(loc) ]\nprint(f\"{loc}'s latitude is {lat}\")\n\nSo to print the location of ‘2 bedroom 8th floor serviced apartment.’:\n\nQuestion\n\nloc = \"2 bedroom 8th floor serviced apartment.\"\nprint(f\"The listing for {loc} can be found at \" + \n      f\"{abs(myData[??][ ?? ])}ºW, {myData['Latitude'][ ?? ]}ºN\")\n\nIf you need help, then you can find it below.\nTo find the easternmost listing we need to adapt what we’ve done above and think about how we’d get the maximum value out:\n\n\nQuestion\n\nlisting = myData['Name'][ myData['Longitude'].index( ??(myData[??]) ) ]\nprint(f\"The easternmost listing is: {listing}\")\n\nIf you need help, then you can find it below with some additional tips on just finding a longitude that really breaks it all down into small steps.\nTo find the mean number of bedrooms we can use a package called numpy. numpy (Numerical Python) is used so much that most people simply refer to it as np. This is a huge package in terms of features, but right now we’re interested only in the simple arithmatic mean.\n\n\nQuestion\n\nimport numpy as np\nmean = np.??(myData['Bedrooms'])\nprint(f\"The mean number of bedrooms is: {mean}\")\n\nAgain, you can find help below.\n\n\n\n\n\n\nWarning\n\n\n\nStop! Look closely at what is going on. There’s a lot of content to process in the code above, so do not rush blindly on if this is confusing. Try pulling it apart into pieces and then reassemble it. Start with the bits that you understand and then add complexity."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#the-location-of-lerwick",
    "href": "practicals/Practical-04-Reuse.html#the-location-of-lerwick",
    "title": "Practical 4: Efficient Code",
    "section": "The Location of Lerwick",
    "text": "The Location of Lerwick\nLet’s take a detour… Lerwick is a small town in the Shetlands, way up to the North of mainland U.K. and somewhere I’ve wanted to go ever since I got back from Orkney–but then I spent my honeymoon in the far North of Iceland, so perhaps I just don’t like being around lots of people… 🙃\nAnyway, this one might be a tiny bit easier conceptually than the other problems, except that I’ve deliberately used a slightly different way of showing the output that might be confusing:\nTo print the location of Lerwick:\n#| eval: False\n# This won't actually run, it's an example\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n    f\"{abs(myData['Longitude'][myData['Name'].index(city)])}ºW, \" +\n    f\"{myData['Latitude'][myData['Name'].index(city)]}ºN\")\nThe first thing to do is to pull apart the print statement: you can see that this is actually just two ‘f-strings’ joined by a +–having that at the end of the line tells Python that it should carry on to the next line. That’s a handy way to make your code a little easier to read. If you’re creating a list and it’s getting a little long, then you can also continue a line using a , as well!\n\n1. The first f-string\nThe first string will help you to make sense of the second: f-strings allow you to ‘interpolate’ a variable into a string directly rather than having to have lots of str(x) + \" some text \" + str(y). You can write f\"{x} some text {y}\" and Python will automatically convert the variables x and y to strings and replace {x} with the value of x and {y} with the value of y.\nSo here f\"The town of {city} can be found at \" becomes f\"The town of Lerwick can be found at \" because {city} is replaced by the value of the variable city. This makes for code that is easier for humans to read and so I’d consider that a good thing.\n\n\n2. The second f-string\nThis one is hard because there’s just a lot of code there. But, again, if we start with what we recognise that it gets just a little bit more manageable… Also, it stands to reason that the only difference between the two outputs is that one asks for the ‘Longitude’ and the other for the ‘Latitude’. So if you can make sense of one you have automatically made sense of the other and don’t need to work it all out.\nLet’s start with a part that you might recognise:\n\nmyData['Name'].index(listing)\n\nYou’ve got this. This is just asking Python to work out the index of Lerwick (because city = 'Lerwick'). So it’s a number. 5 in this case. And we can then think, ’OK so what does this return:\n\nmyData['Longitude'][4]\n\nAnd the answer is -1.145. That’s the Longitude of Lerwick! There’s just one last thing: notice that we’re talking about degrees West here. So the answer isn’t a negative (because negative West degrees would be East!), it’s the absolute value. And that is the final piece of the puzzle: abs(...) gives us the absolute value of a number!\n\nhelp(abs)"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#the-longitude-of-a-listing",
    "href": "practicals/Practical-04-Reuse.html#the-longitude-of-a-listing",
    "title": "Practical 4: Efficient Code",
    "section": "The Longitude of a Listing",
    "text": "The Longitude of a Listing\nThe code we’ve just seen can look pretty daunting, so let’s break it down into two parts. What would you get if you ran just this code?\n\nmyData['Longitude'][2]\n\nRemember that this is a dictionary-of-lists (DoL). So, Python first looks for a key named Longitude in the myData dictionary. It finds out that the value associated with this key is a list and, in this example, it just pulls out the second value (index 1). Does that part make sense?\nNow, to the second part:\n\nmyData['Name'].index('2 bedroom 8th floor serviced apartment.')\n\nHere we look in the dictionary for the key Name and find that that’s also a list. All we’re doing here is asking Python to find the index of ‘2 bedroom 8th floor serviced apartment.’ for us in that list. And myData['Name'].index('2 bedroom 8th floor serviced apartment.') gives us back a 2, so instead of just writing myData['Longitude'][2] we can replace the 2 with myData['Name'].index('2 bedroom 8th floor serviced apartment.')! Crucially, notice the complete absence of a for loop, which means that this code is fast!\nDoes that make sense? If it does then you should be having a kind of an 🤯 moment because what we’ve done by taking a column view, rather than a row view, is to make Python’s index() command do the work for us. Instead of having to look through each row for a field that matches ‘Name’ and then check to see if it’s ‘2 bedroom 8th floor serviced apartment.’, we’ve pointed Python at the right column immediately and asked it to find the match (which it can do very quickly). Once we have a match then we also have the row number to go and do the lookup in the ‘Longitude’ column because the index is the row number!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#the-easternmost-listing",
    "href": "practicals/Practical-04-Reuse.html#the-easternmost-listing",
    "title": "Practical 4: Efficient Code",
    "section": "The Easternmost Listing",
    "text": "The Easternmost Listing\nWhere this approach really comes into its own is on problems that involve maths. To figure out the easternmost city in this list we need to find the maximum Longitude and then use that value to look up the city name. So let’s do the same process of pulling this apart into two steps. Let start with the easier bit:\n\nmyData['Name'][0]\n\nThat would give us the name of a city, but we don’t just want the first city in the list, we want the one with the maximum longitude. To achieve that we need to somehow replace the 0 with the index of the maximum longitude. Let’s break this down further:\n\nWe first need to find the maximum longitude.\nWe then need to find the index of that maximum longitude.\n\nSo Step 1 would be:\n\nmax_lon = max(myData['Longitude'])\n\nBecause the max(...) helps us to find the maximum longitude in the Longitude list. Now that we have that we can proceed to Step 2:\n\nmyData['Longitude'].index(max_lon)\n\nSo now we ask Python to find the position of max_lon in the list. But rather than doing this in two steps we can combine into one if we write it down to make it easier to read:\n\nmyData['Longitude'].index(\n    max(myData['Longitude'])\n)\n\nThere’s the same .index which tells us that Python is going to look for something in the list associated with the Longitude key. All we’ve done is change what’s inside that index function to max(myData['Longitude']). This is telling Python to find the maximum value in the myData['Longitude'] list. So to explain this in three steps, what we’re doing is:\n\nFinding the maximum value in the Longitude column (we know there must be one, but we don’t know what it is!),\nFinding the index (position) of that maximum value in the Longitude column (now that we know what the value is!),\nUsing that index to read a value out of the Name column.\n\nI am a geek, but that’s pretty cool, right? In one line of code we managed to quickly find out where the data we needed was even though it involved three discrete steps. Think about how much work you’d have to do if you were still thinking in rows, not columns!\n\nloc = myData['Name'][\n    myData['Longitude'].index(\n        max(myData['Longitude'])\n    )\n]\nprint(f\"The easternmost listing is {loc}.\")\n\n\nThe Average Number of Bedrooms\nSo here we’re going to ‘cheat’ a little bit: rather than writing our own function, we’re going to import a package and use someone else’s function. The numpy package contains a lot of useful functions that we can call on (if you don’t believe me, add “dir(np)” on a new line after the import statement), and one of them calculates the average of a list or array of data.\n\nimport numpy as np\nprint(f\"The mean number of bedrooms is {np.mean(myData['Bedrooms'])}\")\n\nThis is where our new approach really comes into its own: because all of the population data is in one place (a.k.a. a series or column), we can just throw the whole list into the np.mean function rather than having to use all of those convoluted loops and counters. Simples, right?\nNo, not simple at all, but we’ve come up with a way to make it simple.\n\n\nRecap!\nSo the really clever bit in all of this isn’t switching from a list-of-lists to a dictionary-of-lists, it’s recognising that the dictionary-of-lists is a better way to work with the data that we’re trying to analyse and that that there are useful functions that we can exploit to do the heavy lifting for us. Simply by changing the way that we stored the data in a ‘data structure’ (i.e. complex arrangement of lists, dictionaries, and variables) we were able to do away with lots of for loops and counters and conditions, and reduce many difficult operations to something that could be done on one line!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#bringing-it-all-together",
    "href": "practicals/Practical-04-Reuse.html#bringing-it-all-together",
    "title": "Practical 4: Efficient Code",
    "section": "Bringing it all together…",
    "text": "Bringing it all together…\nConceptually, this is one of the hardest practicals in the entire term because it joins up so many of the seemingly simple ideas that you covered in Code Camp into a very complex ‘stew’ – all our basic ingredients (lists, dictionaries, etc.) have simmered for a bit, been stirred up together, and become something entirely new and more complex.\nSo if this practical doesn’t make sense to you on the first runthrough, I’d suggest going back through the second half of the practical again in a couple of days’ time – that will give your brain a little time to wrap itself around the basics before you throw the hard stuff at it again. Don’t panic if it doesn’t all make sense on the second runthrough either – this is like a language, you need to practice! With luck, the second time you went through this code a little bit more made sense. If you need to do it a third time you’ll find that even more makes sense… and so on."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html",
    "href": "practicals/Practical-02-Foundations_1.html",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "",
    "text": "This week is focussed on ensuring that you’re able to run Jupyter notebooks locally (i.e. on your own computer) and are comfortable with the basics of Python: if you’ve already done Code Camp then this will be a refresher and you’ll have plenty of time to get to grips with Git and GitHub, which often presents significant practical challenges. You should find this notebook quite straightforward, but take any challenges as a sign that you need to keep practicing since subsequent weeks will build on these foundational concepts."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#learning-outcomes",
    "href": "practicals/Practical-02-Foundations_1.html#learning-outcomes",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou have familiarised yourself with how to add and synchronise new files in Git/GitHub.\nYou have refamiliarised yourself with Python operators and precedence.\nYou have refamiliarised yourself with Python logic and lists.\n\nThese are the foundations for subsequent weeks, so if you are still confused at the end of the practical please look at Code Camp and re-run this practical from scratch if need be!"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#download-the-notebook",
    "href": "practicals/Practical-02-Foundations_1.html#download-the-notebook",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Download the Notebook",
    "text": "Download the Notebook\nSo for this week, visit the Week 2 of FSDS page you’ll see that there is a ‘preview’ link and a a ‘download’ link in the Practical section. If you click the preview link you will be taken to the GitHub page for the notebook where it has been ‘rendered’ as a web page. But to make the notebook useable on your computer, you need to download the IPYNB file.\nSo now:\n\nClick on the Download link.\nThe file should download automatically, but if you see a page of raw code, select File then Save Page As....\nMake sure you know where to find the file (e.g. Downloads or Desktop).\nMove the file to your Git repository folder (e.g. ~/Documents/CASA/fsds/)\nCheck to see if your browser has added .txt to the file name:\n\nIf no, then you can move to adding the file.\nIf yes, then you can either fix the name in the Finder/Windows Explore, or you can do this in the Terminal using mv &lt;name_of_practical&gt;.ipynb.txt &lt;name_of_practical&gt;.ipynb (you can even do this in JupyterLab’s terminal if it’s already running)."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#add-the-notebook-to-git",
    "href": "practicals/Practical-02-Foundations_1.html#add-the-notebook-to-git",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Add the Notebook to Git",
    "text": "Add the Notebook to Git\n\nNow you can add it to Git and synchronise it with GitHub:\n\ngit add &lt;name_of_practical&gt;.ipynb\ngit commit -m \"Adding Practical 2\"\ngit push\n\n\nThe file should now be in your GitHub repository in its ‘original’ format (before you write or run any code)."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#sec-available",
    "href": "practicals/Practical-02-Foundations_1.html#sec-available",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Switch to JupyterLab",
    "text": "Switch to JupyterLab\nI am assuming that you are already running JupyterLab via Podman using the command that we saw in Week 1 and on the CASA Computing Platform. However, if you called podman run ... from the ‘wrong place’ then you might not be able to see the recently added practical in your work folder.\nIt that’s the case, then you will need to stop the container so please see the section below. You need to remember that the startup command is something like:\npodman run ... -v \"$(pwd):/home/jovyan/work\" ...\nThe -v (short for volume) tells Podman what part of your computer should be connected to place in container where we store the work (/home/jovyan/work). pwd is short for ‘print working directory’ and is the location where you ran the startup command! So we’re talking about the location on your computer when you access the work folder from Jupyter Lab:\n\nOn a Mac, if you just opened a new Terminal and run the Podman startup command then it will be your $HOME (also known as ~) directory (e.g. /Users/&lt;your username&gt;/) because that’s where new Terminal windows start by default.\nOn a Windows machine it may be your $HOME directory but we can’t promise.\n\nPerhaps a video will help clarify?\nNote: Docker and Podman are basically the same!"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#is-the-container-running-sec-containers",
    "href": "practicals/Practical-02-Foundations_1.html#is-the-container-running-sec-containers",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Is the Container Running?",
    "text": "Is the Container Running?\nPodman will happily keep a container running in the background even if you close every open window. So how do you know if the sds2025 container is already running? There are two ways:\n\nOpen the Podman Desktop from the menu and make sure that you select the Containers tab. You should see something like this if the container is running and available:\n\n\n\n\nContainer running\n\n\n\nFrom the Terminal you should be able to run: podman ps. This will give you output something like this:\n\n\n\n\nContainer running from Terminal\n\n\nIf the sds2025 container is not running then you’ll need to run the startup command (docker run...) covered in the last practical session. If it is running but in the wrong place, then you should stop it, use cd to navigate to the correct location, and then restart it.\n\nConnect to Jupyter Lab\nOnce you know the container is running you can connected to Jupyter Lab on localhost:8888 and should see something like this:\n\n\n\nScreenshot of JupyterLab\n\n\nYou’re connected!"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#hello-world",
    "href": "practicals/Practical-02-Foundations_1.html#hello-world",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Hello World!",
    "text": "Hello World!\nNearly every introduction to programming course starts with the famous ‘Hello World!’, so why do anything different?\nFrom within Jupyter Lab you should now able to create a new notebook:\n\nClick on the Python 3 tile in the Notebook section.\nYou should see a new tab open with a new notebook (title: Untitled.ipynb).\nIn the first cell type print('Hello World!').\nClick the Run button (▶) in the menu above the notebook.\nYou should see Python output Hello World! below the cell.\n\nAny time you want to run code you click on the right-triangle (▶); it’s in the area between the clipboard 📋 (for copying) and the ■ (for stopping running code).\nSo when you run:\n\nprint('Hello World!')\n\nHopefully, the following appears directly below the code:\nHello World!\n\n\n\n\n\n\nTip\n\n\n\nYou can always click the ▶ icon above, but it will be much faster to get into the habit of type Ctrl+Enter instead when you have placed your cursor in a code cell. This is also how to turn a Markdown cell back into display text.\n\n\nYou can now close this notebook. Unless you created this notebook in the work folder, you will not be able to save it permanently. That’s fine."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#starting-the-practical-notebook",
    "href": "practicals/Practical-02-Foundations_1.html#starting-the-practical-notebook",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Starting the Practical Notebook",
    "text": "Starting the Practical Notebook\nNow from within Jupyter Lab you should start the Practical 2 notebook:\n\nStarting from the work folder, navigate to your repo folder where you saved this notebook (Practical-02-Foundations_1.ipynb).\nDouble-click the notebook file and the notebook should appear on the right-hand side. If all you see is reams of plain text then there is probably still a .txt extension on your notebook file that hasn’t been removed.\n\nNow you can run code directly in your browser, so let’s try it!"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#variable-names",
    "href": "practicals/Practical-02-Foundations_1.html#variable-names",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Variable Names",
    "text": "Variable Names\n\n\n\n\n\n\nNoteLook closer!\n\n\n\nPay attention to the colour of the code, while it might be subtle (a single character in a different colour), it is giving you clues as to where problems might be found because it means the Python ‘interpreter’ is reading what you wrote differently from how you probably meant it…\n\n\nSome of the lines of code below are valid Python, but others may have a problem that will cause Python to generate an error. Each error can be fixed by changing one character. See if you can work out which ones you need to fix before running the code:\n\nQuestion\n\nPi = 3.14159      # Valid Python\npi = 3.14159      # ??\n3pi = 3.14159*3   # ??\npi_2 = 3.14159**2 # ??\npi^2 = 3.14159**2 # ??\nmy radius = 5     # ??\nMy_Radius = 5     # ??\nclass = 5         # ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#variable-types",
    "href": "practicals/Practical-02-Foundations_1.html#variable-types",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Variable Types",
    "text": "Variable Types\nUsing only the values for x, y, and z, what operations allow you to print the following?\n6\n22\n22\n125\n0.4\n\nQuestion\n\nx = '2'\ny = 2\nz = 3\n\nprint(y ?? ) # 6\nprint(x ?? ) # 22\nprint(x ?? ) # 22 (but a different way!)\nprint((y+z) ?? ) # 125\nprint(y ?? ) # 0.4"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#assignment",
    "href": "practicals/Practical-02-Foundations_1.html#assignment",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Assignment",
    "text": "Assignment\nBefore running the code, work out what the values of x, y and z will be after every line of code in the block has been executed.\n\nQuestion\n\nx = 12\ny = 10\n\nz = x + y # ??\nx = x + y # ??\ny = z + y # ??\n\nprint(x)\nprint(y)\nprint(z)\n\nOnce you have worked out what you think x, y and z are, run the code to check your answers!\nMake sure you understand the results you find. Ask someone if you need help to understand."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#operators-precedence",
    "href": "practicals/Practical-02-Foundations_1.html#operators-precedence",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Operators & Precedence",
    "text": "Operators & Precedence\nBefore running the code, work out what the values of x, y and z will be after every line of code in the block has been executed. Feel free to use a calculator.\n\n\n\n\n\n\nTip\n\n\n\nThis question is about what operations (i.e. multiplication, division, powers, etc.) are done first based on the type of operation and the presence of parentheses… it’s the same as it would be for a maths problem!\n\n\n\nQuestion\nBy adding parentheses adjust the equations below to output the following:\n\n2.5\n2.25\n2.5\n\n# Add parentheses to get the outputs above\nprint(1 + 2 * 3 / 4)\nprint(1 + 2 * 3 / 4)\nprint(1 + 2 * 3 / 4)\n\n\nOnce you have calculated what you think x, y and z are, run the code to check."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#test-your-operator-knowledge",
    "href": "practicals/Practical-02-Foundations_1.html#test-your-operator-knowledge",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Test Your Operator Knowledge",
    "text": "Test Your Operator Knowledge\nNow let’s look at some of the stranger operators. Many of these can be very useful in more complex code but can seem a little pointless now.\nWork out what operator should replace the ? in each of the lines of code below to produce the output I’ve shown in the comments. I’ve mixed in ones you have seen above with ones that we’ve not seen before.\n\nQuestion\n\nx = 10\ny = 3\n\nprint( x ?? y ) # 1\nprint( x ?? y ) # 13\nprint( x ?? y ) # False\nprint( x ?? y ) # 1000\nprint( x ?? y ) # 7\nprint( x ?? y ) # 3"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#applying-what-weve-learned",
    "href": "practicals/Practical-02-Foundations_1.html#applying-what-weve-learned",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Applying What We’ve Learned",
    "text": "Applying What We’ve Learned\nNow we are going to take what we’ve learned and apply it in a more ‘abstract’ way: how do we translate some well-known mathematical formulae into code? In particular, I’m interested in the formula for the volume of a sphere (and this gives me a chance to show that Notebooks can show formulae as well!):\n\\[\nV = \\frac{4}{3}\\pi r^{3}\n\\]\n\nCalculate the Volume of a Sphere\nSo, given a sphere with a diameter of 12cm, calculate its volume:\n\n\n\n\n\n\nTip\n\n\n\nI would strongly advise you to Google: python constant pi and look for code that will save you having to write down the value of \\(\\pi\\).\n\n\n\nQuestion\n\nfrom math import ??\nv = ??\nprint(f\"{v:0.3f} cm3\")\n\nI get an answer of 904.779cm\\(^3\\).\n\n\n\nCalculate the Radius of a Sphere\nNow, given a sphere of volume 14,137cm\\(^3\\) calculate its radius as a whole number. The formula for this can be worked out as:\n\\[\\begin{align*}\n\\frac{3}{4}V &= \\pi r^{3} \\\\\n\\frac{3}{4}\\frac{V}{\\pi} &= r^{3} \\\\\n(\\frac{3}{4}\\frac{V}{\\pi})^{1/3} &= r\n\\end{align*}\\]\nIf you can’t remember how to rearrange formulae this would be a good skill to refresh!\n\n\n\n\n\n\nTip\n\n\n\nThere are three ways to get a “whole number” from a float:\n\nWhen you’re starting out, the easiest is to change the variable’s type\nThe next step up is to make use of Google to find out if there are ways of rounding to the nearest integer\nThe third step is to change what’s visible to the user without altering the actual number\n\n\n\nI get an answer of either 14 or 15… can you work out why?\n\nQuestion\n\nfrom math import pi\nv = 14137\nr = ??\nprint(??)"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#working-with-conditions",
    "href": "practicals/Practical-02-Foundations_1.html#working-with-conditions",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Working with Conditions",
    "text": "Working with Conditions\nUse if, elif, and else so that you get the following output:\n\nWhen hours is 10 or more, then the code prints At least 10 hours worked!\nWhen hours is exactly 2, then the code prints Exactly 2 hours worked.\n\nWhen hours is 9 or less but not 2, then the code prints Less than 10 hours worked!\n\n\n\n\n\n\n\nTipHint\n\n\n\nYou will also need to think about the order in which these conditions are tested.\n\n\n\nQuestion\n\nhours = 2\n\nif hours ??:\n    print(\" \")\nelif hours ??:\n    print(\" \")\nelse:\n    print(\" \")"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#flow-control",
    "href": "practicals/Practical-02-Foundations_1.html#flow-control",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Flow Control",
    "text": "Flow Control\nUsing the flow chart shown in the image below as a model, write the code to make this condition work. You will need to complete the code such that it produces the following: 1. When a = 2 and b = 2 four lines of output will be written 2. When a = 1 and b = 2 one line of output will be written\n\n\nQuestion\n\na = 1\nb = 1\n\n# &lt;your code here&gt;\n??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#its-all-quite-logical",
    "href": "practicals/Practical-02-Foundations_1.html#its-all-quite-logical",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "It’s All Quite Logical…",
    "text": "It’s All Quite Logical…\nBefore adding a value for x and running the code below, try to answer the following questions:\n\nQuestion\n\nWhat names are name(s) are printed when x = 5?\nWhat value(s) can x be when the names Joe and Aled are printed?\nWhat name(s) are printed when x = -1?\n\nIs there any value for which all three names will be printed?\n\n\nx = ??\n\nif x &gt; 0 and x &lt; 5:\n    print(\"Joe\")\n\nif x &gt; 0 or x &lt; 5:\n    print(\"Aled\")\n\nif not(x &gt; 0):\n    print(\"Sarah\")"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#logic-contd",
    "href": "practicals/Practical-02-Foundations_1.html#logic-contd",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Logic (Cont’d)",
    "text": "Logic (Cont’d)\nStudy the flow chart below.\n\n\n\n\n\n\n\nTip\n\n\n\nThis will require you to combine logic with one of the operators that we saw earlier. Also note the new iterator that we’ve got here: range(&lt;start&gt;, &lt;stop&gt;) to create a range of numbers between two other numbers.\n\n\nIn the cell below, use the for loop already set up to as a starting point for implementing the flow chart shown above for values of x between 0 and 9.\n\nQuestion\n\nfor x in range(0,9):\n    #... do something...\n    ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#nested-conditionals",
    "href": "practicals/Practical-02-Foundations_1.html#nested-conditionals",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "‘Nested’ Conditionals",
    "text": "‘Nested’ Conditionals\nConditional statements can be nested within one another. That is, Python evaluates the first, or ‘outer’, condition and can then evaluate secondary, or ‘inner’, conditions. The code below shows an example of this.\n\nQuestion\n\nx = 5\ny = 4\n\nif x != y:                                 #line 1\n    print(\"x is not equal to y\")\n    \n    if(x &gt; y):                             #line 4\n        print(\"x is greater than y\")\n    \n    else:                                  #line 7\n        print(\"x is less than y\")\n\nelse:\n    print(\"&lt;insert conclusion here&gt;\")\n\nNote how the indentation makes it easier to work out which ‘level’ (outer or inner condition) the code is operating on. In the code above, lines 4 and 7 are at the same indentation meaning that both will be skipped if the initial condition (on line 1) is False.\nTo check you understand how the code above works:\n\nChange &lt;insert conclusion here&gt; to a string that explains the condition of x and y\nFor x = 2 and y = 3, type what line(s) will be output here: …\n\nGreat! You should now have a pretty good understanding of how conditional and logical operators work. This understanding will be handy in future as we work through other computational concepts."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#whos-in-the-list",
    "href": "practicals/Practical-02-Foundations_1.html#whos-in-the-list",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Who’s in the List?",
    "text": "Who’s in the List?\nHere we are looking to interact with lists in a straightforward way that will help you to understand accessing them using indexes and slices, and searching.\n\nQuestion\n\ncities = ['New York', 'London', 'Beijing', 'Tokyo', 'Delhi']\n\n# Print out London from cities:\nprint( ?? )\n\n# Print out Tokyo using *negative* indexing:\nprint( ?? )\n\n# Print out Beijing *and* Tokyo using a list slice\nprint( ?? )\n\n# Print out London to Delhi using a slice\nprint( ?? ) # You could also do cities[1:5] but this way is neater\n\n# Combine positive and negative indexing to print out London, Beijing and Tokyo using a list slice\nprint( ?? )\n\n# Print out the index for New York by searching for it (i.e. you can't just type 0)\nprint( ?? )"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#manipulating-lists",
    "href": "practicals/Practical-02-Foundations_1.html#manipulating-lists",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Manipulating Lists",
    "text": "Manipulating Lists\nLet’s break a few things…\n\nCreate an IndexError\n\nQuestion\n\n# Cause an 'IndexError: list index out of range' errr\n??\n\n\n\n\nCreate a ValueError\n\nQuestion\n\n# Cause a ValueError using the city of Toronto\n??\n\n\n\n\nSort the List\nSort the list in place in reverse alphabetical order (i.e. z…a) and then print the sorted list\n\nQuestion\n\n??\nprint(cities)\n\nThe output from this should be: [‘Tokyo’, ‘New York’, ‘London’, ‘Delhi’, ‘Beijing’]"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#addingremoving-values",
    "href": "practicals/Practical-02-Foundations_1.html#addingremoving-values",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Adding/Removing Values",
    "text": "Adding/Removing Values\n\nInserting into a List\nAdd the city of Toronto to the list after New York in the sorted list.\n\nQuestion\n\n# Just in case you make a mistake...\ncities = ['Tokyo', 'New York', 'London', 'Delhi', 'Beijing']\n\n??\nprint(cities)\n\nThe output should be: [‘Tokyo’, ‘New York’, ‘Toronto’, ‘London’, ‘Delhi’, ‘Beijing’]\n\n\n\nRemoving from a List\nNow pop New York from the list without specifying its index (i.e. the number 1 should not appear in your code). Print out the value that you popped and the print out the cities list to check you’ve done the right thing…\n\nQuestion\n\n??\nprint(p)\nprint(cities)\n\nThe output should be:\n\nNew York\n[‘Tokyo’, ‘Toronto’, ‘London’, ‘Delhi’, ‘Beijing’]\n\n\n\n\nChecking Lists\nFinally, how can you check if the city of Moscow is in the list and let the user know if it is or is not?\n\nQuestion\n\nif ??\n    ??\nelse:\n    ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#youre-done",
    "href": "practicals/Practical-02-Foundations_1.html#youre-done",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "You’re Done!",
    "text": "You’re Done!\nThis is quite a lot to get through. If you’ve managed it in under 2 hours then congratulations! Either you must have paid a lot of attention when doing Code Camp, or you might want to check in with us as to whether you should really be doing this module…\n\nNo Wait, One More Thing…\nYou now want to add/commit/push your completed notebook to your GitHub reposistory. Using the Terminal you need to:\n\nNavigate to your repository (e.g.$HOME/Documents/CASA/&lt;your repository&gt;).\nCheck the status of your notebooks using git status (you should see that Practical-02-Foundations_1.ipynb has been modified).\nAdd the changes to Git using git add  Practical-02-Foundations_1.ipynb\nCommit this changed notebook with a message using git commit -m \"&lt;your message here...&gt;\"\nPush this change to GitHub using: git push\n\nYou should now be able to visit your repository on GitHub and see that your changes are now stored there as well!"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#find-a-postgis-image",
    "href": "lectures/demo_postgis_and_sds.html#find-a-postgis-image",
    "title": "Running Multiple Containers",
    "section": "Find a PostGIS Image",
    "text": "Find a PostGIS Image\nSearch Quay.io for an image:\n\nOn a modern M-chip Mac you need to use arm64 images,\nOn Windows it’s usually amd64\n\n\n\n\n\n\n\nTip\n\n\nHowever, in most cases you don’t need to search for these explicitly because ‘builds’ for most images are completed for both."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#create-a-pod",
    "href": "lectures/demo_postgis_and_sds.html#create-a-pod",
    "title": "Running Multiple Containers",
    "section": "Create a ‘Pod’",
    "text": "Create a ‘Pod’\nBy default, containers can’t talk to each other for security reasons. We need to join them together in a ‘pod’ to allow this to happen.\nThis command creates a pod that exposes two ‘ports’ (8888 and 5432) to the wider world.\npodman pod create -p 8888:8888 -p 5432:5432 myapp\n\n\n\n\n\n\nTip\n\n\nThis ‘maps’ port 8888 inside the pod to port 8888 outside the pod, but we could change it to -p 7777:8888 so that requests for 7777 from the outside world are ‘forwarded’ to 8888 inside the pod. That would allow copies of containers to run at the same time in different pods."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#attach-postgis-to-the-pod",
    "href": "lectures/demo_postgis_and_sds.html#attach-postgis-to-the-pod",
    "title": "Running Multiple Containers",
    "section": "Attach PostGIS to the Pod",
    "text": "Attach PostGIS to the Pod\nThere’s a lot going on here that took quite some to figure out1, but the key thing turned out to be the pg_hba.conf file which tells Postgres on which ports it can use.\npodman run --rm -d --name postgres --pod myapp \\\n-e POSTGRES_USER=postgres \\\n-e POSTGRES_PASSWORD=test \\\n-e POSTGRES_DB=test \\\n-e PGDATA=/var/lib/postgresql/data/pgdata \\\n-v \"${PWD}\"/data/postgres:/var/lib/postgresql/data \\\n-v /tmp:/tmp \\\n-v \"${PWD}\"/data/postgres/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf \\\nquay.io/taolu/postgis:14-3.5-alpine\n\nThis command is telling Podman to start a postgis image (14-3.5-alpine) it downloads from Quay with a test of startup options set at launch. The first three are specifying the databasei, user, and password. The remainder are connecting various ‘mount points’ on the container to locations on the host computer. So the data that is added to the database will be stored under the current working directory (PWD == Print Working Directory). We allow Postgis to the use the computer’s /tmp folder for working data. And the final bit is taking a local copy that we’ve set up of pg_hba.conf and putting that in the place that a regular Postgres server would expect to find it.\n\nPart of an ESRC research project I was supporting."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#attach-sds-to-the-pod",
    "href": "lectures/demo_postgis_and_sds.html#attach-sds-to-the-pod",
    "title": "Running Multiple Containers",
    "section": "Attach SDS to the Pod",
    "text": "Attach SDS to the Pod\nOnly containers inside the Pod can talk to other containers in the pod. So for the SDS container to talk to PostGIS, they both need to be attached to the pod using myapp.\npodman run --rm -d --name sds --pod=myapp \\\n-v \"$(pwd):/home/jovyan/work\" \\\ndocker.io/jreades/sds:2025-amd \\\nstart.sh \\\njupyter lab --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\nYou can now connect using your browser: http://localhost:8888/\n\n\n\n\n\n\nTip\n\n\nThe rest of this short tutorial is all run on the SDS container using your browser as the interface. This is true even for bits about the command line interface: in Jupyter you pick File &gt; New &gt; Terminal."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#install-psycopg2",
    "href": "lectures/demo_postgis_and_sds.html#install-psycopg2",
    "title": "Running Multiple Containers",
    "section": "Install psycopg2",
    "text": "Install psycopg2\nIf I haven’t had time to update the SDS container then you can do this on the SDS Terminal in your browser using the folllowing command:\npip install psycopg2`\nThis is because the sqlalchemy framework is already there but the psycopg2 driver for Postgres isn’t."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#load-data",
    "href": "lectures/demo_postgis_and_sds.html#load-data",
    "title": "Running Multiple Containers",
    "section": "Load Data",
    "text": "Load Data\n\n\n\n\n\n\nRun Python\n\n\nNow you will start a new Notebook (File &gt; New &gt; Notebook) and create code cells for each of the following sections of code.\n\n\n\n# Connect to the database\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://postgres:test@localhost:5432/test')\n\n# Insert data into a new table\nimport geopandas as gpd\ngdf = gpd.read_file('work/data/src/TM_WORLD_BORDERS-0.3.gpkg')\ngdf.to_postgis('world', engine)"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#querying-data",
    "href": "lectures/demo_postgis_and_sds.html#querying-data",
    "title": "Running Multiple Containers",
    "section": "Querying Data",
    "text": "Querying Data\n# Get information about the database\ninsp = inspect(engine) \ninsp.get_table_names()\n# Get data out of the database\nimport geopandas as gpd\n\n# Simple query\ngdf = gpd.read_postgis('SELECT * FROM msoa', geom_col='geometry', con=engine)\n\n# What's in the table\ngdf.head(2)"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#plotting-the-data",
    "href": "lectures/demo_postgis_and_sds.html#plotting-the-data",
    "title": "Running Multiple Containers",
    "section": "Plotting the Data",
    "text": "Plotting the Data\nSince PostGIS is geospatial and Python can ‘talk’ to PostGIS:\n# Plotting the table\ngdf.plot()"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#more-complex-queries",
    "href": "lectures/demo_postgis_and_sds.html#more-complex-queries",
    "title": "Running Multiple Containers",
    "section": "More Complex Queries",
    "text": "More Complex Queries\n# Query the database\ngdf = gpd.read_postgis(\"\"\"\n    SELECT * \n    FROM msoa \n    WHERE \"MSOA21NM\" \n    LIKE 'Waltham%%'\n\"\"\", geom_col='geometry', con=con)\ngdf.plot()"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#querying-the-data-without-python",
    "href": "lectures/demo_postgis_and_sds.html#querying-the-data-without-python",
    "title": "Running Multiple Containers",
    "section": "Querying the Data without Python",
    "text": "Querying the Data without Python\nOne final thing: if you run a Terminal on your computer (so not in the SDS terminal any more) you can directly query the data.\npsql -h localhost -p 5432 -U postgres -d test\n\n\n\n\n\n\nTip\n\n\nThe ‘host’ computer is the only other machine that can access the pod.\n\n\n\nSELECT * FROM world LIMIT 0;\nSELECT \"NAME\", \"ISO3\", \"POP2005\", \"REGION\" FROM world LIMIT 5;\nSELECT \"NAME\", \"POP2005\" FROM world WHERE AREA &gt; 900000;"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#section",
    "href": "lectures/9.2-Linking_Spatial_Data.html#section",
    "title": "Linking (Spatial) Data",
    "section": "",
    "text": "Data Set 1\n\n\n\nSensorID\nLatitude\nLongitude\n\n\n\n\n1\n51.5070\n-0.1347\n\n\n2\n51.5071\n-0.0042\n\n\n3\n51.5074\n-0.1223\n\n\n4\n51.5073\n-0.1122\n\n\n5\n51.5072\n0.1589\n\n\n\ndf = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Latitude': [51.5070, 51.5071, 51.5074, 51.5073, 51.5073],\n  'Longitude': [-0.1347, -0.0042, -0.1223, -0.1122, 0.1589]\n})\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1\nTemperature\n5ºC\n\n\n1\nHumidity\n15%\n\n\n3\nTemperature\n7ºC\n\n\n4\nTemperature\n7ºC\n\n\n6\nHumidity\n18%\n\n\n\ndf1 = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Parameter': ['Temperature','Humidity','Temperature','Temperature','Humidity'], \n  'Value': ['5ºC', '15%', '7ºC', '7ºC', '18%']\n})\n\n\n\nObviously, we can use non-spatial operations on spatial data sets.\n\n\nThis wouldn’t be a particularly good design for a data structure… why?"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking (Spatial) Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking (Spatial) Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking (Spatial) Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking (Spatial) Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking (Spatial) Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking (Spatial) Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking (Spatial) Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking (Spatial) Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\nA. Here’s how I’d do this:\n\nCity = New York (probably a key)\n\nCuisine = Italian (probably a key)\n\nStyle = Family (probably an enumeration/free text)\n\nLocation = Within Distance of X from Request (probably a buffered spatial query)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking (Spatial) Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking (Spatial) Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server.\n\n\n\n\n\nSee also Carto and competitors."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking (Spatial) Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich, but £££ for commercial use.\nMySQL: free, unless you want dedicated support; was feature-poor (though this looks to have changed with MySQL8); heavyweight.\nPostgreSQL: free, unless you want dedicated support; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\nGeoParquet+DuckDB: not as full-featured as Postgres, but evolving quickly and much simpler to configure.\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#dummy-variables",
    "href": "lectures/8.4-Analysing_Text.html#dummy-variables",
    "title": "Analysing Text",
    "section": "Dummy Variables",
    "text": "Dummy Variables\nThe concept of ‘dummy variables’ in economics/regression is a useful point to start thinking about text:\n\n\n\nTopic\nDummy\n\n\n\n\nNews\n0\n\n\nCulture\n1\n\n\nPolitics\n2\n\n\nEntertainment\n3\n\n\n\n\nWhat’s the problem with this approach when you’re thinking about the topics in a document? You either have to assign each document to one, and only one, topic, or you need a lot of dummies."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#one-hot-encoders",
    "href": "lectures/8.4-Analysing_Text.html#one-hot-encoders",
    "title": "Analysing Text",
    "section": "One-Hot Encoders",
    "text": "One-Hot Encoders\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n1\n1\n0\n1\n\n\nCulture item\n0\n1\n1\n0\n\n\nPolitics item\n1\n0\n0\n1\n\n\nEntertainment item\n1\n1\n1\n1\n\n\n\n\nOne-Hot encoders are not often used this way, but for keyword detection or keyword-based classification this might be appropriate: i.e. this keyword was used in this document!\nSo the big difference is One Hot == \\(n\\) variables, Dummy == \\(n-1\\).\nDefinitely some ‘gotchas’ in deployment: one-hot models shouldn’t have an intercept unless you apply a ‘ridge shrinkage penalty’. Standardisation affects whether or not an intercept is needed."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#the-bag-of-words",
    "href": "lectures/8.4-Analysing_Text.html#the-bag-of-words",
    "title": "Analysing Text",
    "section": "The ‘Bag of Words’",
    "text": "The ‘Bag of Words’\nJust like a one-hot (binarised approach) on preceding slide but now we count occurences:\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n4\n2\n0\n6\n\n\nCulture item\n0\n4\n7\n0\n\n\nPolitics item\n3\n0\n0\n3\n\n\nEntertainment item\n3\n4\n8\n1"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#bow-in-practice",
    "href": "lectures/8.4-Analysing_Text.html#bow-in-practice",
    "title": "Analysing Text",
    "section": "BoW in Practice",
    "text": "BoW in Practice\nEnter, stage left, scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\n# Non-reusable transformer\nvectors = vectorizer.fit_transform(texts)\n\n# Reusable transformer\nvectorizer.fit(texts)\nvectors1 = vectorizer.transform(texts1)\nvectors2 = vectorizer.transform(texts2)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'All vectors: {vectors.toarray()}')"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#tfidf",
    "href": "lectures/8.4-Analysing_Text.html#tfidf",
    "title": "Analysing Text",
    "section": "TF/IDF",
    "text": "TF/IDF\nBuilds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:\n\\[\nW(t,d) = TF(t,d) / log(N/DF_{t})\n\\]\nFor example:\n\nIf the term ‘cat’ appears 3 times in a document of 100 words then Term Frequency given by: \\(TF(t,d)=3/100\\), and\nIf there are 10,000 documents and cat appears in 1,000 documents then Normalised Document Frequency given by: \\(N/DF_{t}=10000/1000\\) so the Inverse Document Frequency is \\(log(10)=1\\),\nSo IDF=1 and TF/IDF=0.03."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#tfidf-in-practice",
    "href": "lectures/8.4-Analysing_Text.html#tfidf-in-practice",
    "title": "Analysing Text",
    "section": "TF/IDF in Practice",
    "text": "TF/IDF in Practice\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Non-reusable form:\nvectors=vectorizer.fit_transform(texts)\n\n# Reusable form:\nvectorizer.fit(texts)\nvectors = vectorizer.transform(texts)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'Full vector: {vectors.toarray()}')\n\nWhat do you notice about how this code differs from the CountVectorizer?"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "href": "lectures/8.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "title": "Analysing Text",
    "section": "Term Co-Occurence Matrix (TCM)",
    "text": "Term Co-Occurence Matrix (TCM)\nThree input texts with a distance weighting (\\(d/2\\), where \\(d&lt;3\\)):\n\nthe cat sat on the mat\nthe cat sat on the fluffy mat\nthe fluffy ginger cat sat on the mat\n\n\n\n\n\nfluffy\nmat\nginger\nsat\non\ncat\nthe\n\n\n\n\nfluffy\n\n1\n1\n\n0.5\n0.5\n2.0\n\n\nmat\n\n\n\n\n0.5\n\n1.5\n\n\nginger\n\n\n\n0.5\n0.5\n1.0\n1.5\n\n\nsat\n\n\n\n\n3.0\n3.0\n2.5\n\n\non\n\n\n\n\n\n1.5\n3.0\n\n\ncat\n\n\n\n\n\n\n2.0\n\n\nthe"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#how-big-is-a-tcm",
    "href": "lectures/8.4-Analysing_Text.html#how-big-is-a-tcm",
    "title": "Analysing Text",
    "section": "How Big is a TCM?",
    "text": "How Big is a TCM?\nThe problem:\n\nA corpus with 10,000 words has a TCM of size \\(10,000^{2}\\) (100,000,000)\nA corpus with 50,000 words has a TCM of size \\(50,000^{2}\\) (2,500,000,000)\n\nCleaning is necessary, but it’s not sufficient to create a tractable TCM on a large corpus.\n\nAlthough usually used in the context of clustering, there’s also a curse of dimensionality here!"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#enter-document-embeddings",
    "href": "lectures/8.4-Analysing_Text.html#enter-document-embeddings",
    "title": "Analysing Text",
    "section": "Enter Document Embeddings",
    "text": "Enter Document Embeddings\nTypically, some kind of 2 or 3-layer neural network that ‘learns’ how to embed the TCM into a lower-dimension representation: from \\(m \\times m\\) to \\(m \\times n, n &lt;&lt; m\\).\nSimilar to PCA in terms of what we’re trying to achieve, but the process is utterly different.\n\nMany different approaches, but GloVe (Stanford), word2vec (Google), fastText (Facebook), and ELMo (Allen) or BERT (Google) are probably the best-known."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#sentiment-analysis",
    "href": "lectures/8.4-Analysing_Text.html#sentiment-analysis",
    "title": "Analysing Text",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nRequires us to deal in great detail with bi- and tri-grams because negation and sarcasm are hard. Also tends to require training/labelled data.\n\nSource."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#clustering",
    "href": "lectures/8.4-Analysing_Text.html#clustering",
    "title": "Analysing Text",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\nCluster\nGeography\nEarth Science\nHistory\nComputer Science\nTotal\n\n\n\n\n1\n126\n310\n104\n11,018\n11,558\n\n\n2\n252\n10,673\n528\n126\n11,579\n\n\n3\n803\n485\n6,730\n135\n8,153\n\n\n4\n100\n109\n6,389\n28\n6,626\n\n\nTotal\n1,281\n11,577\n13,751\n11,307\n37,916"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#topic-modelling",
    "href": "lectures/8.4-Analysing_Text.html#topic-modelling",
    "title": "Analysing Text",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nLearning associations of words (or images or many other things) to hidden ‘topics’ that generate them:"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#word-clouds",
    "href": "lectures/8.4-Analysing_Text.html#word-clouds",
    "title": "Analysing Text",
    "section": "Word Clouds",
    "text": "Word Clouds"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#additional-resources",
    "href": "lectures/8.4-Analysing_Text.html#additional-resources",
    "title": "Analysing Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOne-Hot vs Dummy Encoding\nCategorical encoding using Label-Encoding and One-Hot-Encoder\nCount Vectorization with scikit-learn\nCorpus Analysis with Spacy\nThe TF*IDF Algorithm Explained\nHow to Use TfidfTransformer and TfidfVectorizer\nSciKit Learn Feature Extraction\nYour Guide to LDA\nMachine Learning — Latent Dirichlet Allocation LDA\nA Beginner’s Guide to Latent Dirichlet Allocation(LDA)\nAnalyzing Documents with TF-IDF\n\nBasically any of the lessons on The Programming Historian."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#additional-resources-1",
    "href": "lectures/8.4-Analysing_Text.html#additional-resources-1",
    "title": "Analysing Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nIntroduction to Word Embeddings\nThe Current Best of Universal Word Embeddings and Sentence Embeddings\nUsing GloVe Embeddings\nWorking with Facebook’s FastText Library\nWord2Vec and FastText Word Embedding with Gensim\nSentence Embeddings. Fast, please!\n\n\n\nPlasticityAI Embedding Models\nClustering text documents using k-means\nTopic extraction with Non-negative Matrix Factorization and LDA\nTopic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#can-we-describe-text",
    "href": "lectures/8.2-Patterns_in_Text.html#can-we-describe-text",
    "title": "Patterns in Text",
    "section": "Can We Describe Text?",
    "text": "Can We Describe Text?\nConsider the following character sequences:\n\nfoo@bar.com\nhttps://www.ucl.ac.uk/bartlett/casa/\n(555) 102-1111\nE17 5RS\nNow, fair Hippolyta, our nuptial hour / Draws on apace. Four happy days bring in / Another moon. But, oh, methinks how slow / This old moon wanes. She lingers my desires, / Like to a stepdame or a dowager / Long withering out a young man’s revenue. (I.i.)\n\n\nWe need ways to distinguish: Upper and Lower Case, Digits, Space Characters, Other Characters, Repetition, Type… Can you do those with strings alone?"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "href": "lectures/8.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "title": "Patterns in Text",
    "section": "Strings Methods are Not Enough",
    "text": "Strings Methods are Not Enough\n'123foo456'.index('foo') # 2\n'123foo456'.split('foo') # ['123', '456']\n' 123 foo 456 '.strip()  # '123 foo 456'\n'HOW NOW BROWN COW?'.lower() # 'how now brown cow?'\n'How now brown cow?'.replace('brown ','green-')\n# 'How now green-cow?'\nSee: dir(str) for full list of string methods."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#regular-expressions",
    "href": "lectures/8.2-Patterns_in_Text.html#regular-expressions",
    "title": "Patterns in Text",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegexes are a way for talking about patterns observed in text, although their origins are rooted in philosophy and linguistics.\nImplemented in Python as:\nimport re\n# re.search(&lt;regex&gt;, &lt;str&gt;)\ns = '123foo456'\nif re.search('123',s):\n  print(\"Found a match.\")\nelse:\n  print(\"No match.\")\nPrints 'Found a match.'"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#capturing-matches",
    "href": "lectures/8.2-Patterns_in_Text.html#capturing-matches",
    "title": "Patterns in Text",
    "section": "Capturing Matches",
    "text": "Capturing Matches\nm = re.search('123',s)\nprint(m.start())\nprint(m.end())\nprint(m.span())\nprint(m.group())\nOutputs:\n0\n3\n(0,3)\n123\n\nSo, we have None if a search fails, but if it succeeds then we have attributes of the match objection like start, end, span, and group (this last is going to be particularly interesting since it tells us what matched)."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#configuring-matches",
    "href": "lectures/8.2-Patterns_in_Text.html#configuring-matches",
    "title": "Patterns in Text",
    "section": "Configuring Matches",
    "text": "Configuring Matches\ns = '123foo456'\nm = re.search('FOO',s)\nprint(m)\nm = re.search('FOO',s,re.IGNORECASE)\nprint(m)\nOutputs:\nNone\n&lt;re.Match object; span=(3, 6), match='foo'&gt;\nThe third parameter allows us to: match newlines (re.DOTALL), ignore case (re.IGNORECASE), take language into account (re.LOCALE), match across lines (re.MULTILINE), and write patterns across multiple lines (re.VERBOSE). If you need multiple options it’s re.DOTALL | re.IGNORECASE. Bitwise again!"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#more-than-one-match",
    "href": "lectures/8.2-Patterns_in_Text.html#more-than-one-match",
    "title": "Patterns in Text",
    "section": "More Than One Match",
    "text": "More Than One Match\ns = '123foo456foo789'\nlst = re.findall('foo',s)\nprint(lst)\nlst = re.finditer('foo',s)\n[x for x in lst]\nrs  = re.sub('foo',' ',s)\nprint(rs)\nrs  = re.split(' ',rs)\nprint(rs)\nOutputs:\n['foo','foo']\n[&lt;re.Match object; span=(3, 6), match='foo'&gt;, &lt;re.Match object; span=(9, 12), match='foo'&gt;]\n'123 456 789'\n['123', '456', '789']"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "href": "lectures/8.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "title": "Patterns in Text",
    "section": "Regular Expressions Do Much More",
    "text": "Regular Expressions Do Much More\nimport re\nm = re.search(r'\\$((\\d+,){2,}\\d+)',\n        \"'That will be $1,000,000 he said...'\")\nprint(m.group(1)) # '1,000,000'\nThis looks for sequences of 1-or-more digits followed by a comma… and for those sequences to repeat two or more times:\n# Look for a literal '$'\nre.search(r'\\$') \n# Group of &gt;=1 digits followed by a comma...\nre.search(r'(\\d+,)') \n# Repeated two or more times...\nre.search(r'(\\d+,){2,}') \n\n\nAlso notice the r'&lt;regex&gt;' with an r in front of the string. This means ‘raw’ and is often a required modifier for regular expression patterns. Simple ones don’t need it, but from here on out you will."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#character-classes",
    "href": "lectures/8.2-Patterns_in_Text.html#character-classes",
    "title": "Patterns in Text",
    "section": "Character Classes",
    "text": "Character Classes\n\n\n\n\n\n\n\n\nCharacters\nRegex Meta Class Options\n‘Antonyms’\n\n\n\n\na…z\n[a-z], \\w (word-like characters)\n[^a-z], \\W\n\n\nA…Z\n[A-Z], \\w (word-like characters)\n[^A-Z], \\W\n\n\n0…9\n[0-9], \\d (digits)\n[^0-9], \\D\n\n\n' ', \\n, \\t, \\r, \\f, \\v\n\\s\n\\S\n\n\n., [, ], +, $, ^, \\|, {, }, *, (, ), ?\nFor safety always precede character with a \\.\nNone\n\n\n\n\n\n\\w will include _. And \\ is, once again, important as it ‘escapes’ various characters, and options."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#metacharacters",
    "href": "lectures/8.2-Patterns_in_Text.html#metacharacters",
    "title": "Patterns in Text",
    "section": "Metacharacters",
    "text": "Metacharacters\n\n\n\nMetacharacter\nMeaning\nExample\n\n\n\n\n.\nAny character at all\nc.t\n\n\n^\nStart of a string/line\n^start\n\n\n$\nEnd of a string/line\nend$\n\n\n*\n0 or more of something\n-*\n\n\n+\n1 or more of something\n-+\n\n\n?\n0 or 1 of something; also lazy modifier\n,?\n\n\n{m,n}\nRepeat between m and n times\n\\d{1,4}\n\n\n[ ]\nA set of character literals\n[1-5]\n\n\n( )\nGroup/remember this sequence of characters\n(\\d+)\n\n\n|\nOr\n(A|B)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#building-blocks",
    "href": "lectures/8.2-Patterns_in_Text.html#building-blocks",
    "title": "Patterns in Text",
    "section": "Building Blocks",
    "text": "Building Blocks\n\n\n\n\n\n\n\nRegex\nInterpretation\n\n\n\n\nr'\\s*'\n0 or more spaces\n\n\nr'\\d+'\n1 or more digits\n\n\nr'[A-Fa-f0-7]{5}'\nExactly 5 hexadecimal ‘digits’\n\n\nr'\\w+\\.\\d{2,}'\n1 or more ‘wordish’ characters, followed by a full-stop, then 2 or more digits\n\n\nr'^[^@]+@\\w+'\nOne more non-@ characters at the start of a line, followed by a ‘@’ then 1 or more ‘wordish’ characters.\n\n\nr'(uk|eu|fr)$'\nThe characters ‘uk’ or ‘eu’ or ‘fr’ at the end of a line."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#exploring",
    "href": "lectures/8.2-Patterns_in_Text.html#exploring",
    "title": "Patterns in Text",
    "section": "Exploring",
    "text": "Exploring\nRegex101 can be a useful way to build a regex interactively:"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^[^@]+@([a-z0-9\\-]+\\.){1,5}[a-z0-9\\-]+$', s)\n\ns should be replaced with any string you want to check."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-1",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-1",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'\\d{4}-\\d{2}-\\d{2}', s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-2",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-2",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^\\s*$', s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-3",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-3",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^(http|https|ftp):[\\/]{2}([a-zA-Z0-9\\-]+\\.){1,4}[a-zA-Z]{2,5}(:[0-9]+)?\\/?([a-zA-Z0-9\\-\\._\\?\\'\\/\\\\\\+\\&\\%\\$#\\=~]*)',s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-4",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-4",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})',s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#to-help",
    "href": "lectures/8.2-Patterns_in_Text.html#to-help",
    "title": "Patterns in Text",
    "section": "To Help…",
    "text": "To Help…\nre.VERBOSE to the rescue:\nregex = r\"\"\"\n([GIR] 0[A]{2})|    # Girobank \n(\n  (\n    ([A-Z][0-9]{1,2})| # e.g A00...Z99\n      (\n        ([A-Z][A-HJ-Y][0-9]{1,2})|  # e.g. AB54...ZX11\n          (([A-Z][0-9][A-Z])|  # e.g. A0B...Z9Z \n          ([A-Z][A-HJ-Y][0-9][A-Z]?))  # e.g. WC1 or WC1H\n        )\n      )\n    \\s?[0-9][A-Z]{2} # e.g. 5RX\n  )\n\"\"\"\nre.match(regex,s,re.VERBOSE|re.IGNORECASE) # Can also use: re.X|re.I\n\nThis is the government’s own regex but is probably not 100% accurate."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "href": "lectures/8.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "title": "Patterns in Text",
    "section": "Applications of Regular Expressions",
    "text": "Applications of Regular Expressions\nIf our problem follows some set of articulable rules about permissible sequences of characters then we can probably validate it using a regex:\n\n\n\n\n\n\n\nExamples\nMore Examples\n\n\n\n\nEmail\nPassword\n\n\nPostcode\nPhone number\n\n\nDate\nCredit cards\n\n\nWeb scraping\nSyntax highlighting\n\n\nSentence structure\nData wrangling\n\n\nSearching for/withinfiles/content\nLexical analysis/Language detection\n\n\n\n\nThese are all good problems…"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#additional-resources",
    "href": "lectures/8.2-Patterns_in_Text.html#additional-resources",
    "title": "Patterns in Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRegexle\nRegex 101\nPython Documentation\nReal Python: Regular Expressions 1\nReal Python: Regular Expressions 2\nData Camp RegEx Tutorial\nIntroduction to Regex\nUnderstanding RegExes in Python\nDemystifying RegExes in Python\nPython RegExes\nMastering String Methods in Python\n\nThanks to Yogesh Chavan and Nicola Pietroluongo for examples."
  },
  {
    "objectID": "lectures/7.5-ESDA.html#getting-spatial-with-boroughs",
    "href": "lectures/7.5-ESDA.html#getting-spatial-with-boroughs",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Getting Spatial (with Boroughs)",
    "text": "Getting Spatial (with Boroughs)\nimport geopandas as gpd\nurl = 'https://bit.ly/3neINBV'\nboros = gpd.read_file(url, driver='GPKG')\nboros.plot(color='none', edgecolor='red');"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#convex-hull",
    "href": "lectures/7.5-ESDA.html#convex-hull",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Convex Hull",
    "text": "Convex Hull\nboros['hulls'] = boros.geometry.convex_hull\nboros = boros.set_geometry('hulls')\nboros.plot(column='NAME', categorical=True, alpha=0.5);"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#dissolve",
    "href": "lectures/7.5-ESDA.html#dissolve",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Dissolve",
    "text": "Dissolve\nboros['region'] = 'London'\nboros = boros.set_geometry('geometry') # Set back to original geom\nldn   = boros.dissolve(by='region')    # And dissolve to a single poly\n\nf,ax = plt.subplots(figsize=(10,8))    # New plot\nldn.plot(ax=ax)                        # Add London layer to axis"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#simplify",
    "href": "lectures/7.5-ESDA.html#simplify",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Simplify",
    "text": "Simplify\nldn.simplify(500).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#buffer",
    "href": "lectures/7.5-ESDA.html#buffer",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Buffer",
    "text": "Buffer\nldn.buffer(500).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#buffer-simplify",
    "href": "lectures/7.5-ESDA.html#buffer-simplify",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Buffer & Simplify",
    "text": "Buffer & Simplify\nldn.buffer(1000).simplify(1000).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#difference",
    "href": "lectures/7.5-ESDA.html#difference",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Difference",
    "text": "Difference\nAnd some nice chaining…\nldn.buffer(3000).simplify(2500).difference(ldn.geometry).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#legendgrams",
    "href": "lectures/7.5-ESDA.html#legendgrams",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Legendgrams",
    "text": "Legendgrams\n\n\n\nCode begins on next slide."
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-legendgrams",
    "href": "lectures/7.5-ESDA.html#implementing-legendgrams",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing Legendgrams",
    "text": "Implementing Legendgrams\nimport pysal as ps\n# https://github.com/pysal/mapclassify\nimport mapclassify as mc\n# https://jiffyclub.github.io/palettable/\nimport palettable.matplotlib as palmpl\nfrom legendgram import legendgram\n\nf,ax = plt.subplots(figsize=(10,8))\ngdf.plot(column='price', scheme='Quantiles', cmap='magma', k=5, ax=ax)\nq = mc.Quantiles(gdf.price.array, k=5)\n\n# https://github.com/pysal/legendgram/blob/master/legendgram/legendgram.py\nlegendgram(f, ax, \n               gdf.price, q.bins, pal=palmpl.Magma_5,\n               legend_size=(.4,.2), # legend size in fractions of the axis\n               loc = 'upper left', # mpl-style legend loc\n               clip = (0,500), # clip range of the histogram\n               frameon=True)\n\nNote that the number of colours need to match k, which is 5 in this case.\nIt should be possible to set up the colormap and bins such that they can be passed to both GeoPandas and Legendgram."
  },
  {
    "objectID": "lectures/7.5-ESDA.html#knn-weights",
    "href": "lectures/7.5-ESDA.html#knn-weights",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "KNN Weights",
    "text": "KNN Weights"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-knn",
    "href": "lectures/7.5-ESDA.html#implementing-knn",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing KNN",
    "text": "Implementing KNN\nfrom pysal.lib import weights\nw = weights.KNN.from_dataframe(gdf, k=3)\ngdf['w_price'] = weights.lag_spatial(w, gdf.price)\ngdf[['name','price','w_price']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nname\nprice\nw_price\n\n\n\n\n83\nSouthfields Home\n85.0\n263.0\n\n\n53\nFlat in Islington, Central London\n55.0\n190.0\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\n470.0\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\n186.0\n\n\n44\nAvni Kensington Hotel\n430.0\n821.0"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#spatial-lag-of-distance-band",
    "href": "lectures/7.5-ESDA.html#spatial-lag-of-distance-band",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Spatial Lag of Distance Band",
    "text": "Spatial Lag of Distance Band"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-db",
    "href": "lectures/7.5-ESDA.html#implementing-db",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing DB",
    "text": "Implementing DB\nw2 = weights.DistanceBand.from_dataframe(gdf, threshold=2000, alpha=-0.25)\ngdf['price_std'] = (gdf.price - gdf.price.mean()) / gdf.price.std()\ngdf['w_price_std'] = weights.lag_spatial(w2, gdf.price_std)\ngdf[['name','price_std','w_price_std']].sample(5, random_state=42)\n\n\n\n\nname\nprice_std\nw_price_std\n\n\n\n\n83\nSouthfields Home\n-0.27\n0.00\n\n\n53\nFlat in Islington, Central London\n-0.51\n-0.58\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n0.83\n0.46\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n-0.07\n-0.82\n\n\n44\nAvni Kensington Hotel\n2.52\n3.25"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#morans-i",
    "href": "lectures/7.5-ESDA.html#morans-i",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Moran’s I",
    "text": "Moran’s I\nmi = esda.Moran(gdf['price'], w)\nprint(f\"{mi.I:0.4f}\")\nprint(f\"{mi.p_sim:0.4f}\")\nmoran_scatterplot(mi)"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#local-morans-i",
    "href": "lectures/7.5-ESDA.html#local-morans-i",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-local-morans-i",
    "href": "lectures/7.5-ESDA.html#implementing-local-morans-i",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing Local Moran’s I",
    "text": "Implementing Local Moran’s I\nlisa = esda.Moran_Local(gdf.price, w)\n# Break observations into significant or not\ngdf['sig'] = lisa.p_sim &lt; 0.05\n# Store the quadrant they belong to\ngdf['quad'] = lisa.q\ngdf[['name','price','sig','quad']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nname\nprice\nsig\nquad\n\n\n\n\n83\nSouthfields Home\n85.0\nFalse\n3\n\n\n53\nFlat in Islington, Central London\n55.0\nFalse\n3\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\nFalse\n1\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\nFalse\n3\n\n\n44\nAvni Kensington Hotel\n430.0\nFalse\n1"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#full-lisa",
    "href": "lectures/7.5-ESDA.html#full-lisa",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Full LISA",
    "text": "Full LISA\nplot_local_autocorrelation(lisa, gdf, 'price')"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#additional-resources",
    "href": "lectures/7.5-ESDA.html#additional-resources",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\nThere’s so much more to find, but:\n\nPandas Reference\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nVisualising Missing Data\nChoosing Map Colours"
  },
  {
    "objectID": "lectures/7.3-EDA.html#epicyclic-feedback",
    "href": "lectures/7.3-EDA.html#epicyclic-feedback",
    "title": "ExploratoryData Analysis",
    "section": "Epicyclic Feedback",
    "text": "Epicyclic Feedback\nPeng and Matsui, The Art of Data Science, p.8\n\n\n\n\n\n\n\n\n\n\nSet Expectations\nCollect Information\nRevise Expectations\n\n\n\n\nQuestion\nQuestion is of interest to audience\nLiterature search/experts\nSharpen question\n\n\nEDA\nData are appropriate for question\nMake exploratory plots\nRefine question or collect more data\n\n\nModelling\nPrimary model answers question\nFit secondary models / analysis\nRevise model to include more predictors\n\n\nInterpretation\nInterpretation provides specific and meaningful answer\nInterpret analyses with focus on effect and uncertainty\nRevise EDA and/or models to provide more specific answers\n\n\nCommunication\nProcess & results are complete and meaningful\nSeek feedback\nRevises anlyses or approach to presentation"
  },
  {
    "objectID": "lectures/7.3-EDA.html#approaching-eda",
    "href": "lectures/7.3-EDA.html#approaching-eda",
    "title": "ExploratoryData Analysis",
    "section": "Approaching EDA",
    "text": "Approaching EDA\nThere’s no hard and fast way of doing EDA, but as a general rule you’re looking to:\n\nClean\nCanonicalise\nClean More\nVisualise & Describe\nReview\nClean Some More\n…\n\nThe ‘joke’ is that 80% of Data Science is data cleaning.\n\nCleaning Part 1: testing validity of records (possibly while tracking rejected records for subsequent analysis)\nCanonicalisation: controling for variation (e.g. typos, capitalisation, formatting, leading/trailing whitespace, different types of NULL values, etc.) and in a spatial context deal with projection and geo-data issues.\nCleaning Part 2: further testing of records (e.g. deciding what to do with NaNs, missing values, outside of study area, etc.)\nVisualise & Describe: covered in QM but we’ll take a high-level look at this."
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-related-take",
    "href": "lectures/7.3-EDA.html#a-related-take",
    "title": "ExploratoryData Analysis",
    "section": "A Related Take",
    "text": "A Related Take\nEDA—Don’t ask how, ask what:\n\nDescriptive Statistics: get a high-level understanding of your dataset.\nMissing values: come to terms with how bad your dataset is.\nDistributions and Outliers: and why countries that insist on using different units make our jobs so much harder.\nCorrelations: and why sometimes even the most obvious patterns still require some investigating."
  },
  {
    "objectID": "lectures/7.3-EDA.html#another-take",
    "href": "lectures/7.3-EDA.html#another-take",
    "title": "ExploratoryData Analysis",
    "section": "Another Take",
    "text": "Another Take\nHere’s another view of how to do EDA:\n\nPreview data randomly and substantially\nCheck totals such as number of entries and column types\nCheck nulls such as at row and column levels\nCheck duplicates: do IDs recurr, did the servers fail\nPlot distribution of numeric data (univariate and pairwise joint distribution)\nPlot count distribution of categorical data\nAnalyse time series of numeric data by daily, monthly and yearly frequencies"
  },
  {
    "objectID": "lectures/7.3-EDA.html#getting-started",
    "href": "lectures/7.3-EDA.html#getting-started",
    "title": "ExploratoryData Analysis",
    "section": "Getting Started",
    "text": "Getting Started\nYou can follow along by loading the Inside Airbnb sample:\nimport pandas as pd\nimport geopandas as gpd\nurl='https://bit.ly/3I0XDrq'\ndf = pd.read_csv(url)\ndf.set_index('id', inplace=True)\ndf['price'] = df.price.str.replace('$','',regex=False).astype('float')\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.to_file('Airbnb_Sample.gpkg', driver='GPKG')\n\nNote that this (re)loads the sampled Airbnb data from GitHub every time you run it. For a large data set on someone else’e server you might want to save and (re)load it locally. A simple helper function would then just check if the file already existed locally before trying to download the file again: that would allow you to work while offline and speed up your code substantially too!"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-can-we-do-series",
    "href": "lectures/7.3-EDA.html#what-can-we-do-series",
    "title": "ExploratoryData Analysis",
    "section": "What Can We Do? (Series)",
    "text": "What Can We Do? (Series)\nThis is by no means all that we can do…\n\nSeries-level Methods.\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(f\"Host count is {gdf.host_name.count()}\")\nprint(f\"Mean is {gdf.price.mean():.0f}\")\nprint(f\"Max price is {gdf.price.max()}\")\nprint(f\"Min price is {gdf.price.min()}\")\nprint(f\"Median price is {gdf.price.median()}\")\nprint(f\"Standard dev is {gdf.price.std():.2f}\")\nprint(f\"25th quantile is {gdf.price.quantile(q=0.25)}\")\nCount of non-nulls\nMean\nHighest value\nLowest value\nMedian\nStandard deviation\n25th quantile"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-can-we-do-data-frame",
    "href": "lectures/7.3-EDA.html#what-can-we-do-data-frame",
    "title": "ExploratoryData Analysis",
    "section": "What Can We Do? (Data Frame)",
    "text": "What Can We Do? (Data Frame)\n\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(df.mean())\nprint(df.count())\nprint(df.max())\n# ...\nprint(df.corr())\nprint(df.describe())\nMean of each column\nNumber of non-null values in each column\nHighest value in each column\n$\\vdots$\nCorrelation between columns\nSummarise\n\n\n\n\nNotice how we have the same functionality, but it operates at the level of the data set itself now. We gain a few new functions as well that relate to interactions between columns (a.k.a. data series)."
  },
  {
    "objectID": "lectures/7.3-EDA.html#measures",
    "href": "lectures/7.3-EDA.html#measures",
    "title": "ExploratoryData Analysis",
    "section": "Measures",
    "text": "Measures\nSo pandas provides functions for commonly-used measures:\nprint(f\"{df.price.mean():.2f}\")\nprint(f\"{df.price.median():.2f}\")\nprint(f\"{df.price.quantile(0.25):.2f}\")\nOutput:\n118.4542\n80.50\n40.75"
  },
  {
    "objectID": "lectures/7.3-EDA.html#more-complex-measures",
    "href": "lectures/7.3-EDA.html#more-complex-measures",
    "title": "ExploratoryData Analysis",
    "section": "More Complex Measures",
    "text": "More Complex Measures\nBut Pandas also makes it easy to derive new variables… Here’s the z-score:\n\\[ z = \\frac{x - \\bar{x}}{s}\\]\ndf['zscore'] = (df.price - df.price.mean())/df.price.std()\ndf.plot.box(column='zscore')"
  },
  {
    "objectID": "lectures/7.3-EDA.html#and-even-more-complex",
    "href": "lectures/7.3-EDA.html#and-even-more-complex",
    "title": "ExploratoryData Analysis",
    "section": "And Even More Complex",
    "text": "And Even More Complex\nAnd here’s the Interquartile Range Standardised score:\n\\[ x_{iqrs} = \\frac{x - \\widetilde{x}}{Q_{75} - Q_{25}} \\]\ndf['iqr_std'] = (df.price - df.price.median())/ \\\n      (df.price.quantile(q=0.75)-df.price.quantile(q=0.25))\ndf.plot.box(column='iqr_std')"
  },
  {
    "objectID": "lectures/7.3-EDA.html#the-plot-thickens",
    "href": "lectures/7.3-EDA.html#the-plot-thickens",
    "title": "ExploratoryData Analysis",
    "section": "The Plot Thickens",
    "text": "The Plot Thickens\nWe’ll get to more complex plotting over the course of the term, but here’s a good start for exploring the data! All plotting depends on matplotlib which is the ogre in the attic to R’s ggplot.\nimport matplotlib.pyplot as plt\nGet used to this import as it will allow you to save and manipulate the figures created in Python. It is not the most intuitive approach (unless you’ve used MATLAB before) but it does work."
  },
  {
    "objectID": "lectures/7.3-EDA.html#boxplot",
    "href": "lectures/7.3-EDA.html#boxplot",
    "title": "ExploratoryData Analysis",
    "section": "Boxplot",
    "text": "Boxplot\ndf.price.plot.box()\nplt.savefig('pboxplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#frequency",
    "href": "lectures/7.3-EDA.html#frequency",
    "title": "ExploratoryData Analysis",
    "section": "Frequency",
    "text": "Frequency\ndf.room_type.value_counts().plot.bar()\nplt.savefig('phistplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-correlation-heatmap",
    "href": "lectures/7.3-EDA.html#a-correlation-heatmap",
    "title": "ExploratoryData Analysis",
    "section": "A Correlation Heatmap",
    "text": "A Correlation Heatmap\nWe’ll get to these in more detail in a couple of weeks, but here’s some output…"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-map",
    "href": "lectures/7.3-EDA.html#a-map",
    "title": "ExploratoryData Analysis",
    "section": "A ‘Map’",
    "text": "A ‘Map’\ndf.plot.scatter(x='longitude',y='latitude')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-fancy-map",
    "href": "lectures/7.3-EDA.html#a-fancy-map",
    "title": "ExploratoryData Analysis",
    "section": "A Fancy ‘Map’",
    "text": "A Fancy ‘Map’\ndf.plot.scatter(x='longitude',y='latitude',\n                c='price',colormap='viridis',\n                figsize=(10,5),title='London',\n                grid=True,s=24,marker='x')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#an-actual-map",
    "href": "lectures/7.3-EDA.html#an-actual-map",
    "title": "ExploratoryData Analysis",
    "section": "An Actual ‘Map’",
    "text": "An Actual ‘Map’\ngdf.plot(column='price', cmap='viridis', \n         scheme='quantiles', markersize=8, legend=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#additional-resources",
    "href": "lectures/7.3-EDA.html#additional-resources",
    "title": "ExploratoryData Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\nThere’s so much more to find, but:\n\n\n\nPandas Reference\nA Guide to EDA in Python (Looks very promising)\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nEDA: DataPrep.eda vs Pandas-Profiling [Requires module installation]\n\n\n\nA Data Science Project for Beginners (EDA)\nEDA: A Pracitcal Guide and Template for Structured Data\nEDA—Don’t ask how, ask what (Part 1)\nPreparing your Dataset for Modeling – Quickly and Easily (Part 2)\nHandling Missing Data\nIntroduction to Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#two-cultures",
    "href": "lectures/7.1-Mapping.html#two-cultures",
    "title": "Computers+Maps",
    "section": "Two Cultures",
    "text": "Two Cultures\n\n\nPurely Computational\nBoth analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\nMostly Computational\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#the-challenge",
    "href": "lectures/7.1-Mapping.html#the-challenge",
    "title": "Computers+Maps",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#constituency-cards",
    "href": "lectures/7.1-Mapping.html#constituency-cards",
    "title": "Computers+Maps",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nClone and reproduce: github.com/alasdairrae/wpc and explanation: cconstituency cards."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/7.1-Mapping.html#short-term-lets-in-scotland",
    "title": "Computers+Maps",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#every-building-in-america",
    "href": "lectures/7.1-Mapping.html#every-building-in-america",
    "title": "Computers+Maps",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#think-it-through",
    "href": "lectures/7.1-Mapping.html#think-it-through",
    "title": "Computers+Maps",
    "section": "Think it Through!",
    "text": "Think it Through!"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/7.1-Mapping.html#a-deceptively-simple-problem",
    "title": "Computers+Maps",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#classification",
    "href": "lectures/7.1-Mapping.html#classification",
    "title": "Computers+Maps",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment",
    "title": "Computers+Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment-1",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment-1",
    "title": "Computers+Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment-2",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment-2",
    "title": "Computers+Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#consider",
    "href": "lectures/7.1-Mapping.html#consider",
    "title": "Computers+Maps",
    "section": "Consider",
    "text": "Consider\nWe want to:\n\nGroup features with similar values together.\nShow these in a way that doesn’t mislead the viewer.\n\nBut we have the following problems:\n\nToo many classes confuse the viewer.\nToo few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#choices-choices",
    "href": "lectures/7.1-Mapping.html#choices-choices",
    "title": "Computers+Maps",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#look-at-the-data",
    "href": "lectures/7.1-Mapping.html#look-at-the-data",
    "title": "Computers+Maps",
    "section": "Look at the Data!",
    "text": "Look at the Data!\nDifferent colour and break schemes not only give us different views of the data, they give us different understandings of the data! Each scheme changes how the data looks and, consequently, how we perceive the distribution."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/7.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Computers+Maps",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#additional-resources",
    "href": "lectures/7.1-Mapping.html#additional-resources",
    "title": "Computers+Maps",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation\nWorking with Spatial Data in Python\nWeb Mapping Notes"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#why-pandas",
    "href": "lectures/6.4-Pandas.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy, numpy, and sklearn) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-data-frame",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-index",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-series",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-slice",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#using-pandas",
    "href": "lectures/6.4-Pandas.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nHere’s code to read a (remote) CSV file:\nimport pandas as pd      # import package\n# Bitly: https://raw.githubusercontent.com/jreades/fsds/master/data/2019-sample-crime.csv\nurl='https://bit.ly/39SJpfp'\ndf = pd.read_csv(url)       # load a (remote) CSV\nprint(type(df))             # not simple data type\nprint(df.columns.to_list()) # column names\nprint(df.columns.values)    # Also works but deprecated\nOutput:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n['ID' 'Case Number' 'Date' 'Primary Type' 'Description'\n 'Location Description' 'Arrest' 'Domestic' 'Year' 'Latitude' 'Longitude']"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#summarise-a-data-frame",
    "href": "lectures/6.4-Pandas.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\ndf.describe() # Information about each Series\ndf.info()     # Information about each Series and the df\ndf.info is more about data types and memory usage. df.describe is for summarising information about the distribution of values in every series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#familiar",
    "href": "lectures/6.4-Pandas.html#familiar",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\nThis should be looking eerily familiar:\nprint(type(df['Latitude']))          # type for column\nprint(type(df['Latitude'].array))    # type for values\nprint(df['Latitude'].array[:5])     # first five values\nprint(f\"1: {df['Latitude'].mean()}\") # summarise a series/column\nprint(f\"2: {df.Latitude.mean()}\")    # if no spaces in name\nProduces:\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'numpy.ndarray'&gt;\n[41.75130706 41.90399688 41.88032861 41.92438396 41.75579713]\n1: 41.84550008439\n2: 41.84550008439\n\nNotice that we’ve got two ways of accessing a pandas Series:\n\nThe dictionary-like way: df['Latitude']; this works for all columns, always.\nThe method-like way: df.Latitude; this works for ‘reading’ columns without spaces in their names."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#jupyter-formatting",
    "href": "lectures/6.4-Pandas.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter in ‘fancy’ ways:"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#familiar-1",
    "href": "lectures/6.4-Pandas.html#familiar-1",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                       # First 3 rows of df\ndf[['ID','Date','Year']].tail(3) # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nOn one level, this is what we’ve been building towards! We’ve got head and tail which we saw in the Command Line lecture. We’ve got random sampling with seeds which we saw in the Randomness lecture. We’ve even got LoLs, which we saw way back in the Lists of Lists lecture!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#data-frames-vs-series",
    "href": "lectures/6.4-Pandas.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nPandas operates on two principles:\n\nAny operation on a Data Frame returns a Data Frame.\nAny operation on a Series returns a Series.\n\n\nWe’ll see in a moment why this is useful!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#shallow-copies",
    "href": "lectures/6.4-Pandas.html#shallow-copies",
    "title": "Pandas",
    "section": "‘Shallow’ Copies",
    "text": "‘Shallow’ Copies\nMore subtly, operations on a Series or Data Frame return a shallow copy, which is like a ‘view’ in a database…\n\nThe original is unchanged unless you specify inplace=True (where supported).\nAttempts to change a subset of the data frame will often trigger a SettingWithCopyWarning warning.\n\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy()).\n\n\nDataQuest has a nice overview of how SettingWithCopyWarning is triggered and what to do about it."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#putting-these-ideas-together",
    "href": "lectures/6.4-Pandas.html#putting-these-ideas-together",
    "title": "Pandas",
    "section": "Putting These Ideas Together",
    "text": "Putting These Ideas Together\n# Returns a series but not a column\ndf.Latitude - 1 \n# Saves returned series as a new column\ndf['lat'] = df.Latitude - 1\n# Returns a new data frame w/o 'lat' \ndf.drop(columns=['lat']) \n# Modifies df directly\ndf.drop(columns=['lat'], inplace=True) \n# Try to modify a view of df (triggers warning)\ndf[df['Primary Type']=='BURGLARY'].Latitude = 41.7"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#chaining",
    "href": "lectures/6.4-Pandas.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\ndf.sort_values(by=['Year','ID'], ascending=False).sample(frac=0.5).head(20).median()"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#selection",
    "href": "lectures/6.4-Pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n# Returns a selection (Boolean series)\ndf['Primary Type']=='ASSAULT'\n\n# All rows where Primary Type is ASSAULT\ndf[ df['Primary Type']=='ASSAULT' ]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['Primary Type']=='ASSAULT'][['Longitude','Latitude']].mean()\n\n# Two conditions with a bit-wise AND\ndf[\n  (df['Primary Type']=='ASSAULT') &\n  (df['Description']=='AGGRAVATED: HANDGUN')\n]\n\n# Two conditions with a bit-wise OR\ndf[\n  (df['Primary Type']=='ASSAULT') |\n  (df['Primary Type']=='THEFT')\n]"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#dealing-with-types",
    "href": "lectures/6.4-Pandas.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\n\n\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns (including arrays)\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#changing-the-type",
    "href": "lectures/6.4-Pandas.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\nprint(df['Primary Type'].unique())   # Find unique values\nprint(df['Primary Type'].dtype.name) # Confirm is 'object'\ndf['Primary Type'] = df['Primary Type'].astype('category')\nprint(df['Primary Type'].dtype.name) # Confirm is 'category'\nprint(df['Primary Type'].describe()) # Category column info\nOutputs:\n['BURGLARY' 'DECEPTIVE PRACTICE' 'BATTERY'...]\nobject   # &lt; before `as type`\ncategory # &lt; after `as type`\ncount       100\nunique       15\ntop       THEFT\nfreq         28\nName: Primary Type, dtype: object # category==special class of object"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#datetime-data",
    "href": "lectures/6.4-Pandas.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\nprint(df.Date.dtype.name)\n# object\ndf.Date.to_list()[:3]\n# ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nThis shows that Date is currently a string of dates+times.\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.array, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nprint(df.dt.dtype.name)\n# datetime64[ns]\ndf.dt.to_list()[:3]\n# [Timestamp('2019-04-20 11:00:00'), Timestamp('2019-12-02 10:35:00'), Timestamp('2019-10-06 04:50:00')]\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#datetime-formats",
    "href": "lectures/6.4-Pandas.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM\n\n\n\nSo that is why:\npd.to_datetime(df.Date.array, format=\"%m/%d/%Y %H:%M:%S %p\")\nNote the other things happening here:\n\npd.to_datetime(...) is not a method, it’s a function from the pandas package.\ndf.Date.array (and df.Date.to_numpy() and df.Data.tolist()) gives access to the data directly, whereas df.Date gives access to the Series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#deprecation-warning",
    "href": "lectures/6.4-Pandas.html#deprecation-warning",
    "title": "Pandas",
    "section": "Deprecation Warning!",
    "text": "Deprecation Warning!\nFrom time to time, real-world software projects will change the way things work. Pandas is just such a project!\n\n\n\n\n\n\nWarning\n\n\nWe recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array. See API Documenation.\n\n\n\nSo while Series.values still works, and will continue to work for some time, you are being advised to start using Series.array or Series.to_numpy() instead. Meaning, we should consider using df.Date.array."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#tidying-up",
    "href": "lectures/6.4-Pandas.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many options and subtleties…\n# Fix categories\nmapping = {}\n\n# df['Primary Type'].unique().to_list() also works\nfor x in df['Primary Type'].cat.categories.to_list():\n  mapping[x]=x.title()\n\n# And update\ndf['Primary Type'] = df['Primary Type'].cat.rename_categories(mapping)\nHow would you work out what this code does? 1\nTo deal with pricing information treated as a string:\ndf2['price'].str.replace('$','').astype(float)\nMany more examples accessible via Google!\n\nAnother thing you might notice here: adding .cat allows us to access category methods for the Series; adding .str allows us to access string methods for the Series.\n\nThere are at least two ways: 1) print out mapping; 2) before running the code comment out the ‘update’ line and print out x and x.title(); 3) search for title python."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#dropping-rows-and-columns",
    "href": "lectures/6.4-Pandas.html#dropping-rows-and-columns",
    "title": "Pandas",
    "section": "Dropping Rows and Columns",
    "text": "Dropping Rows and Columns\nThere are multiple ways to drop ‘stuff’:\ndf2 = df.copy()\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf2.drop(index=range(5,10), inplace=True) # Row 'numbers' or index values\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf.drop(columns=['Year'], inplace=True)   # Column name(s)\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\nThere is also df.dropna() which can apply to rows or columns with NULL or np.nan values.\nI often prefer df = df[df.index &gt; 15] (negative selection) to df.drop(index=range(0,14)) (positive selection).\n\nWhy might you want the default to not be in_place?"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#accessing-data-by-location",
    "href": "lectures/6.4-Pandas.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index:\n\ndf.loc returns the rows labeled 0, 1, and 2 ([0..2]), whereas\ndf.iloc returns the range 0..2 ([0..2))!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#indexes",
    "href": "lectures/6.4-Pandas.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you print out the head:\ndf.head(3)\nThe left-most column (without) a name is the index.\ndf.set_index('ID', inplace=True)\ndf.head(3)\nNow we see:\n         Case Number                    Date  ...  Longitude                  dt\nID                                            ...\n11667185    JC237601  04/20/2019 11:00:00 PM  ... -87.603468 2019-04-20 11:00:00\n11909178    JC532226  12/02/2019 10:35:00 AM  ... -87.643230 2019-12-02 10:35:00\n11852571    JC462365  10/06/2019 04:50:00 PM  ... -87.758473 2019-10-06 04:50:00\n\nSo ID is now the index and is not accessible as a column: df.ID will now throw an error because it’s not longer part of the Column Index."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#indexes-contd",
    "href": "lectures/6.4-Pandas.html#indexes-contd",
    "title": "Pandas",
    "section": "Indexes (cont’d)",
    "text": "Indexes (cont’d)\nNotice the change to the data frame:\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nAnd now:\nprint(df.loc[11667185,:])\nprint(df.loc[11667185:11852571,'Case Number':'Date'])\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#saving",
    "href": "lectures/6.4-Pandas.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, here are some of the more popular ones:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(&lt;path&gt;)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(&lt;path&gt;)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_feather(&lt;path&gt;)\nDirectly usable by R. Requires pyarrow to be installed to access the options.\n\n\ndf.to_parquet(&lt;path&gt;)\nDirectly usable by many languages. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(&lt;path&gt;))\nWrite a LaTeX-formatted table to a file. Display requires booktabs. Could do copy+paste with print(df.to_latex()).\n\n\ndf.to_markdown(&lt;path&gt;)\nWrite a Markdown-formatted table to a file. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is detected automatically (e.g. df.to_csv('file.csv.gz')) but you can also specify it (e.g. df.to_csv('file.csv.gz', compression='gzip')).1\nFor instance, a bit.ly link to a Gzipped file requires compression='gzip' because there’s nothing in the link itself to tell Pandas what to expect."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#additional-resources",
    "href": "lectures/6.4-Pandas.html#additional-resources",
    "title": "Pandas",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#reproducibility-good-or-bad",
    "href": "lectures/6.2-Randomness.html#reproducibility-good-or-bad",
    "title": "Randomness",
    "section": "Reproducibility: Good or Bad?",
    "text": "Reproducibility: Good or Bad?\nDepends on the problem:\n\nBanking and encryption?\nSampling and testing?\nReproducing research/documentation?\n\n\nOK, technically, even encryption needs to be reproducible to allow for decryption, but you sure don’t want it to be easy."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#not-very-good-encryption",
    "href": "lectures/6.2-Randomness.html#not-very-good-encryption",
    "title": "Randomness",
    "section": "Not Very Good Encryption",
    "text": "Not Very Good Encryption\n\n\n\nCyphertext\nOutput\n\n\n\n\nROT0\nTo be or not to be, That is the question\n\n\nROT1\nUp cf ps opu up cf, Uibu jt uif rvftujpo\n\n\nROT2\nVq dg qt pqv vq dg, Vjcv ku vjg swguvkqp\n\n\n…\n…\n\n\nROT9\nCx kn xa wxc cx kn, Cqjc rb cqn zdnbcrxw\n\n\n\nROT is known as the Caesar Cypher, but since the transformation is simple (A..Z+=x) decryption is easy now. How can we make this harder?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#python-is-random",
    "href": "lectures/6.2-Randomness.html#python-is-random",
    "title": "Randomness",
    "section": "Python is Random",
    "text": "Python is Random\nimport random\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nSee also: random.randrange, random.choice, random.sample, random.random, random.gauss, etc."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#and-repeat",
    "href": "lectures/6.2-Randomness.html#and-repeat",
    "title": "Randomness",
    "section": "And Repeat…",
    "text": "And Repeat…\nimport random\nsize = 10\nresults = [0] * size\n\ntests = 100000\nwhile tests &gt; 0:\n    results[random.randint(0,len(results)-1)] += 1\n    tests -= 1\n\nfor i in range(0,len(results)):\n    print(f\"{i} -&gt; {results[i]}\")\n\nWhat will this return?\nWill it hold for more than 10 numbers?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat",
    "href": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat\nimport random \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nsize = 1000\ndata = [0] * size\n\ntests = 10000000\nwhile tests &gt; 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nfig = plt.figure()\nplt.bar(np.arange(0,len(data)), data)\nfig.savefig('Random.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "href": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#just-add-salt",
    "href": "lectures/6.2-Randomness.html#just-add-salt",
    "title": "Randomness",
    "section": "Just add salt",
    "text": "Just add salt"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#hashing",
    "href": "lectures/6.2-Randomness.html#hashing",
    "title": "Randomness",
    "section": "Hashing",
    "text": "Hashing\nChecking for changes (usally in a security context).\nimport hashlib # Can take a 'salt' (similar to a 'seed')\n\nr1 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r1 is: {r1.hexdigest()}\")\n\nr2 = hashlib.md5('CASA Intro to Programming '.encode())\nprint(f\"The hashed equivalent of r2 is: {r2.hexdigest()}\")\n\nr3 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r3 is: {r3.hexdigest()}\")\nOutputs:\n\"The hashed equivalent of r1 is: acd601db5552408851070043947683ef\"\n\"The hashed equivalent of r2 is: 4458e89e9eb806f1ac60acfdf45d85b6\"\n\"The hashed equivalent of r3 is: acd601db5552408851070043947683ef\"\n\nThis is like generating a ‘fingerprint’ of an application or file. In fact, it’s what is going on behind the scenes when you download something to install on macOS or Windows and you’re told that the installer is being ‘verified’ before it will run: the computer is generating a hash of the application’s codebase, and sending that to the Apple Store or Windows Store or direct to the developer in order to check that the file hasn’t been tampered with."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#and-note",
    "href": "lectures/6.2-Randomness.html#and-note",
    "title": "Randomness",
    "section": "And Note…",
    "text": "And Note…\nimport requests\nnight = requests.get(\"http://www.gutenberg.org/ebooks/1514.txt.utf-8\")\nprint(f\"The text is {night.text[30:70]}\")\nprint(f\"The text is {len(night.text):,} characters long\")\nhash = hashlib.md5(night.text.encode())\nprint(f\"This can be hashed into: {hash.hexdigest()}\")\nOutputs:\n\"The text is A Midsummer Night's Dream by Shakespeare\"\n\"The text is 112,127 characters long\"\n\"This can be hashed into: cce0d35b8b2c4dafcbde3deb983fec0a\"\n\nCan be applied to anything: even one byte’s difference (e.g. in a application) can lead to a different hash output.\nBut notice that hashes are always the same length. This property is quite useful for databases and verifying the integrity of applications (MD5 Checksums)."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#jupyterlab-password",
    "href": "lectures/6.2-Randomness.html#jupyterlab-password",
    "title": "Randomness",
    "section": "JupyterLab Password",
    "text": "JupyterLab Password\nTo set a password in JupyterLab you need something like this:\n'sha1:5b1c205a53e14e:0ce169b9834984347d62b20b9a82f6513355f72d'\nHow this was generated:\nimport uuid, hashlib\nsalt = uuid.uuid4().hex[:16] # Truncate salt\npassword = 'casa2021'        # Set password\n\n# Here we combine the password and salt to \n# 'add complexity' to the hash\nhashed_password = hashlib.sha1(password.encode() + \n                  salt.encode()).hexdigest()\nprint(':'.join(['sha1',salt,hashed_password]))\n\n\nThen you can replace the JUPYTER_PWD parameter in the start-up string for Podman/Docker if you want to set a password.\n\n\nDon’t set your passwords this way."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#encryption-security",
    "href": "lectures/6.2-Randomness.html#encryption-security",
    "title": "Randomness",
    "section": "Encryption & Security",
    "text": "Encryption & Security\nSimple hashing algorithms are not normally secure enough for full encryption. Genuine security training takes a whole degree + years of experience.\nAreas to look at if you get involved in applications:\n\nPublic and Private Key Encryption (esp. OpenSSL)\nPrivileges used by Applications (esp. Podman vs Docker)\nRevocable Tokens (e.g. for APIs)\nInjection Attacks (esp. for SQL using NULL-byte and similar)"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#other-types-of-seeds",
    "href": "lectures/6.2-Randomness.html#other-types-of-seeds",
    "title": "Randomness",
    "section": "Other Types of Seeds",
    "text": "Other Types of Seeds\nTwo main libraries where seeds are set:\nimport random\nrandom.seed(42)\n\nimport numpy as np\nnp.random.seed(42)\n\nTurning now from the security aspects, let’s look at other types seeds.\nWhy do you often see 42 used as a seed?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#seeds-and-state",
    "href": "lectures/6.2-Randomness.html#seeds-and-state",
    "title": "Randomness",
    "section": "Seeds and State",
    "text": "Seeds and State\nimport random\nrandom.seed(42)\nst = random.getstate()\nfor r in range(0,3):\n    random.setstate(st)\n    print(f\"Repetition {r}:\")\n    ints = []\n    for i in range(0,10):\n        ints.append(random.randint(0,10))\n    print(f\"\\t{ints}\")"
  },
  {
    "objectID": "lectures/5.4-Errors.html#helpfully",
    "href": "lectures/5.4-Errors.html#helpfully",
    "title": "Errors",
    "section": "Helpfully…",
    "text": "Helpfully…\nPython will always try to tell you what it thinks went wrong: “I didn’t understand what you meant by this…” or “I’m sorry, I can’t let you do that Dave…”\nThe challenges are:\n\nPython tends to give you a lot of information about the error: this can be very helpful for programmers dealing with complex problems and totally overwhelming for beginners.\nThat what Python thinks the problem is doesn’t always line up with where the problem actually is. In cases of syntax, for instance, the problem could be an unclosed parenthesis three lines earlier!"
  },
  {
    "objectID": "lectures/5.4-Errors.html#challenge-1",
    "href": "lectures/5.4-Errors.html#challenge-1",
    "title": "Errors",
    "section": "Challenge 1",
    "text": "Challenge 1\nThat the ‘error’ isn’t always the error…\ntotal = 0\nprint(\"About to start loop\"\nfor i in range(1,10):\n  total += i\nprint(total)\nThis outputs:\nprint(\"About to start loop\"\n... for i in range(1,10):\n  File \"&lt;stdin&gt;\", line 2\n    for i in range(1,10):\n                        ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "lectures/5.4-Errors.html#errors-have-types",
    "href": "lectures/5.4-Errors.html#errors-have-types",
    "title": "Errors",
    "section": "Errors Have Types",
    "text": "Errors Have Types\nIn the same way that variables have types, so do errors:\n\nModuleNotFoundError\nIndexError\nKeyError\nOSError\n…\n\nWe can add our own messages:\nraise Exception(\"Sorry, I can't let you do that, Dave.\")\n\n\nWhy might different types of errors be useful?\nWe might reasonably want to distinguish between errors that we could reasonably expect or that are not serious, from those that we did not expect or that call the results of the program into question.\n\n\nLots more built-in error types in the Python documentation and imported packages will provide their own as well."
  },
  {
    "objectID": "lectures/5.4-Errors.html#custom-errors",
    "href": "lectures/5.4-Errors.html#custom-errors",
    "title": "Errors",
    "section": "Custom Errors",
    "text": "Custom Errors\nWe can create our own types (classes) of error:\nclass CustomError(Exception):\n  pass # We do nothing except create a new type\nThis can then be triggered with:\nraise CustomError(\"Our custom error\")\nAnd (very importantly) this can be caught with:\nexcept CustomError: \n  #... do something with CustomError ..."
  },
  {
    "objectID": "lectures/5.4-Errors.html#why-customise",
    "href": "lectures/5.4-Errors.html#why-customise",
    "title": "Errors",
    "section": "Why Customise?",
    "text": "Why Customise?\nCustom exceptions can perform a variety of tasks:\n\nDistinguish between generic issues and those specific to your application.\nTriage issues based on your understanding of your application and the severity.\nProvide detailed insight based on fuller access to the state of your application.\nPerform important tidying-up or rollback operations, etc."
  },
  {
    "objectID": "lectures/5.4-Errors.html#so-errors-can-be-trapped",
    "href": "lectures/5.4-Errors.html#so-errors-can-be-trapped",
    "title": "Errors",
    "section": "So Errors can be Trapped",
    "text": "So Errors can be Trapped\nPython calls errors exceptions, so this leads to:\ntry:\n  #... some code that might fail...\nexcept &lt;Named_Error_Type&gt;:\n  #... what do it if it fails for a specific reason...\nexcept:\n  #... what to do if it fails for any other reason...\nfinally:\n  #... always do this, even if it fails...\nYou can use any or all of these together: you can have multiple named excepts to handle different types of errors from a single block of code; you do not have to have a catch-all except or a finally.\n\nSo it makes sense to think: “Well, let’s try this and see what happens. If we have a problem of this type then it’s not serious and we should carry on. But if we have a problem that type then we need to stop what we’re doing right away.”\nSome of the intricacies of errors can seem quite confusing. What’s the point of having a finally (which we’ll get to in a moment), for instance? Well, if your application is connected to a database then finally gives your application a chance to disconnect cleanly (freeing up resources for the database) and even to rollback incomplete changes (e.g. a new user whose details were only partially inputted when the application crashed)."
  },
  {
    "objectID": "lectures/5.4-Errors.html#trapping-errors",
    "href": "lectures/5.4-Errors.html#trapping-errors",
    "title": "Errors",
    "section": "Trapping Errors",
    "text": "Trapping Errors\nThis code fails:\nx,y = 10,0\nprint(x/y)\nAnd it generates this error:\n&gt; Traceback (most recent call last):\n&gt;   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n&gt; ZeroDivisionError: division by zero"
  },
  {
    "objectID": "lectures/5.4-Errors.html#trapping-errors-contd",
    "href": "lectures/5.4-Errors.html#trapping-errors-contd",
    "title": "Errors",
    "section": "Trapping Errors (cont’d)",
    "text": "Trapping Errors (cont’d)\nBut if you ‘trap’ the error using except then:\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\nexcept:\n  print(\"Something has gone very wrong.\")\nfinally: \n  print(\"Division is fun!\")\nThis will print\n&gt; You can't divide by zero!\n&gt; Division is fun!\n\n\nNote: if we need to access the actual exception: except ZeroDivisionError as e:"
  },
  {
    "objectID": "lectures/5.4-Errors.html#raising-hell",
    "href": "lectures/5.4-Errors.html#raising-hell",
    "title": "Errors",
    "section": "Raising Hell",
    "text": "Raising Hell\nYou can trigger your own exceptions using raise.\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")"
  },
  {
    "objectID": "lectures/5.4-Errors.html#understanding-multiple-errors",
    "href": "lectures/5.4-Errors.html#understanding-multiple-errors",
    "title": "Errors",
    "section": "Understanding Multiple Errors",
    "text": "Understanding Multiple Errors\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")\n\nThe code we try triggers the ZeroDivisionError block.\nThis prints \"You can't divide by zero!\"\nWe then raise a new exception that is not caught.\nThe finally code executes because it always does before Python exits.\nPython exits with the message from our newly raised Exception.\n\nThus: ‘During handling of above (ZeroDivisionError) another exception (our Exception) occurred…’"
  },
  {
    "objectID": "lectures/5.4-Errors.html#test-based-development",
    "href": "lectures/5.4-Errors.html#test-based-development",
    "title": "Errors",
    "section": "Test-Based Development",
    "text": "Test-Based Development\nWe can actually think of exceptions as a way to develop our code.\nHere’s some ‘pseudo-code’:\n# Testing the 'addition' operator\ntest(1+1, 2)           # Should equal 2\ntest(1+'1', TypeError) # Should equal TypeError\ntest('1'+'1', '11')    # Should equal '11'\ntest(-1+1, 0)          # Should equal 0 \nOur test(A,B) function takes an input (A) and the expected output (B) and then compares them. The test returns True if A==B and False otherwise."
  },
  {
    "objectID": "lectures/5.4-Errors.html#unit-tests",
    "href": "lectures/5.4-Errors.html#unit-tests",
    "title": "Errors",
    "section": "Unit Tests",
    "text": "Unit Tests\nEach test is a Unit Test because it tests one thing and one thing only. So if you had three functions to ‘do stuff’ then you’d need at least three unit tests.\nA Unit Test may be composed of one or more assertions. Our pseudo-code on the previous slide contained 4 assertions.\nA Unit Test does not mean that your code is correct or will perform properly under all circumstances. It means that your code returns the expected value for a specified input.\nPython considers this approach so important that it’s built in."
  },
  {
    "objectID": "lectures/5.4-Errors.html#approach-1",
    "href": "lectures/5.4-Errors.html#approach-1",
    "title": "Errors",
    "section": "Approach 1",
    "text": "Approach 1\nThis is an explict assertion to test fun:\nimport unittest\n\ndef fun(x):\n  return x + 1\n\nclass MyTest(unittest.TestCase):\n  def test(self):\n    self.assertEqual(fun(3), 4)\n    print(\"Assertion 1 passed.\")\n    self.assertEqual(fun(3), 5)\n    print(\"Assertion 2 passed.\")\n\nm = MyTest()\nm.test()\nThe critical output is:\nAssertionError: 4 != 5"
  },
  {
    "objectID": "lectures/5.4-Errors.html#approach-2",
    "href": "lectures/5.4-Errors.html#approach-2",
    "title": "Errors",
    "section": "Approach 2",
    "text": "Approach 2\nThis approach uses the ‘docstring’ (the bits between \"\"\") to test the results of the function. This is intended to encourage good documentation of functions using examples:\ndef square(x):\n    \"\"\"Return the square of x.\n\n    &gt;&gt;&gt; square(2)\n    4\n    &gt;&gt;&gt; square(-2)\n    4\n    &gt;&gt;&gt; square(-1)\n    2\n    \"\"\"\n    return x * x\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()\n\n\nNotice the __name__ (what could that possibly mean???) is tested to see if it’s '__main__'. The doctest code only runs when this code is executed in '__main__'."
  },
  {
    "objectID": "lectures/5.4-Errors.html#collaboration-continuous-integration",
    "href": "lectures/5.4-Errors.html#collaboration-continuous-integration",
    "title": "Errors",
    "section": "Collaboration & Continuous Integration",
    "text": "Collaboration & Continuous Integration\nThe Unit Test approach is often used on collaborative projects, especially in the Open Source world. PySAL, for instance, asks for unit tests with every new feature or integration.\nThe running of all tests for multiple components is called ‘integration testing’.\nA commit, merge, or pull on GitHub can trigger the unit testing process for the entire software ‘stack’. This is known as Continuous Integration because you are always checking that the code works as expected, rather than leaving testing to the end.\n\nThis is heavily used by PySAL and other robust FOSS projects since TravisCI is free for FOSS projects!"
  },
  {
    "objectID": "lectures/5.4-Errors.html#additional-resources",
    "href": "lectures/5.4-Errors.html#additional-resources",
    "title": "Errors",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nHandling exceptions\nReporting errors\nPython Custom Exceptions\nWriting and Using Custom Exceptions in Python\nPython Documentation\nHow to Define Custom Exception Classes\n\n\n\nUnit Testing in Python\nUnderstanding Unit Testing\nTesting Your Code\nGetting Started with Testing in Python\nPython’s unittest Library\nVideo: Unit Testing Your Code"
  },
  {
    "objectID": "lectures/5.2-Classes.html#whats-an-object",
    "href": "lectures/5.2-Classes.html#whats-an-object",
    "title": "Classes",
    "section": "What’s an Object?",
    "text": "What’s an Object?\nObjects are instantiated versions of classes:\n\n\"hello world\" is an instance of a string, and\n['A','B',1,3] is an instance of a list.\n\nThe class is your recipe, the object is your 🍕…"
  },
  {
    "objectID": "lectures/5.2-Classes.html#really-like-a-pizza",
    "href": "lectures/5.2-Classes.html#really-like-a-pizza",
    "title": "Classes",
    "section": "Really… Like a Pizza!",
    "text": "Really… Like a Pizza!\nclass pizza(object):\n  base = 'sourdough'\n  \n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\n    \n  def add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\n  \n  def get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/5.2-Classes.html#class-definition",
    "href": "lectures/5.2-Classes.html#class-definition",
    "title": "Classes",
    "section": "Class Definition",
    "text": "Class Definition\nclass pizza(object):\n\n    base = 'sourdough'\n    ...\nFollows the pattern: class &lt;name&gt;(&lt;parent class&gt;).\nYou can find many examples in: /opt/conda/envs/sds2020/lib/python3.7/site-packages (Docker)."
  },
  {
    "objectID": "lectures/5.2-Classes.html#the-constructor",
    "href": "lectures/5.2-Classes.html#the-constructor",
    "title": "Classes",
    "section": "The Constructor",
    "text": "The Constructor\n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce    = sauce\n    self.cheese   = cheese\nFollows the pattern: def __init__(self, &lt;params&gt;)\n\n\nNotice also the namespace: the parameters sauce and cheese are the same as the instance variables self.sauce and self.cheese because they occupy different namespaces."
  },
  {
    "objectID": "lectures/5.2-Classes.html#adding-toppings",
    "href": "lectures/5.2-Classes.html#adding-toppings",
    "title": "Classes",
    "section": "Adding Toppings",
    "text": "Adding Toppings\ndef add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\nFollows the pattern: def &lt;function&gt;(self, &lt;params&gt;):"
  },
  {
    "objectID": "lectures/5.2-Classes.html#getting-the-pizza",
    "href": "lectures/5.2-Classes.html#getting-the-pizza",
    "title": "Classes",
    "section": "Getting the Pizza",
    "text": "Getting the Pizza\ndef get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/5.2-Classes.html#pizza-in-action",
    "href": "lectures/5.2-Classes.html#pizza-in-action",
    "title": "Classes",
    "section": "Pizza in Action",
    "text": "Pizza in Action\np = pizza(sauce='white')\np.add_topping('peppers')\np.add_topping('chillis')\np.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chillis']"
  },
  {
    "objectID": "lectures/5.2-Classes.html#check-it-out",
    "href": "lectures/5.2-Classes.html#check-it-out",
    "title": "Classes",
    "section": "Check it Out",
    "text": "Check it Out\np1 = pizza(sauce='white')\np1.add_topping('peppers')\np1.add_topping('chilis')\n\np2 = pizza()\np2.base = \"Plain old base\"\np2.add_topping('pineapple')\np2.add_topping('ham')\n\np1.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()\n&gt; ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']"
  },
  {
    "objectID": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza",
    "href": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza",
    "text": "Recap: How to Make a Pizza\nA class is defined by:\nclass &lt;name&gt;(&lt;parent class):\n  ...\nA class is initialised by:\n  def __init__(self, &lt;any_parameters&gt;):\n    ...\nAll methods have to have this:\n  def &lt;method&gt;(self, &lt;any_parameters&gt;):\n    ..."
  },
  {
    "objectID": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "href": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza (cont’d)",
    "text": "Recap: How to Make a Pizza (cont’d)\nThis is an instance variable:\n  self.&lt;var&gt; = &lt;something&gt;\nThis is a class variable (in the class definition):\n  &lt;var&gt; = &lt;something&gt;"
  },
  {
    "objectID": "lectures/5.2-Classes.html#decorating-a-class",
    "href": "lectures/5.2-Classes.html#decorating-a-class",
    "title": "Classes",
    "section": "Decorating a Class",
    "text": "Decorating a Class\nClasses have additional decorators you can use:\n\n@staticmethod: binds any function to a class so objects can use it (but no object or class information is passed to static methods).\n@classmethod: decorates a function for use by a class, not an object (accesses and modifies class state).\n@property: create ‘managed’ attributes in classes to ‘hide’ them from users but keep code clean."
  },
  {
    "objectID": "lectures/5.2-Classes.html#class-and-static-methods",
    "href": "lectures/5.2-Classes.html#class-and-static-methods",
    "title": "Classes",
    "section": "Class and Static Methods1",
    "text": "Class and Static Methods1\n\nfrom datetime import date\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    # a class method to create a Person object by birth year.\n    @classmethod\n    def fromBirthYear(cls, name, year):\n        return cls(name, date.today().year - year)\n\n    # a static method to check if a Person is adult or not.\n    @staticmethod\n    def isAdult(age):\n        return age &gt; 18\n\nperson1 = Person('Andy', 31)\nperson2 = Person.fromBirthYear('Jon', 1976)\n\nprint(person1.age)\nprint(person2.age)\nprint(Person.isAdult(22))\n\n31\n49\nTrue\n\n\nFrom Geeks for Geeks."
  },
  {
    "objectID": "lectures/5.2-Classes.html#properties",
    "href": "lectures/5.2-Classes.html#properties",
    "title": "Classes",
    "section": "Properties1",
    "text": "Properties1\n\nclass Circle:\n    def __init__(self, radius):\n        self._radius = radius\n\n    @property\n    def radius(self):\n        \"\"\"The radius property.\"\"\"\n        print(\"Get radius\")\n        return self._radius\n\n    @radius.setter\n    def radius(self, value):\n        print(\"Set radius\")\n        self._radius = value\n\n    @radius.deleter\n    def radius(self):\n        print(\"Delete radius\")\n        del self._radius\n\ncircle = Circle(42.0)\nprint(circle.radius)\ncircle.radius = 100\nprint(circle.radius)\ndel(circle.radius)\n\nGet radius\n42.0\nSet radius\nGet radius\n100\nDelete radius\n\n\n\nThis is a simple example, but notice how we are accessing an attribute with the potential to ‘do things’ before or after accessing the value of interest. This could have all sorts of useful applications in the real world where lots of things are happening to an application at the same time.\n\nExample from Real Python."
  },
  {
    "objectID": "lectures/5.2-Classes.html#additional-resources",
    "href": "lectures/5.2-Classes.html#additional-resources",
    "title": "Classes",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nClasses\nObjects\nBasic class definition\nInstance methods and attributes\nChecking instance types\nClass methods and members\n\n\n\nCreating a class\nConstructing an object\nClass methods\nClass vs Instance Variables\nObject data\nInheritance\nAn Introduction to Object-Oriented Programming (by a CASA alum)"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#many-models",
    "href": "lectures/4.5-Group_Working.html#many-models",
    "title": "Group Working",
    "section": "Many Models…",
    "text": "Many Models…\n\nArtist Collectives — shared responsibilities for ideas and outputs; a lot of freedom and fuzziness in roles, but propensity for struggles over direction and power.\nTheatre — every member has a specific role; there is freedom within the role and clear lines of responsibility for delivery, but can blow up spectacularly.\nCo-Creation — emphasis on participation and recognition of diverse strengths; problem- and communication-focused; lots of effort and uncertainty, but results can be much more meaningful and durable.\n…\n\n\nThe arts world is particularly innovative when it comes to project work because teams are always being formed and reformed around individual outputs (a play, an artistic collaboration, a theatre production)…\nFields such as consultancy and software development have their own norms, but they are for the most part less experimental in their structure.\nHowever, the point is that there is no one-size-fits-all pattern for a successful piece of group work."
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#challenges",
    "href": "lectures/4.5-Group_Working.html#challenges",
    "title": "Group Working",
    "section": "Challenges",
    "text": "Challenges\n\nWe make superficial assessments of the flaws/strengths of others.\nWe make a range of assumptions about the motivations of others.\nWe make a range of assumptions about the situations of others.\nWe respond differently to stimuli and stresses.\nWe have a hard time talking about any of this.\n\n\nThese are just some of the challenges to setting up your group.\nIn particular, when we meet a new group, it’s often easy to be impressed by someone who is self-confident and well-spoken: they must know what they’re doing! Group dynamics can be like a popularity contest in which the person who is easy-going or ‘fun’ is making a wonderful contribution, while the one who asks difficult questions is ‘not constructive’ and ‘not contributing’. Someone who is ‘quiet’ doesn’t seem to have much to say…"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#identifying-your-strengths",
    "href": "lectures/4.5-Group_Working.html#identifying-your-strengths",
    "title": "Group Working",
    "section": "Identifying Your Strengths",
    "text": "Identifying Your Strengths\n\n\nDominance\n\nResults-oriented\nInnovative\nCompetitive\nDirect\n\nInfluence\n\nPeople-oriented\nEnthusiastic\nOptimistic\nCreative\n\n\nSteadiness\n\nSincere\nDependable\nPatient\nModest\n\nConscientiousness\n\nAccurate\nCautious\nPrecise\nAnalytical\n\n\n\nHere is one way of moving beyond the stereotype that the person doing the talking is also doing the thinking. The DISC model is apparently connected back to Wonder Woman by her creator W.M Marston. Like Wonder Woman, this is a model that reflects its cultural context: it was created for American teams and if you aren’t American then you might find that these four personality types don’t ‘fit’ very well. But it’s still a good place to start talking about preferences, behaviours, and the way they impact others."
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#too-much-of-a-good-thing",
    "href": "lectures/4.5-Group_Working.html#too-much-of-a-good-thing",
    "title": "Group Working",
    "section": "Too Much of a Good Thing",
    "text": "Too Much of a Good Thing\n\n\nDominance\n\nFails to involve others\nImpatient\nOffensive\n\nInfluence\n\nToo social\nEasily distracted\nOverly optimistic\n\n\nSteadiness\n\nIndirect\nAvoids conflict\nDelays difficult decisions\n\nConscientiousness\n\nPerfectionist\nAvoids unsystematic people\nDelays decisions over risks\n\n\n\nAnything that is a strength in one context can become a weakness in another.\nFor instance, a dominant personality might assume that the fact no one else is objecting means that there is support for their proposal; however, it could just be that the rest of the group wants to avoid conflict even though they think there are significant risks.\nOr someone who is motivated by the social component might spend so long bringing people together to talk about directions (and constantly adding new ideas to the mix) that very little progress is made on the actual project!"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#consequences",
    "href": "lectures/4.5-Group_Working.html#consequences",
    "title": "Group Working",
    "section": "Consequences1",
    "text": "Consequences1\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#every-good-plan",
    "href": "lectures/4.5-Group_Working.html#every-good-plan",
    "title": "Group Working",
    "section": "Every Good Plan1…",
    "text": "Every Good Plan1…\n\n\n\n\n\nSource: Zentao"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#nothing-ever-goes-to-plan",
    "href": "lectures/4.5-Group_Working.html#nothing-ever-goes-to-plan",
    "title": "Group Working",
    "section": "Nothing Ever Goes to Plan",
    "text": "Nothing Ever Goes to Plan\nFrom relational to practical…\n\nAgile principles1: iterative delivery of successful projects focussed on individuals and interactions; working software, customer collaboration; and responding to change.\nScrum methodology2: organised around stand ups, sprints, sprint review, and retrospectives.\nKanban methodology: use of ‘sticky notes’ in columns to organise and prioritise visually.\nXP methdology: one person writes/codes while a partner researches/solves/feeds back.\n\n\nOne challenge that teams face in the world of software is failure due to complexity and change.\nYou may have seen ‘Waterfall charts’ of the sort produced by project management software where one task cascades into the next until the project is delivered at the end. This is recipe for failure. The Mythical Man Month comprehensively debunked this approach in 1975.\nThe general conclusion was that “Adding more people to a project that is late makes it later.” The reason this happens is that you cannot break a complex task into discrete parts, send someone away to work on it in isolation, and then have them report back at the end with a finished product. I have seen this happen in previous years’ group work too: one person did a lit review while someone else was writing code but they didn’t line up at the end!\nI saw this in graphic design too: a client writes a brief for exactly what they want. You deliver something exactly like what they asked for. And they say that’s not what they meant.\nThe Agile philosophy emerged in response to these kinds of failures, with the approach that the product should be finished as quickly as possible, but as provisionally as possible. So you always have a working application, even if it only does a fraction of what you intend for it to do in the long run.\nThis has been elevated by companies like Facebook to slogans along the lines of ‘Fail Fast’, but the point is to recognise as quickly as possible when something isn’t working or isn’t headed in the direction you planned.\nFor the group work at the heart of this module, the point of the iterative process that begins in Week 6 is to help you develop competency in using the tools required to make your submission, to start you thinking along the lines that are required for the more complex questions, and to tease out points of failure in the group’s ability to work together long before you are under the tight deadline at the end of term!\n\nSource: SimpleLearnSource: Northeastern University"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#additional-resources",
    "href": "lectures/4.5-Group_Working.html#additional-resources",
    "title": "Group Working",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nBartlett Guide to Group Work\nThe Five Dysfunctions of a Team\nThe Mythical Man Month"
  },
  {
    "objectID": "lectures/4.3-Packages.html#popular-packages",
    "href": "lectures/4.3-Packages.html#popular-packages",
    "title": "Packages",
    "section": "Popular Packages",
    "text": "Popular Packages\n\n\n\nPackage\nDoes\n\n\n\n\nos\nOperating System stuff (paths, processes, etc.)\n\n\ncsv\nReading and Writing CSV files.\n\n\nmath\nMaths constants and functions.\n\n\nstatistics\nSimple statistical tests & measures.\n\n\nurllib\nURL Reading (e.g. web pages).\n\n\nnumpy\nNumerical Python (scientific computing).\n\n\nscipy\nScientific Python (more scientific computing)\n\n\nsklearn\nMachine learning, clustering, etc."
  },
  {
    "objectID": "lectures/4.3-Packages.html#importing-a-package",
    "href": "lectures/4.3-Packages.html#importing-a-package",
    "title": "Packages",
    "section": "Importing a Package",
    "text": "Importing a Package\nIf a package is installed, then it’s as simple as:\nimport &lt;packagename&gt;\nYou normally do this at the start of a program so that it’s easy to see what the program requires to run:\nimport math\nprint(math.pi) # Prints 3.141592653589793"
  },
  {
    "objectID": "lectures/4.3-Packages.html#what-can-a-package-do",
    "href": "lectures/4.3-Packages.html#what-can-a-package-do",
    "title": "Packages",
    "section": "What Can a Package Do?",
    "text": "What Can a Package Do?\nThere are many ways to find this out:\n\nRead the documentation\nSearch Google, and\nSearch StackOverflow\n\nThere’s even a web site python.readthedocs.io.\nBut we can also ask the package:\nimport math\ndir(math)\n# ['__doc__', '__file__', '__name__', '__package__', ..., \n# 'log', 'log10', 'log1p', 'modf', 'pi', ...]\nhelp(math.log10)\n\nSo, remember what I said in the talk about functions: that the numbers and total variables in the function aren’t the same as the ones outside the function? I said the answer had to do with something called the namespace. Here is the math namespace."
  },
  {
    "objectID": "lectures/4.3-Packages.html#so",
    "href": "lectures/4.3-Packages.html#so",
    "title": "Packages",
    "section": "So…",
    "text": "So…\n\ndir(&lt;package name&gt;) lists all ‘things’ that &lt;package&gt; contains.\nBy convention, things that start with __ are ‘private’ (you shouldn’t change them) and things that start and end with __ are metadata (e.g. __version__).\nEverything else you can interrogate with help(&lt;package name&gt;.&lt;thing in package&gt;).\n\nWith help(math.log10) you get an answer like this:\nHelp on built-in function log10 in module math:\n\nlog10(x, /)\n    Return the base 10 logarithm of x.\nWith help(math.pi) you get an answer Help on float object…\n\nThis tells you that pi is a float, it doesn’t tell you what Pi is (an irrational number). So here’s another case where the computer gives you a technically correct but not always helpful answer. In the context of the math package, Pi is a float constant."
  },
  {
    "objectID": "lectures/4.3-Packages.html#why-namespaces-matter",
    "href": "lectures/4.3-Packages.html#why-namespaces-matter",
    "title": "Packages",
    "section": "Why Namespaces Matter",
    "text": "Why Namespaces Matter\nConsider this:\nimport math\npi = 4\nprint(math.pi)\nprint(pi)\nSo math.pi and pi are not the same variable!\n\nThe latter is implicitly main.pi but Python doesn’t have a ‘main’ programme namespace. Everything is ‘main’ unless you put it in a package."
  },
  {
    "objectID": "lectures/4.3-Packages.html#more-laziness-aliases",
    "href": "lectures/4.3-Packages.html#more-laziness-aliases",
    "title": "Packages",
    "section": "More Laziness: Aliases",
    "text": "More Laziness: Aliases\nProgrammers hate typing more than they have to:\nimport math\nr = 5\narea = math.pi * r**2\nln = math.log(area)\nprint(ln)\nSo we can use an alias instead:\nimport math as m\nr = 5\narea = m.pi * r**2\nln = m.log(area)\nprint(ln)\nYou will see this used a lot with more complex libraries like Pandas (pd), Geopandas (gpd), and PySAL (ps)."
  },
  {
    "objectID": "lectures/4.3-Packages.html#importing-part-of-a-package",
    "href": "lectures/4.3-Packages.html#importing-part-of-a-package",
    "title": "Packages",
    "section": "Importing Part of a Package",
    "text": "Importing Part of a Package\nSometimes even that is too much typing… or sometimes we only really want one or two things from a much larger package. In that case we can select these specifically:\nfrom math import pi, log10\nprint(pi)\nhelp(log10)\nThis import pi and log10 from math into the ‘main’ namespace."
  },
  {
    "objectID": "lectures/4.3-Packages.html#gotcha",
    "href": "lectures/4.3-Packages.html#gotcha",
    "title": "Packages",
    "section": "Gotcha!",
    "text": "Gotcha!\nNotice the subtle differences here:\n\n\nApproach 1\npi = 3.1415\nprint(pi)      # 3.1415\n\nimport math as m\nprint(m.pi)    # 3.141592...\nprint(pi)      # 3.1415\nprint(math.pi) # Error!\n\nApproach 2\npi = 3.1415\nprint(pi) # 3.1415\n\nfrom math import pi\nprint(pi)      # 3.141592...\nprint(m.pi)    # Error!\nprint(math.pi) # Error!"
  },
  {
    "objectID": "lectures/4.3-Packages.html#packages-make-your-life-easier",
    "href": "lectures/4.3-Packages.html#packages-make-your-life-easier",
    "title": "Packages",
    "section": "Packages Make Your Life Easier",
    "text": "Packages Make Your Life Easier"
  },
  {
    "objectID": "lectures/4.3-Packages.html#additional-resources",
    "href": "lectures/4.3-Packages.html#additional-resources",
    "title": "Packages",
    "section": "Additional Resources",
    "text": "Additional Resources\nA bit of a mish-mash of different explanations:\n\n\n\nLearnPython.org\nRealPython: Namespaces and Scope\nReal Python: Modules and Packages\nProgramiz\nPythonCourse.eu\nTutorialsTeacher.com\nTutorialsPoint.com\n\n\n\nHow to Make a Package in Python\nHow to Create Your First Python Package\nCreate Python Packages for Your Python Code [This is more for distributing your own packages via Pip]"
  },
  {
    "objectID": "lectures/4.1-Functions.html#what-does-a-function-look-like",
    "href": "lectures/4.1-Functions.html#what-does-a-function-look-like",
    "title": "Functions",
    "section": "What Does a Function Look Like?",
    "text": "What Does a Function Look Like?\nlen(&lt;some_list&gt;) is a function\nSo len(...) encapsulates the process of figuring out how long something with ‘countable units’ actually is, whether it’s a string or a list.\nprint(&lt;some_value&gt;) is also a function\nBecause print(...) encapsulates the process of sending output to the command line, a file, or even a database or API!\n\nlen(123) is a Type Error.\nlen(‘123’) is not.\nCan you think why?"
  },
  {
    "objectID": "lectures/4.1-Functions.html#so-what-does-a-function-look-like",
    "href": "lectures/4.1-Functions.html#so-what-does-a-function-look-like",
    "title": "Functions",
    "section": "So What Does a Function Look like?",
    "text": "So What Does a Function Look like?\nAll function ‘calls’ looking something like this:\nfunction_name(...)\nWhere the ‘...’ are the inputs to the function; it could be one variable, 25 variables, a list, even another function!\nAnd if the function ‘returns’ something it will look like this:\nreturn_data = function_name(...)"
  },
  {
    "objectID": "lectures/4.1-Functions.html#in-action",
    "href": "lectures/4.1-Functions.html#in-action",
    "title": "Functions",
    "section": "In Action!",
    "text": "In Action!\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data)) # 8.666666666666666\ndata2 = [200000,2500000,-4,1400000,70,900000]\nprint(calc_mean(data2)) # 833344.3333333334"
  },
  {
    "objectID": "lectures/4.1-Functions.html#but-notice",
    "href": "lectures/4.1-Functions.html#but-notice",
    "title": "Functions",
    "section": "But Notice!",
    "text": "But Notice!\ndata    = [1,25,-4,14,7,9]\ntotal   = 1\nnumbers = []\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\nprint(calc_mean(data))\n\n# Why haven't these changed????\nprint(total)\nprint(numbers)\n\nFunctions encapsulate information: the total and numbers used within the function are different from the variables with the same name that we created outside the function. When we get to libraries and packages you’ll understand why, but the key concept here is ‘namespace’ and that these variables might have the same name but they sit in different namespaces."
  },
  {
    "objectID": "lectures/4.1-Functions.html#simple-function",
    "href": "lectures/4.1-Functions.html#simple-function",
    "title": "Functions",
    "section": "Simple Function",
    "text": "Simple Function\nBy ‘simple’ I don’t mean easy, I mean it does one thing only:\ndef hello():\n  print(\"Hello world!\")\nWe then run it with:\nhello()\nAnd that produces:\nHello world!"
  },
  {
    "objectID": "lectures/4.1-Functions.html#passing-in-information",
    "href": "lectures/4.1-Functions.html#passing-in-information",
    "title": "Functions",
    "section": "Passing in Information",
    "text": "Passing in Information\nWe can pass information to a function if we tell the function what to expect:\ndef hello(name:str):\n  print(f\"Hello {name}!\")\nNow we can do this:\nhello(\"new programmers\")\nAnd that produces:\nHello new programmers!"
  },
  {
    "objectID": "lectures/4.1-Functions.html#getting-information-out",
    "href": "lectures/4.1-Functions.html#getting-information-out",
    "title": "Functions",
    "section": "Getting Information Out",
    "text": "Getting Information Out\nWe can also get information out of them!\ndef hello(name:str) -&gt; str:\n  return f\"Hello {name}!\"\nNow we can do this:\noutput = hello(\"new programmers\")\nprint(output.title())\n# Same as: print(hello(\"new programmers\").title())\nAnd this produces:\n'Hello New Programmers!'"
  },
  {
    "objectID": "lectures/4.1-Functions.html#writing-a-function",
    "href": "lectures/4.1-Functions.html#writing-a-function",
    "title": "Functions",
    "section": "Writing a Function",
    "text": "Writing a Function\ndef &lt;function_name&gt;(&lt;var_name&gt;: &lt;var_type&gt;) -&gt; &lt;var_type&gt;:\n  ...\n  return &lt;var&gt;\nThis can also be written:\ndef &lt;function_name&gt;(&lt;var_name&gt;):\n  ...\n  return &lt;var&gt;\nPython is ‘friendly’ in the sense that all of the &lt;var_type&gt; information is optional, but it will help you (and Python) to know what you were expecting to see happen."
  },
  {
    "objectID": "lectures/4.1-Functions.html#complicating-things",
    "href": "lectures/4.1-Functions.html#complicating-things",
    "title": "Functions",
    "section": "Complicating Things…",
    "text": "Complicating Things…\nds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz': [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ndef get_city_info(city:str, field:str, city_lookup:str='name', data:dict=ds2) -&gt; str:\n  return str(data[field][ data[city_lookup].index(city) ])\n\ncity = 'New York'\nprint(f\"The latitude of {city} is {get_city_info(city,'lat')}\")\n# The latitude of New York is 40.71"
  },
  {
    "objectID": "lectures/4.1-Functions.html#additional-resources",
    "href": "lectures/4.1-Functions.html#additional-resources",
    "title": "Functions",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nWhat is a function?\nPython functions\nBuilt-in functions\nDefine your own functions\nTypes of functions\nDefining a function\n\n\n\nFunction arguments\nArgument lists\nKeyword arguments\nReturn values\nDecorators\nVariable Scopes\nRobust Python with Type Hints"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nConsider how these two data structures differ:\ncities = [\n  {'name': 'London', 'loc': [51.5072, 0.1275], 'tz': +0}, \n  {'name': 'New York', 'loc': [40.7127, 74.0059], 'tz': -5}, \n  {'name': 'Tokyo', 'loc': [35.6833, 139.6833], 'tz': +8}\n]\nOr:\ncities = {\n  'London': {'loc': [51.5072, 0.1275], 'tz': +0}, \n  'New York': {'loc': [40.7127, 74.0059], 'tz': -5}, \n  'Tokyo': {'loc': [35.6833, 139.6833], 'tz': +8}\n}\n\nWhy don’t you copy the code and then see how to access different fields/values? What might be the pros and cons of each?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#implications",
    "href": "lectures/3.3-DOLs_to_Data.html#implications",
    "title": "Data Structures",
    "section": "Implications",
    "text": "Implications\n\nSo we can mix and match dictionaries and lists in whatever way we need to store… ‘data’. The question is then: what’s the right way to store our data?\n\n\nAnswer: the way that makes the most sense to a human while also being the most robust for coding."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nHow do these data structures differ?\n\n\nOption 1\nds1 = [\n  ['lat','lon','name','tz'],\n  [51.51,0.13,'London',+0],\n  [40.71,74.01,'New York',-5],\n  [35.69,139.68,'Tokyo',+8]\n]\n\nOption 2\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\n\nTo understand why I’m asking this question, here are two example questions I’d like you to try to answer:\n\nWhat’s the average latitude of these three cities?\nWhat’s the time zone of Tokyo?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "href": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "title": "Data Structures",
    "section": "Thinking it Through",
    "text": "Thinking it Through\nWhy does this work for both computers and people?\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nWe are doing away with the idea that the order of columns matters (humans don’t care that a city’s name is in the first column, and a city’s latitude in the second). We just want to find the column. But because we have a dictionary-of-lists we can ensure that the row order is preserved. Let’s see this in action."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#examples",
    "href": "lectures/3.3-DOLs_to_Data.html#examples",
    "title": "Data Structures",
    "section": "Examples",
    "text": "Examples\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nprint(ds2['name'][0]) # London\nprint(ds2['lat'][0])  # 51.51\nprint(ds2['tz'][0])   # 0\nSo 0 always returns information about London, and 2 always returns information about Tokyo. But it’s also easy to ask for the latitude (ds2['lat'][0]) or time zone (ds2['tz'][0]) value once you know that 0 is London!\n\nBut there’s another advantage that’s not quite so obvious: for the computer because everything of type ‘lat’ is a float, everything of type ‘tz’ is an integer, and everything of type ‘name’ is a string, it’s a lot easier to work with each column as data."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "href": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "title": "Data Structures",
    "section": "How is that easier???",
    "text": "How is that easier???\nRemember that we can use any immutable ‘thing’ as a key. This means…\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ncity_nm = 'Tokyo'\ncity_idx = ds2['name'].index(city_nm)\n\nprint(f\"The time zone of {city_nm} is {ds2['tz'][city_idx]}\")\nWe can re-write this into a single line as:\ncity_nm = 'New York'\nprint(f\"The time zone of {city_nm} is {ds2['tz'][ ds2['name'].index(city_nm)]}\")\n\nThis achieves several useful things:\n\nIt is fast: faster than iterating over a list-of-lists or dictionary-of-dictionaries. In other words, there is no iteration at all!\nAll data in a list is of the same type so we can easily add checks to make sure that it’s valid.\nWe can also easily calculate an average/max/min/median and so on (as we’ll see later) without even having to look at any other columns!\nWe can add more columns instantly and the process of finding something is just as fast as it is now. And adding more rows doesn’t make it much slower either!\n\nAlso, notice how in these two examples we don’t try to write the second example in one go: first, we work it out as a set of steps: how do we figure out what ‘row’ (position in the list) Tokyo is in? Now that we’ve got that, how do we retrieve the time zone value for Tokyo? We know that code works, now let’s do variable substitution, as we would if we were doing maths: we can replace the city_idx in the time zone lookup with ds2['name'].index('Tokyo')."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#additional-resources",
    "href": "lectures/3.3-DOLs_to_Data.html#additional-resources",
    "title": "Data Structures",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n8 Data Structures Every Data Scientist Should Know (by a CASA alum)"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#so-key---value",
    "href": "lectures/3.1-Dictionaries.html#so-key---value",
    "title": "Dictionaries",
    "section": "So: Key -> Value",
    "text": "So: Key -&gt; Value\nThe key can be almost anything that is immutable (aka. hashable). So these are all ‘legit’:\nlookup[1]             # Int\nlookup(52.1)          # Float\nlookup['1']           # String\nlookup['Jon Reades']  # String\nk = 'Jon Reades'\nlookup[k]             # String variable\nlookup[(52.1, -0.04)] # Tuple\nBut this is not:\nlookup[['Jon','Reades']] # Error, unhashable type\nThat’s because a list is not immutable.\n\nAgain, just like a real dictionary: you don’t have multiple entries for ‘dog’, otherwise the dictionary wouldn’t work. You might have multiple definitions: which is to say, the key might return multiple values."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "href": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "title": "Dictionaries",
    "section": "Deliberately Similar",
    "text": "Deliberately Similar\nNotice the differences when creating them, and the absence of difference when accessing them.\n\n\nList\ncities = [\n  'San Francisco',\n  'London',\n  'Paris',\n  'Beijing']\n  \n# Prints London\nprint(cities[2]) \n\nDict\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\n# Prints pop of London\nprint(cities['London'])\n\n\nSo why might we prefer the dictionary?\n\nDicts are created using: d = { key: value, key: value }\nDicts are accessed using: d[key]\n\nSo the only difference between lists and dicts is: [...] and {...} when they are created.\nOver the next couple of weeks we’ll see ways that you can store more information in a list and also why lists are sometimes more powerful than you realise… if you can think about your data in an entirely new way. But for simple key/value stuff it’s hard to beat a dictionary!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "href": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "title": "Dictionaries",
    "section": "It’s All About Access",
    "text": "It’s All About Access\nSpecifically: do we need sequential or random access?\n\n\nList\n\n\n\nindex\nvalue\n\n\n\n\n0\nSan Francisco\n\n\n1\nLondon\n\n\n2\nParis\n\n\n3\nBeijing\n\n\n\n\nDict\n\n\n\nkey\nvalue\n\n\n\n\nSan Francisco\n837442\n\n\nLondon\n8673713\n\n\nParis\n2229621\n\n\nBeijing\n21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values",
    "href": "lectures/3.1-Dictionaries.html#getting-values",
    "title": "Dictionaries",
    "section": "Getting Values",
    "text": "Getting Values\nThere are two ways to retrieve values from a dictionary:\n\ncities['Beijing']\ncities.get('Beijing')\n\nWhy have two? Consider:\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nprint(cities['Sao Paulo'])     # Throws KeyError\nprint(cities.get('Sao Paulo')) # Returns None\nprint(cities.get('Sao Paulo','No Data')) # Returns 'No Data'\n\nThe first triggers an error, the second returns None. Errors can be unfriendly: do you want your entire Python program to fail because a single city is missing, or would you rather than it did something a little more sensible such as… skipping the row or returning a sensible default?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "href": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "title": "Dictionaries",
    "section": "Getting Values (cont’d)",
    "text": "Getting Values (cont’d)\nIf we want to think about whether a value is in the dictionary (as opposed to just retrieving it) then notice these options:\nc = cities.get('Sao Paulo')\nif not c:\n  print(\"Sorry, no city by that name.\")\n\nif 'Beijing' in cities:\n  print(\"Found Beijing!\")\n\nThe first example works because cities.get returns None, which is the same as ‘undefined’ for Python. So we can use ‘not’ to imply ‘if c is not defined then do something…’\nThe second example works because we are implicitly treating the keys in the cities dictionary as a list and looking to see if Beijing is one of the values in that list.\nPython often benefits and suffers from TMTOWTDI (There’s More Than One Way To Do It): think of these as being different ways to say the same thing, but depending on where you want to put the emphasis you would choose one or the other."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#setting-values",
    "href": "lectures/3.1-Dictionaries.html#setting-values",
    "title": "Dictionaries",
    "section": "Setting Values",
    "text": "Setting Values\nIt’s the same process to update an existing value or create a new one:\ncities = {}  # Empty dictionary\ncities['Beijing'] = 21716620    # Sets key-&gt;value\ncities['Toronto'] = 2930000     # Sets key-&gt;value\n\nprint(cities['Toronto'])        # Prints 2930000\ndel cities['Toronto']           # Deletes Toronto key (and value)\ncities.pop('Toronto','Default') # Prints 'Default' b/c key not found\nprint(cities)\nThis last command outputs:\n{'Beijing': '0.17'16620}"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#iterating",
    "href": "lectures/3.1-Dictionaries.html#iterating",
    "title": "Dictionaries",
    "section": "Iterating",
    "text": "Iterating\nSimilar to iterating over lists but…\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nfor c in cities:\n  print(c)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nOne really important point to note: here, the cities are printed out in the same order that they were added to the dictionary, but that is not guaranteed! Unlike lists, dictionaries are unordered.\nAlso, how would we print out the population of each city?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#section",
    "href": "lectures/3.1-Dictionaries.html#section",
    "title": "Dictionaries",
    "section": "",
    "text": "Keys\nfor k in cities.keys():\n  print(k)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nValues\nfor v in cities.values():\n  print(v)\nPrints:\n837442\n8673713\n2229621\n21716620\n\nBoth\nfor k,v in cities.items():\n  print(f\"{k} -&gt; {v}\")\nPrints:\nSan Francisco -&gt; 837442\nLondon -&gt; 8673713\nParis -&gt; 837442\nBeijing -&gt; 21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#a-final-note",
    "href": "lectures/3.1-Dictionaries.html#a-final-note",
    "title": "Dictionaries",
    "section": "A Final Note!",
    "text": "A Final Note!\nValues can be almost anything, including a dictionary or list! This opens up some interesting possibilities:\n\n\ncities = {\n  'San Francisco': \n    [37.77, -122.43, 'SFO']\n}\n\ncities = {\n  'San Francisco': {\n    'lat': 37.77,\n    'lon': -122.43,\n    'airport':'SFO'}\n}\nprint(cities['San Francisco']['lat'])\n\nSpoiler: you’re going to encounter this kind of thing a lot.\n\nWhat is this starting to look like? This is basically what JSON is."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "href": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "title": "Dictionaries",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\n\nRemember: in most programming languages dictionaries/hashes are unordered and there is no guarantee that things come out in the same order they went in! They complement ordered lists, they don’t replace them!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#additional-resources",
    "href": "lectures/3.1-Dictionaries.html#additional-resources",
    "title": "Dictionaries",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDictionaries and sets\nComprehensions\nThe Complete Guide to Dictionaries (by a CASA alum)"
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#writing-coding-thinking",
    "href": "lectures/2.7-On_Writing.html#writing-coding-thinking",
    "title": "On Writing & Coding",
    "section": "Writing & Coding == Thinking",
    "text": "Writing & Coding == Thinking\n\nIf writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial. ~ Graham (2022)\n\n \n\nWriting is thinking. To write well is to think clearly. That’s why it’s so hard. ~ McCullough (2002)\n\n\nA story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#large-language-models-llms",
    "href": "lectures/2.7-On_Writing.html#large-language-models-llms",
    "title": "On Writing & Coding",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\n\nChatGPT from OpenAI (an increasingly ‘ironic’ name) is simply the most famous of a growing number of Large Language Models that draw on information found on the web and in open texts to perform sophisticated summarisation tasks."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-for-coding",
    "href": "lectures/2.7-On_Writing.html#why-use-it-for-coding",
    "title": "On Writing & Coding",
    "section": "Why Use it for Coding?",
    "text": "Why Use it for Coding?\nMany programmers use LLMs in coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\n\nThese accelerate code production, but there are significant doubts about code quality."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-code",
    "href": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-code",
    "title": "On Writing & Coding",
    "section": "Why Use it to Learn to Code?",
    "text": "Why Use it to Learn to Code?\nMany students use LLMs to write their code for three reasons:\n\nThey are available at all hours as a personal tutor and advisor.\nThey write better code, more quickly than a learner.\nThey can employ advanced techniques right away.\n\n\n\n\n\n\n\n\nPro-Tip\n\n\nOnly one of these is a good reason."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-for-writing",
    "href": "lectures/2.7-On_Writing.html#why-use-it-for-writing",
    "title": "On Writing & Coding",
    "section": "Why Use It for Writing?",
    "text": "Why Use It for Writing?\nMany writers use LLMs for three reasons:\n\nThey can help to ensure consistency and suggest changes in tone or word choice (i.e. feedback).\nThey can provide information about different strategies the writer can use to tackle a problem (i.e. guidance or training).\nThey can help to ‘translate’ text between languages (i.e. re-use).\n\n\nThese accelerate word production, but there are significant doubts about quality."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-write-academic-english",
    "href": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-write-academic-english",
    "title": "On Writing & Coding",
    "section": "Why Use it to Learn to Write (Academic English)?",
    "text": "Why Use it to Learn to Write (Academic English)?\nMany students use LLMs to write their documents for three reasons:\n\nThey are available at all hours as a personal tutor and advisor.\nThey write better sentences, more quickly than a learner.\nThey draw on a wider range of sources and styles than a learner.\n\n\n\n\n\n\n\n\nPro-Tip\n\n\nOnly one of these is a good reason."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#dont-take-my-word-for-it",
    "href": "lectures/2.7-On_Writing.html#dont-take-my-word-for-it",
    "title": "On Writing & Coding",
    "section": "Don’t Take My Word for It",
    "text": "Don’t Take My Word for It\n\nUsing AI in education is like using a forklift at the gym. The weights do not actually need to be moved from place to place. That is not the work. The work is what happens within you. ~ Seemingly a conversation on YouTube?\n\n\nExcellent! I’ve always heard students say, “When will I ever use this in real life?” And no teacher ever said, “These classes are not the work. The work is what happens within you.” Critical thinking, problem solving, the idea that the world is knowable. That is the work happening with you.\nThe problem is that many students don’t come to uni to learn to write essays, even those that come to learn (not just ‘get a degree’) often see learning as an accumulation of knowledge rather than the strengthening of a skill. So this gym analogy is really helpful for that too"
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#as-well",
    "href": "lectures/2.7-On_Writing.html#as-well",
    "title": "On Writing & Coding",
    "section": "As well…",
    "text": "As well…\nFrom Richmond (2025):\n\nResearch has long shown that meaningfully processing new pieces of information – in other words, thinking hard about something – is vital for retaining that information in your long-term memory… if GenAI tools are used uncritically, there is a risk that they could disrupt or even circumvent the acquisition of new knowledge and the enhancement of existing knowledge – leading to poorer outcomes for learners of all ages.\n\n\n\nMoreover, the impact of AI tools appears to differ between groups of students. Younger students and those with lower prior attainment were more likely to use it in ways that are detrimental to their learning (for example, copying and pasting work without reflection). Research has also shown similar effects of GenAI tools among workers, not just students."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#danger-will-robinson",
    "href": "lectures/2.7-On_Writing.html#danger-will-robinson",
    "title": "On Writing & Coding",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nHere’s what we’ve noticed about LLM use so far:\n\nOver-use of flowery language/complex code.\nOver-confidence in recommending solutions.\nLack of overall structure/coherence.\nUse of non-existent or irrelevant references/coding libraries.1\n\n\n\n\n\n\n\n\nThe Underlying Issue\n\n\n\nLack of actual learning by the student.\nUndermining of learning by other students.\nPointless work by staff.\nDeluge of bullshit (Bergstrom and West 2025)\n\n\n\n\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly.\n\nWe think that ‘intricate methodologies’ and ‘exhaustive reviews’ aren’t just overblown, they actually invite you to be marked down because youre methodology or review are not intricate or exhaustive.\nWe find individual paragraphs that seem reasonable but the whole doesn’t ‘work’ as a single output.\nOne of our PhD students has created a tool to help us find the papers in your bibliography that don’t actually exist, or that seem superficially useful but are not relevant in practice.\nFundamentally, if you’re substituting the LLM for thinking then you’re not going to learn and that means that you are not going to be able to draw the wider connections that mark out strong submissions on assessments and the kind of person that we, or any company, might want to hire!\n\nBasically, they are overconfident and, as a result, tend to either: a) lead the student to think it’s not worth the effort of learning (leading the student into deep trouble); or b) lead the student to overconfidence in their self-assessment of submissions or skills (leading the student into deep trouble).\n\n\nThis has even fueled security vulnerabilities in code using ‘slopsquatting’!"
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#we-recommend",
    "href": "lectures/2.7-On_Writing.html#we-recommend",
    "title": "On Writing & Coding",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder or writer by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose and rationale.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#additional-resources",
    "href": "lectures/2.7-On_Writing.html#additional-resources",
    "title": "On Writing & Coding",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nThe Myth of the ‘Genius Programmer’ (by Google Devs)\nProgram-Aided Language Models\nChain of Thought Prompting\nChatGPT is a blurry JPEG of the Internet 1\nWhy Meta’s latest large language model survived only three days online 2\nModern-Day Oracles or Bullshit Machines?\n\n\n\n‘How can I know what I think till I see what I say?’: How AI is changing education and writing\nWhat does it mean if students think that AI is more intelligent than they are?\nA Student’s Guide to Not Writing with ChatGPT\nA nice tutorial on distinguishing between plagiarism and paraphrasing.\nGreene, A. E. (2013). Writing science in plain English. University of Chicago Press.3\nSword, H. (2017). Air & light & time & space: How successful academics write. Harvard University Press.4\nLLMs can’t stop making up software dependencies and sabotaging everything\n\n\nProbably the best ‘lay person’s’ explanation of how LLMs work/fall apart you’ll ever read.And this one was trained on scientific articles!Not seemingly available for free, but I found a nice little summary (with typos) here.Available for free via JStor."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#as-in",
    "href": "lectures/2.5-The_Command_Line.html#as-in",
    "title": "The CLI",
    "section": "As in…",
    "text": "As in…\n\nWhy are you torturing me with this arcane knowledge?\nWhy do I need to do this when we have slick IDEs now?"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#the-answer",
    "href": "lectures/2.5-The_Command_Line.html#the-answer",
    "title": "The CLI",
    "section": "The Answer?",
    "text": "The Answer?\nNo matter how long you try to avoid it, eventually you’ll find things that can only be solved (or that can be much more quickly solved) using the Command Line Interface (CLI).\nThings like:\n\nInteracting with git is actually easier on the Command Line.\nMaking the most of developer-oriented tools (e.g. docker, GDAL, proj4/6).\nPeeking and poking at (large) files efficiently…\nAutomating things that would be hard/annoying to do manually…\n\nA lot of this ties back to data and servers.\n\nTrue story: 25 years ago I used to process more than 40GB of compressed plain-text data every day from my Titanium PowerBook. But that’s because it was all running on a server in New Jersey while I was in Manhattan. Everything was done using the Command Line and SSH (secure shell).\nMore recently, processing OSM data for the entire UK was possible on my MacBook Air using GDAL and bash scripts but not possible using R/RStudio directly. Basically, the work took so long (&gt; 13 hours) that RStudio thought the script had died and tried to kill it."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#anatomy-of-a-command",
    "href": "lectures/2.5-The_Command_Line.html#anatomy-of-a-command",
    "title": "The CLI",
    "section": "Anatomy of a Command",
    "text": "Anatomy of a Command\ncurl -L http://bit.ly/2vrUFKi | \n  head -3 |  \n  awk -F\",\" '{ print $2, $4, $6; }' &gt; results.txt\nThis command does four things in one ‘line’ on the CLI:\n\ncurl downloads the file and passes the contents to…\nhead which takes the first three rows and passes those to…\nawk which splits the rows on \",\" and takes the 2nd, 4th, and 6th fields and directs them into…\nA file called results.txt\n\n\nNote that results.txt is created if it doesn’t already exist, or overwritten if it does.\nIf you wanted to append to an existing file you would use &gt;&gt; instead of &gt;."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#interacting-with-files",
    "href": "lectures/2.5-The_Command_Line.html#interacting-with-files",
    "title": "The CLI",
    "section": "Interacting with Files",
    "text": "Interacting with Files\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nls\nList\nls .\n\n\ncd\nChange Directory\ncd $HOME or cd ~\n\n\npwd\nPrint Working Directory\npwd\n\n\nmv\nRename/Move file a to b\nmv a.txt b.txt\n\n\nfind\nFind files matching some criteria\nfind . -name \"*.md\"\n\n\n\n\nNotice that most commands on the Command Line involve typing mnemonics (the shortest possible combination of letters that is unique memorable)."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#common-shortcuts",
    "href": "lectures/2.5-The_Command_Line.html#common-shortcuts",
    "title": "The CLI",
    "section": "Common Shortcuts",
    "text": "Common Shortcuts\n\n\n\n\n\n\n\n\nShortcut\nMeans\nExample\n\n\n\n\n.\nThe current working directory\nls .\n\n\n..\nThe directory above the current working one\ncd ..\n\n\n~1\nThe current user’s home directory.\ncd ~\n\n\n/\nThe ‘root’ directory for the entire computer\nls /\n\n\n\"*\"\nA ‘wildcard’ meaning any number of characters in a filename\nfind . -name \"*.md\"\n\n\n\"?\"\nA ‘wildcard’ meaning one character in a filename\nfind . -name \"2.?-*.md\"\n\n\n\n\nThe main reason we care about all this is that all data is stored somewhere and all code executes somewhere. So we want a way to traverse the device efficiently when looking in directories, creating new files, writing different types of data to different places, and so forth. These shortcuts therefore crop up all over the place ‘in the wild’–if you don’t know what they’re telling you then you’ll wonder why your code doesn’t run or you can’t find the data you saved!\n\nThis may be easier to remember and write as cd $HOME, which does the same thing."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "href": "lectures/2.5-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "title": "The CLI",
    "section": "A Simulated Walk Across My Laptop",
    "text": "A Simulated Walk Across My Laptop\ncd /\npwd\n&gt; /\nls\n&gt; Applications  Library  System  Users Volumes ...\ncd $HOME\npwd\n&gt; /Users/casa\nls\n&gt; Applications  Desktop  Dropbox  ...\ncd Dropbox\npwd\n&gt; /Users/casa/Dropbox\nls\n&gt; CASA  Lectures  Practicals ...\n\nModern computers (especially if you’ve grown up around iPhones and Android phones/tablets) are really good at hiding this fact, but that’s because people using phones or tablets really don’t want to be thinking about where their data is being stored, they just want to click save. But when you start coding then you need to start caring a lot more about where something is happening."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#finding-things-in-files",
    "href": "lectures/2.5-The_Command_Line.html#finding-things-in-files",
    "title": "The CLI",
    "section": "Finding Things in Files",
    "text": "Finding Things in Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nless\nPeek at contents of a text file\nless file.txt\n\n\ngrep\nFind lines matching a ‘pattern’ in a file\ngrep 'pattern' file.txt\n\n\nhead\nPeek at first x rows of a text file\nhead -n 10 file.txt\n\n\ntail\nPeek at last x rows of a text file\ntail -n 10 file.txt\n\n\nwc\nCount things (rows, words, etc.)\nwc -l file.txt\n\n\nsed/awk\nComplicated, but powerful, things\nawk -F\",\" '{ print $1, $3; }' file.csv\n\n\n\n\nThe really crucial thing about all of these utilities is that they don’t load the entire file into memory. So you can ‘peek’ into a 15GB text file instantly without waiting four hours for it to load into memory (and then crash your machine). It’s kind of like the anti-Excel."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#time-to-escape",
    "href": "lectures/2.5-The_Command_Line.html#time-to-escape",
    "title": "The CLI",
    "section": "Time to Escape!",
    "text": "Time to Escape!\nSome characters are ‘special’ and need to be escaped. You’ll encounter these both in the shell (a.k.a. command line) and in Python:\n\n\n\n\n\n\n\n\nEscape\nDoes\nExample\n\n\n\n\n\\\nAllows spaces in file names\nless My\\ File\\ with\\ Spaces.txt\n\n\n\\t\nCreates/matches a tab character\n\\tThe start of a paragraph...\n\n\n\\n\nCreates/matches a newline character\nThe end of a row/para...\\n\n\n\n\\r\nCreates/matches a carriage return\nThe end of a row/para...\\r\\n\n\n\n\\$\nLiteral dollar sign (since $ often marks a variable)\nIt costs \\$1,000,000\n\n\n\\!\nLiteral exclamation mark (since ! can mean a number of things)\nDon't forget me\\!\n\n\n\nThis also becomes relevant when you’re dealing with quotes:\n\"\"This is a problem,\" she said.\"\nvs. \n\"\\\"This is a problem,\\\" she said.\"\n\nThe carriage return is only ever encountered on files that have been opened on Windows machines."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#compressingdecompressing-files",
    "href": "lectures/2.5-The_Command_Line.html#compressingdecompressing-files",
    "title": "The CLI",
    "section": "Compressing/Decompressing Files",
    "text": "Compressing/Decompressing Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\ngzip\nCompress/Decompress files\ngzip file.txt\n\n\ngunzip\nDecompress files\ngunzip file.txt.gz1\n\n\n\nThis can also be done using ‘switches’ passed to gzip: gzip -cd (where -d means ‘decompress’)."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#chaining-commands",
    "href": "lectures/2.5-The_Command_Line.html#chaining-commands",
    "title": "The CLI",
    "section": "Chaining Commands",
    "text": "Chaining Commands\nThe CLI becomes much useful with command chaining:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\"\nThe ‘pipe’ (|) takes output from command and ‘pipes’ (aka. passes) it to another.\n\nThis will give you an ‘answer’ much, much, much faster than trying to open the whole file in, say, Excel, Numbers, or even Python."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#redirecting-output",
    "href": "lectures/2.5-The_Command_Line.html#redirecting-output",
    "title": "The CLI",
    "section": "Redirecting Output",
    "text": "Redirecting Output\nWe can redirect outputs in to new files with &gt;, and inputs out of existing files using &lt;:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\" &gt; matches.txt\nSo the output from the previous commands goes into matches.txt as plain-text. The reverse &lt; is only used in very special circumstances so you probably won’t encounter it very often."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#practical-applications",
    "href": "lectures/2.5-The_Command_Line.html#practical-applications",
    "title": "The CLI",
    "section": "Practical Applications",
    "text": "Practical Applications\nMost developers will use one or more of these on a daily basis:"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#a-complex-example",
    "href": "lectures/2.5-The_Command_Line.html#a-complex-example",
    "title": "The CLI",
    "section": "A (Complex) Example",
    "text": "A (Complex) Example\nI do not expect you to understand this, but I do want you to understand why this is important:\ndocker run -v conda:/home/jovyan/work --rm ${DOCKER_NM} start.sh \\\n   conda env export -n ${ENV_NM} | sed '1d;$d' | sed '$d' \\\n   | perl -p -e 's/^([^=]+=)([^=]+)=.+$/$1$2/m' \\\n   | grep -Ev '\\- _|cpp|backports|\\- lib|\\- tk|\\- xorg' &gt; conda/environment_py.yml\n\nThis is how I generated the YAML file used by Anaconda Python installers: it is running a command on a virtual machine, collecting the output, filtering out lines by both row number and textual pattern, and directing this all in the environment_py.yml file. This can be run as part of my ‘build’ of the programming environment. It’s all automated!"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#getting-help",
    "href": "lectures/2.5-The_Command_Line.html#getting-help",
    "title": "The CLI",
    "section": "Getting Help",
    "text": "Getting Help\nThe Software Carpentry people have a whole set of lessons around working with ‘the shell’ (a.k.a. Command Line) that might help you.\n\nThe UNIX Shell.\nMIT’s ‘Missing Semester’ on Vim\n\nIndeed all of MIT’s Missing Semester content could be useful!"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#additional-resources",
    "href": "lectures/2.5-The_Command_Line.html#additional-resources",
    "title": "The CLI",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\nThe Shell/Terminal in general:\n\nAbsolute BEGINNER Guide to the Mac OS Terminal\nLinux Bash Shell for Beginners: Tutorial 1\nBeginner’s Guide to the Bash Terminal\nShell Novice\nHow to use the Command Line\n\n\nSome specific commands:\n\nCat\nGzip/Tar (also a good point about spaces in a file name!)\nGrep\nFind\n\n\nAnd lots more here on using the file system and shell commands"
  },
  {
    "objectID": "lectures/2.3-Lists.html#whats-in-a-list",
    "href": "lectures/2.3-Lists.html#whats-in-a-list",
    "title": "Lists",
    "section": "What’s in a List?",
    "text": "What’s in a List?\nIn the same way that a paper shopping list holds many ‘types’ of shopping in one place, a Python list holds many ‘types’ of data in one place.\nmyList = [1, 3, 5, 7]     # homogenous list\nmyList = [1, \"dog\", 7.01] # heterogenous list \nmyList = []               # empty list\nPython lists are always recognisable by their “square brackets”: [...]"
  },
  {
    "objectID": "lectures/2.3-Lists.html#whats-in-a-list-part-2",
    "href": "lectures/2.3-Lists.html#whats-in-a-list-part-2",
    "title": "Lists",
    "section": "What’s in a List? (Part 2)",
    "text": "What’s in a List? (Part 2)\nIn fact, when I say lists can hold many types of data, I should have said that they can hold any type of data:\nx = 3\ny = \"Foo\"\nz = [\"A\", \"list\", 42]\n\na = [x, y, z] # Holds x, y, *and* list z\nThe output of print(a) is:\n[3, 'Foo', ['A', 'list', 42]]\n\nWe’re going to come back to this a lot later, but for now notice that a list can hold lists!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#using-list-indexes",
    "href": "lectures/2.3-Lists.html#using-list-indexes",
    "title": "Lists",
    "section": "Using List Indexes",
    "text": "Using List Indexes\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nLists are ‘indexed’ numerically from the zero-th element:\n\n\n\n\n\n\n\n\n\n\ngeographers [\n0\n1\n2\n]\n\n\n\n\n\nMassey 1\nHarvey 2\nRose 3\n\n\n\n\nprint(geographers[1]) # Harvey\nprint(geographers[2]) # Rose\nprint(geographers[3]) # Error: List index out of range\n\nAnd notice this error: Python tells you waht the problem is. The issue is understanding what the message means if you don’t know the vocabulary.\n\nhttps://en.wikipedia.org/wiki/Doreen_Massey_(geographer)https://en.wikipedia.org/wiki/David_Harveyhttps://en.wikipedia.org/wiki/Gillian_Rose_(geographer)"
  },
  {
    "objectID": "lectures/2.3-Lists.html#interpolation",
    "href": "lectures/2.3-Lists.html#interpolation",
    "title": "Lists",
    "section": "Interpolation",
    "text": "Interpolation\nWe can also use variables as list indexes:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 0\nprint(geographers[i]) # Massey\nAnything that evaluates (i.e. resolves) to a number can be used as an index:\ni = 1\nprint(geographers[i+1]) # Rose\nprint(geographers[ (i-2+1)*2 ]) # Massey"
  },
  {
    "objectID": "lectures/2.3-Lists.html#countdown",
    "href": "lectures/2.3-Lists.html#countdown",
    "title": "Lists",
    "section": "Countdown!",
    "text": "Countdown!\nWe can ‘count’ backwards from the end of the list using negative numbers:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[-1] ) # Rose\nprint( geographers[-2] ) # Harvey"
  },
  {
    "objectID": "lectures/2.3-Lists.html#does-not-compute",
    "href": "lectures/2.3-Lists.html#does-not-compute",
    "title": "Lists",
    "section": "Does Not Compute!",
    "text": "Does Not Compute!\nErrors can be scary… but informative!\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[4] )\n\nIndexError: list index out of range\n\nAnd then try:\nprint( geographers[1.25] )\n\nTypeError: list indices must be integers or slices, not float\n\nNotice that Python gives us important hints about the source of the problem!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#slicing-dicing-lists",
    "href": "lectures/2.3-Lists.html#slicing-dicing-lists",
    "title": "Lists",
    "section": "Slicing & Dicing Lists",
    "text": "Slicing & Dicing Lists\nYou can access more than one element at a time using a slice:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[0:2] ) # ['Massey','Harvey']\nprint( geographers[1:] )  # ['Harvey', 'Rose']\nprint( geographers[-2:] ) # ['Harvey', 'Rose']\nThe syntax for a slice is: list[ &lt;start_idx&gt;, &lt;end_idx&gt; ], but end_idx is not included in the slice. And notice:\nprint( geographers[1:2] ) # ['Harvey']\nprint( geographers[1] )   #   Harvey\n\nIt’s really subtle, but notice that a slice always returns a list, even if it’s just a list containing one thing. So geographers[1]=='Harvey' but geographers[1:2]==['Harvey']. Not the same thing!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself",
    "href": "lectures/2.3-Lists.html#test-yourself",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat do you think this will produce?\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 2\nprint( geographers[ (i-3)**2-4:-1 ] )\nSee if you can work out in your head before typing it!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#wheres-wally",
    "href": "lectures/2.3-Lists.html#wheres-wally",
    "title": "Lists",
    "section": "Where’s Wally?",
    "text": "Where’s Wally?\nlist.index(...) tells you where something can be found in a list:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index(\"Harvey\") # 1\ngeographers.index(\"Massey\") # 0\nCombining ideas that will become very useful later:\nprint(geographers[ geographers.index(\"Massey\") ])\nWhat do you think this prints? Why does it work at all?\n\nThis last example looks a little strange, but what if I had a separate list with first names, or Wikipedia links, or other information about these geographers? Because list.index(x) returns an integer we can use it as an index for accessing another list."
  },
  {
    "objectID": "lectures/2.3-Lists.html#wheres-wally-part-2",
    "href": "lectures/2.3-Lists.html#wheres-wally-part-2",
    "title": "Lists",
    "section": "Where’s Wally (Part 2)",
    "text": "Where’s Wally (Part 2)\nlist.index(...) has one flaw:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index('Batty')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: 'Batty' is not in list\nIf ‘throwing an error’ is overkill, then here’s another way:\nif 'Batty' in geographers:\n    print(\"Found Mike!\")\nelse:\n    print(\"Not a geographer!\")"
  },
  {
    "objectID": "lectures/2.3-Lists.html#sorting",
    "href": "lectures/2.3-Lists.html#sorting",
    "title": "Lists",
    "section": "Sorting",
    "text": "Sorting\nWe can sort lists in alpha-numerical order:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.sort()\nprint(geographers) # ['Harvey', 'Massey', 'Rose']\nAnd we can reverse-sort too:\ngeographers.sort(reverse=True)\nprint(geographers) # ['Rose', 'Massey', 'Harvey']"
  },
  {
    "objectID": "lectures/2.3-Lists.html#lists-are-mutable",
    "href": "lectures/2.3-Lists.html#lists-are-mutable",
    "title": "Lists",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\nMutable == “liable or subject to change or alteration”\nLet’s replace Rose with Jefferson1 in the list.\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers[2] = \"Jefferson\"\nprint(geographers) # ['Massey','Harvey','Jefferson']\nthe-women-cartographers-who-mapped-art-and-science-in-the-20th-century"
  },
  {
    "objectID": "lectures/2.3-Lists.html#addingremoving-items",
    "href": "lectures/2.3-Lists.html#addingremoving-items",
    "title": "Lists",
    "section": "Adding/Removing Items",
    "text": "Adding/Removing Items\nWhen we insert() items into, or pop() items out of, a list we normally need to specify the index.\ngeographers = [\"Massey\", \"Harvey\", \"Jefferson\"]\ngeographers.insert(0,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.insert(3,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'von Humboldt', 'Jefferson']\nAnd in ‘reverse’:\ngeographers.pop(3) # 'von Humboldt'\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\n\nNotice also that insert modifies the list and returns nothing, while pop modifies the list and returns the value that you ‘popped’."
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself-1",
    "href": "lectures/2.3-Lists.html#test-yourself-1",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nThere are two ways to remove David Harvey from the list of geographers without writing this:\ngeographers = ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.pop(2) # Do not use this answer!\n\nYou can adapt an example we saw earlier in ‘Finding Things’.\nYou can use Google to see if there are list operations we’ve not covered.\n\n\nHints: remove and del are both options for 2."
  },
  {
    "objectID": "lectures/2.3-Lists.html#concatenating",
    "href": "lectures/2.3-Lists.html#concatenating",
    "title": "Lists",
    "section": "Concatenating",
    "text": "Concatenating\nWe combine lists using addition:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = female_geographers + male_geographers\nprint(all_geographers)    # ['Rose', ..., 'Hägerstrand']\nprint(all_geographers[0]) # Rose"
  },
  {
    "objectID": "lectures/2.3-Lists.html#appending",
    "href": "lectures/2.3-Lists.html#appending",
    "title": "Lists",
    "section": "Appending",
    "text": "Appending\nNote that this is not the same!\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers   = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\n all_geographers.append(male_geographers)\nprint(all_geographers) # [['Rose',...], [..., 'Hägerstrand']]\nprint(all_geographers[0]) # ['Rose', ..., 'Jefferson']\nWhat do you think has happened here?"
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself-2",
    "href": "lectures/2.3-Lists.html#test-yourself-2",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nmale_geographers.append('Batty')\nprint(male_geographers)\nWhat do you think this will produce? And why do you think that append appears to do something different in these two examples?"
  },
  {
    "objectID": "lectures/2.3-Lists.html#how-many-geographers-do-i-know",
    "href": "lectures/2.3-Lists.html#how-many-geographers-do-i-know",
    "title": "Lists",
    "section": "How many geographers do I know?",
    "text": "How many geographers do I know?\nlen(...) gives you the length of ‘countable’ things:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nlen(geographers) # 3\nBut…\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nprint( len(all_geographers) ) # 2"
  },
  {
    "objectID": "lectures/2.3-Lists.html#whos-on-the-list",
    "href": "lectures/2.3-Lists.html#whos-on-the-list",
    "title": "Lists",
    "section": "Who’s on the List?",
    "text": "Who’s on the List?\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers) # True\nprint(\"Batty\" in geographers)  # False\nBut…\ngeographers.index('Batty')\nis a ValueError that causes your Python code to fail.\n\nWhy might you choose one of these over the other?"
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself-3",
    "href": "lectures/2.3-Lists.html#test-yourself-3",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nHow would you change this code:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers)\nprint(\"Batty\" in geographers)\nSo that it prints:\nFalse\nTrue\nYou will have seen the answer to this in Code Camp, but you can also Google it†!\n\n\n† I’d suggest looking first at Stack Overflow answers in most cases."
  },
  {
    "objectID": "lectures/2.3-Lists.html#tuples-not-actually-a-list",
    "href": "lectures/2.3-Lists.html#tuples-not-actually-a-list",
    "title": "Lists",
    "section": "Tuples: Not Actually a List",
    "text": "Tuples: Not Actually a List\nBecause they come up a lot in geo-data, it’s worth knowing about tuples, which are basically immutable lists:\nt = (52.124021, -0.0012012)\nprint(type(t)) # &lt;class 'tuple'&gt;\nprint(t)       # (52.124021, -0.0012012)\nprint(t[0])    # 52.124021\nBut this…\nt[0] = 25.1203210\nwill throw an error:\n\nTypeError: ‘tuple’ object does not support item assignment"
  },
  {
    "objectID": "lectures/2.3-Lists.html#additional-resources",
    "href": "lectures/2.3-Lists.html#additional-resources",
    "title": "Lists",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nLists in Python\nTuples in Python\nRange and lists\nSequence types\nThe Complete Guide to Lists (by a CASA alum!)"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#variables-have-types",
    "href": "lectures/2.1-Python_the_Basics-1.html#variables-have-types",
    "title": "Python: the Basics",
    "section": "Variables Have Types…",
    "text": "Variables Have Types…\n\n\n\nName\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#but-we-can-change-that",
    "href": "lectures/2.1-Python_the_Basics-1.html#but-we-can-change-that",
    "title": "Python: the Basics",
    "section": "… But We Can Change That",
    "text": "… But We Can Change That\nMessage starts as a string:\nmsg = '42'\ntype(msg) # str\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg) # change to int\nAnd back to a string:\nmsg = str(msg)\ntype(msg) # back to str\nAnd notice:\nprint(str(int('42'))) # string from int from string\n\nThat last line of code is intended to start familiarising you with Python syntax: programmers rarely do one operation per line of code if they can do more than one, so you’ll often see nested parentheses like this and you need to learn how to read this kind of code starting from the inner-most parentheses (the int()) and working outwards from there to str() and finally print()."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#and-they-are-all-objects",
    "href": "lectures/2.1-Python_the_Basics-1.html#and-they-are-all-objects",
    "title": "Python: the Basics",
    "section": "…And They Are All Objects",
    "text": "…And They Are All Objects\nOne to remember for the session on objects and classes:\nisinstance(msg,object)     # True\nisinstance(answer,object)  # True\nisinstance(pi,object)      # True\nisinstance(c,object)       # True\nisinstance(correct,object) # True\n\nLike str(), int(), and print(), you see here another command — or ‘function’ in Python terminology — called ininstance. An ‘instance’ of something is just a way of asking ‘is it a type of’: so the first line asks if string msg is a kind of object, and the answer to that is: True. But it’s the same for answer (an integer), pi (a floating point number), and so on. They are all a kind of object, and we’ll see why that is a useful answer in a few weeks’ time."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#variables-have-names",
    "href": "lectures/2.1-Python_the_Basics-1.html#variables-have-names",
    "title": "Python: the Basics",
    "section": "Variables Have Names…",
    "text": "Variables Have Names…\nRules for variable names:\n\nThey cannot start with a number: item3 is valid, 3items is not.\nWhite space and symbols are not allowed, but _ is allowed: my_variable is valid, my-variable, my$variable, and my variable are not.\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used; but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#but-not-all-words-are-allowed",
    "href": "lectures/2.1-Python_the_Basics-1.html#but-not-all-words-are-allowed",
    "title": "Python: the Basics",
    "section": "…But Not All Words Are Allowed",
    "text": "…But Not All Words Are Allowed\nDo not try to use any of these as variable names. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#simple-operations-on-variables",
    "href": "lectures/2.1-Python_the_Basics-1.html#simple-operations-on-variables",
    "title": "Python: the Basics",
    "section": "Simple Operations on Variables",
    "text": "Simple Operations on Variables\nLet’s start with x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n15\n\n\nDifference\nx - y\n5\n\n\nProduct\nx * y\n50\n\n\nQuotient\nx / y\n2.0\n\n\n‘Floored’ Quotient\nx // y\n2\n\n\nRemainder\nx % y\n0\n\n\nPower\npow(x,y) or x**y\n100000\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#strings-are-different",
    "href": "lectures/2.1-Python_the_Basics-1.html#strings-are-different",
    "title": "Python: the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nWhen you do things with strings the answers can look a little different. Let’s start with x=\"Hello\" and y=\"You\" and z=2…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n'HelloYou'\n\n\nDifference\nx - y\nTypeError\n\n\nProduct\nx * z\nHelloHello\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#using-strings-to-output-information",
    "href": "lectures/2.1-Python_the_Basics-1.html#using-strings-to-output-information",
    "title": "Python: the Basics",
    "section": "Using Strings to Output Information",
    "text": "Using Strings to Output Information\nPython has no fewer than three ways output information:\n\nstring concatenation using +;\nstring formatting using &lt;str&gt;.format(&lt;variables&gt;); and\nf-strings using f\"{variable_1} some text {variable_n}\".\n\nThere are pros and cons to each:\nx = 24\ny = 'Something'\nprint(\"The value of \" + y + \" is \" + str(x))\nprint(\"The value of {0} is {1}\".format(y, x))\nprint(f\"The value of {y} is {x}\")\n\nObviously, a really common requirement that programmers have is ‘output a nicely formatted string containing information about the variables in my program’.\nI rather like f-strings because they can actually contain any code you like (you could, for instance, write `f”The square root of {y} is {x**(1/2)}” and it would work. However, concatenation is the easiest to learn."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#operators-assemble",
    "href": "lectures/2.1-Python_the_Basics-1.html#operators-assemble",
    "title": "Python: the Basics",
    "section": "Operators Assemble!",
    "text": "Operators Assemble!\nAlways pay attention to precedence:\nx, y = 10, 5\nx + y * 2         # == 20\n(x + y) * 2       # == 30\nx + y * 2 / 3     # == 13.3333333333334\nx + y * (2/3)     # == 13.3333333333332\n(x + y) * (2/3)   # == 10.0\nAnd here’s a subtle one:\n(x * y) ** 2/3    # == 833.333333333334\n(x * y) ** (2/3)  # == 13.5720880829745\nThe full list is here.\n\nIf you’re a little rusty on exponents, that last example is the cube root of (x*y)**2. But the key point is that formatting is not what matters here, it’s operator precedence: one has parentheses, the other does not, so they are evaluated differently. To make the first one more clear we might use (x * y)**2 / 3 or even ((x*y)**2)/3.\nAlso notice that with the two floats in the first block you do not always get the same result from operations that should give the same answer. Non-terminating decimals (e.g. 1/3) will always be rounded, unpredictably, by the computer because it doesn’t have infinite memory. The process of rounding means that you need to be very careful comparing floats (more on this later)."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#comparing-variables",
    "href": "lectures/2.1-Python_the_Basics-1.html#comparing-variables",
    "title": "Python: the Basics",
    "section": "Comparing Variables",
    "text": "Comparing Variables\nFor numeric variables comparisons are easy.\n\n\n\nOperator\nInput\nResult\n\n\n\n\n==\n10 == 5\nFalse\n\n\n!=\n10 != 5\nTrue\n\n\n&lt;, &lt;=\n10 &lt; 5\nFalse\n\n\n&gt;, &gt;=\n10 &gt; 5\nTrue\n\n\n\n\nThe result of any (successful) comparison is a Boolean (True/False). We can save the output of this comparison to a new variable (e.g. z = x &gt; y).\nThis last example has to do with the way that strings are compared."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#strings-are-different-1",
    "href": "lectures/2.1-Python_the_Basics-1.html#strings-are-different-1",
    "title": "Python: the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nBut notice:\nw, x, y, z = '4a','4a','365','42'\nw == x  # True\nw != y  # True\nx &gt; y   # True\nx &gt; z   # True\nWhy is 4a greater than both 365 and 42?"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#danger-will-robinson",
    "href": "lectures/2.1-Python_the_Basics-1.html#danger-will-robinson",
    "title": "Python: the Basics",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nNotice the very subtle visual difference between = and ==!\nx = 5\ny = 10\nx = y   # Probably a mistake: setting x to the value of y\nx == y  # True, because x and z are now both set to 10\nRemember this!\n\nConfusing these two operators is the most common source of mistakes early on when learning to code in Python! One (=) does assignment, the other (==) does comparison."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#common-mistakes",
    "href": "lectures/2.1-Python_the_Basics-1.html#common-mistakes",
    "title": "Python: the Basics",
    "section": "Common Mistakes",
    "text": "Common Mistakes\nHere’s the output from some attempts at comparison:\nx, y = '42', 42\nx==y   # False\nx&gt;y    # An error!\nThis last line produces:\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: '&gt;' not supported between instances of 'str' and 'int'\nIf we want to compare them then we’ll need to change their type:\nx &gt; str(y)   # False\nint(x) &lt;= y  # True\nx &gt; = str(y) # Also True\n\nA really common mistake is to think that string (str) \"42\" is the same as the integer (int) 42.\nNotice that in the first example we can say that 42 is clearly not the same as ‘42’, but we can’t say whether it’s more or less because that’s non-sensical in this context. So this is a computer being totally logical but not always sensible.\nAlso notice the syntax for this: we have str(&lt;something&gt;) and int(&lt;something&gt;) to convert between types. These are functions, which we’ll spend a lot more time on next week!\nWhy might it be (fractionally) faster to compare integers than strings?"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#additional-resources",
    "href": "lectures/2.1-Python_the_Basics-1.html#additional-resources",
    "title": "Python: the Basics",
    "section": "Additional Resources",
    "text": "Additional Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\nTypes of Data\nVariables and expressions\nStrings\nThe string type\nCommon string methods\nFormatting strings\nSplitting and joining\nNumeric types\nThe bool type\nStoring Data in Variables"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#spot-the-difference",
    "href": "lectures/12.3-Clustering.html#spot-the-difference",
    "title": "Clustering",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nClassification\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms (A) present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in 1–2 dimensions\n\n\nClustering\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms A present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in &lt; 9 dimensions\n\n\n\nClustering algorithms can suffer from the ‘curse of dimensionality’ such that high-dimensional spaces cluster poorly without either dimensionality reduction or the use of specialist algorithms such as Spherical k-Means."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#the-first-geodemographic-classification",
    "href": "lectures/12.3-Clustering.html#the-first-geodemographic-classification",
    "title": "Clustering",
    "section": "The First Geodemographic Classification?",
    "text": "The First Geodemographic Classification?\n\nSource: booth.lse.ac.uk/map/"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#more-than-100-years-later",
    "href": "lectures/12.3-Clustering.html#more-than-100-years-later",
    "title": "Clustering",
    "section": "More than 100 Years Later",
    "text": "More than 100 Years Later\n\nSource: vis.oobrien.com/booth/"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "href": "lectures/12.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "title": "Clustering",
    "section": "Intimately Linked to Rise of The State",
    "text": "Intimately Linked to Rise of The State\n\nGeodemographics only possible in context of a State – without a Census it simply wouldn’t work… until now?\nClearly tied to social and economic ‘control’ and intervention: regeneration, poverty & exclusion, crime, etc.\nPresumes that areas are the relevant unit of analysis; in geodemographics these are usually called neighbourhoods… which should ring a few bells.\nIn practice, we are in the realm of ‘homophily’, a.k.a. Tobler’s First Law of Geography"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#where-is-it-used",
    "href": "lectures/12.3-Clustering.html#where-is-it-used",
    "title": "Clustering",
    "section": "Where is it used?",
    "text": "Where is it used?\nAnything involving grouping individuals, households, or areas into larger ‘groups’…\n\nStrategic marketing (above the line, targeted, etc.)\nRetail analysis (store location, demand modelling, etc.)\nPublic sector planning (resource allocation, service development, etc.)\n\nCould see it as a subset of customer segmentation."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#problem-domains",
    "href": "lectures/12.3-Clustering.html#problem-domains",
    "title": "Clustering",
    "section": "Problem Domains",
    "text": "Problem Domains\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nSupervised\nRegression\nClassification\n\n\nUnsupervised\nDimensionality Reduction\nClustering\n\n\n\n\n\nIn classification we ‘know’ the answer (we test against labels).\nIn clustering we don’t ‘know’ the answer (we look for clusters)."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#measuring-fit",
    "href": "lectures/12.3-Clustering.html#measuring-fit",
    "title": "Clustering",
    "section": "Measuring ‘Fit’",
    "text": "Measuring ‘Fit’\n\nUsually working towards an ‘objective criterion’ for quality… these are known as cohesion and separation measures."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#how-your-data-looks",
    "href": "lectures/12.3-Clustering.html#how-your-data-looks",
    "title": "Clustering",
    "section": "How Your Data Looks…",
    "text": "How Your Data Looks…\nClustering is one area where standardisation (and, frequently, normalisation) are essential:\n\nYou don’t (normally) want scale in any one dimension to matter more than scale in another.\nYou don’t want differences between values in one dimension to matter more than differences in another.\nYou don’t want skew in one dimension to matter more than skew in another.\n\nYou also want uncorrelated variables… why?"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#first-steps",
    "href": "lectures/12.3-Clustering.html#first-steps",
    "title": "Clustering",
    "section": "First Steps",
    "text": "First Steps\nYou will normally want a continuous variable… so these types of data are especially problematic:\n\nDummies / One-Hot Encoded\nCategorical / Ordinal\nPossible solutions: k-modes, CCA, etc."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#performance",
    "href": "lectures/12.3-Clustering.html#performance",
    "title": "Clustering",
    "section": "Performance",
    "text": "Performance\nTypically about trade-offs between:\n\n\n\nAccuracy\nGeneralisation"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#trade-offs",
    "href": "lectures/12.3-Clustering.html#trade-offs",
    "title": "Clustering",
    "section": "Trade-Offs",
    "text": "Trade-Offs\nNeed to balance:\n\nAbility to cluster at speed.\nAbility to replicate results.\nAbility to cope with fuzzy/indeterminate boundaries.\nAbility to cope with curse of dimensionality.\nUnderlying representation of group membership…"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#visualising-the-trade-offs",
    "href": "lectures/12.3-Clustering.html#visualising-the-trade-offs",
    "title": "Clustering",
    "section": "Visualising the Trade-Offs",
    "text": "Visualising the Trade-Offs\n\n\n\nNotice the limitations to k-means: it may be fast but it’s got problems if your data is non-linear/non-Gaussian.\nAnd this doesn’t even include options like HDBSCAN, HAC/Hierarchical Clustering, and many more!\n\n\nDetails on scikit-learn.org."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#putting-it-all-into-context",
    "href": "lectures/12.3-Clustering.html#putting-it-all-into-context",
    "title": "Clustering",
    "section": "Putting it All into Context",
    "text": "Putting it All into Context"
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#location-quotient",
    "href": "lectures/12.1-Diverse_Data.html#location-quotient",
    "title": "Diverse Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R. \\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[&lt; 1\\]\n\n\nLow Regional Share\n\\[&gt; 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#herfindahl-hirschman-index",
    "href": "lectures/12.1-Diverse_Data.html#herfindahl-hirschman-index",
    "title": "Diverse Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry: \\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[&lt; 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#shannon-entropy",
    "href": "lectures/12.1-Diverse_Data.html#shannon-entropy",
    "title": "Diverse Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure: \\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#additional-resources",
    "href": "lectures/12.1-Diverse_Data.html#additional-resources",
    "title": "Diverse Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nStop aggregating away the signal in your data"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#what-is-it",
    "href": "lectures/11.2-Transformation.html#what-is-it",
    "title": "Transformation",
    "section": "What is it?",
    "text": "What is it?\nData transformation just means changing the raw data in some way to make it more tractable for analysis.\nFor example:\n\nCentering the mean on zero is an obvious example.\nBut we can also do quite complicated things (with caution!) in order to get a distribution that we can work with using statistical tests that have certain expectations about the data we’re feeding them.\n\n\nSo, even though we add or subtract, multiply or divide, square or log the data, because we are doing the same thing to every observation the underlying relationships between the data are ‘unchanged’."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#transformation-in-1d",
    "href": "lectures/11.2-Transformation.html#transformation-in-1d",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D\n\n\n\\[\nx-\\bar{x}\n\\]\n\n\n\nInput\nOutput\n\n\n\n\n12\n-2\n\n\n13\n-1\n\n\n14\n0\n\n\n15\n+1\n\n\n16\n+2\n\n\n\n\n\nHow is this any different?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#so",
    "href": "lectures/11.2-Transformation.html#so",
    "title": "Transformation",
    "section": "So…",
    "text": "So…\n\nTransformations are mathematical operations applied to every observation in a data set that preserve some of the relationships between them."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#for-example",
    "href": "lectures/11.2-Transformation.html#for-example",
    "title": "Transformation",
    "section": "For Example",
    "text": "For Example\nIf we subtract the mean from everyone’s height then we can immediately tell if someone is taller or shorter than we would expect.\nIf we subtract the mean from everyone’s income then we cannot immediately tell if someone is earning more or less that we would expect.\nSo what is a useful transformation in one context, may not be in another!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#fleshing-this-out",
    "href": "lectures/11.2-Transformation.html#fleshing-this-out",
    "title": "Transformation",
    "section": "Fleshing This Out",
    "text": "Fleshing This Out\nQuestion: How can you tell if you did better than everyone else on the Quiz or on the Final Report?\n\nAnswer: Just subtracting the mean is not enough because the distributions are not the same. For that we also need to standardise the data in some way.\n\\[\nz = \\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nDivide through by the distribution!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#z-score-standardisation",
    "href": "lectures/11.2-Transformation.html#z-score-standardisation",
    "title": "Transformation",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\\[\n\\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nprint(scaler.mean_)\nscaler.transform(x)\n\nThe important thing to note is that if transform data that has not been fit and you get values outside the range used for fitting then you can no longer assume a standard normal."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#interquartile-standardisation",
    "href": "lectures/11.2-Transformation.html#interquartile-standardisation",
    "title": "Transformation",
    "section": "Interquartile Standardisation",
    "text": "Interquartile Standardisation\n\\[\n\\dfrac{x_{i}-x_{Q2}}{x_{Q3}-x_{Q1}}\n\\]\nfrom sklearn.preprocessing import RobustScaler\ntrf = RobustScaler(\n        quantile_range=(25.0,75.0)).fit(x)\ntrf.transform(x)"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#interdecile-standardisation",
    "href": "lectures/11.2-Transformation.html#interdecile-standardisation",
    "title": "Transformation",
    "section": "Interdecile Standardisation",
    "text": "Interdecile Standardisation\n\\[\n\\dfrac{x_{i}-x_{50^{th}}}{x_{90^{th}}-x_{10^{th}}}\n\\]\nprint(\"You've got this...\")\n\nWhy standardise:\n\nWe understand the properties of normal-ish distributions, and can simulate them easily.\nMore ‘power’ in the statistical tools available.\nMany analyses assume that ‘error’ is random and symmetric (homoscedastic, not skewed)."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#group-standardisation",
    "href": "lectures/11.2-Transformation.html#group-standardisation",
    "title": "Transformation",
    "section": "Group Standardisation",
    "text": "Group Standardisation\n\\[\nx'_{a,i} = \\dfrac{x_{ai}}{\\sum_{g} r_{N,g} P_{a,g}}\n\\]\n\nDetails:\n\n\\(x_{a,i}\\) = Value of attribute i in area a.\n\\(P_{a,g}\\) = Population of group g in area a.\n\\(r_{N,g}\\) = National ratio N of group g\n\\(\\sum\\) = Sum for all groups.\n\\(x'_{a,i}\\) = Standardised value of i in area a."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#proportional-normalisation",
    "href": "lectures/11.2-Transformation.html#proportional-normalisation",
    "title": "Transformation",
    "section": "Proportional Normalisation",
    "text": "Proportional Normalisation\n\\[\n\\dfrac{x_{i}}{\\sum{x}_{i=1}^{n}}\n\\]\nimport numpy as np\nx/np.sum(x)\n\nNumpy has a fair few options for implementing this, but note that log means natural log, not log10!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#range-normalisation",
    "href": "lectures/11.2-Transformation.html#range-normalisation",
    "title": "Transformation",
    "section": "Range Normalisation",
    "text": "Range Normalisation\n\\[\n\\dfrac{x_{i}-x_{min}}{x_{max}-x_{min}}\n\\]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(x)\nprint(scaler.data_max_)\nscaler.transform(x)\n\nNormalisation helps in several ways:\n\nScaling is important for comparability\nClustering is particularly sensitive to scale"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#log-transformation",
    "href": "lectures/11.2-Transformation.html#log-transformation",
    "title": "Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nRecall: logs are the inverse of exponentiation!\n\nSo if \\(10^{3} = 1,000\\) then \\(log_{10}(1,000) = 3\\).\nAnd if \\(10^{0} = 1\\) then \\(log_{10}(1) = 0\\)\n\nimport numpy as np\nxhat = np.log(x)\n\n\nWhy is this so common? Esp. in social sciences?\n\n\nNote that numpy uses the natural log in base \\(e\\)."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#why-log-transform",
    "href": "lectures/11.2-Transformation.html#why-log-transform",
    "title": "Transformation",
    "section": "Why Log Transform?",
    "text": "Why Log Transform?\nLet’s assume that \\(x = \\{10, 100, 1000, 10000\\}\\), consider what happens if:\n\nThe formula for the mean is \\(\\frac{\\sum{x}}{n}\\).\nThe formula for variance is \\(\\frac{(x-\\bar{x})^{2}}{n}\\).\n\nThe Natural Log (\\(e\\)) has certain advantages over other logs and should probably be your default choice for log transformations."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#other-transforms",
    "href": "lectures/11.2-Transformation.html#other-transforms",
    "title": "Transformation",
    "section": "Other Transforms…",
    "text": "Other Transforms…\n\nQuantile (maps the PDF of each feature to a uniform distribution)\nSquare Root (often with count data)\nArcsine/Angular (with percentages, proportions, text)\nRank (with care on extreme distributions)\nBox-Cox and Yeo-Johnson (arbitrary power transformations)\n\n\n\nTo report measures of central tendency it’s usually helpful to convert back to the original units.\nThe more extreme the transformation the less meaningful measures of dispersion.\nCorrelation can be significantly affected in either direction.\nCount data can be tricky because you should not have negative values (especially \\(-\\infty\\))."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#when-transforms-dont-help",
    "href": "lectures/11.2-Transformation.html#when-transforms-dont-help",
    "title": "Transformation",
    "section": "When Transforms Don’t Help",
    "text": "When Transforms Don’t Help\nArbitrarily transforming data isn’t a panacea. ‘Robust’ tests can be another approach when all else fails and two common approaches are:\n\nTrimming: cutting off, say, the top and bottom 5% of scores would start to remove skew and offer a more useful view of the central tendency of the data.\nBootstrapping: taking many sub-samples (usually of \\(n-1\\) data points or similar) we can build a picture of how certain metrics vary."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#one-last-note",
    "href": "lectures/11.2-Transformation.html#one-last-note",
    "title": "Transformation",
    "section": "One Last Note",
    "text": "One Last Note\n\nThe term normalization is used in many contexts, with distinct, but related, meanings. Basically, normalizing means transforming so as to render normal. When data are seen as vectors, normalizing means transforming the vector so that it has unit norm. When data are though of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be normal, normalizing means transforming to unit variance.\n\nSource: Stack Exchange"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#whats-wrong-with-this-map",
    "href": "lectures/11.2-Transformation.html#whats-wrong-with-this-map",
    "title": "Transformation",
    "section": "What’s Wrong with this Map?",
    "text": "What’s Wrong with this Map?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#thats-better",
    "href": "lectures/11.2-Transformation.html#thats-better",
    "title": "Transformation",
    "section": "That’s Better!",
    "text": "That’s Better!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#whats-a-projection",
    "href": "lectures/11.2-Transformation.html#whats-a-projection",
    "title": "Transformation",
    "section": "What’s a Projection?",
    "text": "What’s a Projection?\n\nSource"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#additional-resources",
    "href": "lectures/11.2-Transformation.html#additional-resources",
    "title": "Transformation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nNormalisation vs Standardisation – Quantitative analysis\nTransforming Data with R\nData Transformation and Normality Testing\nIntroduction to Logarithms\nWhat is ‘e’ and where does it come from?\nLogarithms - What is e?\nsklearn API reference\nCompare effects of different scalers on data with outliers\nThe things you’ll find in higher dimensions (useful brief discussion of manifolds)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#its-all-connected",
    "href": "lectures/10.2-Wrap-Up.html#its-all-connected",
    "title": "The End is Nigh",
    "section": "It’s All Connected",
    "text": "It’s All Connected\n\n…Engage [with] GIS as an always-interconnected set of technical and social practices… [we approach] critical GIS as an orientation to GIS praxis that ‘does’ GIS from within a questioning stance about how we know. (Elwood and Wilson 2017)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#beta-the-business-problem",
    "href": "lectures/10.2-Wrap-Up.html#beta-the-business-problem",
    "title": "The End is Nigh",
    "section": "\\(\\beta\\) (The Business Problem)",
    "text": "\\(\\beta\\) (The Business Problem)\nRequirements:\n\nUnderstanding of the problem domain (Why?).\n\nWhat is the context?\nWhat are the priorities?\nWhat has (not) been done before?\n\nCritical thinking wrt to how the problem can be solved (How?).\n\nNot all problems are data problems.\nNot all problems are coding problems.\n\nAbility to communicate with decision-makers/budget-holders (What?).\n\nDevelop a narrative.\nFocus on the takeaways.\n\n\n\n\nHow do you develop a narrative? Through story-telling and analogy/metaphor.\nRefer to experience with telco CRM team."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#understand-the-problem",
    "href": "lectures/10.2-Wrap-Up.html#understand-the-problem",
    "title": "The End is Nigh",
    "section": "Understand the Problem",
    "text": "Understand the Problem\n\nA standard template emerges when reading such work. A cursory review of existing research is reeled off and then set aside. Central questions are reframed in terms of reproducing statistical features of the phenomenon, such as the highly skewed dis- tribution of earthquake magnitudes or city populations. A highly stylized computational, statistical, or mathematical model is demonstrated to exhibit the required statistical features, implying that the model explains the real-world phenomenon. Filling in the details of real-world mechanisms that explain why the model works can be done later by domain-specific experts. The implication is that a general explanatory mechanism is more significant than incidental details of any specific field. (O’Sullivan and Manson 2015)\n\n\n\nDiscussion with telco re: modelling sub-cell level mobility."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#delta-the-data",
    "href": "lectures/10.2-Wrap-Up.html#delta-the-data",
    "title": "The End is Nigh",
    "section": "\\(\\delta\\) (The Data)",
    "text": "\\(\\delta\\) (The Data)\nRequirements:\n\nUnderstanding of the DGP (Data Generating Process).\n\nWhere does it come from and how is it generated?\nWhat is not in the data?\nIs the DGP stable?\n\nUnderstanding of the ethics of the data.\n\nHow can it be used?\nHow should it be used?\n\nAbility to map data on to the problem.\n\nCleaning and transforming.\nLinking and integrating."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#whats-a-disaster",
    "href": "lectures/10.2-Wrap-Up.html#whats-a-disaster",
    "title": "The End is Nigh",
    "section": "What’s a Disaster?",
    "text": "What’s a Disaster?\n\nSocial media datasets depict a specific time period, typically defined by the spike in Twitter messages or the use of particular hashtags. This can make it difficult to understand both the causes of disaster and the entire period of aftermath where the impact is realized. This further reifies a problematic short-term conceptions of disaster. In this sense, the analysis of social media during and after a disaster can resemble traditional media coverage, which has been often accused of paying attention to only the most sensational stories in a truncated timeframe… (Crawford and Finn 2015)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#whats-a-poi",
    "href": "lectures/10.2-Wrap-Up.html#whats-a-poi",
    "title": "The End is Nigh",
    "section": "What’s a POI?",
    "text": "What’s a POI?\n\nStephens (2013) for example analyses the gendered politics of spatial data crowdsourcing initiatives like Open Street Map, which enable the creation of endless categorizations associated with particular masculinities (brothels, strip clubs) while often wholly omitting feminized places (baby hatches, after-school childcare centres) from these schemas. (Elwood and Leszczynski 2018)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#kappa-the-code",
    "href": "lectures/10.2-Wrap-Up.html#kappa-the-code",
    "title": "The End is Nigh",
    "section": "\\(\\kappa\\) (The Code)",
    "text": "\\(\\kappa\\) (The Code)\nRequirements:\n\nAbility to develop/use appropriate tools.\n\nWhat can I reuse or recycle?\nIs this solution the right one?\n\nAbility to visualise an abstract goal.\n\nHow does this bit of code serve that goal?\nWhat is the next step towards that goal?\n\nAbility to work withing constraints.\n\nWhat is the budget?\nWhat is the timeline?\nWhat are the limitations?\n\n\n\nAll of these are about constraints. The ‘correct answer’ is defined in part by the constraints you are working within, not the ideal set of knowledge, data, and resources!"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#iota-the-infrastructure",
    "href": "lectures/10.2-Wrap-Up.html#iota-the-infrastructure",
    "title": "The End is Nigh",
    "section": "\\(\\iota\\) (The Infrastructure)",
    "text": "\\(\\iota\\) (The Infrastructure)\nRequirements:\n\nUnderstanding of the pipeline.\n\nHow is the code deployed?\nHow is the data managed?\nHow is the solution maintained?\n\nUndertanding of the wider ecosystem.\n\nHow are issues with the code identified and resolved?\nHow are changes to the data identified and managed?\nHow are reports generated and shared?\nHow are resources allocated?"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#the-big-picture",
    "href": "lectures/10.2-Wrap-Up.html#the-big-picture",
    "title": "The End is Nigh",
    "section": "The Big Picture",
    "text": "The Big Picture\n\n… we need to enroll the practical and technical aspects of GIS use and development directly into modules that address its history of development and implications of use. In doing so, hardware, software, analytic techniques, and data/representation are always presented as both technical and social. From these origins, critical GIS pedagogies aim beyond only building students’ conceptual apparatus for critiquing GIS or explaining its social and political implications, toward instilling this technopositionality as their orientation to doing GIS. (Elwood and Wilson 2017)\n\n\n\nWhen we think ecosystem, we should think about everything."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#pi-the-pies",
    "href": "lectures/10.2-Wrap-Up.html#pi-the-pies",
    "title": "The End is Nigh",
    "section": "\\(\\pi\\) (The Pies)",
    "text": "\\(\\pi\\) (The Pies)\nRequirements:\n\n🍕🍕🍕"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#so",
    "href": "lectures/10.2-Wrap-Up.html#so",
    "title": "The End is Nigh",
    "section": "So…",
    "text": "So…\n\nLet \\(\\sigma\\) be the spatial data science solution to a business problem that requires the integration of data, code, infrastructure, and pies…"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#but-in-an-urban-context",
    "href": "lectures/10.2-Wrap-Up.html#but-in-an-urban-context",
    "title": "The End is Nigh",
    "section": "But in an urban context…",
    "text": "But in an urban context…\nHave you heard of IBM’s Smarter Cities lab?\n\n\nThis is not easy.\nAfter being a big part of IBM’s strategy in the early 2000s, the Smarter Cities initiative was quietly dropped in 2016.\nImportance of the ‘assemblage’"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#instrumenting-the-city",
    "href": "lectures/10.2-Wrap-Up.html#instrumenting-the-city",
    "title": "The End is Nigh",
    "section": "Instrumenting the City",
    "text": "Instrumenting the City\n\nThe risk here is that the dashboard’s seeming comprehensiveness and seamlessness suggest that we can “govern by Blackberry” — or “fly by instrument” — alone. (Mattern 2015)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#literate-programming",
    "href": "lectures/1.4-Tools.html#literate-programming",
    "title": "Tools of the Trade",
    "section": "Literate Programming",
    "text": "Literate Programming\nIdeally, we want to ‘do’ data science in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct. ~ Knuth (1996)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#key-tenets",
    "href": "lectures/1.4-Tools.html#key-tenets",
    "title": "Tools of the Trade",
    "section": "Key Tenets",
    "text": "Key Tenets\nWhat we want:\n\nWeaving: the code and its documentation are together.\nTangling: the code can be run directly.\n\nIn an ideal world, these are the same file…"
  },
  {
    "objectID": "lectures/1.4-Tools.html#and-how-do-we-do-this",
    "href": "lectures/1.4-Tools.html#and-how-do-we-do-this",
    "title": "Tools of the Trade",
    "section": "And how do we do this?",
    "text": "And how do we do this?\nHint: it’s more than just one thing…\n\nJupyterLab: how we do ‘data science’.\nVirtualisation: separate your computer from your coding environment.\nVersion Control: manage your code, your data, and even your reports.\nMarkup: focus on the structure while you write!\nRender: creating documents and web pages from code and markup."
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-jupyterlab",
    "href": "lectures/1.4-Tools.html#why-use-jupyterlab",
    "title": "Tools of the Trade",
    "section": "Why Use JupyterLab?",
    "text": "Why Use JupyterLab?\nCoding in JupyterLab has a number of advantages over ‘point-and-click’:\n\nCoding requires our instructions to be unambiguous and logical.1\nComputers are infinitely patient so we can re-run as many times as necessary to get it ‘right’.\nThere is nothing to install (runs in your web browser).\nYou can run code from anywhere (runs in your web browser).\n\nThis does not guarantee that they’ll be correct."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture",
    "href": "lectures/1.4-Tools.html#the-bigger-picture",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nIf we can’t explain it simply enough that a computer can do it, perhaps we don’t actually understand it?\n\n\nTogether with the other tools in this talk, you can largely stop worrying about where code is running.\nIt’s easy to forget how you obtained a particular result when you are clicking around inside software like ArcGIS; this is much harder when using code.\nIn analysing the problem so that we can submit it to the computer we often develop a better understanding of the problem ourselves!\nWhy spend your time doing the boring stuff???\n\n\nConversely, perhaps the real value of humans over AI lies in what cannot be explained to the computer."
  },
  {
    "objectID": "lectures/1.4-Tools.html#jupyterlab-python",
    "href": "lectures/1.4-Tools.html#jupyterlab-python",
    "title": "Tools of the Trade",
    "section": "JupyterLab + Python",
    "text": "JupyterLab + Python"
  },
  {
    "objectID": "lectures/1.4-Tools.html#two-basic-flavours",
    "href": "lectures/1.4-Tools.html#two-basic-flavours",
    "title": "Tools of the Trade",
    "section": "Two Basic ‘Flavours’",
    "text": "Two Basic ‘Flavours’\nBoth do the same thing: separate the platform from the hardware, but they do this in defferent ways for different reasons.\n\nA ‘full’ Virtual Machine (VM) includes the Operating System and behaves like a separate computer even though it may share hardware with other VMs.\nA ‘container’ is a ‘lightweight’ VM running only the application and its dependencies; everything else is managed by the host Operating System so the resulting ‘image’ is small and easy to distribute.\n\nShort version: if you have to install an Operating System you are using a full VM; otherwise you are probably using a containerisation tool/\nMany things, including storage, networks, CPUs, GPUs, etc. can be virtualised."
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-containers",
    "href": "lectures/1.4-Tools.html#why-use-containers",
    "title": "Tools of the Trade",
    "section": "Why Use Containers?",
    "text": "Why Use Containers?\nWe gain quite a few benefits:\n\nEasier installation and ‘everyone’ has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nEasy to scale up and scale down, or to link them together via ‘microservices’.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-1",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-1",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nRather than having one environment for every project, we have one environment for each project.\n\n\n‘Computing contexts’ are disposable, while data and code are persistent when I need them.\nI don’t care where my code and data are, so long as they’re accessible when I need them.\nI don’t care if containers are created or destroyed, so long as they’re available when I need them.\nI rebuild or update the computing context when I am ready to do so."
  },
  {
    "objectID": "lectures/1.4-Tools.html#podman",
    "href": "lectures/1.4-Tools.html#podman",
    "title": "Tools of the Trade",
    "section": "Podman",
    "text": "Podman\n\n\nPodman is an open source container and image management engine. Podman makes it easy to find, run, build, and share containers."
  },
  {
    "objectID": "lectures/1.4-Tools.html#using-podman",
    "href": "lectures/1.4-Tools.html#using-podman",
    "title": "Tools of the Trade",
    "section": "Using Podman",
    "text": "Using Podman\nPodman makes configuring a development environment (fairly) simple. If a Podman image works for us then we know1 it works for you.\nUse either:\n\njreades/sds:2025-amd (Windows and Older Macs)\njreades/sds:2025-arm (Newer Macs)\n\nUsually, but not always, true."
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-version-control",
    "href": "lectures/1.4-Tools.html#why-use-version-control",
    "title": "Tools of the Trade",
    "section": "Why use Version Control?",
    "text": "Why use Version Control?\n\n… If a mistake is made, developers can turn back the clock and compare earlier versions of the code to help fix the mistake while minimizing disruption to all team members.\nSource: Altassian\n\nIn addition:\n\nWe can share code with others (directly) as source code or (indirectly) as the product of compiling that source code.\nWe can rewind, fast forward, and combine changes by different people working on different features.\nWe gain detailed, incremental backups that help us tro track down the changes that introduced a bug when something goes wrong.\n\n\nDiscuss comparison with Dropbox or OneDrive. How are they similar, how are they different? File level vs. row-level views."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-2",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-2",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nIn open source projects there may be no one view of what the ‘right’ solution/version of a project is, so differences need to be negotiated.\n\n\nEvery computer with version control might have the ‘right’ version of the code for a given user, so there is no ‘master’ view of a project.\nWe need to be able to choose whether to merge other people’s changes with our changes, rather than having everything forced on us.\nWe still want to be able to share our version of the code / outputs of the code with other people, and a web site is a good way to do that."
  },
  {
    "objectID": "lectures/1.4-Tools.html#git",
    "href": "lectures/1.4-Tools.html#git",
    "title": "Tools of the Trade",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout.\nstash changes that we don’t want to commit.\nstatus the repository."
  },
  {
    "objectID": "lectures/1.4-Tools.html#github",
    "href": "lectures/1.4-Tools.html#github",
    "title": "Tools of the Trade",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.4-Tools.html#gitgithub-is-for-anything",
    "href": "lectures/1.4-Tools.html#gitgithub-is-for-anything",
    "title": "Tools of the Trade",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course is on GitHub."
  },
  {
    "objectID": "lectures/1.4-Tools.html#oh-my-git",
    "href": "lectures/1.4-Tools.html#oh-my-git",
    "title": "Tools of the Trade",
    "section": "Oh My Git!",
    "text": "Oh My Git!\n\n\nSource: OhMyGit"
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-markup",
    "href": "lectures/1.4-Tools.html#why-use-markup",
    "title": "Tools of the Trade",
    "section": "Why use Markup?",
    "text": "Why use Markup?\n\nQuickly sketch out the structure of a document.\nFocus on the substance, not the style.\nWorks well with version control (line-by-line changes + GitHub.io web site).\nCombine code, documentation, and narrative easily."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-3",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-3",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nI spend a lot less time ‘faffing’ writing in Markdown than I used to. Spend more time on what you want to say and worry about the how later.\n\n\n\n### A Subtitle\n\nSome text in **bold** and *italics* with a [link](https://jreades.github.io/).\n\n&gt; A blockquote\n\nA Subtitle\nSome text in bold and italics with a link.\n\nA blockquote"
  },
  {
    "objectID": "lectures/1.4-Tools.html#markdown-examples",
    "href": "lectures/1.4-Tools.html#markdown-examples",
    "title": "Tools of the Trade",
    "section": "Markdown Examples",
    "text": "Markdown Examples\nSee CommonMark and the Markdown Guide for more:\n\n\n\n\n\n\n\nFormat\nOutput\n\n\nPlain text...\nPlain text\n\n\n## A Large Heading\nA Large Heading\n\n\n### A Medium Heading\nA Medium Heading\n\n\n- A list\n- More list\n\nA list\nMore list\n\n\n\n1. An ordered list\n2. More ordered list\n\nAn ordered list\nMore ordered list\n\n\n\n[A link](http://casa.ucl.ac.uk)\nA link\n\n\n\n\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…"
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-render",
    "href": "lectures/1.4-Tools.html#why-render",
    "title": "Tools of the Trade",
    "section": "Why Render?",
    "text": "Why Render?\n\nOutputs can be: web pages, Jupyter notebooks, Word documents, PDFs, presentations…\nIt can be really useful to have a single input and multiple outputs because requirements and needs always change.\nIt teaches you to focus on the process, not the minutiae."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-4",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-4",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nEverything this week was created using these basic tools and techniques. It has transformed the way I teach, do research, and write! It embodies the potential of ‘literate programming’ (Knuth 1984)."
  },
  {
    "objectID": "lectures/1.4-Tools.html#programming-languages-used",
    "href": "lectures/1.4-Tools.html#programming-languages-used",
    "title": "Tools of the Trade",
    "section": "Programming Languages (Used)",
    "text": "Programming Languages (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#databases-used",
    "href": "lectures/1.4-Tools.html#databases-used",
    "title": "Tools of the Trade",
    "section": "Databases (Used)",
    "text": "Databases (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#frameworks-libraries-used",
    "href": "lectures/1.4-Tools.html#frameworks-libraries-used",
    "title": "Tools of the Trade",
    "section": "Frameworks & Libraries (Used)",
    "text": "Frameworks & Libraries (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#virtualisation-other-tools-used",
    "href": "lectures/1.4-Tools.html#virtualisation-other-tools-used",
    "title": "Tools of the Trade",
    "section": "Virtualisation & Other Tools (Used)",
    "text": "Virtualisation & Other Tools (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#programming-languages-desiredadmired",
    "href": "lectures/1.4-Tools.html#programming-languages-desiredadmired",
    "title": "Tools of the Trade",
    "section": "Programming Languages (Desired/Admired)",
    "text": "Programming Languages (Desired/Admired)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#databases-desiredadmired",
    "href": "lectures/1.4-Tools.html#databases-desiredadmired",
    "title": "Tools of the Trade",
    "section": "Databases (Desired/Admired)",
    "text": "Databases (Desired/Admired)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#additional-resources",
    "href": "lectures/1.4-Tools.html#additional-resources",
    "title": "Tools of the Trade",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGetting Started with Markdown\nAn online interactive tutorial for Markdown\nMarkdown Cheatsheet\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nGit for Decolonisation1\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…\n\nAnd once you’re ready to get ‘serious’, check out this tutorial on Sustainable Authorship in Plain Text using Pandoc and Markdown from The Programming Historian! That’s what actually underpins Quarto, but you can do so much more…\nPart art, part activism, part tech!"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#waves",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#waves",
    "title": "Computers in Urban Studies",
    "section": "3 Waves?",
    "text": "3 Waves?\n\n“A computer in every institution”\n“A computer in every office”\n“A computer in every thing”\n\n\nWave 1\n\nRoughly the 1950s–70s\nComputers as ‘super-human’ calculators for data\nData + models as theory-testing tool\n\nRetrospectively: the 1st quantitative revolution. But can see this as incorrect and focus on the theoretical aspect.\nWave 2\n\nRoughly the 1980s–2000s\nComputers as tools for thinking about spatial relationships\nExplicit modelling of local spatial effects\n\nRetrospectively: the GIS revolution. But I personally see this as incorrect because GIS is Wave 1.\nShift from computers as processors of data to integrated, pervasive systems that spew out data on everything.\nWave 3\n\nRoughly the mid-2000s–?\nComputers as tools for generating data (pace ABM researchers)\nGeodata being continuously produced as byproduct of other activities\nShift from researching attributes to behaviours (pace Hägerstrand)\n\nRetrospectively: the big data revolution or 2nd quantitative revolution."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#all-waves-still-going",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#all-waves-still-going",
    "title": "Computers in Urban Studies",
    "section": "All Waves Still Going!",
    "text": "All Waves Still Going!\nWave 1: Computers help me do it faster\n\nGIS is ‘just’ the industrial revolution impacting cartography.\n\n\nWave 2: Computers help me to think\n\nGeocomputation & local stats are qualitatively & meaningfully different.\n\n\n\nWave 3: Computers help me to learn\n\nNot ‘just’ about the ‘bigness’ of data, though that is important.\n\n\nWave 2 is about implementing ideas such as recursion and iteration – these could, in theory, have been tackled in Wave 1, but in practice that’s not what people were doing.\nWave 3 is about more explicitly allowing computers to learn about data so that we can extract insight from these models – these could also, in theory, have been tackled in Wave 2 but in practice that’s not what people were doing.\nI’m not totally happy about my description of Wave 3 and will try to dig into this in a little more detail but suggestions welcome!"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#anticipated-by-hägerstrand-1967",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#anticipated-by-hägerstrand-1967",
    "title": "Computers in Urban Studies",
    "section": "Anticipated by Hägerstrand (1967)",
    "text": "Anticipated by Hägerstrand (1967)\n\nI think that the computer can do three different and useful things for us. The first and simplest operation is… descriptive mapping the second… is the analytical one The third kind of service is… to run process models by which we might try to reproduce observed or create hypothetical chains of events of a geographical nature.\nHägerstrand (1967)"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#but-persistent-critiques",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#but-persistent-critiques",
    "title": "Computers in Urban Studies",
    "section": "But Persistent Critiques",
    "text": "But Persistent Critiques\n\nMore important, there is a clear disparity between the sophisticated theoretical and methodological framework which we are using and our ability to say anything really meaningful about events as they unfold around us. There are too many anomalies between what we purport to explain and manipulate and what actually happens. There is an ecological problem, an urban problem, an international trade problem, and yet we seem incapable of saying anything of any depth or profundity about any of them. When we do say something it appears trite and rather ludicrous.\nHarvey (2008 [1972], 17)"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#data-science",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#data-science",
    "title": "Computers in Urban Studies",
    "section": "Data science?",
    "text": "Data science?\n\n\n\nSource: Davenport and Patil (2012)\n\n\nIndustry-led\nSpatially ignorant (often)\nDisciplinarily greedy (often)"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#what-is-different",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#what-is-different",
    "title": "Computers in Urban Studies",
    "section": "What is Different?",
    "text": "What is Different?\nAccording to Donoho (2017) ‘data science’ differs from plain old ‘statistics’ through an interest in:\n\nData gathering, preparation, and exploration;\nData representation and transformation;\nComputing with data;\nData visualisation and presentation;\nData modelling; and\nA reflexive ‘science of data science’."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#in-practice",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#in-practice",
    "title": "Computers in Urban Studies",
    "section": "In Practice…",
    "text": "In Practice…\nI think there are several distinguishing features that I encounter in day-to-day (geography) work:\n\nData-driven methods development & deployment\nExplicit tuning/meta-parameterisation\nExplicit feature optimisation/engineering\nExplicit training/testing from ‘one shot’ data\n‘Black boxes’ feature prominently & ‘online learning’ emerging quickly\n\nData science as process and pipeline, not just input to research."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#do-we-need-it",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#do-we-need-it",
    "title": "Computers in Urban Studies",
    "section": "Do We Need It?",
    "text": "Do We Need It?\n\nSource: xkcd"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#additional-resources",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#additional-resources",
    "title": "Computers in Urban Studies",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Data of Urban Spaces — a nice collection of pieces on Towards Data Science"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Jon Reades1\nThe Foundations of Spatial Data Science (FSDS) module is an optional element of CASA’s MSc programmes and is intended provide an introduction to doing data science in Python for students who are new to programming or whose previous exposure to coding is fairly limited. The module seeks to enable students to access, understand, and communicate data in a spatial context. FSDS is not about pushing buttons, but about using logic, programming, and your growing analytical skills to tackle real-world problems in a creative, reproducible, and open manner.\nFSDS is not easy: in order to make the most of the module—and the foundation that it provides both for Term 2 modules on the MSc and for post-Masters employment—you will need to work hard. This does not mean cramming before each practical, it means practicing between practicals, doing the readings, and really watching the videos. UCL expectations for a Masters-level module is 150 hours of study time: there are 4 hours of timetabled activity per week, and about an hour of videos to watch before each workshop, leaving up to 100 hours of ‘self-study’. By implication, you should expect to spend about 1.25 hours/day studying for this module: reading, coding, and (above all) practicing.\nIn exchange for your hard work, there is a pressing need for analysts, planners, and geographers able to think computationally using programming, analytics, and data manipulation skills that are anchored in the needs of policy-makers, businesses, and non-profits. There is a severe skills shortage in this domain across all sectors and, consequently, significant opportunity for those who can ‘make sense’ of data+code."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWhile this module in indebted to both feedback from students and colleagues over the years, several people played a particularly outsize role in my thinking and deserve special acknowledgement:\n\nDani for help with Docker, geopandas, and any number of other new tools with which I’ve had to familiarise myself.\nAndy for somehow knowing about all kinds of new web apps that I could use to support the module.\nThe Geocomp team at King’s College London, who supported my hare-brained scheme to teach Geography undergrads to code and offered all manner of useful feedback on what we could/could not feasibly cover."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Bartlett Centre for Advanced Spatial Analysis↩︎"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html",
    "href": "extra/Live-04-Objects-1.html",
    "title": "Intro",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 4: Object-Oriented Programming 1&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with Classes &amp; Objects&lt;/h2&gt;\nYou will be encountering classes and dealing with inheritance from next week and this is fundamental to how Python works. In Python, everything is an object and that means that everything – absolutely everything – is part of the Python class hierarchy, at the bottom of which sits the base object class.\nFor our purposes, the place you’re going to encounter this every day is that Pandas is our main data analysis tool, but when we deal with geography we’ll be using GeoPandas, which inherits from Pandas and adds (we say “extends”) functionality for processing geo-data.\nThere’s even a Moving Pandas for trajectory analysis. These all work on the basis of inheritance."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-1-creating-a-class-hierarchy",
    "href": "extra/Live-04-Objects-1.html#task-1-creating-a-class-hierarchy",
    "title": "Intro",
    "section": "Task 1: Creating a Class Hierarchy",
    "text": "Task 1: Creating a Class Hierarchy\nWe want to create a set of ideal shape classes with methods allowing us to derive various properties of that shape:\n\nDiameter: which we’ll define as the longest line that can be drawn across the inside of the shape.\nVolume: the total volume of the shape.\nSurface Area: the total outside area of the shape.\n\nWe will create all of these shape classes in the notebook so that we know they work and then will move them to an external package file so that they can be imported and re-used easily in other notebooks.\nWe’re also going to make use of a few features of Python:\n\nYou can access the class name of an instance using: self.__class__.__name__. And here’s one key point: self refers to the instance, not to the class… we’ll see why this matters.\nYou can raise your own exceptions easily if you don’t want to implement a particular method yet.\nYou can have an ‘abstract’ base class that does nothing except provide a template for the ‘real’ classes so that they can be used interchangeably.\n\n\nprint(\"Hello world\".__class__.__name__)\n\nstr\n\n\n\nTask 1.1: Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\nfrom math import pi\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        print(\"I'm a shape!\")\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\n\n\nTask 1.2: Demonstrate\nBriefly discuss what’s going on, paying particular attention to:\n\nclass shape(object) – this means that shape extends the object base class.\ndef __init__ – what is self (reference to the instance) and what does dimension:float=None mean?\nCreate a new shape object:\n\n\nWhat happens when you run s = shape()?\nWhat happens when you try to call s.diameter()?\nWhat happens when you try to call s.type()?"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-2-cube",
    "href": "extra/Live-04-Objects-1.html#task-2-cube",
    "title": "Intro",
    "section": "Task 2: Cube",
    "text": "Task 2: Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nTask 2.1: Cube Class\n\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        print(\"I'm a cube!\")\n        return\n    \n    def diameter(self):\n        return (3 * self.dim**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return 6*(self.dim**2)\n\n\n\nTask 2.2: Demonstrate\n\nCreate a cube and do the same things we did with shape (show that it’s been overridden).\nLook at the print statements created from the __init__ and discuss.\nShow how the type method works for Cubes even though we never specified it."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-3-sphere",
    "href": "extra/Live-04-Objects-1.html#task-3-sphere",
    "title": "Intro",
    "section": "Task 3: Sphere",
    "text": "Task 3: Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2d\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nTask 3.1: Sphere Class\n\n# Sphere class\nclass sphere(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return self.dim*2\n    \n    def volume(self):\n        return (4/3) * pi * self.dim**3\n    \n    def surface(self):\n        return 4 * pi * (self.dim**2)\n\n\n\nTask 3.2: Demonstrate how this works\n\nCreate one Cube and one Sphere with the same parameter (dimension)\nShow how they return different values for diameter, volume, surface\nShow that both have a type function that works without being defined."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-4-pyramids",
    "href": "extra/Live-04-Objects-1.html#task-4-pyramids",
    "title": "Intro",
    "section": "Task 4: Pyramids",
    "text": "Task 4: Pyramids\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them!\n\nTask 4.1: Regular Pyramids\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n    \n    has_mummies = True # This is for *all* regular pyramids\n    \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n    \n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n    \n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**2 * self.height() / 3\n    \n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)\n\n\n\nTask 4.2: Demonstrate\nShow how we added a new method for regular pyramids:\n\nDoesn’t work for any other class\nBut all the rest of the methods do\n\nDiscuss: what are the pros and cons of this new method?\nAnswer: in general, we try to extend and never to break functionality of other classes in the hierarchy. So you wouldn’t create a shape sub-class that broke the diameter or surface methods by returning something like “I don’t have this dimension” as a text string, but you could throw an error or return a Null value. It depends on what makes sense.\n\n\nTask 4.3: Triangular Pyramid\nWe want triangular pyramid to inherit from regular pyramid, and all sides are equal so it’s an equilateral triangular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nAnyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n    \n    has_mummies = False # This is for all triangular pyramids\n    \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n    \n    def diameter(self):\n        return self.dim\n    \n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n    \n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n    \n    def volume(self):\n        return (1/3) * self.base() * self.height()\n    \n    def surface(self):\n        return 4 * self.base()\n\n\n\nTask 4.4: Demonstrate\n\nCompare regular and triangular pyramids: notice that the has_mummies class variable has changed its default value.\nNotice too that you can override/change a class variable or an instance variable at run-time. There is a very important difference between doing something like self.has_mummies = has_mummies and t_pyramid.has_mummies = True.\n\nHere’s an example of everything run together:\n\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaagh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaagh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\n\nI'm a shape!\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nI'm a shape!\nI'm a cube!\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nI'm a shape!\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaagh!\n\nI'm a shape!\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'\n\n\n\n\nprint(p2.__class__)\nprint(p2.__class__.__mro__)\n\n&lt;class '__main__.t_pyramid'&gt;\n(&lt;class '__main__.t_pyramid'&gt;, &lt;class '__main__.pyramid'&gt;, &lt;class '__main__.shape'&gt;, &lt;class 'object'&gt;)\n\n\nThis shows the class name and then the full hierarchy, so you know in which order Python will search for a method before throwing an error. Some, such as print, exist at the very basic level of the object."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-5-packaging",
    "href": "extra/Live-04-Objects-1.html#task-5-packaging",
    "title": "Intro",
    "section": "Task 5: Packaging",
    "text": "Task 5: Packaging\nNow that we’ve created our classes, we want to move them to a separate file that can be imported and re-used by code elsewhere. We need to turn this into a package. Formally packaging things up for distribution requires a lot more work(see this and this). But for something that we’re only going to use ourselves it’s basically as simple as:\n\nCreate a directory that will become the package name (e.g. shapes).\nWrite the code in a file called __init.py__ inside that new directory (i.e. ./shapes/__init__.py).\n\nWe could do this by hand by copy-pasting each cell above into a new file, but because we prefer to be lazy let’s try automating it…\n\nTask 5.1: Shell Commands\nFirst, we need to create the shapes directory. To do this, although you can use Python code to create and delete files and folders, it’s helpful to know that you can actually execute shell commands directly from a notebook. Oviously, the degree to which this works depends on which Operating System you’re using!\nFor instance, here’s our trusty list command:\n\n!ls\n\ncss                                   Live-06-Spatial_Data.bak\nimg                                   Live-07-Textual_Data.bak\nLive-02-Foundations_1.html            Live-08-Dimensions_in_Data.bak\nLive-02-Foundations_1.qmd             Live-08-Dimensions_in_Shakespeare.bak\nLive-03-Foundations_2_Intro.html      Live-09-Grouping_Data.bak\nLive-03-Foundations_2_Intro.qmd       Live-10-Visualising_Data.bak\nLive-03-Foundations_2.html            Note_Hint_Danger.bak\nLive-03-Foundations_2.qmd             shapes\nLive-04-Objects-1.qmd                 textual\nLive-04-Objects-1.quarto_ipynb        world.geojson\nLive-04-Objects-2.bak                 XX-Function_Development.ipynb\nLive-05-Numeric_Data.bak\n\n\nSo we can simply ask mkdir (make directory) to do this for us. The -p means that it creates directory hierarhices if they’re missing and doesn’t complain if a directory already exists.\n\n!mkdir -p shapes\n\n\n\nTask 5.2: Creating an init file\nThe easiest and most intelligible way to create the shapes package is to copy and paste each of the class definitions above into a new file called __init__.py manually.\nI would demonstrate this approach; however…\n\nAside: Notebook Convert\nIt turns out that jupyter also offers a nbconvert (Notebook Convert) utility that helps you to convert notebooks to other formats including HTML, LaTeX, PDF, Markdown, Executable Python and so on! We’re going to take advantage of the fact that __init__.py is just plain old python following a particular naming and placement scheme…\n\n!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \\\n    --to python --output=shapes/__init__.py \\\n    Live-04-Objects-1.ipynb\n\n[NbConvertApp] WARNING | pattern 'Live-04-Objects-1.ipynb' matched no files\nThis application is used to convert notebook files (*.ipynb)\n        to various other formats.\n\n        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n\nOptions\n=======\nThe options below are convenience aliases to configurable class-options,\nas listed in the \"Equivalent to\" description-line of the aliases.\nTo see all configurable class-options for some &lt;cmd&gt;, use:\n    &lt;cmd&gt; --help-all\n\n--debug\n    set log level to logging.DEBUG (maximize logging output)\n    Equivalent to: [--Application.log_level=10]\n--show-config\n    Show the application's configuration (human-readable format)\n    Equivalent to: [--Application.show_config=True]\n--show-config-json\n    Show the application's configuration (json format)\n    Equivalent to: [--Application.show_config_json=True]\n--generate-config\n    generate default config file\n    Equivalent to: [--JupyterApp.generate_config=True]\n-y\n    Answer yes to any questions instead of prompting.\n    Equivalent to: [--JupyterApp.answer_yes=True]\n--execute\n    Execute the notebook prior to export.\n    Equivalent to: [--ExecutePreprocessor.enabled=True]\n--allow-errors\n    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n--stdin\n    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n    Equivalent to: [--NbConvertApp.from_stdin=True]\n--stdout\n    Write notebook output to stdout instead of files.\n    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n--inplace\n    Run nbconvert in place, overwriting the existing notebook (only\n            relevant when converting to notebook format)\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n--clear-output\n    Clear output of current file and save in place,\n            overwriting the existing notebook.\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n--coalesce-streams\n    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n--no-prompt\n    Exclude input and output prompts from converted document.\n    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n--no-input\n    Exclude input cells and output prompts from converted document.\n            This mode is ideal for generating code-free reports.\n    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n--allow-chromium-download\n    Whether to allow downloading chromium if no suitable version is found on the system.\n    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n--disable-chromium-sandbox\n    Disable chromium security sandbox when converting to PDF..\n    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n--show-input\n    Shows code input. This flag is only useful for dejavu users.\n    Equivalent to: [--TemplateExporter.exclude_input=False]\n--embed-images\n    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n    Equivalent to: [--HTMLExporter.embed_images=True]\n--sanitize-html\n    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n    Equivalent to: [--HTMLExporter.sanitize_html=True]\n--log-level=&lt;Enum&gt;\n    Set the log level by value or name.\n    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n    Default: 30\n    Equivalent to: [--Application.log_level]\n--config=&lt;Unicode&gt;\n    Full path of a config file.\n    Default: ''\n    Equivalent to: [--JupyterApp.config_file]\n--to=&lt;Unicode&gt;\n    The export format to be used, either one of the built-in formats\n            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n            or a dotted object name that represents the import path for an\n            ``Exporter`` class\n    Default: ''\n    Equivalent to: [--NbConvertApp.export_format]\n--template=&lt;Unicode&gt;\n    Name of the template to use\n    Default: ''\n    Equivalent to: [--TemplateExporter.template_name]\n--template-file=&lt;Unicode&gt;\n    Name of the template file to use\n    Default: None\n    Equivalent to: [--TemplateExporter.template_file]\n--theme=&lt;Unicode&gt;\n    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n    as prebuilt extension for the lab template)\n    Default: 'light'\n    Equivalent to: [--HTMLExporter.theme]\n--sanitize_html=&lt;Bool&gt;\n    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n    should be set to True by nbviewer or similar tools.\n    Default: False\n    Equivalent to: [--HTMLExporter.sanitize_html]\n--writer=&lt;DottedObjectName&gt;\n    Writer class used to write the\n                                        results of the conversion\n    Default: 'FilesWriter'\n    Equivalent to: [--NbConvertApp.writer_class]\n--post=&lt;DottedOrNone&gt;\n    PostProcessor class used to write the\n                                        results of the conversion\n    Default: ''\n    Equivalent to: [--NbConvertApp.postprocessor_class]\n--output=&lt;Unicode&gt;\n    Overwrite base name use for output files.\n                Supports pattern replacements '{notebook_name}'.\n    Default: '{notebook_name}'\n    Equivalent to: [--NbConvertApp.output_base]\n--output-dir=&lt;Unicode&gt;\n    Directory to write output(s) to. Defaults\n                                  to output to the directory of each notebook. To recover\n                                  previous default behaviour (outputting to the current\n                                  working directory) use . as the flag value.\n    Default: ''\n    Equivalent to: [--FilesWriter.build_directory]\n--reveal-prefix=&lt;Unicode&gt;\n    The URL prefix for reveal.js (version 3.x).\n            This defaults to the reveal CDN, but can be any url pointing to a copy\n            of reveal.js.\n            For speaker notes to work, this must be a relative path to a local\n            copy of reveal.js: e.g., \"reveal.js\".\n            If a relative path is given, it must be a subdirectory of the\n            current directory (from which the server is run).\n            See the usage documentation\n            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n            for more details.\n    Default: ''\n    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n--nbformat=&lt;Enum&gt;\n    The nbformat version to write.\n            Use this to downgrade notebooks.\n    Choices: any of [1, 2, 3, 4]\n    Default: 4\n    Equivalent to: [--NotebookExporter.nbformat_version]\n\nExamples\n--------\n\n    The simplest way to use nbconvert is\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to html\n\n            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n\n            &gt; jupyter nbconvert --to latex mynotebook.ipynb\n\n            Both HTML and LaTeX support multiple output templates. LaTeX includes\n            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n            'classic'. You can specify the flavor of the format used.\n\n            &gt; jupyter nbconvert --to html --template lab mynotebook.ipynb\n\n            You can also pipe the output to stdout, rather than a file\n\n            &gt; jupyter nbconvert mynotebook.ipynb --stdout\n\n            PDF is generated via latex\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to pdf\n\n            You can get (and serve) a Reveal.js-powered slideshow\n\n            &gt; jupyter nbconvert myslides.ipynb --to slides --post serve\n\n            Multiple notebooks can be given at the command line in a couple of\n            different ways:\n\n            &gt; jupyter nbconvert notebook*.ipynb\n            &gt; jupyter nbconvert notebook1.ipynb notebook2.ipynb\n\n            or you can specify the notebooks list in a config file, containing::\n\n                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n\n            &gt; jupyter nbconvert --config mycfg.py\n\nTo see all available configurables, use `--help-all`.\n\n\n\n\n\nTidying Up the Output\nUsing the file browser provided by Jupyter Lab (on the left), open up the new __init__.py file created in the shapes directory. You will want (at the very least) to search for ‘Task 2’ and delete everything in the file after that point.\n\nReally Important: if you do not delete everything from Task 2: Packaging onwards then every time you try to (re)load the shapes you will also output another copy of this notebook because this code is also outputted by nbconvert. In fact, you might find it easier to search for the line “How would you test these changes?” and delete everything from there onwards in the file.\n\n\n\n\nTask 5.3: Importing the Classes\n\nTask 5.3.1: Autoreload Cell Magic\nWhen writing a new package from a notebook, you need to ensure that Python knows to reload the package every time you run the code. Otherwise, Python will keep running the version of the package that you loaded when you first ran import &lt;package name&gt;!\nThis the first time you’ll have seen this special type of code block: % at the start of a line in a codeblock indicates that a ‘magic command’ is being issued. autoreload is one such magic command. There is also the ! at the start of a line which indicates a shell command to be executed (e.g. !ls).\nThere are other magics we will use later like %%time which track the running time of an operation.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nTask 5.3.2: Import\n\nimport shapes\n\nstr\nI'm a shape!\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nI'm a shape!\nI'm a cube!\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nI'm a shape!\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaagh!\n\nI'm a shape!\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'\n\n&lt;class 'shapes.t_pyramid'&gt;\n(&lt;class 'shapes.t_pyramid'&gt;, &lt;class 'shapes.pyramid'&gt;, &lt;class 'shapes.shape'&gt;, &lt;class 'object'&gt;)\ncss                                   Live-06-Spatial_Data.bak\nimg                                   Live-07-Textual_Data.bak\nLive-02-Foundations_1.html            Live-08-Dimensions_in_Data.bak\nLive-02-Foundations_1.qmd             Live-08-Dimensions_in_Shakespeare.bak\nLive-03-Foundations_2_Intro.html      Live-09-Grouping_Data.bak\nLive-03-Foundations_2_Intro.qmd       Live-10-Visualising_Data.bak\nLive-03-Foundations_2.html            Note_Hint_Danger.bak\nLive-03-Foundations_2.qmd             shapes\nLive-04-Objects-1.qmd                 textual\nLive-04-Objects-1.quarto_ipynb        world.geojson\nLive-04-Objects-2.bak                 XX-Function_Development.ipynb\nLive-05-Numeric_Data.bak\n[NbConvertApp] WARNING | pattern 'Live-04-Objects-1.ipynb' matched no files\nThis application is used to convert notebook files (*.ipynb)\n        to various other formats.\n\n        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n\nOptions\n=======\nThe options below are convenience aliases to configurable class-options,\nas listed in the \"Equivalent to\" description-line of the aliases.\nTo see all configurable class-options for some &lt;cmd&gt;, use:\n    &lt;cmd&gt; --help-all\n\n--debug\n    set log level to logging.DEBUG (maximize logging output)\n    Equivalent to: [--Application.log_level=10]\n--show-config\n    Show the application's configuration (human-readable format)\n    Equivalent to: [--Application.show_config=True]\n--show-config-json\n    Show the application's configuration (json format)\n    Equivalent to: [--Application.show_config_json=True]\n--generate-config\n    generate default config file\n    Equivalent to: [--JupyterApp.generate_config=True]\n-y\n    Answer yes to any questions instead of prompting.\n    Equivalent to: [--JupyterApp.answer_yes=True]\n--execute\n    Execute the notebook prior to export.\n    Equivalent to: [--ExecutePreprocessor.enabled=True]\n--allow-errors\n    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n--stdin\n    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n    Equivalent to: [--NbConvertApp.from_stdin=True]\n--stdout\n    Write notebook output to stdout instead of files.\n    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n--inplace\n    Run nbconvert in place, overwriting the existing notebook (only\n            relevant when converting to notebook format)\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n--clear-output\n    Clear output of current file and save in place,\n            overwriting the existing notebook.\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n--coalesce-streams\n    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n--no-prompt\n    Exclude input and output prompts from converted document.\n    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n--no-input\n    Exclude input cells and output prompts from converted document.\n            This mode is ideal for generating code-free reports.\n    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n--allow-chromium-download\n    Whether to allow downloading chromium if no suitable version is found on the system.\n    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n--disable-chromium-sandbox\n    Disable chromium security sandbox when converting to PDF..\n    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n--show-input\n    Shows code input. This flag is only useful for dejavu users.\n    Equivalent to: [--TemplateExporter.exclude_input=False]\n--embed-images\n    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n    Equivalent to: [--HTMLExporter.embed_images=True]\n--sanitize-html\n    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n    Equivalent to: [--HTMLExporter.sanitize_html=True]\n--log-level=&lt;Enum&gt;\n    Set the log level by value or name.\n    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n    Default: 30\n    Equivalent to: [--Application.log_level]\n--config=&lt;Unicode&gt;\n    Full path of a config file.\n    Default: ''\n    Equivalent to: [--JupyterApp.config_file]\n--to=&lt;Unicode&gt;\n    The export format to be used, either one of the built-in formats\n            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n            or a dotted object name that represents the import path for an\n            ``Exporter`` class\n    Default: ''\n    Equivalent to: [--NbConvertApp.export_format]\n--template=&lt;Unicode&gt;\n    Name of the template to use\n    Default: ''\n    Equivalent to: [--TemplateExporter.template_name]\n--template-file=&lt;Unicode&gt;\n    Name of the template file to use\n    Default: None\n    Equivalent to: [--TemplateExporter.template_file]\n--theme=&lt;Unicode&gt;\n    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n    as prebuilt extension for the lab template)\n    Default: 'light'\n    Equivalent to: [--HTMLExporter.theme]\n--sanitize_html=&lt;Bool&gt;\n    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n    should be set to True by nbviewer or similar tools.\n    Default: False\n    Equivalent to: [--HTMLExporter.sanitize_html]\n--writer=&lt;DottedObjectName&gt;\n    Writer class used to write the\n                                        results of the conversion\n    Default: 'FilesWriter'\n    Equivalent to: [--NbConvertApp.writer_class]\n--post=&lt;DottedOrNone&gt;\n    PostProcessor class used to write the\n                                        results of the conversion\n    Default: ''\n    Equivalent to: [--NbConvertApp.postprocessor_class]\n--output=&lt;Unicode&gt;\n    Overwrite base name use for output files.\n                Supports pattern replacements '{notebook_name}'.\n    Default: '{notebook_name}'\n    Equivalent to: [--NbConvertApp.output_base]\n--output-dir=&lt;Unicode&gt;\n    Directory to write output(s) to. Defaults\n                                  to output to the directory of each notebook. To recover\n                                  previous default behaviour (outputting to the current\n                                  working directory) use . as the flag value.\n    Default: ''\n    Equivalent to: [--FilesWriter.build_directory]\n--reveal-prefix=&lt;Unicode&gt;\n    The URL prefix for reveal.js (version 3.x).\n            This defaults to the reveal CDN, but can be any url pointing to a copy\n            of reveal.js.\n            For speaker notes to work, this must be a relative path to a local\n            copy of reveal.js: e.g., \"reveal.js\".\n            If a relative path is given, it must be a subdirectory of the\n            current directory (from which the server is run).\n            See the usage documentation\n            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n            for more details.\n    Default: ''\n    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n--nbformat=&lt;Enum&gt;\n    The nbformat version to write.\n            Use this to downgrade notebooks.\n    Choices: any of [1, 2, 3, 4]\n    Default: 4\n    Equivalent to: [--NotebookExporter.nbformat_version]\n\nExamples\n--------\n\n    The simplest way to use nbconvert is\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to html\n\n            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n\n            &gt; jupyter nbconvert --to latex mynotebook.ipynb\n\n            Both HTML and LaTeX support multiple output templates. LaTeX includes\n            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n            'classic'. You can specify the flavor of the format used.\n\n            &gt; jupyter nbconvert --to html --template lab mynotebook.ipynb\n\n            You can also pipe the output to stdout, rather than a file\n\n            &gt; jupyter nbconvert mynotebook.ipynb --stdout\n\n            PDF is generated via latex\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to pdf\n\n            You can get (and serve) a Reveal.js-powered slideshow\n\n            &gt; jupyter nbconvert myslides.ipynb --to slides --post serve\n\n            Multiple notebooks can be given at the command line in a couple of\n            different ways:\n\n            &gt; jupyter nbconvert notebook*.ipynb\n            &gt; jupyter nbconvert notebook1.ipynb notebook2.ipynb\n\n            or you can specify the notebooks list in a config file, containing::\n\n                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n\n            &gt; jupyter nbconvert --config mycfg.py\n\nTo see all available configurables, use `--help-all`.\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nI'm a shape!\nI'm a cube!\n&lt;shapes.cube object at 0x15c78b5f0&gt;\n1000\n17.320508075688775\n600\nI'm a shape!\n150\n122.47448713915888\n397747.5644174329\n38971.143170299736\nHelp on class shape in module shapes:\n\nclass shape(builtins.object)\n |  shape(dimension: float = None)\n |\n |  # Base class shape\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension: float = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |\n |  diameter(self)\n |\n |  surface(self)\n |\n |  type(self)\n |\n |  volume(self)\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\nHelp on function type in module shapes:\n\ntype(self)\n\n\n\n\n\nTask 5.3.3: Demonstrate\nNotice that because we just imported shapes we need to call shapes.cube() below. We could also have done this as from shapes import cube which would allow us to call cube() directly.\nPros and Cons: This is about namespaces. If I dump everything into the main namespace and I have several classes/packages that implement different aspects of cubes the issue is that only the last cube class I imported will be accessible (from shapes import cube), but if I import just shapes then I can use the namespace (shapes) to specify that I mean exactly that class. This is a choice based on what you want to do.\n\nb = shapes.cube(10)\n\nI'm a shape!\nI'm a cube!\n\n\nWhat’s particularly interesting below is the output of dir: notice all of the methods we didn’t create but which are available! And notice that many of them start __ (like __init__ up above). That means the method is ‘private’ and shouldn’t be called directly by the programmer. But what might __gt__ do and why might the person creating the shape classes need to do something here?\n\nprint(b)\nprint(b.volume())\nprint(b.diameter())\nprint(b.surface())\ndir(b)\n\n&lt;shapes.cube object at 0x162852ba0&gt;\n1000\n17.320508075688775\n600\n\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'diameter',\n 'dim',\n 'shape_type',\n 'surface',\n 'type',\n 'volume']\n\n\n\np = shapes.t_pyramid(150)\nprint(p.diameter())\nprint(p.height())\nprint(p.volume())\nprint(p.surface())\n\nI'm a shape!\n150\n122.47448713915888\n397747.5644174329\n38971.143170299736"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-6.-adding-documentation",
    "href": "extra/Live-04-Objects-1.html#task-6.-adding-documentation",
    "title": "Intro",
    "section": "Task 6.: Adding Documentation",
    "text": "Task 6.: Adding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\n\nimport shapes\nhelp(shapes.shape)\nhelp(shapes.shape.type)\n\nHelp on class shape in module shapes:\n\nclass shape(builtins.object)\n |  shape(dimension: float = None)\n |\n |  # Base class shape\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension: float = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |\n |  diameter(self)\n |\n |  surface(self)\n |\n |  type(self)\n |\n |  volume(self)\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\nHelp on function type in module shapes:\n\ntype(self)"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html",
    "href": "extra/Live-03-Foundations_2.html",
    "title": "Task 0: LoLs and DoLs",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 3: Foundations (Part 2)&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with Dictionaries, LOLs and DOLs, Packages and Functions&lt;/h2&gt;\nCome to class prepared to present/discuss:"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-1-reading-a-remote-file",
    "href": "extra/Live-03-Foundations_2.html#task-1-reading-a-remote-file",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 1: Reading a Remote File",
    "text": "Task 1: Reading a Remote File\n\nTask 1.1: Research Solution\n\nGoogle: read remote CSV file Python\nOooh look, Stack Overflow link\nLet’s review this–note: Python2 vs Python3\n\nLook at dates on answers to see if reasonably useful\nRead responses to what looks like useful answer\nWhich answer was accepted?\n\nOK, so it looks like urllib and csv will be useful.\n\nHow do we work out what’s possible?\nHow do we read help for a function?\n\n\n\nfrom urllib.request import urlopen\n?urlopen\n\n\n\nTask 1.2: Implementing Solution\nOK, so now we know what to do, how do we do it?\n\nSet a url variable\nCapture the response\nRead it, what’s the decoding thing?\nLook at the ‘.’ cropping up: we’ll deal with that later.\n\n\nfrom urllib.request import urlopen\n\n# Given the info you were given above, what do you \n# think the value of 'url' should be? What\n# type of variable is it? int or string? \nurl = 'https://github.com/jreades/fsds/raw/master/data/src/2022-sample-Crime.csv'\n\n# Read the URL stream into variable called 'response'\n# using the function that we imported above\nresponse = urlopen(url)\n\n# Now read from the stream, decoding so that we get actual text\ndatafile = response.read().decode('utf-8')\n\n# You might want to explore what `__class__` and `__name__`\n# offer, but basically the give us a way of finding out what\n# is 'behind' more complex variables\nprint(\"datafile variable is of type: '\" + datafile.__class__.__name__ + \"'.\\n\")\n\ndatafile variable is of type: 'str'.\n\n\n\n\n\nTask 1.3: Checking Solution\nIt’s tempting to just print out the contents of datafile, but what should we do? - Print out the size of the variable (how do we do this for a string?) - Print out some of the top of the file (how do we do this for the first n chars in a string?)\n\nprint(len(datafile))\nprint(datafile[:600])\n\n3809403\nID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location\n11570600,JB377499,08/03/2018 01:16:00 PM,046XX S ST LAWRENCE AVE,2024,NARCOTICS,POSSESS - HEROIN (WHITE),VEHICLE NON-COMMERCIAL,true,false,0221,002,4,38,18,,,2018,09/10/2022 04:50:59 PM,,,\n12457866,JE332953,08/10/2021 04:20:00 PM,016XX W VAN BUREN ST,2018,NARCOTICS,MANUFACTURE / DELIVER - SYNTHETIC DRUGS,VEHICLE NON-COMMERCIAL,true,false,1231,012,27,28,18,,,2021,09/10/2022 04:50:59 PM,,,\n128"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-2-parsing-a-csv-file-using-a-package",
    "href": "extra/Live-03-Foundations_2.html#task-2-parsing-a-csv-file-using-a-package",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 2: Parsing a CSV file using a Package",
    "text": "Task 2: Parsing a CSV file using a Package\nRight, so we’ve got our data in datafile, what are we going to do with it now?\n\nTask 2.1: Research Solution\n\nWe need to turn it into data by reading the CSV\n\nGoogle what to do\nHmmm, this looks useful,\nMaybe also try read csv file python example\n\nOK, so it looks like we need to splitlines first.\nThen let’s read it into a list (What data structure is this?)\nNow, how would we print out the number of rows and columns?\n\n\nimport csv \n\nurlData = [] # Somewhere to store the data\n\ncsvfile = csv.reader(datafile.splitlines())\n\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(f\"urlData has {len(urlData)} rows and {len(urlData[0])} columns.\")\n\nurlData has 16956 rows and 22 columns.\n\n\n\n\nTask 2.2: Selecting a Sample Row\nHow could we print out a random row? Let’s Google it. - Library random seems promising - Which function do we want?\n\nimport random\ndir(random)\nhelp(random.randint)\n\nHelp on method randint in module random:\n\nrandint(a, b) method of random.Random instance\n    Return random integer in range [a, b], including both end points.\n\n\n\n\nurlData[random.randint(1,len(urlData))]\n\n['12584665',\n 'JF102781',\n '10/23/2019 12:00:00 PM',\n '021XX N Cleveland Ave',\n '1153',\n 'DECEPTIVE PRACTICE',\n 'FINANCIAL IDENTITY THEFT OVER $ 300',\n '',\n 'false',\n 'false',\n '1812',\n '018',\n '43',\n '7',\n '11',\n '',\n '',\n '2019',\n '01/05/2022 03:49:10 PM',\n '',\n '',\n '']"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-3-reading-file-as-dictionary-of-lists",
    "href": "extra/Live-03-Foundations_2.html#task-3-reading-file-as-dictionary-of-lists",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 3: Reading File as Dictionary of Lists",
    "text": "Task 3: Reading File as Dictionary of Lists\n\nTask 3.1: Finding the Header Row\nSometimes this is easy (it’s the very first row in a CSV file), but often (especially with Excel data from, e.g., the Office for National Statistics) it’s not. So here is where doing some quick, initial checks using head can be helpful.\n\nurlData[0]\n\n['ID',\n 'Case Number',\n 'Date',\n 'Block',\n 'IUCR',\n 'Primary Type',\n 'Description',\n 'Location Description',\n 'Arrest',\n 'Domestic',\n 'Beat',\n 'District',\n 'Ward',\n 'Community Area',\n 'FBI Code',\n 'X Coordinate',\n 'Y Coordinate',\n 'Year',\n 'Updated On',\n 'Latitude',\n 'Longitude',\n 'Location']\n\n\nOK, some questions: - What does row 0 give us? Are these data, or something else? - If we were making a dictionary-of-lists, how would we use Row 0? - What do we need to do to set this up?\n\n\nTask 3.2: Creating a DOL from Data\nHow would we use the header row to initialise our Dictionary-of-Lists.\n\nds = {}\ncol_names = urlData[0]\nfor c in col_names:\n    ds[c] = []\n\nNext… - How would we print out all of the column names? - How would we go about adding all of the data? - What kind of loop would this use?\n\nprint(ds.keys())\n\ndict_keys(['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude', 'Location'])\n\n\n\nfor r in urlData[1:len(urlData)]:\n    for c in range(0,len(col_names)):\n        ds[ col_names[c] ].append( r[c] )\n\n\n\nTask 3.3: Validating/Checking\nLet’s check a few columns to see if the data makes sense!\n\nprint(ds['Case Number'][:20])\nprint()\nprint(ds['Primary Type'][:20])\nprint()\n\n['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989', 'JF413871', 'JF380003', 'JF378902', 'JF415101', 'JF379014', 'JF378985', 'JF415451', 'JF415893', 'JF378620', 'JF380820', 'JF380344', 'JF380066', 'JF378882', 'JF379811', 'JF381657']\n\n['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n\n\n\nActually, is there a better way to do this? Rather than printing each out in turn, could we do this as a loop?\n\nfor c in ds.keys():\n    print(ds[c][:5])\n\n['11570600', '12457866', '12842196', '12842170', '12842271']\n['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989']\n['08/03/2018 01:16:00 PM', '08/10/2021 04:20:00 PM', '03/01/2020 12:00:00 AM', '02/28/2018 09:00:00 AM', '01/01/2014 08:00:00 PM']\n['046XX S ST LAWRENCE AVE', '016XX W VAN BUREN ST', '069XX S CALUMET AVE', '128XX S LOWE AVE', '035XX S STATE ST']\n['2024', '2018', '1153', '1153', '1153']\n['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n['POSSESS - HEROIN (WHITE)', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300']\n['VEHICLE NON-COMMERCIAL', 'VEHICLE NON-COMMERCIAL', 'RESIDENCE', 'RESIDENCE', 'CTA TRAIN']\n['true', 'true', 'false', 'false', 'false']\n['false', 'false', 'false', 'false', 'false']\n['0221', '1231', '0322', '0523', '0213']\n['002', '012', '003', '005', '002']\n['4', '27', '6', '9', '3']\n['38', '28', '69', '53', '35']\n['18', '18', '11', '11', '11']\n['', '', '', '', '']\n['', '', '', '', '']\n['2018', '2021', '2020', '2018', '2014']\n['09/10/2022 04:50:59 PM', '09/10/2022 04:50:59 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM']\n['', '', '', '', '']\n['', '', '', '', '']\n['', '', '', '', '']\n\n\nHow would we improve this?\n\nfor c in ds.keys():\n    print(f\"{c}:\\t{ds[c][:5]}\")\n    print()\n\nID: ['11570600', '12457866', '12842196', '12842170', '12842271']\n\nCase Number:    ['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989']\n\nDate:   ['08/03/2018 01:16:00 PM', '08/10/2021 04:20:00 PM', '03/01/2020 12:00:00 AM', '02/28/2018 09:00:00 AM', '01/01/2014 08:00:00 PM']\n\nBlock:  ['046XX S ST LAWRENCE AVE', '016XX W VAN BUREN ST', '069XX S CALUMET AVE', '128XX S LOWE AVE', '035XX S STATE ST']\n\nIUCR:   ['2024', '2018', '1153', '1153', '1153']\n\nPrimary Type:   ['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n\nDescription:    ['POSSESS - HEROIN (WHITE)', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300']\n\nLocation Description:   ['VEHICLE NON-COMMERCIAL', 'VEHICLE NON-COMMERCIAL', 'RESIDENCE', 'RESIDENCE', 'CTA TRAIN']\n\nArrest: ['true', 'true', 'false', 'false', 'false']\n\nDomestic:   ['false', 'false', 'false', 'false', 'false']\n\nBeat:   ['0221', '1231', '0322', '0523', '0213']\n\nDistrict:   ['002', '012', '003', '005', '002']\n\nWard:   ['4', '27', '6', '9', '3']\n\nCommunity Area: ['38', '28', '69', '53', '35']\n\nFBI Code:   ['18', '18', '11', '11', '11']\n\nX Coordinate:   ['', '', '', '', '']\n\nY Coordinate:   ['', '', '', '', '']\n\nYear:   ['2018', '2021', '2020', '2018', '2014']\n\nUpdated On: ['09/10/2022 04:50:59 PM', '09/10/2022 04:50:59 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM']\n\nLatitude:   ['', '', '', '', '']\n\nLongitude:  ['', '', '', '', '']\n\nLocation:   ['', '', '', '', '']"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-4-fixing-column-types",
    "href": "extra/Live-03-Foundations_2.html#task-4-fixing-column-types",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 4: Fixing Column Types",
    "text": "Task 4: Fixing Column Types\nOK, so we have a few columns that aren’t really of the right type. We have date-time types in Python that we’re not going to get stuck into now, but we also very obviously have numbers and booleans as well that we need to deal with!\nSo how we would do this? The process for each float would be the same. The process for each int would be the same. The process for each boolean would be the same. Sounds like a good opportunity for a function!\n\nTask 4.1: What Type Am I?\nGiven these data… What Python data type should each one be?\n\nID:\nCase Number:\nDate:\nPrimary Type:\nDescription:\nLocation Description:\nArrest:\nDomestic:\nYear:\nLatitude:\nLongitude:\n\n\n\nTask 4.2 Converting One Column\n\ndef to_bool(col_data):\n    fdata = []\n    for c in col_data:\n        fdata.append( c=='true' )\n    return fdata\n\nbool_cols = ['Arrest','Domestic']\nfor b in bool_cols:\n    ds[ b ] = to_bool( ds[b] )\n\n\nds['Arrest'][:6]\n\n[True, True, False, False, False, False]\n\n\n\nimport numpy as np\n\n\narrest_made = list(np.where(ds['Arrest']))[0]\n\narrest_reasons = set()\n\nfor a in arrest_made:\n    arrest_reasons.add(ds['Description'][a])\n\nprint(arrest_reasons)\nprint()\nprint(\", \".join([i.title() for i in sorted(arrest_reasons)]))\n\n{'POSSESS - METHAMPHETAMINE', 'ATTEMPT POSSESSION NARCOTICS', 'DECEPTIVE COLLECTION PRACTICES', 'CREDIT CARD FRAUD', 'MANUFACTURE / DELIVER - HEROIN (TAN / BROWN TAR)', 'POSSESS - HEROIN (WHITE)', 'POSSESS - HALLUCINOGENS', 'POSSESS - AMPHETAMINES', 'ILLEGAL POSSESSION CASH CARD', 'ATTEMPT - FINANCIAL IDENTITY THEFT', 'CRIMINAL DRUG CONSPIRACY', 'EMBEZZLEMENT', 'POSSESS - COCAINE', 'MANUFACTURE / DELIVER - PCP', 'MANUFACTURE / DELIVER - HALLUCINOGEN', 'BOGUS CHECK', 'POSSESS - HEROIN (TAN / BROWN TAR)', 'POSSESS - HYPODERMIC NEEDLE', 'ALTER / FORGE PRESCRIPTION', 'MANUFACTURE / DELIVER - CANNABIS OVER 10 GRAMS', 'UNAUTHORIZED VIDEOTAPING', 'FORGERY', 'THEFT OF LOST / MISLAID PROPERTY', 'FOUND SUSPECT NARCOTICS', 'IMPERSONATION', 'POSSESS - CANNABIS 30 GRAMS OR LESS', 'MANUFACTURE / DELIVER - COCAINE', 'SALE / DELIVER - DRUG PARAPHERNALIA', 'FORFEIT PROPERTY', 'SOLICIT NARCOTICS ON PUBLIC WAY', 'POSSESS - BARBITURATES', 'MANUFACTURE / DELIVER - CRACK', 'POSSESS - CANNABIS MORE THAN 30 GRAMS', 'POSSESS - PCP', 'THEFT BY LESSEE, MOTOR VEHICLE', 'MANUFACTURE / DELIVER -  HEROIN (WHITE)', 'ILLEGAL USE CASH CARD', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'POSSESS - CRACK', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'COUNTERFEITING DOCUMENT', 'COUNTERFEIT CHECK', 'POSSESS - HEROIN (BLACK TAR)', 'MANUFACTURE / DELIVER - BARBITURATES', 'MANUFACTURE / DELIVER - METHAMPHETAMINE', 'MANUFACTURE / DELIVER - CANNABIS 10 GRAMS OR LESS', 'FINANCIAL EXPLOITATION OF AN ELDERLY OR DISABLED PERSON', 'MANUFACTURE / DELIVER - AMPHETAMINES', 'THEFT OF LABOR / SERVICES', 'POSSESSION OF DRUG EQUIPMENT', 'POSSESS - SYNTHETIC DRUGS', 'FRAUD OR CONFIDENCE GAME', 'STOLEN PROPERTY BUY / RECEIVE / POSSESS', 'MANUFACTURE / DELIVER - HEROIN (BLACK TAR)'}\n\nAlter / Forge Prescription, Attempt - Financial Identity Theft, Attempt Possession Narcotics, Bogus Check, Counterfeit Check, Counterfeiting Document, Credit Card Fraud, Criminal Drug Conspiracy, Deceptive Collection Practices, Embezzlement, Financial Exploitation Of An Elderly Or Disabled Person, Financial Identity Theft Over $ 300, Forfeit Property, Forgery, Found Suspect Narcotics, Fraud Or Confidence Game, Illegal Possession Cash Card, Illegal Use Cash Card, Impersonation, Manufacture / Deliver -  Heroin (White), Manufacture / Deliver - Amphetamines, Manufacture / Deliver - Barbiturates, Manufacture / Deliver - Cannabis 10 Grams Or Less, Manufacture / Deliver - Cannabis Over 10 Grams, Manufacture / Deliver - Cocaine, Manufacture / Deliver - Crack, Manufacture / Deliver - Hallucinogen, Manufacture / Deliver - Heroin (Black Tar), Manufacture / Deliver - Heroin (Tan / Brown Tar), Manufacture / Deliver - Methamphetamine, Manufacture / Deliver - Pcp, Manufacture / Deliver - Synthetic Drugs, Possess - Amphetamines, Possess - Barbiturates, Possess - Cannabis 30 Grams Or Less, Possess - Cannabis More Than 30 Grams, Possess - Cocaine, Possess - Crack, Possess - Hallucinogens, Possess - Heroin (Black Tar), Possess - Heroin (Tan / Brown Tar), Possess - Heroin (White), Possess - Hypodermic Needle, Possess - Methamphetamine, Possess - Pcp, Possess - Synthetic Drugs, Possession Of Drug Equipment, Sale / Deliver - Drug Paraphernalia, Solicit Narcotics On Public Way, Stolen Property Buy / Receive / Possess, Theft By Lessee, Motor Vehicle, Theft Of Labor / Services, Theft Of Lost / Mislaid Property, Unauthorized Videotaping\n\n\n\n\nTask 4.3: Converting Another Column\nFloats first!\n\nimport numpy as np\n\ndef to_float(col_data):\n    fdata = []\n    for c in col_data:\n        try:\n            fdata.append( float(c) )\n        except (ValueError, TypeError):\n            fdata.append( np.nan )\n    return fdata\n\nfloat_cols = ['Latitude','Longitude']\nfor f in float_cols:\n    ds[ f ] = to_float( ds[f] )\n\n\nds['Latitude'][:6]\n\n[nan, nan, nan, nan, nan, nan]\n\n\n\nprint(f\"Have found {len(list(np.where(~np.isnan(ds['Latitude'])))[0]):,} records with lat/long coordinates\")\n\nHave found 9,801 records with lat/long coordinates\n\n\n\nfor i in list(np.where(~np.isnan(ds['Latitude'])))[0][:15]:\n    print(ds['Latitude'][i])\n\n41.885739108\n41.893676531\n41.885931086\n41.786188054\n41.788414065\n41.701933304\n41.976290414\n41.772998982\n41.881857098\n41.722597651\n41.80184271\n41.763268313\n41.986788545\n41.793924823\n41.894327846\n\n\nThen booleans!\nThen ints! (Drop if time is short)\n\ndef to_int(col_data):\n    fdata = []\n    for c in col_data:\n        fdata.append( int(c) )\n    return fdata\n\nint_cols = ['ID','Year']\nfor i in int_cols:\n    ds[ i ] = to_int( ds[i] )\n\n\nds['Year'][:6]\n\n[2018, 2021, 2020, 2018, 2014, 2020]\n\n\n\nprint(f\"There are {len(np.where(np.asarray(ds['Arrest']))[0]):,} arrests\")\nprint(f\"There are {len(set(np.where(np.asarray(ds['Year']) &gt; 2021)[0])):,} records from 2022 onwards\")\n\nThere are 4,108 arrests\nThere are 13,845 records from 2022 onwards\n\n\n\nintersect = set(np.where(np.asarray(ds['Arrest']))[0]).intersection(set(np.where(np.asarray(ds['Year']) &gt; 2021)[0]))\nprint(f\"There are {len(intersect):,} records that are both.\")\n\nThere are 3,058 records that are both."
  },
  {
    "objectID": "assessments/resources.html",
    "href": "assessments/resources.html",
    "title": "Resources",
    "section": "",
    "text": "To get you started, we’ve created a set of templates that you’ll want to keep together in your Git/GitHub repo. You are free to modify the template and its defaults as needed for stylistic or other purposes.\n\nThe template file (.qmd) that we have provided: Group_Work.qmd. You can see both PDF and HTML output (and may find rendering to HTML faster when writing and testing), but please only submit the raw QMD and rendered PDF in their respective areas. Zip files and other submissions of multiple files will be ignored.\nThe CSL file (.csl) that governs how references are done: harvard-cite-them-right.csl.\nThe BibTeX file (.bib) that will contain any references you use: bio.bib (you may also make use of the one used for the module to simplify the creation/management of your bibliography)\n\nNote that in our reproducibility tests we are starting with only the qmd file since you could choose to change the CSL style and will be producing your own references.\n\n\n\n\n\n\nNoteReally Look at the Template!\n\n\n\nYou’ll notice that the template specifies three fonts (mainfont, monofont, sansfont). These are installed in the Docker image. You are welcome to change the fonts used and can see what’s available in the existing Docker image by asking matplotlib or using the Terminal… which will help you to learn about font-management.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Resources"
    ]
  },
  {
    "objectID": "assessments/resources.html#the-templates",
    "href": "assessments/resources.html#the-templates",
    "title": "Resources",
    "section": "",
    "text": "To get you started, we’ve created a set of templates that you’ll want to keep together in your Git/GitHub repo. You are free to modify the template and its defaults as needed for stylistic or other purposes.\n\nThe template file (.qmd) that we have provided: Group_Work.qmd. You can see both PDF and HTML output (and may find rendering to HTML faster when writing and testing), but please only submit the raw QMD and rendered PDF in their respective areas. Zip files and other submissions of multiple files will be ignored.\nThe CSL file (.csl) that governs how references are done: harvard-cite-them-right.csl.\nThe BibTeX file (.bib) that will contain any references you use: bio.bib (you may also make use of the one used for the module to simplify the creation/management of your bibliography)\n\nNote that in our reproducibility tests we are starting with only the qmd file since you could choose to change the CSL style and will be producing your own references.\n\n\n\n\n\n\nNoteReally Look at the Template!\n\n\n\nYou’ll notice that the template specifies three fonts (mainfont, monofont, sansfont). These are installed in the Docker image. You are welcome to change the fonts used and can see what’s available in the existing Docker image by asking matplotlib or using the Terminal… which will help you to learn about font-management.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Resources"
    ]
  },
  {
    "objectID": "assessments/resources.html#rendering",
    "href": "assessments/resources.html#rendering",
    "title": "Resources",
    "section": "Rendering",
    "text": "Rendering\nThe method for rendering the document will be the same on your computer as on ours, this is how we are testing for reproducibility. From the Terminal in Docker you use the following command to render a document in the current directory:\nquarto render Group_Work.qmd --to pdf \nThis is the command we will be running to generate the PDF output as well, though you can change the name of the input file if you like.\nHowever, there are a few things to note:\n\nWe run this command in the same directory as your QMD file (i.e. in Docker we would run cd work/&lt;your group's name&gt;/ then quarto render &lt;your group's file&gt;.qmd --to pdf).\nWe will have only your raw QMD file and no other files, so you need to ensure that all supporting files, data, and code are downloaded near the start of the QMD file to be available for inclusion in the render pipeline.\nYou will want to test that your QMD file renders correctly on a ‘new’ machine or ‘clean’ folder before submitting it.\n\nGood luck!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Resources"
    ]
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "Context",
    "section": "",
    "text": "The overall assessment package is intended to test students’ comprehension of, and ability to integrate, technical skills with a broader understanding of, and reflection upon, computational approaches to urban research and spatial data science.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#assessment-elements",
    "href": "assessments/index.html#assessment-elements",
    "title": "Context",
    "section": "Assessment Elements",
    "text": "Assessment Elements\nThe assessments are grounded in a mixture of critical reflection and group work that map on to real-world data science challenges, including:\n\nTimed Open Book Examination (30%) to ensure that you are acquiring the fundamentals of data analysis in Python, and simulate the real-world challenge of having to write code to a deadline (to be completed between 11am-11pm or 1pm-3pm on Friday, 21 November 2025);\nCollaboratively evaluating and analysing a data set (two parts, worth 25% and 35%) as part of a small group you will be determining the suitability of a data set for tackling an analytical ‘problem’ using a mix of coding, analysis, and presentation skills in a reproducible format Tuesday, 16 December 2025 @ 10:00);\nReflecting on the process (10%) to better-understand why a project succeeded/failed so as to improve future outcomes and recognise the contributions of individual members of the group to the success of the project (due Tuesday, 16 December 2025; the individual component is due @ 10:00 and your peer ratings are due @ 12:00).",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#rationale",
    "href": "assessments/index.html#rationale",
    "title": "Context",
    "section": "Rationale",
    "text": "Rationale\nCollectively, these assessments seek to provide multiple opportunities to ‘shine’ both individually and as part of a group. You do not need to be the best programmer in the class in order to do well on these assessments. Indeed, focussing only on the programming is likely to result in a low mark because it missed the context in which data science and data analysis ‘work’. As a budding data scientist/analyst your job is just as much to understand your audience and their needs: you will work with clients who can’t really articulate what they want or why, so good project management often involves putting yourself in your client’s shoes and working out how to translate what they say they want into what they actually need.\nYou will therefore do poorly on the assessments if you do not do the readings, watch the pre-recorded lectures, or participate in discussions (both online and in-person during practicals and classes). These provide you with context for the work that is being done when you start typing and running code in a Jupyter Notebook. Code is the how. Context is the why.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/exam.html",
    "href": "assessments/exam.html",
    "title": "Timed Open Book Exam",
    "section": "",
    "text": "This ‘exam’ (to be completed on Friday, 21 November 2025 between 11am-11pm or 1pm-3pm and worth 30%) will be administered as a quiz through Moodle. Most questions require little or no coding, but for those where an answer that includes code (or where coding helps you to check your answer) you can use Docker or Python directly (as you like) to do your work. Include code only if instructed to do so in the question.\nFor the exam itself you have 1h 20m (2 hours with a relevant SORA) to submit your answers. The ‘clock’ runs for the window period of those with a SORA (11am-11pm or 1pm-3pm). All incomplete exams are closed at the end of the window. If you do not have a SORA all questions submitted more than 1h 20m after the start of the exam will be discarded.",
    "crumbs": [
      "Elements",
      "Timed Open Book Exam"
    ]
  },
  {
    "objectID": "assessments/exam.html#expectations",
    "href": "assessments/exam.html#expectations",
    "title": "Timed Open Book Exam",
    "section": "Expectations",
    "text": "Expectations\nAs a general set of expectations:\n\nAny content covered prior to Reading Week may be encountered in the exam.\nThe exam is ‘open book’, but there are three important limitations:\n\nYou may not obtain direct assisstance from another person (either in-person or online);\nYou may not ask questions relating to the exam on StackOverflow or other ‘online help boards’ (you can search for existing answers).\nYou may use ChatGPT or another LLM for help.\n\nHowever, if you make use of an external resource (e.g. you search for and find relevant code on StackOverflow or use ChatGPT) you must acknowledge this (a template has been included in free text answers).\nAnyone found to have cheated on this submission will receive a mark of 0 and the resit assessment will involve producing an analysis in a timed context under direct supervision of module staff: you will be required to be present in-person for the resit (this is likely to be during the summer period) and will have to complete a set of coding tasks specified by the module leader.\n\nIn other words: don’t do it.\nGood luck!",
    "crumbs": [
      "Elements",
      "Timed Open Book Exam"
    ]
  },
  {
    "objectID": "assessments/conflicts.html",
    "href": "assessments/conflicts.html",
    "title": "Dealing with Conflict",
    "section": "",
    "text": "WarningGroup Disputes\n\n\n\nIn the event that there is irreconcilable disagreement within a group, we will use GitHub to determine contributions and inform individual marks.\nWorking on a group project through Git and GitHub will invetiably result in version control conflicts: two (or more) students will have made changes that need to be reconciled using Git’s conflict resolution mechanisms. Depending on the nature of the conflict, these can be trivial or very, very hard to resolve. If you think about it, the obvious conflicts will broadly fall into three classes:\nMaking this all a lot harder is that a ‘change’ could be anything from a direct edit to a section of a line, to deleting a file or folder! There’s a lot more to this, and there are undoubtedly lots of good examples of about (e.g. Example 1, Example 2).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#spotting-a-conflict",
    "href": "assessments/conflicts.html#spotting-a-conflict",
    "title": "Dealing with Conflict",
    "section": "Spotting a Conflict",
    "text": "Spotting a Conflict\nYou will most likely discover a conflict when pushing local changes or pulling remote changes. So let’s imagine that you’ve done something like this:\ngit add [file]\ngit commit -m \"[your message]\"\ngit push\nYou then see the message:\nfailed to push some refs to [url].\nUpdates were rejected because the remote contains work that you do not have locally.  \nSince you can’t push, what do you do?",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#first-things-first",
    "href": "assessments/conflicts.html#first-things-first",
    "title": "Dealing with Conflict",
    "section": "First Things First",
    "text": "First Things First\nCheck the status of the repository:\ngit status\nThis will tell you both the status of your local repository and how it compares to the remote one, helping you to work out where the likely problems are. Before doing anything else you should run this command and keep the output handy (either open a new Terminal for subsequent steps, or copy the output into a Markdown editor).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-1-hope-for-the-best",
    "href": "assessments/conflicts.html#option-1-hope-for-the-best",
    "title": "Dealing with Conflict",
    "section": "Option 1: Hope for the Best",
    "text": "Option 1: Hope for the Best\nIf you’re lucky then some conflicts can be resolved simply with a git pull. This works when there are remote changes that have zero impact on your local changes. Pulling the remote changes will update your local copy and then you can just turn around and push your local changes back to the remote (usually: GitHub).\nWhat do you do if that doesn’t work?",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-2-resolve-conflicts-locally",
    "href": "assessments/conflicts.html#option-2-resolve-conflicts-locally",
    "title": "Dealing with Conflict",
    "section": "Option 2: Resolve Conflicts Locally",
    "text": "Option 2: Resolve Conflicts Locally\nFetch the latest changes from the remote repository:\ngit fetch origin\nMerge the remote version of the repository (that you just fetched) with the local version:\ngit merge origin/main (or git merge origin/master)\n\n\n\n\n\n\nWarningWhat Branch?\n\n\n\nThe command above assumes that you are working in the main branch, which is the most likely branch unless your group is very, very advanced. But how do you know what branch you’re working on?\nRunning git branch --show-current will give you the currently active branch. The other way (git branch) shows you a list of local branches, the one with a * next to it is the one you’re currently using. To exit the list of branches just hit the letter q for ‘Quit’.\n\n\nYou will likely then see a message similar to:\nCONFLICT (content): Merge conflict in your-file.md\n\nFormat of a Conflict\nWhen you have a conflict, the format of a conflict is (roughly) as follows2:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:file.txt\nHello world\n=======\nGoodbye\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 77976da35a11db4580b80ae27e8d65caf5208086:file.txt\nThe first section (from &lt;&lt;&lt;&lt; HEAD to ====) is what you have in your local file. The second section (from ==== to &gt;&gt;&gt;&gt; &lt;hexadecimal number&gt;) is the change that is coming from the remote repository that Git wants to merge on to your local file but can’t because of the changes that you’ve made locally.\nOpen the conflicting files and manually edit them. You will see annotations like this indicating where there are conflicts:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; origin/main\nChange #1\n=======\nChange #2\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\nResolve the conflicts and remove the annotations (the &lt;&lt;&lt;..., ===... and &gt;&gt;&gt;... lines):\nChange #1\nChange #2\nCommit your changes and push the merged results back to the remote repository.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-3-stash",
    "href": "assessments/conflicts.html#option-3-stash",
    "title": "Dealing with Conflict",
    "section": "Option 3: Stash",
    "text": "Option 3: Stash\nAnother option is to ‘stash’ your local changes and then try to reapply them when you’ve got the latest version from the repository. The sequence is:\n\nStash your work (git stash deals with files you’re already tracking in Git; git stash -u includes untracked files).\nPull the version with the conflict (git pull if you’re dealing with the HEAD on the remote repository)\nRe-apply your stashed changes (Read the documentation carefully since there are several options here).\nCommit.\nPush.\n\nAltassian has a nice overview of how this works.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-4-open-a-pull-request",
    "href": "assessments/conflicts.html#option-4-open-a-pull-request",
    "title": "Dealing with Conflict",
    "section": "Option 4: Open a Pull Request",
    "text": "Option 4: Open a Pull Request\nTo resolve, create a new branch:\ngit checkout -b new-branch-name\nCheck that you are in the new branch:\ngit branch -v\nCheck that your local changes are committed on to the new branch and then push the new branch to the remote repository:\ngit push --set-upstream origin new-branch-name\nYou then create a Pull Request on Github (you will see a green button pop up “Compare & Pull Request”):\n\nGit will say “Can’t automatically merge”. That’s OK. Click “Create pull request”.\nGit will “Check for ability to merge automatically” and will then say:\n\n“This branch has conflicts that must be resolved”\nClick “Resolve conflicts”\nGit will show you text like this that shows the conflicting changes in each branch and the resolution process is the same as above:\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; new-branch-name\nYet another one\n=======\nAnother one\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n\nNow that you’ve resolved the conflict, click “Mark as resolved” and “Commit merge”.\nThen, “Merge pull request” and “Confirm merge”.\n\nWhen you go back to the main page of your repository, you will see the new changes.\nOne more thing! On your computer, make sure to exit your branch and return to the main (or master) branch.\ngit checkout main\nThen, pull your latest changes from the Github repository.\ngit pull\nCongratulations (🎉), you have successfully resolved a merge conflict!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#footnotes",
    "href": "assessments/conflicts.html#footnotes",
    "title": "Dealing with Conflict",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example: you’ve deleted lines 6-9 and changed line 10, the other person has modified lines 17-18 of the same file, so Git needs to determine that 17-18 of the other change maps on to 14-15 of your change.↩︎\nTaken from: https://stackoverflow.com/a/7901901↩︎",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/Group_Work.html",
    "href": "assessments/Group_Work.html",
    "title": "Group Name’s Group Project",
    "section": "",
    "text": "We, [insert your group’s names], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\nDate:\nStudent Numbers:"
  },
  {
    "objectID": "assessments/Group_Work.html#declaration-of-authorship",
    "href": "assessments/Group_Work.html#declaration-of-authorship",
    "title": "Group Name’s Group Project",
    "section": "",
    "text": "We, [insert your group’s names], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\nDate:\nStudent Numbers:"
  },
  {
    "objectID": "assessments/Group_Work.html#priorities-for-feedback",
    "href": "assessments/Group_Work.html#priorities-for-feedback",
    "title": "Group Name’s Group Project",
    "section": "Priorities for Feedback",
    "text": "Priorities for Feedback\nAre there any areas on which you would appreciate more detailed feedback if we’re able to offer it?"
  },
  {
    "objectID": "assessments/Group_Work.html#word-counts",
    "href": "assessments/Group_Work.html#word-counts",
    "title": "Group Name’s Group Project",
    "section": "Word Counts",
    "text": "Word Counts\nWe will only count the text that you’ve added in the final PDF. Headings, guidance, etc. do no count towards the total. References do. And please also see the rest of guidance relating to Content."
  },
  {
    "objectID": "assessments/Group_Work.html#is-airbnb-out-of-control-in-london",
    "href": "assessments/Group_Work.html#is-airbnb-out-of-control-in-london",
    "title": "Group Name’s Group Project",
    "section": "1. Is Airbnb out of control in London?",
    "text": "1. Is Airbnb out of control in London?\n\n( points; Answer due Week 7 )"
  },
  {
    "objectID": "assessments/Group_Work.html#how-many-professional-landlords-are-there",
    "href": "assessments/Group_Work.html#how-many-professional-landlords-are-there",
    "title": "Group Name’s Group Project",
    "section": "2. How many professional landlords are there?",
    "text": "2. How many professional landlords are there?\n\n( points; Answer due Week 8 )"
  },
  {
    "objectID": "assessments/Group_Work.html#how-many-properties-will-be-impacted-by-the-oppositions-proposal",
    "href": "assessments/Group_Work.html#how-many-properties-will-be-impacted-by-the-oppositions-proposal",
    "title": "Group Name’s Group Project",
    "section": "3. How many properties will be impacted by the opposition’s proposal?",
    "text": "3. How many properties will be impacted by the opposition’s proposal?\n\n( points; Answer due Week 9 )"
  },
  {
    "objectID": "assessments/Group_Work.html#what-are-the-pros-and-cons-to-the-oppositions-proposal",
    "href": "assessments/Group_Work.html#what-are-the-pros-and-cons-to-the-oppositions-proposal",
    "title": "Group Name’s Group Project",
    "section": "4. What are the pros and cons to the opposition’s proposal?",
    "text": "4. What are the pros and cons to the opposition’s proposal?\n\n( points; Answer due Week 7 )"
  },
  {
    "objectID": "assessments/Group_Work.html#can-we-somehow-spin-this-scandal-into-a-positive-story-about-social-mobility",
    "href": "assessments/Group_Work.html#can-we-somehow-spin-this-scandal-into-a-positive-story-about-social-mobility",
    "title": "Group Name’s Group Project",
    "section": "5. Can we somehow spin this scandal into a positive story about social mobility?",
    "text": "5. Can we somehow spin this scandal into a positive story about social mobility?\n\n( points; Answer due Week 7 )"
  },
  {
    "objectID": "assessments/code.html",
    "href": "assessments/code.html",
    "title": "Code",
    "section": "",
    "text": "The Reproducible Analysis is worth 25% of your module grade.\nThe Reproducible Analysis must be written using Python in a Quarto Markdown Document (QMD file). You are free to draw on concepts and methods covered in both Quantitative Methods and GIS, but must still write the code in Python (e.g. adapting something from R in the GIS module to Python).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#preparing-your-submission",
    "href": "assessments/code.html#preparing-your-submission",
    "title": "Code",
    "section": "Preparing Your Submission",
    "text": "Preparing Your Submission\nYou are expected to use sustainable authorship tools for this submission. You may be asked to provide evidence of this. A template will be provided, and you should also look at the Quarto Guide and, in particular, the PDF options in order to customise your project.\nYou are strongly advised to develop and maintain the submission in GitHub so that we can review contributions if necessary. In the absence of a GitHub commit history or everything being committed by only one member of the group we will be unable to determine individual contributions when marking the submission as a whole.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#how-we-measure-reproducibility",
    "href": "assessments/code.html#how-we-measure-reproducibility",
    "title": "Code",
    "section": "How We Measure Reproducibility",
    "text": "How We Measure Reproducibility\nYou will be submitting a runnable markdown document (.qmd file) that we will run on our own computers in order to generate a PDF output. If you have made use of one or more libraries that are not part of the Docker image then you can install these using ! pip install; however, if you take this approach then you should also ‘place nice’ by checking first to see if the library is already installed using try... except code that you can find on Stack Overflow and elsewhere (you will need to look this up).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#data-and-resources-used",
    "href": "assessments/code.html#data-and-resources-used",
    "title": "Code",
    "section": "Data and Resources Used",
    "text": "Data and Resources Used\nIt is also up to you to ensure that all relevant data are available via a valid URL for downloading and running. You may host your data anywhere you like, but please bear in mind that the markers will be based in the U.K. so some servers may be inaccessible. For very small data sets we’d recommend a GitHub repo, but for larger ones a Dropbox or OneDrive link would be more appropriate (you will need to check that the link you’ve created gives permission to anyone to download).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#for-largelong-workflows",
    "href": "assessments/code.html#for-largelong-workflows",
    "title": "Code",
    "section": "For Large/Long Workflows",
    "text": "For Large/Long Workflows\nIf your analysis has a particularly time-consuming stage (e.g. Named-Entity Recognition or Part-of-Speech tagging) then you can provide partially-processed data via a download in the QMD file: comment out the code up to the point where you have generated the ‘expensive’ data set but leave it in the markdown document. That way we can see how you generated the data without it being part of the reproducibility stage.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/content.html",
    "href": "assessments/content.html",
    "title": "Content",
    "section": "",
    "text": "The Content is worth 35% of your module mark. You will be writing for a non-technical audience: the incumbent Mayor has asked you to rapidly respond to a political scandal and a proposal for regulation by the opposition candidate.\nThe responses to the set questions may be written without substantially new modelling or coding through the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature).\nStudents may use data from more than one time period if they wish, but this is not required. You can see the available data sets on Orca.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#set-questions",
    "href": "assessments/content.html#set-questions",
    "title": "Content",
    "section": "Set Questions",
    "text": "Set Questions\nIn the midst of an election, a newspaper has broken the scandal of an advisor on social mobility to the Mayor having three homes, two of which are let on Airbnb (one of which is still council-owned housing!). The opposition has announced a plan to force all professional landlords to register their properties and face higher Council Tax rates saying that Airbnb is ‘out of control’ in the capital. The Mayor wants to understand the scale of the ‘problem’ and the likely impacts of the opposition’s proposal so that they can either adopt it (and show how responsive they are) or demonstrate how poorly thought-through the opposition’s proposal is (and show that they’re not ready to govern). They have come to you—their team of crack data crunchers and advisors—for guidance in the form of a briefing, and are looking for evidence and visualisations that they can use in their campaign comms.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#is-airbnb-out-of-control-in-london",
    "href": "assessments/content.html#is-airbnb-out-of-control-in-london",
    "title": "Content",
    "section": "1. Is Airbnb out of control in London?",
    "text": "1. Is Airbnb out of control in London?\n\n( points; Answer due Week 7 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#how-many-professional-landlords-are-there",
    "href": "assessments/content.html#how-many-professional-landlords-are-there",
    "title": "Content",
    "section": "2. How many professional landlords are there?",
    "text": "2. How many professional landlords are there?\n\n( points; Answer due Week 8 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#how-many-properties-will-be-impacted-by-the-oppositions-proposal",
    "href": "assessments/content.html#how-many-properties-will-be-impacted-by-the-oppositions-proposal",
    "title": "Content",
    "section": "3. How many properties will be impacted by the opposition’s proposal?",
    "text": "3. How many properties will be impacted by the opposition’s proposal?\n\n( points; Answer due Week 9 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#what-are-the-pros-and-cons-to-the-oppositions-proposal",
    "href": "assessments/content.html#what-are-the-pros-and-cons-to-the-oppositions-proposal",
    "title": "Content",
    "section": "4. What are the pros and cons to the opposition’s proposal?",
    "text": "4. What are the pros and cons to the opposition’s proposal?\n\n( points; Answer due Week 7 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#can-we-somehow-spin-this-scandal-into-a-positive-story-about-social-mobility",
    "href": "assessments/content.html#can-we-somehow-spin-this-scandal-into-a-positive-story-about-social-mobility",
    "title": "Content",
    "section": "5. Can we somehow spin this scandal into a positive story about social mobility?",
    "text": "5. Can we somehow spin this scandal into a positive story about social mobility?\n\n( points; Answer due Week 7 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#style",
    "href": "assessments/content.html#style",
    "title": "Content",
    "section": "Style",
    "text": "Style\nThis is not an essay, and students who submit answers using a traditional essay style will see their overall mark impacted as a result. You must preserve the question/response format, and your responses should be readily-grasped by an intelligent, but non-technical audience. This doesn’t mean that you don’t need citations, but you should not employ an academic writing style. See the models provided below for insights into how to write for a less technical audience.\nThere will also be opportunities to discuss the submission during the second half of term.\n\n\n\n\n\n\nWarningWrite for Your Audience\n\n\n\nWhat makes writing a good briefing hard—and not just about writing good code—is finding the right balance of technical detail and high-level explanation: you can’t just say ‘here are the five types of accommodation we found…’, but you also can’t say ‘we tested clustering solutions in the range 3–50 and found the optimal result at k=12…’ You should have a look at the examples.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#word-counts-figures",
    "href": "assessments/content.html#word-counts-figures",
    "title": "Content",
    "section": "Word Counts & Figures",
    "text": "Word Counts & Figures\n\nWord Counts\nEach figure or table counts for 150 words, and so students should give careful consideration to the trade-offs involved: more figures may serve to illustrate your points but leave you with much less space to synthesise and present and argument.\nThe overall word limit for this assessment is 2,500 words.\n\n\nFigures & Tables\nUnless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used must be generated by Python code cells included in the markdown file. You may not modify or create figures in another application since this undermines the reproducibility of the analysis.\n\n\nA/B Figures\nA figure with A/B/C elements will count as one figure, but only where the parts parts are conceptually related (e.g. before/after; non-spatial/spatial distribution; type 1 and type 2; etc.). The output from PySAL’s LISA analysis library, for instance, is pre-formatted as 3 figures. Seaborn’s jointplot will only be considered to be one plot even though it is technically three because the distribution plots in the margin are related to the scatter plot that is the focus of the plot.\nIn principle, a briefing with 16 figures would have no space for any text or interpretation; this choice is deliberate because its purpose is to focus your attention on which charts and tables best-communicate your findings. In practice, using A/B/C figure layouts then you are looking at up to 48 separate figures before hitting the limit, though you would at this point be producing an infographic and not a briefing.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#assumptions",
    "href": "assessments/content.html#assumptions",
    "title": "Content",
    "section": "Assumptions",
    "text": "Assumptions\nIt is highly likely that you will need to make substantial use of assumptions in developing your briefing for the Mayor. These should be plausible and, where possible, documented. For example (and using randomly selected references):\n\nBased on Travers, Sims, and Bosetti (2016), we assume that the average visitor to London walks at a rate of 3m/s (180m/minute). We further assume that a 10-minute walk is reasonable, giving us a limit of 1,800m…\n\nOr:\n\nWe assume that tourists spend approximately £X/day in their local area (see, e.g., Wachsmuth and Weisler 2018), implying that each property generates a maximum of £Y/year in local spending (assuming continuous occupation). In practice, we adopt the approach of XXX (2019) to estimate occupancy from reviews to generate a more realistic impact of…\n\nSo you can see that neither of these requires more than a couple of citations to allow you to estimate some reasonable threshold for a metric of interest. This will be a lot simpler than trying to look up global tourist spending information or TfL travel stats. Yes, it’s a rough-and-ready estimate, but it also doesn’t pull the number out of thin air.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#referencing",
    "href": "assessments/content.html#referencing",
    "title": "Content",
    "section": "Referencing",
    "text": "Referencing\nYou will need to make use of BibTeX and Markdown referencing in Quarto. ‘Hard-coded’ references will not be considered.\n\n\n\n\n\n\nTipReferencing in Quarto\n\n\n\nAlthough you can create BibTeX entries by hand, you will probaly want to make use of BibDesk (Mac) or JabRef (Mac/Windows). Zotero shuould also work to edit the BibTeX file.\nIn Google Scholar, if you want to add a reference to your BibTeX file there’s an option in the Cite functionality to copy a BibTeX entry to the clipboard and then pasted this into BibDesk or JabRef.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#models",
    "href": "assessments/content.html#models",
    "title": "Content",
    "section": "Models",
    "text": "Models\nAlthough the following examples are all much longer than permitted under the assessment format, they are exemplary in their communication of the data and key findings in a manner that is clear, straightforward, and well-illustrated using maps, charts, and tables. So while they wouldn’t work for a busy Mayor per se, they can still help you to think about how best to present information for non-expert audiences:\n\nTravers et al. (2016), Housing and Inequality in London, Centre for London; URL.\nBivens, J. (2019), The economic costs and benefits of Airbnb, Economic Policy Institute; URL.\nWachsmuth et al. (2018), The High Cost of Short-Term Rentals in New York City, Urban Politics and Governance research group, McGill University; URL.\n\n\n\n\n\n\n\nTipLonger Questions\n\n\n\nNotice how these ‘models’ differ from a traditional essay format. So instead of Introduction, Literature, etc. you will see the evidence is developed in parallel with the background material. This format provides for more flexibility in style and presentation, though you will note that they all refer to a mix of academic and grey literature as well!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#partial-bibliography",
    "href": "assessments/content.html#partial-bibliography",
    "title": "Content",
    "section": "Partial Bibliography",
    "text": "Partial Bibliography\nYou will also want to expand on the partial bibliography shown in the Templates section. This is by means complete and you will likely find other relevant work ‘out there’, but this gives you a good starting point.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/group.html",
    "href": "assessments/group.html",
    "title": "Rationale",
    "section": "",
    "text": "This is group work due Tuesday, 16 December 2025 @ 10:00 that you will undertake in a small group of no more than four students. The project is intended to resemble real-world data science ways of working: you will be part of a small team thrown together at short notice, you will need to figure out how to work effectively together, and you will need to jointly produce an output in which you all have confidence.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Rationale"
    ]
  },
  {
    "objectID": "assessments/group.html#applying-your-knowledge",
    "href": "assessments/group.html#applying-your-knowledge",
    "title": "Rationale",
    "section": "Applying Your Knowledge",
    "text": "Applying Your Knowledge\nThe focus of this assessment is therefore the student’s ability to make use of concepts and methods covered in class as part of an analytical process to support decision-making by a non-expert. It is not necessary that you employ every technique covered in class. It is necessary that you justify your choices, results, and conclusions usingacademic and ‘grey’ literature as needed. It is perfectly possible to obtain a distinction-level grade without the use of any advanced analytical techniques (e.g. clustering, NLP, or Random Forests); however, it is unlikely that you would be able to complete this assessment to a high standard without some graphs and some maps chosen for their ability to advance your argument.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Rationale"
    ]
  },
  {
    "objectID": "assessments/group.html#new-code",
    "href": "assessments/group.html#new-code",
    "title": "Rationale",
    "section": "New Code?",
    "text": "New Code?\nSo the assessment may be completed by drawing on the code written in the practicals and the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature) for examples of how much can be achieved in this way. However, it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was covered in class by connecting concepts and demonstrating a deeper understanding of how to apply what has been learned across FSDS, QM, and GIS to the problem at hand.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Rationale"
    ]
  },
  {
    "objectID": "assessments/group.html#two-part-submission",
    "href": "assessments/group.html#two-part-submission",
    "title": "Rationale",
    "section": "Two-Part Submission",
    "text": "Two-Part Submission\nThe submission will have two parts and both are evaluated as part of your overall grade:\n\nA runnable QMD (Quarto Markdown Document) file that addresses the set questions appropriately. The QMD will be evaluated for its reproducibility (25% of your module grade) by us rendering your QMD file on our computer. So we are looking at whether outputs created by the group run fully and without error on a different computer, and whether they show evidence of thought in relation to the quality and clarity of the coding and outputs. The simplest way to ceonceptualise this is: if someone vaguely intelligent were provided with only your group’s QMD file, would they be able to produce the PDF that you submitted in Part 2?\nA rendered PDF file that is the output of your QMD file. The PDF file allows us to focus on your content (35% of your module grade) of what you submitted, regardless of whether or not there are issues with its reproducibility. So we are looking at how the group engages with the questions through a mix of literature, critical and creative thinking, and data analysis. You are being asked to operate in a ‘low information environment’, which implies the need to make choices, to justify these, and to present your findings and conclusions in a way that instill confidence in the reader. You will have considered the issues, anchored them in an understanding (or at least awareness) of the relevant laws/regulation and context, and managed to produce a succinct set of results for a non-technical audience.\n\nPlease see Resources for the templates and information about how to create and render the QMD file.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Rationale"
    ]
  },
  {
    "objectID": "assessments/peer.html",
    "href": "assessments/peer.html",
    "title": "Group Self-Evaluation",
    "section": "",
    "text": "This self- and peer-mark assessment (10%) is also a 2-parter, with the self-assessment at due Tuesday, 16 December 2025 @ 10:00 and the peer-mark component due @ 12:00. This assessment asks you to reflect on the process of ‘doing data science’ as part of a team, since this format is typical of real-world projects. Many companies (e.g. Apple, Google, etc.) employ an Agile project format in which teams undertake a ‘Retrospective’ at the end of a project in order to identify ways to improve how they work in the future. We are not asking you to do this as a group, but we hope that this will help you to develop as a budding analyst or data scientist.",
    "crumbs": [
      "Elements",
      "Group Self-Evaluation"
    ]
  },
  {
    "objectID": "assessments/peer.html#format",
    "href": "assessments/peer.html#format",
    "title": "Group Self-Evaluation",
    "section": "Format",
    "text": "Format\nYou will be asked to score both yourself and the rest of the group on their contribution to the project. Keep in mind that someone who wrote the code may be more ‘visible’ than the person who ensured that the code was actually answering the question, and the fact that you’re much more likely to be aware of your own contributions than you are of the contributions of others.\nThe guidance for this is:\n\nThinking about the many ways — visible and less visible — that a student can contribute to the success of a group project, please rank the contributions made by your team-mates towards the group project as well as provide any comments you feel are pertinent. Write your comments in a professional and constructive way, as this will be seen by your peers afterwards, though commentators will remain anonymous. You also need to assess yourself!\n\nJustify this score with reference to the guidance provided in class (and linked to here, here, and here).",
    "crumbs": [
      "Elements",
      "Group Self-Evaluation"
    ]
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html",
    "href": "extra/Live-02-Foundations_1.html",
    "title": "Context",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 2: Foundations (Part 1)&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with the 'Basics'&lt;/h2&gt;\nWe’re going to start out with the live coding and practical sessions potentially looking quite different, but that’s because I don’t think you all need to spend 60-90 minutes reviewing lists and variables. Instead, the angle in Weeks 1-3 is about getting you organised, starting to explore the data in a way that’s more immediately flexible than using code, and familiarising you with the data we’ll be using across the rest of term.\nIn the live coding sessions we will be using Crime data from 2022 for Chicago. In the practical sessions we’ll be using Inside Airbnb data for London."
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-1.-creating-and-cloning-a-repo",
    "href": "extra/Live-02-Foundations_1.html#task-1.-creating-and-cloning-a-repo",
    "title": "Context",
    "section": "Task 1. Creating and Cloning a Repo",
    "text": "Task 1. Creating and Cloning a Repo\n\nSuggest fsds as private repo\nClone to local machine\nOpen and look around."
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-2.-adding-files-commit",
    "href": "extra/Live-02-Foundations_1.html#task-2.-adding-files-commit",
    "title": "Context",
    "section": "Task 2. Adding Files & Commit",
    "text": "Task 2. Adding Files & Commit\n\ngit status\nAdd a notebook file downloaded from i2p\n\ngit add\ngit commit\n\nNow let’s look on GitHub.\n\nNow let’s update the README\n\nTry to make a change locally and push it\n\nMake a change to README\nTry to push\ngit diff\n\nMerge and resolve.\nPush again"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-3.-more-on-docker",
    "href": "extra/Live-02-Foundations_1.html#task-3.-more-on-docker",
    "title": "Context",
    "section": "Task 3. More on Docker",
    "text": "Task 3. More on Docker\nCan expand this after asking students if they have questions about how Docker works. It is available directly on the jreades.github.io/sds_env/docker/#understanding-docker page\n\nUnderstanding Docker Section\n\nStarting up\nLogging in\nTurning on extensions\nLooking at what the extensions do\n\nEspecially the toc view\nVariable inspector\nGitPlus (untested)\n\nCreating a notebook\nUsing the Terminal"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-4.-investigating-a-file",
    "href": "extra/Live-02-Foundations_1.html#task-4.-investigating-a-file",
    "title": "Context",
    "section": "Task 4. Investigating a File",
    "text": "Task 4. Investigating a File\n\nDo all of this through JupyterLab/Docker\n\n\nTask 4.1. Download the File\n\nThe full file\nThe sample file\n\nIdeally download the files to ~/Downloads/\n\n\nTask 4.2. Check Names of Files\nls ~/Downloads/*.gz\n\nExplain the ~.\nExplain the *.\nExplain the .gz.\n\n\n\nTask 4.3. How big are the files?\ndu -sh *.gz\n\nAsk them to explain the *.gz.\nWhat is du (Anyone want to guess? You can Google the answer right now!)\nHow do we figure out what du does?\n\nMany command line tools have a --help option, so you can run du --help to see the options.\nA full Unix/Linux/macOS installation also has a ‘manual’ option: man du…\n\nOk, how the hell do I navigate this?\n\narrow keys to move up and down one line at a time\nspace bar to move down one page at a time\nq to quit\n/ and then a term to search\n\n\n\n\n\n\nTask 4.4. Moving the file around\nMove the file to the cloned repo.\nmv ~/Downloads/&lt;fn&gt;.gz ~/Documents/&lt;repo&gt;/\ncd ~/Documents/&lt;repo&gt;/\nmkdir data\nmv &lt;fn&gt;.gz data/\n\nHow would we make sense of mkdir?\nWhat does -p option do?\nHow did you find it?\n\n\n\nTask 4.5. How many rows are in the data set?\n\nShow them what open . does on a Mac.\nDecompress a copy of the file (just double-click as compressed file will remain)\nOpen the copy in Excel.\nWhat are we looking at?\n\nDiscuss\nNotice any issues with the data?\n\nDate is actually datetime, and some is in 24-hour, others in 12-hour format\nWhat data types can we see? Think about what you’re learning in QM? (We’ve got floats [both ratio–meaningful zero–and interval], integers [discrete ids that are not case numbers], booleans, categorical…)\n\n\n\nNow back to the file… - How do we count things in a file? - wc - How do we count lines in a file? - Look it up! man wc\nwc -l *-Crime.csv\n\n\nTask 4.6. What are the names of the columns?\nObviously we can look in Excel, but what commands could you use to look at the first line of the file?\n\nWe’ve seen at least two ways to do this in my talk, but can you remember them?\n\nless 2022-Crime-data.csv\nand\nhead -n 1 2022-Crime-data.csv\nWith less, talk through the commands for stepping forwards, backwards, one page down, and quitting. Notice that these are the same as for the manpage help we used above with du.\n\nHow would you get the last 5 lines?\n\ntail -n 5 2022-Crime-data.csv\n\nHow would you get the 50th to 60th lines?\n\nhead -n 60 2022-Crime-data.csv | tail -n 10\n\n\nTask 4.7. How many ‘Narcotics’ charges are there?\nLead them through the fact that case matters:\n\nWhy doesn’t this work?\n\ngrep \"Narcotics\" 2022-Crime-data.csv\n\nHow could we get matches for Narcotics?\n\nOption #1: use “NARCOTICS”\nOption #2: use -i or --ignore-case\nWe get the latter by looking at man grep\n\n\ngrep -i \"Narcotics\" 2022-Crime-data.csv\nThere should be 7 rows.\n\nCan you figure out how many Narcotics charges there are in the file?\n\nYou’ve seen how to use the ‘pipe’ to pass output from head to tail.\nYou’ve seen how to find the rows that match using grep\nYou’ve seen how to count the number of lines\nYou need to put these pieces all together.\n\n\ngrep -i \"Narcotics\" 2022-Crime-data.csv | wc -l\nThere should be 7 rows.\n\n\nTask 4.8. How many charges mention values over $500?\nDemonstrate value of escape commands. Let’s have a look at the output from:\ngrep \"$500\" 2022-Crime-data.csv | wc -l\nNow let’s try:\ngrep '$500' 2022-Crime-data.csv | wc -l\nAnd finally:\ngrep \"\\$500\" 2022-Crime-data.csv | wc -l\n\n\nTask 4.9. More Complex Command Chaining\nOther fun things we can do…\n\nHow do we know what types of primary crimes there are in the data?\n\nawk -F',' '{print $4}' 2022-Crime-data.csv | sort | uniq\n\nOr how about what types of descriptions there are of Narcotics charges?\n\ngrep -i 'Narcotics' 2022-Crime-data.csv | awk -F',' '{print $5}' | sort | uniq\n\nOr how may there are of Possession?\n\ngrep -i 'Narcotics' 2022-Crime-data.csv | grep 'POSS' | wc -l"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#check-in",
    "href": "extra/Live-03-Foundations_2_Intro.html#check-in",
    "title": "Foundations of Spatial Data Science",
    "section": "1. Check In",
    "text": "1. Check In\n\n\n\nSEATS QR Code:"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#organise-your-group",
    "href": "extra/Live-03-Foundations_2_Intro.html#organise-your-group",
    "title": "Foundations of Spatial Data Science",
    "section": "2. Organise (Your Group)",
    "text": "2. Organise (Your Group)\n\n\n\nGroup Sign-up\n\n\nforms.office.com/e/2ij0sWHnpR"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#comment-on-progress",
    "href": "extra/Live-03-Foundations_2_Intro.html#comment-on-progress",
    "title": "Foundations of Spatial Data Science",
    "section": "2. Comment (On Progress)",
    "text": "2. Comment (On Progress)\n\n\n\nwww.menti.com/al8qfmocpxkg"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#on-with-the-show",
    "href": "extra/Live-03-Foundations_2_Intro.html#on-with-the-show",
    "title": "Foundations of Spatial Data Science",
    "section": "3. On with the show…",
    "text": "3. On with the show…\n\nDon’t forget to start Lecture Capture!"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "We all need help from time to time, and while we will always do our best to support you because we know that this module is hard for students who are new to programming, the best way to ‘get help’ will also always be taking steps to ‘help yourself’ first."
  },
  {
    "objectID": "help.html#how-to-help-yourself",
    "href": "help.html#how-to-help-yourself",
    "title": "Getting Help",
    "section": "How to Help Yourself",
    "text": "How to Help Yourself\nHere are at least six things that you can do to ‘help yourself’:\n\nUse the dedicated #fsds channel on Slack–this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours.\nDo the readings–regardless of whether we ask you questions in class about them (or not), the readings are designed to support the module’s learning outcomes, so if you are struggling with a concept or an idea then please look to the week’s readings! You should also review the full bibliography while developing your thinking for the final project.\nUse Google–this is one course where saying “I googled it…” will be taken as a good sign! Probalby the biggest difference between a good programmer and a new programmer is that the good one knows which terms to type into Google to get the answer that they need right away.\nUse Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the right answer right away, but whether you’re a beginner or an expert Stack Overflow is your friend.\nMake use of Drop-in Hours (see also below)–there is no extra credit for struggling in silence, and we can’t help you if we don’t know that you’re lost! That doesn’t mean that we can simply ‘give’ you the answers to challenging questions, but we will do everything that we can to support your learning. Many of the same advice applies here as on our other Getting Help page (with the Soft Skills).\nPractice like it’s a language–set yourself little problems or tasks and see if you can apply what you’ve learned in class to a problem in a different class, or a friend’s problem, or just something you’re curious about! In the same way that practicing your Chinese or French with native speakers will help you to learn those languages, so will practicing your Python.\nSign up for online classes–realistically, you will have a lot on your plate, but if you want or need more practice with Python and are strugging to come up with your own problems to work through then there is a wealth of options out there. You might also find that a different explanation or challenge resonates and gives you new insight into how to code.\n\nRemember: when you are learning to code there is no such thing as a stupid question. Sometimes students have lazy questions when they are frustrated and just want to know ‘the answer’, but anyone finding themselves stuck on a particular problem has a 100% chance that someone else in the class has the same problem as well but hasn’t quite worked up the courage to ask. So please: ask."
  },
  {
    "objectID": "help.html#you-can-book-me",
    "href": "help.html#you-can-book-me",
    "title": "Getting Help",
    "section": "You Can Book Me",
    "text": "You Can Book Me"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#who-is-this-guy",
    "href": "lectures/1.1-Getting_Started.html#who-is-this-guy",
    "title": "Getting Started",
    "section": "Who is this Guy?",
    "text": "Who is this Guy?\n\nA few fun facts about your module lead…\n\n\nJon:\n\nHas a first degree in literature.\nHas never studied computer science and has only ever taken night classes in programming.\nWorked for a dot.com start-up in New York before transferring to London.\nHas taught undergraduate geography at King’s, from which this module evolved, and has taught this module for five years.\nWhen not working on teaching, being head of department, and squeezing in some reseach, Jon can be found looking after his two young daughters, mountain biking with Andy or hiking in Scotland."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#useful-information",
    "href": "lectures/1.1-Getting_Started.html#useful-information",
    "title": "Getting Started",
    "section": "Useful Information",
    "text": "Useful Information\nFoundations is distributed across two web sites:\n\nThe micro-site: jreades.github.io/fsds/ – lectures, practicals, readings, and information about the assessments. This will remain accessible to you after graduation.\nMoodle: moodle…?id=54436 – recorded sessions, booking drop-in hours, group messaging, ‘answer sheets’, and submission of assessments, as well as other formal components. This is tied to your enrolment at UCL.\n\nAnd don’t forget about this quick introduction to Python: jreades.github.io/code-camp/!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#where-does-fsds-fit",
    "href": "lectures/1.1-Getting_Started.html#where-does-fsds-fit",
    "title": "Getting Started",
    "section": "Where Does FSDS Fit?",
    "text": "Where Does FSDS Fit?\n\n\nGeographic Information Systems (GIS)\n\nFoundations of spatial analysis\nWorking with geo-data\n\nQuantitative Methods (QM)\n\nFoundations of statistical analysis\nWorking with data\n\n\nFoundations of Spatial Data Science (FSDS)\n\nFoundations of applied spatial and statistical analysis\nIntegrating and applying concepts from GIS & QM to a problem\nDeveloping programming and practical analysis skills\nSeeing the ‘data science’ pipeline from end to end"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#what-are-we-trying-to-do",
    "href": "lectures/1.1-Getting_Started.html#what-are-we-trying-to-do",
    "title": "Getting Started",
    "section": "What Are We Trying to Do?",
    "text": "What Are We Trying to Do?\nThis class hopes to achieve four things:\n\nTo teach you the basics of how to code.\nTo teach you the basics of how to think through code.\nTo teach you how to engage with data critically.\nTo help you integrate concepts taught across Term 1 and prepare you to apply them in Term 2.\n\nThese skills are intended to be transferrable to post-degree employment or research."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#the-challenges",
    "href": "lectures/1.1-Getting_Started.html#the-challenges",
    "title": "Getting Started",
    "section": "The Challenges",
    "text": "The Challenges\n\nTo learn a bit of programming and to connect it to the bigger picture.\nTo be ok with learning to walk before you run.\nTo learn not to rely (too much) on ChatGPT.\nTo communicate your thoughts through code and text.\n\n\nThis is a new one for us too. We don’t want to pretend that ChatGPT doesn’t exist. It’s how you will do your work. Unquestionably. But it is also a trap. This year we’re hoping to show you that."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#the-rewards",
    "href": "lectures/1.1-Getting_Started.html#the-rewards",
    "title": "Getting Started",
    "section": "The Rewards",
    "text": "The Rewards\n\nSkills that are highly transferrable and highly sought-after professionally.\nProblem-solving and practical skills that are valued by the private and public sectors.\nA whole new way of seeing the world and interacting with it.\nLots of support along the way… if you remember to ask for it!\n\nSee this thread on moving from academia to data science."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#narrative-arc",
    "href": "lectures/1.1-Getting_Started.html#narrative-arc",
    "title": "Getting Started",
    "section": "Narrative ‘Arc’",
    "text": "Narrative ‘Arc’\n\nPart 1: Foundations: Weeks 1–5 to cover the ‘basics’ and set out a data science workflow.\nPart 2: Data: Weeks 6–10 look at the same data through three lenses.\nPart 3: Bonus: Weeks 11–12 additional content if you want it.\n\n\n1-5 means tackling the ‘basics’ of Python, foundational concepts in programming, and practicing with the ‘tools of the trade’ for programmers.\n6-10 means different types of data (numeric, spatial and textual) with a view to understanding how such data can be cleaned, processed, and aggregated for use in a subsequent analysis. It is commonly held that 80% of ‘data science’ involves data cleaning, so this is a critical phase in developing an understanding of data. We also look at selection and visualisation.\n11-12 (Bonus) is about classification, dimensionality reduction, and clustering. These concepts will have been encountered in other modules, so the intention is that the student will see how these fit into the ‘bigger picture’ of applied spatial analysis."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#week-to-week",
    "href": "lectures/1.1-Getting_Started.html#week-to-week",
    "title": "Getting Started",
    "section": "Week-to-Week",
    "text": "Week-to-Week\nThe specific activities for each week can be found on the microsite. These include:\n\nPreparation: readings, pre-recorded lectures, identifying areas for feedback.\nIn-Person: discussing readings and lectures; discussing issues arising from the previous week’s practical, responding to assessment requirements, and some ‘live coding’.\nPracticals: working through a weekly ‘programming notebook’ with support from your PGTAs.\n\n\n\n\n\n\n\nBring Your Computer\n\n\nPlease remember to bring your own computer to the practical sessions! The tools we use are not installed on cluster systems."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#assessments",
    "href": "lectures/1.1-Getting_Started.html#assessments",
    "title": "Getting Started",
    "section": "Assessments",
    "text": "Assessments\n\nTimed, Open Book Exam (30% of module grade): A quiz requiring a mix of numeric and textual answers to short data analysis questions for which you must write the code.\nReproducible Analysis (25% of module grade): A small-group submission of a ‘tangled’ document that demonstrates reproducibility through an exploratory analysis of the assigned data set.\nStructured Report (35% of module grade): A structured, small-group submission which responds to set questions and develops an exploratory analysis of the assigned data set.\nSelf- and Peer-Evaluation (10% of module grade): A short individual reflection combined with numerical scoring by peers on their contribution to the group’s outcomes.\n\n\nAssessment logic:\n\nTeach and test the most challenging aspects of data science ‘work’ without mastery of Python.\nDiscover transferrability of skills and tools across projects, disciplines, and industries.\nBuild on content from QM (e.g. setting quantitative research questions) and GIS (e.g. spatial statistics).\nDevelop experience with non-academic research formats and writing."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#formats-due-dates",
    "href": "lectures/1.1-Getting_Started.html#formats-due-dates",
    "title": "Getting Started",
    "section": "Formats & Due Dates",
    "text": "Formats & Due Dates\n\nTimed, Open Book Exam: is a Moodule quiz due Friday, 21 November 2025 (after Reading Week) and it will focus on content from the first five weeks of class.\nReproducible Analysis: is a Quarto document due Tuesday, 16 December 2025 and written in small groups to demonstrate reproducibility through an exploratory analysis of the assigned data set.\nStructured Report: is a PDF (output from Quarto) due Tuesday, 16 December 2025 written in small groups which responds to set questions and develops an exploratory analysis of the assigned data set.\nSelf- and Peer-Evaluation: is a Moodle ‘IPAC’ assessment due Tuesday, 16 December 2025 combining short individual reflection combined with numerical scoring by peers on their contribution to the group’s outcomes.\n\n\nWe will talk more about these over the course of the term."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#actual-feedback",
    "href": "lectures/1.1-Getting_Started.html#actual-feedback",
    "title": "Getting Started",
    "section": "Actual Feedback…",
    "text": "Actual Feedback…\n\nI was really struggling with the concepts of lists, dictionaries and iterations (I basically could not do any of Practical 3 without panicking) and I was telling  that it felt like Workshop 3 was all in a foreign language - I was so lost. \n But both yesterday and today, I have been over all the content, recordings and even code camp again and I’ve just had a penny drop moment, I could cry woohooo!!!!!! \nI really appreciate all the effort you’ve put into recording the concepts ahead of lectures and the way you’ve structured the module, although it is very fast-moving you have provided all the resources for us to do well."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#more-feedback",
    "href": "lectures/1.1-Getting_Started.html#more-feedback",
    "title": "Getting Started",
    "section": "More Feedback",
    "text": "More Feedback\n\nI just wanted to update you on my progress. Since flipping the content round following your advice, I have been feeling much much better. I followed what you were doing in the workshop and also have completed the practical in about half the time than I usually do. Thanks so much for responding and for your effort with this module."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#last-year-1",
    "href": "lectures/1.1-Getting_Started.html#last-year-1",
    "title": "Getting Started",
    "section": "Last Year 1",
    "text": "Last Year 1\n\nIn the first 3 weeks we were thrown a lot of information about different concepts (Git, Jupiter, Docker, Python) and it was very overwhelming. I think, the amount of information we were supposed to learn within the first month was too much.\n\n… and\n\nIn FSDS, I’ve tried my best to review and catch up but I don’t know why I still struggle a lot figuring the whole picture. Suddenly the script went from simple into complex really quick.\n\n… vs.\n\nThe technical content is very slow, and we don’t write a lot of code in the practicals, so I feel like I’m not learning and remembering it very well."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#last-year-2",
    "href": "lectures/1.1-Getting_Started.html#last-year-2",
    "title": "Getting Started",
    "section": "Last Year 2",
    "text": "Last Year 2\n\nThe discussion in the lecture is just a very low-level recap of the readings… I think it might be better for Jon to give mini lectures (e.g. on ethics, data quality etc.) which tie the points together.\n\nvs. \n\nI think it’s not necessary to spend too much time discussing reading materials at class. Instead, we can ask some questions on slack after class."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#last-year-3",
    "href": "lectures/1.1-Getting_Started.html#last-year-3",
    "title": "Getting Started",
    "section": "Last Year 3",
    "text": "Last Year 3\n\nThese courses need clearer guidance to help students set learning goals and show them what they need to do in each class. For example the tasks and assignments for each class are clearly stated before the reading material is presented.\n\n\nI’m quite confused about The content of practical 1-5. I know python has so many package’s and data types. Maybe combining the real data analysis with detailed data learning (objects, function) is better to understand.\n\n\nThe slides can be a little clearer and write more words."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#lots-of-help-out-there",
    "href": "lectures/1.1-Getting_Started.html#lots-of-help-out-there",
    "title": "Getting Started",
    "section": "Lots of Help ‘Out There’",
    "text": "Lots of Help ‘Out There’\nWhen you need an answer right now:\n\nGoogle\nStack Overflow\nSlack1\nChatGPT / Copilot\n\nWhen you want to learn more:\n\nMedium\nPocket\n\n\nGoogle will become more useful as you learn more and this is definitely one class in which “I Googled it” is a good answer.\nAs of early September 2020, Stack Overflow contains over 1.5 million Python questions alone! Chances are someone else has had your question before.\n\nHere’s the signup link. You must use your UCL address."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#when-to-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#when-to-ask-for-help",
    "title": "Getting Started",
    "section": "When to Ask for Help",
    "text": "When to Ask for Help\n\nWhen you get warning messages from your computer’s Operating System.\nWhen you cannot get the coding environment to run at all.\nWhen even simple commands return line after line of error code.\nWhen you have no clue what is going on or why.\nWhen you have been wrestling with a coding question for more than 20 minutes (but see: How to Ask for Help!)"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#before-you-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#before-you-ask-for-help",
    "title": "Getting Started",
    "section": "Before You Ask for Help",
    "text": "Before You Ask for Help\nFrom the Computer Science Wiki:\n\nDraw a picture of the problem\nExplain the problem out loud to a friend, teddy bear or whatever (really!)\nForget about a computer; how would you solve this with a pencil and paper?\n\nTo which we would add:\n\nUse print(variable) statements liberally in your code!\nicecream is nice, but a little overwhelming.\n\n\nWe’ll cover this last bit as we get more used to coding!\nIn order to learn you do need to struggle, but only up to a point! So we don’t think that giving you the answer to a coding question as soon as you get stuck is a good way for you to learn. At the same time, I remain sad to this day that one of the most insightful students I’ve ever taught in a lecture context dropped out of our module because they were having trouble with their computer and thought it was their fault nothing was working right. By we had realised what was going on it was too late: they were so far behind that they didn’t feel able to catch up. We’d rather that you asked and we said “Close, but try it again” than you didn’t ask and checked out thinking that you couldn’t ‘do’ programming."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#how-to-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#how-to-ask-for-help",
    "title": "Getting Started",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nIn addition to what we have provided, we like the “How to ask programming questions” page provided by ProPublica:\n\nDo some research first.\nBe specific.\nRepeat.\nDocument and share.\n\nIf you find yourself wanting to ask a question on Stack Exchange then they also have a guide, and there are plenty of checklists.\n\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#where-to-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#where-to-ask-for-help",
    "title": "Getting Started",
    "section": "Where to Ask for Help",
    "text": "Where to Ask for Help\nThere is no shame in asking for help. None. We are here to support your learning and we have chosen a range of tools to support that:\n\nSlack: use the #fsds channel for help with coding, practical, and related course questions.\nDrop-in Hours: use Booking Form\nOut-of-Hours: use email to raise personal circumstances and related issues for focussed support.\nEmergencies: contact Bartlett.Postgraduate (CASA)1 for support as-needed and/or to preserve privacy.\n\n\nWe think that this is the best way to get help when you need it. Slack enables us to support you as a community of learners across computer / tablet / phone.\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time, as well as how to read and how to think.\n\nbartlett.pg-casa@ucl.ac.uk"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#learn-from-your-mistakes",
    "href": "lectures/1.1-Getting_Started.html#learn-from-your-mistakes",
    "title": "Getting Started",
    "section": "Learn from Your Mistakes",
    "text": "Learn from Your Mistakes"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#one-more-thing",
    "href": "lectures/1.1-Getting_Started.html#one-more-thing",
    "title": "Getting Started",
    "section": "One More Thing…",
    "text": "One More Thing…\nYou will get things wrong. We will get things wrong. We have tried to offer suggestions for how to read, how to think about what you’re learning, and how to manage distractions. But if you need help we are here for you!\nThe Academic Communication Centre also offers lots of support, including drop-in sessions, tutorials, and writing workshops.\nWe will assume that you are trying your best. Please assume the same about us!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#and-finally",
    "href": "lectures/1.1-Getting_Started.html#and-finally",
    "title": "Getting Started",
    "section": "And Finally…",
    "text": "And Finally…\n\n\n\n\n\n\nAuto-Updates\n\n\nDo not allow your computer to auto-update during term. Inevitably, major upgrades will break developer tools. Do this by choice only when you have time. MacOS Tahoe was released 15 September; if you haven’t already updated, please do not install it!\n\n\n\n\nMany students allowed their computer to update to Big Sur or Sonoma since new releases always come out in September and it broke their entire computing environment. Some did this shortly before a submission was due. Do not do this!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#additional-resources",
    "href": "lectures/1.1-Getting_Started.html#additional-resources",
    "title": "Getting Started",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\n\nACC QR Code\n\n\n\n\nSee the GIS&T Body of Knowledge (BoK) for quick overviews of concepts, techniques, and tools.\nInvestigate UCL’s ACC and sign up to access resources and 1:1s through Moodle using the QR code."
  },
  {
    "objectID": "lectures/1.3-Principles.html#principles",
    "href": "lectures/1.3-Principles.html#principles",
    "title": "Principles into Practices",
    "section": "Principles",
    "text": "Principles\nFoundations was designed around these principles:\n\nSoftware and data should be free (as far as practicable).\nSoftware and data should be open (as far as practicable).\nSoftware should run on all platforms (you get the idea).\nSoftware and data should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.3-Principles.html#how-is-open-source-helpful",
    "href": "lectures/1.3-Principles.html#how-is-open-source-helpful",
    "title": "Principles into Practices",
    "section": "How is Open Source Helpful?",
    "text": "How is Open Source Helpful?\n\nIf you can think of it, someone has probably built it.\nGiven enough eyeballs all bugs are shallow.1\n‘Free as in speech’ (always).\n‘Free as in beer’ (often).2\n\n\nAll of the tools used this week are open source and free.\n\n\nBy this I mean that, if you use Microsoft to do everything then you are dependent on their tools, even when they aren’t appropriate. There are many small, open source applications that do really useful but narrow things.\nWhen you have many people involved and they are working in an open environment then issues are often spotted spotted and solved more quickly. While this claim is open to some debate, there’s certainly no evidence that closed source code is any better than open source.\nA more interesting claim has to do with the potential for interested people to get involved in making code better – for instance, for some languages there may be very few translators available, leading private companies to simply ignore localisation; whereas for open source you can contribute the translation yourself! Or if you really think something isn’t working the way it should then you can offer up a solution that will make it better and, if the people running the project don’t agree, you can still do it and launch your own, competing project.\nFinally, many (though by no means all) open source projects are also free as free food, beer, or rides. If you have been taught to use ESRI’s ArcGIS or Microsoft Excel this seems crazy: how can you have free competition to those? Well, why not give QGIS and LibreOffice a try!\n\n\n\nAlso known as Linus’ law via Eric Raymond’s The Cathedral and the Bazaar.Or chai, thalis, etc."
  },
  {
    "objectID": "lectures/1.3-Principles.html#fair-play",
    "href": "lectures/1.3-Principles.html#fair-play",
    "title": "Principles into Practices",
    "section": "FAIR Play",
    "text": "FAIR Play\nWilkinson et al. (2016) set out the following principles:\n\nFindable: data and metadata should be easy to find for computers and humans.\nAccessible: it should be clear how the data found can be accessed.\nInteroperable: data should work for range of analyses, storage, and processing needs.1\nReusable: metadata and data should be well-described so they can be used/combined easily.\n\n\nThere are strict and loose versions of these principles. Many governments truggle with the ‘R’ part because of licensing restrictions.\n\nShape files fail this test."
  },
  {
    "objectID": "lectures/1.3-Principles.html#how-is-open-data-helpful",
    "href": "lectures/1.3-Principles.html#how-is-open-data-helpful",
    "title": "Principles into Practices",
    "section": "How is Open Data Helpful?",
    "text": "How is Open Data Helpful?\nAccording to Open Data Institute (n.d.) (amongst others):\n\nPromotes transparency and accountability in government and services.\nPromotes efficiency and service delivery.\nPromotes innovation and economic growth.\nEmpowers citizens.\nReduces costs.\n\n\nOpen data is valuable!\n\n\nThis can be scary for many, especially those in government; however, making data open can help to debunk conspiracies and demonstrate that government is not corrupt! It can also help government to spot where there are problems.\nWe get increased efficiency and better service delivery if governments can look across departments or states to compare performance or enable others to turn up opportunities for improvement.\nCompanies can innovate off of government data: Land Registry and EPC data in the UK, for instance, give companies certainty that they won’t have to pay for this data and can build commercial products using it. The biggest gain here is in making government mapping data available.\nThere’s a strong assumption here that citizens are data literate, which I think doesn’t always hold; however, crime mapping, bus route mapping, and so on can help citizens to advocate for resources and support.\nGovernments can also save by not having to constantly respond to FOI requests or rebut other access requests from citizens and companies."
  },
  {
    "objectID": "lectures/1.3-Principles.html#how-is-open-code-helpful",
    "href": "lectures/1.3-Principles.html#how-is-open-code-helpful",
    "title": "Principles into Practices",
    "section": "How is Open Code Helpful?",
    "text": "How is Open Code Helpful?\n\nAllow others to build on your work (reuse, collaboration).\nAllow others to learn from your work (speed, bug detection).\nA way to attract contributors to your project (visibility, collaboration).\nA mechanism for perpetuating a potlatch ecosystem (community building, recognition, visibility).1\n\n\nThere are many ways to share code, including contributing questions and answers to public fora like Stack Overflow,\n\nWhere ‘knowledge is power’, not ‘power is right’."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#start-with-chart-part-2",
    "href": "lectures/10.1-Visualising_Data.html#start-with-chart-part-2",
    "title": "Visualising Data",
    "section": "Start with Chart (Part 2)",
    "text": "Start with Chart (Part 2)\nBuilding on Week 7, here are some deeper links between models and visualistions:\n\nWhat are we trying to model?\nWhat are we trying to penalise?\nWhat isn’t fitting into the model?\n\n\nInspired by this: https://towardsdatascience.com/when-averages-lie-moving-beyond-single-point-predictions-23201e8c04c8/\nLink it to the following kinds of issues:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#what-are-we-modelling",
    "href": "lectures/10.1-Visualising_Data.html#what-are-we-modelling",
    "title": "Visualising Data",
    "section": "What are We Modelling?",
    "text": "What are We Modelling?\n\nIssues of skew and leverage. It’s not (so much) about normality per se with OLS."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#what-are-we-penalising",
    "href": "lectures/10.1-Visualising_Data.html#what-are-we-penalising",
    "title": "Visualising Data",
    "section": "What are We Penalising?",
    "text": "What are We Penalising?"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#what-isnt-fitting",
    "href": "lectures/10.1-Visualising_Data.html#what-isnt-fitting",
    "title": "Visualising Data",
    "section": "What isn’t Fitting?",
    "text": "What isn’t Fitting?"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#choices-choices",
    "href": "lectures/10.1-Visualising_Data.html#choices-choices",
    "title": "Visualising Data",
    "section": "Choices, Choices…",
    "text": "Choices, Choices…\n\nmatplotlib: the ‘big beast’ of visualisation in Python. Similar to MATLAB. Highly customisable. Very complex.\nseaborn: a layer that sits over top of matplotlib and makes it easier to produce good-quality graphics.\nbokeh: web-based visualisation tool that can integrate with Jupyter or output to static HTML files.\nplotly: another web-based visualisation tool that can integrate with Jupyter.\n\nMore emerging all the time: Vega/Altair, HoloViews, etc.\n\n\nIf you’re really wedded to ggplot, plotnine is a clone of ggplot’s interface (Grammer of Graphics) in Python. A brief overview of visualisation libraries could be helpful."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#seaborn",
    "href": "lectures/10.1-Visualising_Data.html#seaborn",
    "title": "Visualising Data",
    "section": "Seaborn",
    "text": "Seaborn\nDesigned to provide ggplot-like quality output using matplotlib:\n\nImprove on default colourmaps and colour defaults.\nIntegration with pandas data frames (Note: not geopandas!).\nOffers more plot types out of the box.\nStill offers access to matplotlib’s back-end."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#plot-types",
    "href": "lectures/10.1-Visualising_Data.html#plot-types",
    "title": "Visualising Data",
    "section": "Plot Types",
    "text": "Plot Types\n\n\n\nPartial Overview of Seaborn Plots\n\n\n\n\nFor the fuller overview see Overview of seaborn plotting functions and the full API reference."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#in-practice",
    "href": "lectures/10.1-Visualising_Data.html#in-practice",
    "title": "Visualising Data",
    "section": "In Practice",
    "text": "In Practice\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\",\n             hue=\"region\", style=\"event\",\n             data=fmri)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#in-practice-2",
    "href": "lectures/10.1-Visualising_Data.html#in-practice-2",
    "title": "Visualising Data",
    "section": "In Practice 2",
    "text": "In Practice 2\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\ndf = sns.load_dataset(\"penguins\")\n\nax = sns.swarmplot(data=df, x=\"body_mass_g\", y=\"sex\", hue=\"species\")\nax.set(ylabel=\"\")"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#configuring-seaborn",
    "href": "lectures/10.1-Visualising_Data.html#configuring-seaborn",
    "title": "Visualising Data",
    "section": "Configuring Seaborn",
    "text": "Configuring Seaborn\nSeaborn ‘themes’ act as shortcuts for setting multiple matplotlib parameters:\n\n\n\nSeaborn Command\nAccomplishes\n\n\n\n\nset_theme(...)\nSet multiple theme parameters in one step.\n\n\naxes_style(...)\nReturn a parameter dict for the aesthetic style of the plots.\n\n\nset_style(...)\nSet the aesthetic style of the plots.\n\n\nplotting_context(...)\nReturn a parameter dict to scale elements of the figure.\n\n\nset_context(...)\nSet the plotting context parameters.\n\n\n\nYou can also access:\n\nPalettes: colormaps can be generated using sns.color_palette(...) and set using sns.set_palette(...).\nAxes Styles: includes darkgrid, whitegrid, dark, white, ticks."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#anatomy-of-a-figure",
    "href": "lectures/10.1-Visualising_Data.html#anatomy-of-a-figure",
    "title": "Visualising Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\nSource."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#writing-a-figure",
    "href": "lectures/10.1-Visualising_Data.html#writing-a-figure",
    "title": "Visualising Data",
    "section": "Writing a Figure",
    "text": "Writing a Figure\nThere are multiple ways to access/write elements of a plot:\n\nFigure: high-level features (e.g. title, padding, etc.). Can be accessed via plt.gcf() (get current figure) or upon creation (e.g. f, ax = plt.subplots(1,1) or f = plt.figure()).\nAxes: axis-level features (e.g. labels, tics, spines, limits, etc.). Can be accessed via plt.gca() (get current axes) or upon creation (e.g. f, ax = plt.subplots(1,1) or ax = f.add_subplot(1,1,1)).\n\nAnnotations, artists, and other features are typically written into the axes using the coordinate space of the figure (e.g. decimal degrees for lat/long, metres for BNG, etc.)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#adding-a-3rd-dimension",
    "href": "lectures/10.1-Visualising_Data.html#adding-a-3rd-dimension",
    "title": "Visualising Data",
    "section": "Adding a 3rd Dimension",
    "text": "Adding a 3rd Dimension\nThis ‘feature’ is less well-developed but does work:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax  = plt.axes(projection='3d')\n# OR\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\n# THEN\nax.contour3D(X, Y, Z, ...)\nax.plot_surface(x, y, z, ...)\nax.plot3D(xline, yline, zline, ...)\nax.scatter3D(x, y, z, ...)\n# ax.plot_surface and ax.plot_wire also give you 3D renderings\nYou can then set the elevation and azimuth using: ax.view_init(&lt;elevation&gt;, &lt;azimuth&gt;)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#saving-outputs",
    "href": "lectures/10.1-Visualising_Data.html#saving-outputs",
    "title": "Visualising Data",
    "section": "Saving Outputs",
    "text": "Saving Outputs\nStraightforward via save figure function, but lots of options!\nplt.savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n    orientation='portrait', papertype=None, format=None,\n    transparent=False, bbox_inches=None, pad_inches=0.1,\n    frameon=None, metadata=None)\nThe format can be largely determined by the file extension in the fname (file name) and the supported formats depends on what you’ve installed! You can find out what’s available to you using: plt.gcf().canvas.get_supported_filetypes()."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#jupyter",
    "href": "lectures/10.1-Visualising_Data.html#jupyter",
    "title": "Visualising Data",
    "section": "Jupyter",
    "text": "Jupyter\nBy default, Jupyter’s output is static matplotlib, but we can extend this in three ways:\n\nMake the static plot zoomable and pannable using %matplotlib widget (declare this at the top of your notebook).\nMake the plot more directly interactive using ipywidgets (import interact and related libs as needed).\nUse a browser-based visualisation tool such as bokeh, plotly, altair/vega, holoviews, or even d3 (format may be very, very different from what you are ‘used to’ in Python)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#widgets",
    "href": "lectures/10.1-Visualising_Data.html#widgets",
    "title": "Visualising Data",
    "section": "Widgets",
    "text": "Widgets\n%matplotlib widget\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = hackney.plot(edgecolor='k', facecolor='none')\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#interact",
    "href": "lectures/10.1-Visualising_Data.html#interact",
    "title": "Visualising Data",
    "section": "Interact()",
    "text": "Interact()\nTaking an example from Dani’s work:\nfrom ipywidgets import interact\n# Alternatives: interactive, fixed, interact_manual\ninteract(\n    &lt;function&gt;, # Function to make interactive\n    &lt;param0&gt;,   # e.g. Data to use\n    &lt;param1&gt;,   # e.g. Range start/end/step\n    &lt;param2&gt;    # e.g. Fixed value\n);"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#bokeh",
    "href": "lectures/10.1-Visualising_Data.html#bokeh",
    "title": "Visualising Data",
    "section": "Bokeh",
    "text": "Bokeh"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#automation",
    "href": "lectures/10.1-Visualising_Data.html#automation",
    "title": "Visualising Data",
    "section": "Automation",
    "text": "Automation\nPlots built on top of matploblib can, to some extent, be automated using functions. For example, to draw circles and place text:\ndef circle(ax, x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, \n                    linewidth=1, edgecolor='black', \n                    facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, \n                                  foreground='w')])\n    ax.add_artist(circle)\n\ndef text(ax, x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n         ha='center', va='top', weight='bold', color='blue')"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nLabel directly1:\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-1",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-1",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nRepeat measurement units1 2:\n\nShamelssly taken from DatawrapperInterestingly, this flies in the face of the ‘chart junk minimisation’ of Tufte and Graves-Morris (1983)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-2",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-2",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nPut the axes where they’re needed1:\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-3",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-3",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nEmphasise and explain with annotation1:\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-4",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-4",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nLead the eye with font sizes, styles, and colors1 2:\n\nShamelssly taken from DatawrapperJust not too many of them in too many different styles!"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-5",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-5",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nSee what I said on the previous slide about ‘too much going on’1:\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-6",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-6",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nFor the sake of all that is holy, please don’t center-align text1:\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-7",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-7",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nMake reading easy and interpretation intuitive1 2:\n\nShamelssly taken from DatawrapperYour readers don’t need to legal names for each country, nor do they like akward hypens."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-8",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-8",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nUse outlines to allow overlaps1 2:\n\nShamelssly taken from DatawrapperNotice how this supports the focus and interpretation."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-9",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-9",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nGet to the point in a way that works for the reader1 2:\n\nShamelssly taken from DatawrapperRephrase for legibility and intelligibility."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#additional-resources",
    "href": "lectures/10.1-Visualising_Data.html#additional-resources",
    "title": "Visualising Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nMatplotlib Cheatsheets\nIntroduction to PyPlot (includes lots of parameter information)\nVisualisation with Seaborn\nSeaborn Tutorial\nElite Data Science Seaborn Tutorial\nDatacamp Seaborn Tutorial\nThree-Dimensional Plotting in Matplotlib\nAn easy introduction to 3D plotting with Matplotlib\n\n\n\nDataWrapper Blog\nUsing text effectively in data viz\nChoosing fonts for charts and tables\nBokeh Gallery\nBokeh User Guide\nProgramming Historian: Visualizing Data with Bokeh and Pandas\nReal Python: Data Viz with Bokeh\nData Viz with Bokeh (Pt. 1)\nUsing Interact\nText in Data Visualizations"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#the-data-generating-process",
    "href": "lectures/11.1-Data_Space.html#the-data-generating-process",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\nSource."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#the-data-generating-process-1",
    "href": "lectures/11.1-Data_Space.html#the-data-generating-process-1",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\nSource."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#cashier-income-as-dgp",
    "href": "lectures/11.1-Data_Space.html#cashier-income-as-dgp",
    "title": "The Data Space",
    "section": "Cashier Income as DGP",
    "text": "Cashier Income as DGP\nQuestion: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#house-prices-as-dgp",
    "href": "lectures/11.1-Data_Space.html#house-prices-as-dgp",
    "title": "The Data Space",
    "section": "House Prices as DGP",
    "text": "House Prices as DGP\n\nSource.\n\nWhat does this suggest about the DGP?"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "href": "lectures/11.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "title": "The Data Space",
    "section": "Is Bill Gates as Rich as He is Tall?",
    "text": "Is Bill Gates as Rich as He is Tall?\nInstinctively, we know that Bill Gates’ wealth is much further from ‘normal’ than is his height. But how?\n\nHow can we compare income and height if they share no common units?\nHow can we compare the biodiversity of sites in the tropics with those of sub-Arctic areas given that there are different numbers of species to begin with?\n\nWe need:\n\nWays to make different dimensions comparable, and\nWays to remove unit effects from distance measures."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-1d",
    "href": "lectures/11.1-Data_Space.html#distance-in-1d",
    "title": "The Data Space",
    "section": "Distance in 1D",
    "text": "Distance in 1D\n\\[\nd(i,j) = |(i_{1}-j_{1})|\n\\]"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-2d",
    "href": "lectures/11.1-Data_Space.html#distance-in-2d",
    "title": "The Data Space",
    "section": "Distance in 2D",
    "text": "Distance in 2D\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}}\n\\]"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-3d-or-more",
    "href": "lectures/11.1-Data_Space.html#distance-in-3d-or-more",
    "title": "The Data Space",
    "section": "Distance in 3D… or More",
    "text": "Distance in 3D… or More\nWe can keep adding dimensions…\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}+(i_{3}-j_{3})^{2}}\n\\]\nYou continue adding dimensions indefinitely, but from here on out you are dealing with hyperspaces!"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#thinking-in-data-space",
    "href": "lectures/11.1-Data_Space.html#thinking-in-data-space",
    "title": "The Data Space",
    "section": "Thinking in Data Space",
    "text": "Thinking in Data Space\nWe can write the coordinates of an observation with 3 attributes (e.g. height, weight, income) as:\n\\[\nx_{i} = { {x_{i1}, x_{i2}, x_{i3} } }\n\\]\nSomething with 8 attributes (e.g. height, weight, income, age, year of birth, …) ‘occupies’ an 8-dimensional space…"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#two-propositions",
    "href": "lectures/11.1-Data_Space.html#two-propositions",
    "title": "The Data Space",
    "section": "Two Propositions",
    "text": "Two Propositions\n\nThat geographical space is no different from any other dimension in a data set.\nThat geographical space is still special when it comes to thinking about relationships."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#implication",
    "href": "lectures/11.1-Data_Space.html#implication",
    "title": "The Data Space",
    "section": "Implication",
    "text": "Implication\nIf you can shift from thinking in columns of data, to thinking of a data space then you’ll have a much easier time dealing with dimensionality reduction and clustering."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#additional-resources",
    "href": "lectures/11.1-Data_Space.html#additional-resources",
    "title": "The Data Space",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nAre Statisticians Cold-Blooded Bosses?\nBeyond 3D: Thinking in Higher Dimensions\nVisualizing beyond 3 Dimensions\nThe things you’ll find in higher dimensions (also useful for dimensionality reduction)\nWhat’s a Tensor? (heavy on the Physics relevance, but a lot of useful terminology and abstraction)"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#curse-and-blessing",
    "href": "lectures/11.3-Dimensionality.html#curse-and-blessing",
    "title": "Dimensionality",
    "section": "Curse and Blessing…",
    "text": "Curse and Blessing…\n\nMore dimensions means more information.\nMore dimensions makes for easier seperation.\nMore dimensions inflates distance.\nMore dimensions increases the risk of overfitting.\n\n\nOr as Analytics India Magazine puts it:\n\nHigh-dimensional spaces have geometrical properties that are counter-intuitive and far from the properties observed in two- or three-dimensional spaces.\nData analysis tools are often designed with intuitive properties and low-dimensional spaces in mind."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#pca",
    "href": "lectures/11.3-Dimensionality.html#pca",
    "title": "Dimensionality",
    "section": "PCA",
    "text": "PCA\nWorkhorse dimensionality reduction method: simple, fast, and effective. Can be thought of as freely rotating axes to align with directions of maximum variance. I like this summary:\n\nPCA (Principal Components Analysis) gives us our ‘ideal’ set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, the second has higher variance than the third, and so on), uncorrelated (all components are orthogonal), and low in number (we can throw away the lower ranked components as they usually contain little signal).\n\nBut I particularly liked this exposition in Towards Data Science."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice",
    "href": "lectures/11.3-Dimensionality.html#in-practice",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\npca.transform(data)\nSee also: Kernel PCA for non-linear problems."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#rtfm",
    "href": "lectures/11.3-Dimensionality.html#rtfm",
    "title": "Dimensionality",
    "section": "RT(F)M",
    "text": "RT(F)M\nWhy was I banging on about transformations? Well, what does this assume about the data?\n\nLinear dimensionality reduction using Singular Value Decomposition projects data into to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\n\nI found a nice explanation of PCA using dinner conversation over several bottles of wine as an example on Stats.StackExhcange.com. There are many good illustrations of this process on stats.stackexchange.com."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#other-considerations",
    "href": "lectures/11.3-Dimensionality.html#other-considerations",
    "title": "Dimensionality",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nPCA is a form of unsupervised learning that does not take output labels into account. Other approaches (such as Linear Discriminant Analysis [note: not Latent Dirichlet Allocation]) consider the output as part of the transformation. PCA is also deterministic.\n\nSee this discussion."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#t-sne",
    "href": "lectures/11.3-Dimensionality.html#t-sne",
    "title": "Dimensionality",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbour Embedding is best understood as a visualisation technique, not an analytical one. This is because it is probabilistic and not deterministic."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice-1",
    "href": "lectures/11.3-Dimensionality.html#in-practice-1",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.manifold import TSNE\nembedded = TSNE(n_components=d).fit_transform(x)\nThe choice of perplexity and n_iter matter, and so does the metric. In practice you will need to experiment with these.\n\nt-SNE is also much harder computationally than PCA and it may be preferrable on very high-D data sets to apply PCA first and then t-SNE to the reduced data set! The output could then be fed to a clustering algorithm to make predictions about where new observations belong, but do not confuse that with meaning."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#umap",
    "href": "lectures/11.3-Dimensionality.html#umap",
    "title": "Dimensionality",
    "section": "UMAP",
    "text": "UMAP\nNon-linear dimensionality reduction that tries to preserve both local and global structure. Puts it between PCA and t-SNE.\n\n\n\nsee examples on umap-learn.readthedocs.io\n\n\nNote that the library is called umap-learn."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice-2",
    "href": "lectures/11.3-Dimensionality.html#in-practice-2",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nimport umap\ntransformer = umap.UMAP(n_components=d)\nembedded = transformer.fit_transform(x)\nThe choice of n_neighbors, min_dist, and metric matter. In practice you may need to experiment with these."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#gotcha",
    "href": "lectures/11.3-Dimensionality.html#gotcha",
    "title": "Dimensionality",
    "section": "Gotcha!",
    "text": "Gotcha!\nt-SNE (less so UMAP) requires very careful handling:\n\nHyperparameters matter a lot\nCluster size means nothing\nCluster distances mean nothing\nClusters may mean nothing (low neighbour count/perplexity)\nOutputs are stochastic (not deterministic)\n\nBoth likely require repeated testing and experimentation."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#other-approaches",
    "href": "lectures/11.3-Dimensionality.html#other-approaches",
    "title": "Dimensionality",
    "section": "Other Approaches",
    "text": "Other Approaches\n\nFeature selection, including forwards/backwards (sklearn.feature_selection here)\nDecomposition (sklearn.decomposition here, especiall SVD)\nOther types of manifold learning (sklearn.manifold here)\nRandom projection (sklearn.random_projection here)\nSupport Vector Machines (sklearn.svm here)\nEnsemble Methods (such as Random Forests: sklearn.ensemble.ExtraTreesClassifier and sklearn.ensemble.ExtraTreesRegressor here and here)"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#additional-resources",
    "href": "lectures/11.3-Dimensionality.html#additional-resources",
    "title": "Dimensionality",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nRethinking ‘distance’ in New York City Medium URL\nFive Boroughs for the 21\\(^{st}\\) Century Medium URL\nCurse of Dimensionality\nThe Curse of Dimensionality\nUnderstanding Curse of Dimensionality\nCurse of Dimensionality – A ‘Curse’ to Machine Learning\nImportance of Feature Scaling\nUnderstanding PCA\nIntroduction to t-SNE in Python\n\n\n\nHow to Use t-SNE Effectively\nHow to tune the Hyperparameters of t-SNE\nUnderstanding UMAP (Compares to t-SNE)\nHow UMAP Works\n3 New Techniques for Data-Dimensionality Reduction in ML\nUMAP for Dimensionality Reduction (Video)\nA Bluffer’s Guide to Dimensionality Reduction (Video)"
  },
  {
    "objectID": "lectures/12.2-Classification.html#spot-the-difference",
    "href": "lectures/12.2-Classification.html#spot-the-difference",
    "title": "Classification",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nOn Maps\n\nGroup observations by ‘class’.\nTypically based on 1-D distribution.\nClasses are assigned by user choice.\n\n\nOn Labels\n\nLabel observations by ‘class’.\nTypically based on model outputs.\nLabels are assigned by user feedback.\n\n\n\nIn this session we are primarily concerned with the first column — classification as a modelling process is better considered a data science/modelling problem that is beyond the scope of this module."
  },
  {
    "objectID": "lectures/12.2-Classification.html#map-classification-choices",
    "href": "lectures/12.2-Classification.html#map-classification-choices",
    "title": "Classification",
    "section": "Map Classification Choices",
    "text": "Map Classification Choices\n\nAssign classes manually.\nSplit range evenly.\nSplit data evenly\nSplit data according to distribution\nSplit data according to their similarity to each other.\n\n\n\nAccording to some logic/theory/regulatory or policy fact or objective.\nEqual intervals for cases without heavy skew\nQuantiles or HeadTailBreaks for cases with heavy skew\nSD for cases with normal distribution; BoxPlot for others.\nNatural breaks/FIsher Jenks for cases where distribution is discontinuous"
  },
  {
    "objectID": "lectures/12.2-Classification.html#mapclassify",
    "href": "lectures/12.2-Classification.html#mapclassify",
    "title": "Classification",
    "section": "Mapclassify",
    "text": "Mapclassify\nMapclassify (part of PySAL) provides a wide range of classifiers:\n\n\n\nNo Parameters\nk Parameter\n\n\n\n\nBoxPlot\nUserDefined\n\n\nStdMean\nPercentiles\n\n\nMaxP\nQuantiles\n\n\nHeadTailBreaks\nNatural Breaks\n\n\nEqualInterval\nMaximum Breaks\n\n\n\nJenksCaspall/Sampled/Forced\n\n\n\nFisherJenks/Sampled\n\n\n\nk will a user-specified number of classes or binning criterion."
  },
  {
    "objectID": "lectures/12.2-Classification.html#raw",
    "href": "lectures/12.2-Classification.html#raw",
    "title": "Classification",
    "section": "Raw",
    "text": "Raw"
  },
  {
    "objectID": "lectures/12.2-Classification.html#user-defined",
    "href": "lectures/12.2-Classification.html#user-defined",
    "title": "Classification",
    "section": "User Defined",
    "text": "User Defined\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, 125000.00]\n0\n\n\n( 125000.00, 250000.00]\n4\n\n\n( 250000.00, 925000.00]\n865\n\n\n( 925000.00, 1500000.00]\n85\n\n\n(1500000.00, 4500000.00]\n29"
  },
  {
    "objectID": "lectures/12.2-Classification.html#box-plot",
    "href": "lectures/12.2-Classification.html#box-plot",
    "title": "Classification",
    "section": "Box Plot",
    "text": "Box Plot\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -31429.25]\n0\n\n\n( -31429.25, 391267.00]\n246\n\n\n( 391267.00, 495010.00]\n246\n\n\n( 495010.00, 673064.50]\n245\n\n\n( 673064.50, 1095760.75]\n175\n\n\n(1095760.75, 4416659.00]\n70"
  },
  {
    "objectID": "lectures/12.2-Classification.html#standard-deviations",
    "href": "lectures/12.2-Classification.html#standard-deviations",
    "title": "Classification",
    "section": "Standard Deviations",
    "text": "Standard Deviations\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -171366.63]\n0\n\n\n(-171366.63, 216174.43]\n0\n\n\n( 216174.43, 991256.55]\n892\n\n\n( 991256.55, 1378797.61]\n53\n\n\n(1378797.61, 4416659.00]\n38"
  },
  {
    "objectID": "lectures/12.2-Classification.html#max-p",
    "href": "lectures/12.2-Classification.html#max-p",
    "title": "Classification",
    "section": "Max P",
    "text": "Max P\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346594.00]\n142\n\n\n( 346594.00, 461577.00]\n279\n\n\n( 461577.00, 529197.00]\n140\n\n\n( 529197.00, 530662.00]\n3\n\n\n( 530662.00, 613465.00]\n115\n\n\n( 613465.00, 842387.00]\n167\n\n\n( 842387.00, 4416659.00]\n137"
  },
  {
    "objectID": "lectures/12.2-Classification.html#head-tail-breaks",
    "href": "lectures/12.2-Classification.html#head-tail-breaks",
    "title": "Classification",
    "section": "Head Tail Breaks",
    "text": "Head Tail Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 603715.49]\n670\n\n\n( 603715.49, 976290.79]\n218\n\n\n( 976290.79, 1508985.73]\n66\n\n\n(1508985.73, 2257581.55]\n16\n\n\n(2257581.55, 2826007.08]\n9\n\n\n(2826007.08, 3553496.25]\n3\n\n\n(3553496.25, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#equal-interval",
    "href": "lectures/12.2-Classification.html#equal-interval",
    "title": "Classification",
    "section": "Equal Interval",
    "text": "Equal Interval\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 825125.00]\n842\n\n\n( 825125.00, 1423714.00]\n108\n\n\n(1423714.00, 2022303.00]\n17\n\n\n(2022303.00, 2620892.00]\n10\n\n\n(2620892.00, 3219481.00]\n4\n\n\n(3219481.00, 3818070.00]\n1\n\n\n(3818070.00, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#quantiles",
    "href": "lectures/12.2-Classification.html#quantiles",
    "title": "Classification",
    "section": "Quantiles",
    "text": "Quantiles\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346009.00]\n140\n\n\n( 346009.00, 405677.86]\n140\n\n\n( 405677.86, 461959.29]\n140\n\n\n( 461959.29, 529612.86]\n141\n\n\n( 529612.86, 639488.86]\n140\n\n\n( 639488.86, 827691.43]\n140\n\n\n( 827691.43, 4416659.00]\n141"
  },
  {
    "objectID": "lectures/12.2-Classification.html#natural-breaks",
    "href": "lectures/12.2-Classification.html#natural-breaks",
    "title": "Classification",
    "section": "Natural Breaks",
    "text": "Natural Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 433543.00]\n356\n\n\n( 433543.00, 605879.00]\n316\n\n\n( 605879.00, 842387.00]\n174\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/12.2-Classification.html#maximum-breaks",
    "href": "lectures/12.2-Classification.html#maximum-breaks",
    "title": "Classification",
    "section": "Maximum Breaks",
    "text": "Maximum Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 1688895.00]\n961\n\n\n(1688895.00, 1926265.50]\n4\n\n\n(1926265.50, 2278155.50]\n5\n\n\n(2278155.50, 2929865.50]\n9\n\n\n(2929865.50, 3349991.00]\n2\n\n\n(3349991.00, 3959682.50]\n1\n\n\n(3959682.50, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#fisher-jenks",
    "href": "lectures/12.2-Classification.html#fisher-jenks",
    "title": "Classification",
    "section": "Fisher Jenks",
    "text": "Fisher Jenks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 435961.00]\n363\n\n\n( 435961.00, 607480.00]\n310\n\n\n( 607480.00, 842387.00]\n173\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/12.2-Classification.html#jenks-caspall",
    "href": "lectures/12.2-Classification.html#jenks-caspall",
    "title": "Classification",
    "section": "Jenks Caspall",
    "text": "Jenks Caspall\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 365741.00]\n188\n\n\n( 365741.00, 441979.00]\n187\n\n\n( 441979.00, 520791.00]\n167\n\n\n( 520791.00, 638474.00]\n160\n\n\n( 638474.00, 890055.00]\n156\n\n\n( 890055.00, 1626454.00]\n103\n\n\n(1626454.00, 4416659.00]\n22"
  },
  {
    "objectID": "lectures/12.2-Classification.html#summary",
    "href": "lectures/12.2-Classification.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\n\nThe choice of classification scheme should be data- and distribution-led. This is simply a demonstration of how different schemes can shape your understanding of the data."
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips",
    "href": "lectures/12.2-Classification.html#code-useful-tips",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the classes:\nkl = 7\ncls = [mapclassify.BoxPlot, ...,  mapclassify.JenksCaspall]\nSetting up the loop:\nfor cl in cls:\n    try: \n        m = cl(ppd.Value, k=kl)\n    except TypeError:\n        m = cl(ppd.Value)\n    \n    f = plt.figure()\n    gs = f.add_gridspec(nrows=2, ncols=1, height_ratios=[1,4])\n\n    ax1 = f.add_subplot(gs[0,0])\n    ...\n\n    ax2 = f.add_subplot(gs[1,0])\n    ..."
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips-1",
    "href": "lectures/12.2-Classification.html#code-useful-tips-1",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the distribution:\n    ax1 = f.add_subplot(gs[0,0])\n    sns.kdeplot(ppd.Value, ax=ax1, color='r')\n    ax1.ticklabel_format(style='plain', axis='x') \n\n    y = ax1.get_ylim()[1]\n    for b in m.bins:\n        ax1.vlines(b, 0, y, linestyles='dotted')"
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips-2",
    "href": "lectures/12.2-Classification.html#code-useful-tips-2",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nAdjusting the legend text:\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k,v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\nSetting up the map:\n    ax2 = f.add_subplot(gs[1,0])\n    ppd.assign(cl=m.yb).plot(column='cl', k=len(m.bins), categorical=True, legend=True, ax=ax2)\n    \n    mapping = dict([(i,s) for i,s in enumerate(m.get_legend_classes())])\n    ax2.set_axis_off()\n    replace_legend_items(ax2.get_legend(), mapping)"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#space-adds-complexity",
    "href": "lectures/12.4-Clustering_and_Geography.html#space-adds-complexity",
    "title": "Clustering & Geography",
    "section": "Space Adds Complexity",
    "text": "Space Adds Complexity\nWe now have to consider two more types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#trade-offs-again",
    "href": "lectures/12.4-Clustering_and_Geography.html#trade-offs-again",
    "title": "Clustering & Geography",
    "section": "Trade-offs (Again)…",
    "text": "Trade-offs (Again)…\nConsider:\n\nClustering algorithms are inherently spatial.\nClustering algorithms do not take space geography into account.\n\nDoes this matter?\n\nAll clustering algorithms are about inter-observation and intra-cluster distances so they have some conceptualisation of ‘space’.\nSpatially-aware clustering algorithms exist but are generally much more computationally-intensive than ‘regular ones’."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#different-approaches",
    "href": "lectures/12.4-Clustering_and_Geography.html#different-approaches",
    "title": "Clustering & Geography",
    "section": "Different Approaches",
    "text": "Different Approaches\n\n\n\nAlgorithm\nPros\nCons\nGeographically Aware?\n\n\n\n\nk-Means\nFast. Deterministic.\nEvery observation to a cluster.\nN.\n\n\nDBSCAN\nAllows for clusters and outliers.\nSlower. Choice of \\(\\epsilon\\) critical. Can end up with all outliers.\nN, but implicit in \\(\\epsilon\\).\n\n\nOPTICS\nFewer parameters than DBSCAN.\nEven slower.\nN, but implicit in \\(\\epsilon\\).\n\n\nHierarchical/ HDBSCAN\nCan cut at any number of clusters.\nNo ‘ideal’ solution.\nY, with connectivity parameter.\n\n\nADBSCAN\nScales. Confidence levels.\nMay need large data set to be useful. Choice of \\(\\epsilon\\) critical.\nY.\n\n\nMax-p\nCoherent regions returned.\nVery slow if model poorly specified.\nY."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "href": "lectures/12.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "title": "Clustering & Geography",
    "section": "Setting the Relevant Distance",
    "text": "Setting the Relevant Distance\nMany clustering algorithms rely on a distance specification (usually \\(\\epsilon\\)). So to set this threshold:\n\nIn high-dimensional spaces this threshold will need to be large.\nIn high-dimensional spaces the scale will be meaningless (i.e. not have a real-world meaning, only an abstract one).\nIn 2- or 3-dimensional (geographical) space this threshold could be meaningful (i.e. a value in metres could work)."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "href": "lectures/12.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "title": "Clustering & Geography",
    "section": "Choosing a Distance Metric",
    "text": "Choosing a Distance Metric\n\n\n\n\n\n\n\n\nn Dimensions\nHow to Set\nExamples\n\n\n\n\n2 or 3\nTheory/Empirical Data\nWalking speed; Commute distance\n\n\n2 or 3\nK/L Measures\nPlot with Simulation for CIs to identify significant ‘knees’.\n\n\n3\nMarked Point Pattern?\n\n\n\n&gt; 3\nkNN\nCalculate average kNN distance based on some expectation of connectivity.\n\n\n\n\nRemember: inter-observation distance increases with dimensionality!"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian",
    "title": "Clustering & Geography",
    "section": "Experian",
    "text": "Experian\nSpecialist in consumer segmentation and geodemographics (bit.ly/2jMRhAW).\n\nMarket cap: £14.3 billion.\nMosaic: “synthesises of 850 million pieces of information… to create a segmentation that allocates 49 million individuals and 26 million households into one of 15 Groups and 66 detailed Types.””\nMore than 450 variables used.\n\nMost retail companies will have their own segmentation scheme. Competitors: CACI, Nielsen, etc."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian-groups",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian-groups",
    "title": "Clustering & Geography",
    "section": "Experian Groups",
    "text": "Experian Groups"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian-mapping",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian-mapping",
    "title": "Clustering & Geography",
    "section": "Experian Mapping",
    "text": "Experian Mapping"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#output-area-classification",
    "href": "lectures/12.4-Clustering_and_Geography.html#output-area-classification",
    "title": "Clustering & Geography",
    "section": "Output Area Classification",
    "text": "Output Area Classification\nOAC set up as ‘open source’ alternative to Mosaic:\n\nWell documented (UCL Geography a major contributor)\nDoesn’t require a license or payment\nCan be tweaked/extended/reweighted by users as needed"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#conditions-consequences",
    "href": "lectures/2.2-Python_the_Basics-2.html#conditions-consequences",
    "title": "Python: the Basics(Part 2)",
    "section": "Conditions & Consequences",
    "text": "Conditions & Consequences\nThe simplest condition only considers one outcome:\nif &lt;condition is true&gt;:\n    #...do something...\nBut you’ll often needs something a little more sophisticated:\nif &lt;condition is true&gt;:\n    #...do something...\nelif &lt;some other condition is true&gt;:\n    #...do something else...\nelse:\n    #...if no conditions are true...\n\n#...code continues...\n\nBut no matter how complex, conditions always ultimately evaluate to True or False."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#for-example",
    "href": "lectures/2.2-Python_the_Basics-2.html#for-example",
    "title": "Python: the Basics(Part 2)",
    "section": "For Example",
    "text": "For Example\nif x &lt; y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x &lt; y:\n  print(\"x is less than y\")\nelif x &gt; y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#conditional-syntax",
    "href": "lectures/2.2-Python_the_Basics-2.html#conditional-syntax",
    "title": "Python: the Basics(Part 2)",
    "section": "Conditional Syntax",
    "text": "Conditional Syntax\nThe most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-input",
    "href": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-input",
    "title": "Python: the Basics(Part 2)",
    "section": "All of Them Together (Input)!",
    "text": "All of Them Together (Input)!\nif hours &gt;= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-output",
    "href": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-output",
    "title": "Python: the Basics(Part 2)",
    "section": "All of Them Together (Output)!",
    "text": "All of Them Together (Output)!\nOutput from the Python interpreter:\n&gt;&gt;&gt; if hours &gt;= 0:\n... print(\"Hours were worked.\")\n  File \"&lt;stdin&gt;\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n&gt;&gt;&gt; else\n  File \"&lt;stdin&gt;\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt;     print \"No hours were worked.\")\n  File \"&lt;stdin&gt;\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#thats-better",
    "href": "lectures/2.2-Python_the_Basics-2.html#thats-better",
    "title": "Python: the Basics(Part 2)",
    "section": "That’s Better!",
    "text": "That’s Better!\nIt’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours &gt; 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#make-your-life-easy-well-easier",
    "href": "lectures/2.2-Python_the_Basics-2.html#make-your-life-easy-well-easier",
    "title": "Python: the Basics(Part 2)",
    "section": "Make Your Life Easy (Well, Easier)",
    "text": "Make Your Life Easy (Well, Easier)\nAlways comment your code:\n\nSo that you know what is going on.\nSo that you know why it is going on.\nSo that others can read your code.\nTo help you plan your code\n\n\nYou are reminding your future self what your code was for and helping to give it structure (explaining==thinking!)."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#different-comment-styles",
    "href": "lectures/2.2-Python_the_Basics-2.html#different-comment-styles",
    "title": "Python: the Basics(Part 2)",
    "section": "Different Comment Styles",
    "text": "Different Comment Styles\n# This is a short comment\nprint(\"Foo\")\nprint(\"Bar\") # Also a short comment\n\n# ------- New Section --------\n# You can have comments span multiple\n# lines just by adding more '#' at the \n# start of the line.\n\n# You can keep code from running\n# print(\"Baz\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#comments-follow-indentation",
    "href": "lectures/2.2-Python_the_Basics-2.html#comments-follow-indentation",
    "title": "Python: the Basics(Part 2)",
    "section": "Comments Follow Indentation",
    "text": "Comments Follow Indentation\n# Function for processing occupational data\n# from the 2001 and 2011 Censuses.\ndef occ_data(df):\n  #  Columns of interest\n  cols = ['Managerial','Professional','Technical']\n    \n  # Integrate results into single dataset -- \n  # right now we don't replicate Jordan's approach of\n  # grouping them into 'knowledge worker' and 'other'. \n  for i in df.iterrows():\n    # For each column...\n    for j in cols:\n      # Do something\n      ..."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#easier-multi-line-comments",
    "href": "lectures/2.2-Python_the_Basics-2.html#easier-multi-line-comments",
    "title": "Python: the Basics(Part 2)",
    "section": "Easier Multi-Line Comments",
    "text": "Easier Multi-Line Comments\nThe below are not real comments, but they can help when you have a really long comment that you want to make. They are also used to help explain what a function does (called a docstring).\n\"\"\"\nSo I was thinking that what we need here is \na way to handle the case where the data is\nincomplete or contains an observation that we\nweren't expecting (e.g. \"N/A\" instead of \"0\").\n\"\"\""
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#tips",
    "href": "lectures/2.2-Python_the_Basics-2.html#tips",
    "title": "Python: the Basics(Part 2)",
    "section": "Tips",
    "text": "Tips\nSome useful tips for commenting your code:\n\nInclude general information at the top of your programming file.\nAssume the person reading the code is a coder themselves.\nGood commenting is sparse in the sense that it is used judiciously, and concise without being gnomic.\nUse comments to track the logic of your code (especially in conditionals and loops)"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#additional-resources",
    "href": "lectures/2.2-Python_the_Basics-2.html#additional-resources",
    "title": "Python: the Basics(Part 2)",
    "section": "Additional Resources",
    "text": "Additional Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\nConditional structures\nIf Statements\nIf-Else Statements\nIf-Elif\nWhitespace and comments\nUsing print()\nConditional syntax\nConditional operators\nConditional assignment"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#iteration",
    "href": "lectures/2.4-Iteration.html#iteration",
    "title": "Iteration",
    "section": "it·er·a·tion",
    "text": "it·er·a·tion\n/itə’rāSHən/\nNoun\nThe repetition of a process or utterance.\n\nrepetition of a mathematical or computational procedure applied to the result of a previous application, typically as a means of obtaining successively closer approximations to the solution of a problem.\na new version of a piece of computer hardware or software. plural noun: iterations\n\n\nBy themselves, lists and variables aren’t very useful if we can only access items either one at a time, or as slice.\nBut if we can iterate over a list (do something to every item in a list) then we gain some serious powers.\nMany programmers also call these loops."
  },
  {
    "objectID": "lectures/2.4-Iteration.html#two-types-of-iteration",
    "href": "lectures/2.4-Iteration.html#two-types-of-iteration",
    "title": "Iteration",
    "section": "Two Types of Iteration",
    "text": "Two Types of Iteration\n\n\n‘For’ loops\n\nUsed with finite lists of definite length\nFor each item in this list do something…\n\n\n‘While’ loops:\n\nUsed with unknown or non-finite lists\nWhile a condition is still True, do something to the list…"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#making-the-difference-memorable",
    "href": "lectures/2.4-Iteration.html#making-the-difference-memorable",
    "title": "Iteration",
    "section": "Making the Difference Memorable",
    "text": "Making the Difference Memorable"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#for-loops-1",
    "href": "lectures/2.4-Iteration.html#for-loops-1",
    "title": "Iteration",
    "section": "For Loops",
    "text": "For Loops\nThis ‘simple’ loop allows us to print out every element of the list in turn:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\nNotice the format:\nfor x in list:\n  # ...do something using the current value of x...\n\nNotice that this is the same in that we saw with if 'Batty' in geographers!"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#while-loops-1",
    "href": "lectures/2.4-Iteration.html#while-loops-1",
    "title": "Iteration",
    "section": "While Loops",
    "text": "While Loops\nThis loop does the same thing, but differently:\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )\n  g += 1\nNotice the format:\nwhile &lt;some condition is true&gt;:\n  # ...do something..."
  },
  {
    "objectID": "lectures/2.4-Iteration.html#nesting-loops",
    "href": "lectures/2.4-Iteration.html#nesting-loops",
    "title": "Iteration",
    "section": "Nesting Loops",
    "text": "Nesting Loops\nWe can use one loop ‘inside’ another loop! What do you think this might print?\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  for h in g:\n    print(h)\nHuh??? Let’s puzzle this out…"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#debugging",
    "href": "lectures/2.4-Iteration.html#debugging",
    "title": "Iteration",
    "section": "Debugging",
    "text": "Debugging\nWhen you see something completely new, it’s often good to:\n\nAdd print(...) statements to see how the values of a variable are changing.\nComment out the parts you don’t understand so that you can focus on the parts you do\nThen iteratively add complexity!"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-1-the-outer-loop",
    "href": "lectures/2.4-Iteration.html#step-1-the-outer-loop",
    "title": "Iteration",
    "section": "Step 1: The ‘Outer Loop’",
    "text": "Step 1: The ‘Outer Loop’\nSo I would start off with:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\n#   for h in g:\n#   print(h)\nThis prints:\n'Rose'\n'Massey'\n'Jefferson'"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-1-summing-up",
    "href": "lectures/2.4-Iteration.html#step-1-summing-up",
    "title": "Iteration",
    "section": "Step 1: Summing Up",
    "text": "Step 1: Summing Up\nOK, so now we know:\n\nThat g is the name of a geographer.\nThe ‘outer’ loop sets g to the name of a new geographer on each iteration.\nSo if g is set to 'Rose' what does for h in g: do?"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-2-the-inner-loop",
    "href": "lectures/2.4-Iteration.html#step-2-the-inner-loop",
    "title": "Iteration",
    "section": "Step 2: The ‘Inner’ Loop",
    "text": "Step 2: The ‘Inner’ Loop\nWe know change it like this:\nfor g in geographers:\n  print(g)\n  for h in g:\n    print(h)\n  break # &lt;-- Notice this!\nThis prints:\nRose\nR\no\ns\ne"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-2-summing-up",
    "href": "lectures/2.4-Iteration.html#step-2-summing-up",
    "title": "Iteration",
    "section": "Step 2: Summing Up",
    "text": "Step 2: Summing Up\nAnd now we know that:\n\nh is looping over the string, meaning that a string can be treated as a list!\nAnd break is a really useful way to control a loop while you’re testing your code!"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#recap",
    "href": "lectures/2.4-Iteration.html#recap",
    "title": "Iteration",
    "section": "Recap",
    "text": "Recap\n\nfor iterates once over a collection items (e.g. a list).\nwhile keeps going until a condition is False."
  },
  {
    "objectID": "lectures/2.4-Iteration.html#test-yourself",
    "href": "lectures/2.4-Iteration.html#test-yourself",
    "title": "Iteration",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat will this code print? I’d suggest that you don’t run it!\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#test-yourself-tricksy-version",
    "href": "lectures/2.4-Iteration.html#test-yourself-tricksy-version",
    "title": "Iteration",
    "section": "Test Yourself (Tricksy Version)",
    "text": "Test Yourself (Tricksy Version)\nHere’s a really tricky one! The following two blocks of code produce the same output, how are they different?\ngeographers = ['Rose','Massey','Jefferson']\ngeographers.reverse()\nfor g in geographers:\n  print(g)\nAnd:\ngeographers = ['Rose','Massey','Jefferson']\ng = len(geographers)-1\nwhile g &gt;= 0:\n  print( geographers[g] )\n  g -= 1"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#one-more-thing",
    "href": "lectures/2.4-Iteration.html#one-more-thing",
    "title": "Iteration",
    "section": "One More Thing…",
    "text": "One More Thing…\nLet’s go back to the Lists examples for a second:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nHave a think about how this code works:\nfor ag in all_geographers:\n  for g in ag:\n    print(g)"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#additional-resources",
    "href": "lectures/2.4-Iteration.html#additional-resources",
    "title": "Iteration",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhat is Iteration?\nLoops\nFor Loop\nWhile Loop\n\nWe don’t cover the concept of recursion, but it’s quite a powerful idea and links nicely with Iteration:\n\nWhat is a recursive function?\nDefine recursive functions"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#the-pipeline",
    "href": "lectures/2.6-What_We_Do.html#the-pipeline",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nCode\nAnalyse\nUnderstand\nCommunicate\nReport\n\n\nHere are five things that we might imagine any data scientists, spatial or otherwise, does.\nDo we think they’re in the right order?\nI can tell you right now that this sequence of steps is how to get a mark of between 45 and 63 in your dissertation."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#the-pipeline-1",
    "href": "lectures/2.6-What_We_Do.html#the-pipeline-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nUnderstand\nCode\nReport\nAnalyse\nCommunicate\n\n\nHere’s a slightly better order, here’s why:\n\nYou can’t answer a question – from your boss, from an academic, from your friends even – if you don’t understand it. So before you run off and start writing some code, the first thing you need to do is understand the problem you’re trying to solve. That problem should not be technical, it should be practical.\nOnce you understand the problem you can start trying to code a solution.\nThe code will allow you to produce reports. These reports might be to do with data quality, they might be diagnostics from a Random Forest Machine Learning algorithm. At each stage in the development of your results you should be generating reports that help you to better-understand your problem and work out if your code is working.\nOnce the reports have given you confidence in your findings now you can actually write the analysis. Your analysis might lead you to realise that you need to go back and write more code and produce more reports, but that’s normal.\nFinally, you need to work out how to communicate your analysis. If you understand the problem then you’ll find this process rewarding. If you don’t then you’ll find it frustrating and want to brush it off quickly.\n\nSo the fact these are in a list is still rather misleading because at each point you get feedback effects, and it’s also a loop."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-understand",
    "href": "lectures/2.6-What_We_Do.html#how-to-understand",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-understand-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-understand-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?\n\n\nWhy are we doing this?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-report",
    "href": "lectures/2.6-What_We_Do.html#how-to-report",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-report-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-report-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number.\n\n\nThese are the takeaways."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-analyse",
    "href": "lectures/2.6-What_We_Do.html#how-to-analyse",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods we can use."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-analyse-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-analyse-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods I can use.\n\n\nThis is the method that matches the need."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-communicate",
    "href": "lectures/2.6-What_We_Do.html#how-to-communicate",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-communicate-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-communicate-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?\n\n\nWho needs to know?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#tell-me-a-story",
    "href": "lectures/2.6-What_We_Do.html#tell-me-a-story",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#tell-me-a-story-1",
    "href": "lectures/2.6-What_We_Do.html#tell-me-a-story-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story.\n\n\nFocus on the ‘so what’."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#tell-me-a-story-2",
    "href": "lectures/2.6-What_We_Do.html#tell-me-a-story-2",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story.\n\n\nFocus on the ‘so what’.\n\n\nDon’t bury the lede.\n\n\nAll data science is, ultimately, a story. A story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#all-change",
    "href": "lectures/2.6-What_We_Do.html#all-change",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "All Change?",
    "text": "All Change?\nWe think the story is the stable part, because everything else in data science is always changing. Here’s one vision from Analyst Uttam (2025):\n\n\n\n\n\n\n\nOld Data Science Role\nNew Job Title\n\n\n\n\nData cleaning & ETL\nAnalytics Engineer (SQL + dbt + Airflow )\n\n\nBusiness dashboards\nBI Developer (Power BI, Tabley, Looker)\n\n\nProduct experimentation\nData Analyst (SQL + experimentation)\n\n\nML Modelling\nML Engineer (MLOps, pipelines, deployment)\n\n\nCausal inference/forecasting\nResearch Scientist (Stats + PhD-level modeling)"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#additional-resources",
    "href": "lectures/2.6-What_We_Do.html#additional-resources",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nVersion Control with Git\nSetting up and managing your GitHub user account\nPersonal Access Tokens on Git\nGit Cheat Sheet"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#what-about-computational-thinking",
    "href": "lectures/2.8-On_Coding.html#what-about-computational-thinking",
    "title": "On Coding",
    "section": "What About Computational Thinking?",
    "text": "What About Computational Thinking?\n\nComputational thinking is not thinking like a Computer Scientist. It is about recognising how to code can help us to understand, and manipulate, the world."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#key-features",
    "href": "lectures/2.8-On_Coding.html#key-features",
    "title": "On Coding",
    "section": "Key Features",
    "text": "Key Features\nAspects of computational thinking include:\n\nRecognising how one problem connects to other problems.\nRecognising when and how to make things simpler and faster\nRecognising how different ways of tackling a problem gives you power to tackle new problems.\n\nSee this keynote by Lorena Barba (2014); esp. from 52:00 onwards.\n\nYou already do a lot of this when you generalise from your readings to your ideas/understanding!"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#why-are-these-virtues",
    "href": "lectures/2.8-On_Coding.html#why-are-these-virtues",
    "title": "On Coding",
    "section": "Why Are These Virtues?",
    "text": "Why Are These Virtues?\nAccording to Larry Wall the three virtues of the programmer are:\n\nLaziness\nImpatience\nHubris\n\nThese are not to be taken literally (see Larry Wall’s “Three Virtues of a Programmer” are Utter Bull💩).\n\nAutomate the boring stuff, focus on the interesting bits! And it’s not about quantity of code, it’s about quality.\nUse code to save time, but don’t just jump head-first into problems.\nWhen something isn’t working well you want to make it work better/faster/more efficiently…"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#four-quotes-to-remember",
    "href": "lectures/2.8-On_Coding.html#four-quotes-to-remember",
    "title": "On Coding",
    "section": "Four Quotes to Remember",
    "text": "Four Quotes to Remember\n\nComputers make very fast, very accurate mistakes.\nA computer program does what you tell it to do, not what you want it to do.\nOnly half of programming is coding. The other 90% is debugging.\nWeeks of coding can save you hours of planning."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#and-one-more",
    "href": "lectures/2.8-On_Coding.html#and-one-more",
    "title": "On Coding",
    "section": "And One More…",
    "text": "And One More…"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#following-a-recipe-is-easy-right",
    "href": "lectures/2.8-On_Coding.html#following-a-recipe-is-easy-right",
    "title": "On Coding",
    "section": "Following a Recipe is Easy, Right?",
    "text": "Following a Recipe is Easy, Right?\n\nSource"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#calculating-the-mean",
    "href": "lectures/2.8-On_Coding.html#calculating-the-mean",
    "title": "On Coding",
    "section": "Calculating the Mean",
    "text": "Calculating the Mean\nGiven these numbers, what’s the average?\n1, 4, 7, 6, 4, 2"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#as-a-recipe",
    "href": "lectures/2.8-On_Coding.html#as-a-recipe",
    "title": "On Coding",
    "section": "As a Recipe",
    "text": "As a Recipe\n\nTake a list of numbers\nStart a count of numbers in the list at 0\nStart a sum of numbers in the list at 0\nTake a number from the list:\n\nAdd 1 to the count\nAdd the value of the number to the sum\n\nRepeat step #4 until no numbers are left in the list.\nDivide the sum by the count\nReport this number back to the user"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#as-python",
    "href": "lectures/2.8-On_Coding.html#as-python",
    "title": "On Coding",
    "section": "As Python",
    "text": "As Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#why-we-still-havent-solved-it",
    "href": "lectures/2.8-On_Coding.html#why-we-still-havent-solved-it",
    "title": "On Coding",
    "section": "Why We Still Haven’t ‘Solved It’",
    "text": "Why We Still Haven’t ‘Solved It’\n\nSource"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#languages",
    "href": "lectures/2.8-On_Coding.html#languages",
    "title": "On Coding",
    "section": "Languages",
    "text": "Languages\nComputer languages come with all of the ‘baggage’ of human languages; they have:\n\nA vocabulary (reserved words)\nA grammar (syntax)\nRules about the kinds of things you can say (grammar)\nStyles and idiosyncrasies all their own (history)\n\nIn this module we will use the Python programming language. We could also teach this same content in R."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#python",
    "href": "lectures/2.8-On_Coding.html#python",
    "title": "On Coding",
    "section": "Python",
    "text": "Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count += 1 # An alternative to count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#r",
    "href": "lectures/2.8-On_Coding.html#r",
    "title": "On Coding",
    "section": "R",
    "text": "R\nnumbers = c(1, 4, 7, 6, 4, 2)\ntotal   = 0\ncount   = 0\nfor (num in numbers) {\n  total = total + num\n  count = count + 1\n}\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#finally-style",
    "href": "lectures/2.8-On_Coding.html#finally-style",
    "title": "On Coding",
    "section": "Finally: Style",
    "text": "Finally: Style\nAlthough all programmers develop their own style (see: writing in any language), Python encourages coders to use a consistent style so that others can pick up your code and make sense of what’s going on (see: comments!).\nTwo useful resources:\n\nThe Hitchhiker’s Guide to Python\nA summary of Python code style conventions"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#additional-resources",
    "href": "lectures/2.8-On_Coding.html#additional-resources",
    "title": "On Coding",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhich programming language should I use? A guide for early-career researchers\n“I mostly had one big, ugly, long, unreadable script”\nFinding your scientific story by writing backwards"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-sense-of-this",
    "href": "lectures/3.2-LOLs.html#making-sense-of-this",
    "title": "LoLs",
    "section": "Making Sense of This",
    "text": "Making Sense of This\nWe can ‘unpack’ my_list in stages in order to make sense of it:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nfor i in my_list:\n  print(i)\nWhat do you think this will print?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "href": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "title": "LoLs",
    "section": "Debugging Our Thinking",
    "text": "Debugging Our Thinking\nLet’s make it a little more obvious:\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [7, 8, 9]\n\nmy_list = [a, b, c]\n\nfor i in my_list:\n  print(i) # Prints a, b, c in turn..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#the-next-step",
    "href": "lectures/3.2-LOLs.html#the-next-step",
    "title": "LoLs",
    "section": "The Next Step",
    "text": "The Next Step\nWe could then try this:\nfor i in my_list:\n  print(f\" &gt;&gt; {i}\")\n  for j in i: # Remember that i is a list!\n    print(j)\nThis produces:\n &gt;&gt; [1, 2, 3]\n1\n2\n3\n &gt;&gt; [4, 5, 6]\n4\n..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#putting-it-together",
    "href": "lectures/3.2-LOLs.html#putting-it-together",
    "title": "LoLs",
    "section": "Putting It Together",
    "text": "Putting It Together\nSome observations:\n\nWe can access i in my_list using either for i in my_list (every element in turn) or my_list[i] (one element only).\nWe can access j in list i using for j in i (every element in turn) or i[j] (one element only).\n\nDoes that mean we can also do this:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\ni,j = 0,1\nprint(my_list[i][j])"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "href": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "title": "LoLs",
    "section": "Let’s Talk It Out!",
    "text": "Let’s Talk It Out!\nSo if we write:\ni,j = 0,1\nprint(my_list[i][j])\nThen:\n\nmy_list[i] returns [1,2,3] (because i==0 and the first list is [1,2,3]), and\nmy_list[i][j] returns 2 (because j==1 and the [1,2,3][1]==2).\n\nSimilarly, my_list[2] grabs the third list ([7,8,9]) and then my_list[2][2] tells Python to get the third item in that third list (i.e. 9).\n\nHow you print the number 5 from this list-of-lists?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-this-useful",
    "href": "lectures/3.2-LOLs.html#making-this-useful",
    "title": "LoLs",
    "section": "Making This Useful",
    "text": "Making This Useful\nIf I rewrite the list this way perhaps it looks a little more useful?\nmy_cities = [\n  ['London', 51.5072, 0.1275, +0], \n  ['New York', 40.7127, 74.0059, -5], \n  ['Tokyo', 35.6833, 139.6833, +8]\n]\nNow we have something that is starting to look like data!"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "href": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "title": "LoLs",
    "section": "Down the Rabbit Hole",
    "text": "Down the Rabbit Hole"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lols-of-lols",
    "href": "lectures/3.2-LOLs.html#lols-of-lols",
    "title": "LoLs",
    "section": "LOLs of LOLs",
    "text": "LOLs of LOLs\nThis is also a legitimate list in Python.\nmy_cities = [\n  ['London', [51.5072, 0.1275], +0], \n  ['New York', [40.7127, 74.0059], -5], \n  ['Tokyo', [35.6833, 139.6833], +8]\n]\nprint(my_cities[0][0])\n&gt; London\nprint(my_cities[0][1][0])\n&gt; 51.5072\n\nWhy might it be a better choice of data structure than the earlier version?"
  },
  {
    "objectID": "lectures/3.4-Git.html#how-it-works",
    "href": "lectures/3.4-Git.html#how-it-works",
    "title": "Getting to Grips with Git",
    "section": "How It Works",
    "text": "How It Works\nThe natural way normal people think about managing versions of a document is to save a copy with a new name that somehow shows which version is most recent.\nThe natural way developers used to think about managing versions of a document is to have a master copy somewhere. Everyone asks the server for the master copy, makes some changes, and then checks those changes back in.\nThis is not how Git works.\n\nThe way normal people approach this problem assumes that, usually, only one or two people are making changes. But how do you coordinate with 20 other people to find out who has the most recent copy then collect all 21 people’s changes?\nThe way developers used to approach this problem assumes that someone is in final charge. That a company or organisation runs a server which will decide whose changes are allowed, and whose are not."
  },
  {
    "objectID": "lectures/3.4-Git.html#how-git-works",
    "href": "lectures/3.4-Git.html#how-git-works",
    "title": "Getting to Grips with Git",
    "section": "How Git Works",
    "text": "How Git Works\nGit is distributed, meaning that every computer where git is installed has its own master copy.\nSo every computer has a full history of any git project (aka. repository or ‘repo’). Indeed, you don’t have to synchronise your repo with any other computer or server at all! 1\n\nIn order to make this useful, you need ways to synchronise changes between computers that all think they’re right.\n\nI’d suggest that this is leaving the benefit of free backups on the table for no good reason!"
  },
  {
    "objectID": "lectures/3.4-Git.html#github",
    "href": "lectures/3.4-Git.html#github",
    "title": "Getting to Grips with Git",
    "section": "GitHub",
    "text": "GitHub\nGitHub is nothing special to Git, just another Git server with which to negotiate changes. Do not think of GitHub as the ‘master’ copy. There isn’t one.\nThere are, however, upstream and remote repositories.\n\nAn ‘upstream’ repository is where there’s a ‘gatekeeper’: e.g. the people who run PySAL have a repo that is considered the ‘gatekeeper’ for PySAL.\nA remote repository is any repository with which your copy synchronises. So the remote repository can be ‘upstream’ or it can just be another computer you run, or you GitHub account."
  },
  {
    "objectID": "lectures/3.4-Git.html#a-dropbox-analogy",
    "href": "lectures/3.4-Git.html#a-dropbox-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Dropbox Analogy",
    "text": "A Dropbox Analogy\n\nThink of JupyterLab as being like Word or Excel: an application that allows you to read/write/edit notebook files.\nThink of GitHub as being like Dropbox: a place somewhere in the cloud that files on your home machine can be backed up.\n\nBut Dropbox doesn’t have the .gitignore file and fine-grained control over what is synchronised when!\n\nGitHub offers a lot of ‘value added’ features (like simple text editing) on top of the basic service of ‘storing files’."
  },
  {
    "objectID": "lectures/3.4-Git.html#getting-started",
    "href": "lectures/3.4-Git.html#getting-started",
    "title": "Getting to Grips with Git",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\nTerm\nMeans\n\n\n\n\nRepository (Repo)\nA project or achive stored in Git.\n\n\ninit\nTo create a new repo on your computer.\n\n\nclone\nTo make a full copy of a repo somewhere else.\n\n\n\nThis creates a local repo that is unsynchronised with anything else:\nmkdir test\ncd test\ngit init\nWhereas this creates a local clone that is fully synchronised with GitHub:\ncd .. # To move out of 'test'\ngit clone https://github.com/jreades/fsds.git"
  },
  {
    "objectID": "lectures/3.4-Git.html#working-on-a-file",
    "href": "lectures/3.4-Git.html#working-on-a-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a File",
    "text": "Working on a File\n\n\n\nTerm\nMeans\n\n\n\n\nadd\nAdd a file to a repo.\n\n\nmv\nMove/Rename a file in a repo.\n\n\nrm\nRemove a file from a repo.\n\n\n\nFor example:\ncd test # Back into the new Repo\ntouch README.md # Create empty file called README.md\ngit add README.md # Add it to the repository\ngit mv README.md fileA.md # Rename it (move it)\ngit rm fileA.md # Remove it... which is an Error!\nThis produces:\nerror: the following file has changes staged in the index:\n    fileA.md\n(use --cached to keep the file, or -f to force removal)\n\nThis is telling you that you can force remove (git rm -f fileA.md) if you really want, but you’d probably be better off commiting the changes that have been ‘staged’… more on this in a second!\nAlso: no one else knows about these changes yet!"
  },
  {
    "objectID": "lectures/3.4-Git.html#looking-at-the-history",
    "href": "lectures/3.4-Git.html#looking-at-the-history",
    "title": "Getting to Grips with Git",
    "section": "Looking at the History",
    "text": "Looking at the History\n\n\n\nTerm\nMeans\n\n\n\n\ndiff\nShow changes between commits.\n\n\nstatus\nShow status of files in repo.\n\n\nlog\nShow history of commits.\n\n\n\nFor example:\ncd ../test/ # In case you weren't already there\ngit status  # What's the status\nThis produces:\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   fileA.md\n\nSo again, git is giving us hints as to the options: ‘changes to be committed’ vs. ‘unstage’ the changes. We can also see what files are to be committed (i.e. have changed)."
  },
  {
    "objectID": "lectures/3.4-Git.html#working-on-a-project-or-file",
    "href": "lectures/3.4-Git.html#working-on-a-project-or-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a Project or File",
    "text": "Working on a Project or File\n\n\n\nTerm\nMeans\n\n\n\n\ncommit\nTo record changes to the repo.\n\n\nbranch\nCreate or delete branches.\n\n\ncheckout\nJump to a different branch.\n\n\n\nFor example:\ngit commit -m \"Added and then renamed the README.\"\ngit status\nYou should see:\n[master (root-commit) e7a0b25] Added and then renamed the README Markdown file.\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 fileA.md\n# ... and then this:\nOn branch master\nnothing to commit, working tree clean\nMake a note of the number after ‘root-commit’!"
  },
  {
    "objectID": "lectures/3.4-Git.html#recovery",
    "href": "lectures/3.4-Git.html#recovery",
    "title": "Getting to Grips with Git",
    "section": "Recovery",
    "text": "Recovery\ngit rm fileA.md\ngit status\ngit commit -m \"Removed file.\"\nls \ngit checkout &lt;number you wrote down earlier&gt;\nls \n\nSo every operation on a file is recorded in the repository: adding, renaming, deleting, and so on. And we can roll back any change at any time. For plain-text files (such as Markdown, Python and R scripts) these changes are recorded at the level of each line of code: so you can jump around through your entire history of a project and trace exactly when and what changes you (or anyone else) made."
  },
  {
    "objectID": "lectures/3.4-Git.html#a-rock-climbing-analogy",
    "href": "lectures/3.4-Git.html#a-rock-climbing-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Rock-Climbing Analogy",
    "text": "A Rock-Climbing Analogy"
  },
  {
    "objectID": "lectures/3.4-Git.html#collaborating-on-a-project",
    "href": "lectures/3.4-Git.html#collaborating-on-a-project",
    "title": "Getting to Grips with Git",
    "section": "Collaborating on a Project",
    "text": "Collaborating on a Project\n\n\n\nTerm\nMeans\n\n\n\n\npull\nTo request changes on a repo from another computer.\n\n\npush\nTo send changes on a repo to another computer.\n\n\n\nFor example:\ngit push"
  },
  {
    "objectID": "lectures/3.4-Git.html#a-note-on-workflow",
    "href": "lectures/3.4-Git.html#a-note-on-workflow",
    "title": "Getting to Grips with Git",
    "section": "A Note on Workflow",
    "text": "A Note on Workflow\nSo your workflow should be:\n\nSave edits to Jupyter notebook.\nRun git add &lt;filename.ipynb&gt; to record changes to the notebook (Note: replace &lt;filename.ipynb&gt; completely with the notebook filename).\nRun git commit -m \"Adding notes based on lecture\" (or whatever message is appropriate: -m means ‘message’).\nThen run git push to push the changes to GitHub.\n\nIf any of those commands indicate that there are no changes being recorded/pushed then it might be that you’re not editing the file that you think you are (this happens to me!).\nOn the GitHub web site you may need to force reload the view of the repository: Shift + the Reload button usually does it in most browsers. You may also need to wait 5 to 10 seconds for the changes to become ‘visible’ before reloading. It’s not quite instantaeous."
  },
  {
    "objectID": "lectures/3.4-Git.html#this-is-not-easy",
    "href": "lectures/3.4-Git.html#this-is-not-easy",
    "title": "Getting to Grips with Git",
    "section": "This is not easy",
    "text": "This is not easy\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/3.4-Git.html#so-here-are-some-cheat-sheets",
    "href": "lectures/3.4-Git.html#so-here-are-some-cheat-sheets",
    "title": "Getting to Grips with Git",
    "section": "So here are some cheat sheets",
    "text": "So here are some cheat sheets"
  },
  {
    "objectID": "lectures/3.4-Git.html#additional-resources",
    "href": "lectures/3.4-Git.html#additional-resources",
    "title": "Getting to Grips with Git",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nUnderstanding Git (Part 1) – Explain it Like I’m Five\nTrying Git\nVisualising Git\nGit Novice\nConfusing Git Terminology\nGit Cheat Sheet: Commands and Best Practices\nAndy’s R-focussed Tutorial\n\n\nI now have everything in Git repos: articles, research, presentations, modules… the uses are basically endless once you start using Markdown heavily (even if you don’t do much coding)."
  },
  {
    "objectID": "lectures/4.2-Decorators.html#a-basic-function",
    "href": "lectures/4.2-Decorators.html#a-basic-function",
    "title": "Decorators",
    "section": "A Basic Function",
    "text": "A Basic Function\n\ndef hello():\n  return('hello world')\n\nprint(hello())\n\nhello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#modifying-the-function",
    "href": "lectures/4.2-Decorators.html#modifying-the-function",
    "title": "Decorators",
    "section": "Modifying the Function",
    "text": "Modifying the Function\nLet’s get meta:\n\ndef better():\n  def hello():\n    print(\"  + Defining hello!\")\n    return('hello world')\n  print(\"+ Calling hello...\")\n  rv = hello().title() + \"!\"\n  print(f\"+ hello returned {rv}\")\n  return(rv)\n\nbetter()\n\n+ Calling hello...\n  + Defining hello!\n+ hello returned Hello World!\n\n\n'Hello World!'"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#decorating-the-function",
    "href": "lectures/4.2-Decorators.html#decorating-the-function",
    "title": "Decorators",
    "section": "Decorating the Function",
    "text": "Decorating the Function\n\ndef better(func):\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())\n\nHello World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#wtf",
    "href": "lectures/4.2-Decorators.html#wtf",
    "title": "Decorators",
    "section": "WTF?",
    "text": "WTF?\nHow did that happen?\n\ndef better(func):\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#the-decorator-function",
    "href": "lectures/4.2-Decorators.html#the-decorator-function",
    "title": "Decorators",
    "section": "The Decorator Function",
    "text": "The Decorator Function\nLet’s unpack the wrapper:\n\ndef better(func):\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#reusing-a-decorator",
    "href": "lectures/4.2-Decorators.html#reusing-a-decorator",
    "title": "Decorators",
    "section": "Reusing a Decorator",
    "text": "Reusing a Decorator\nEverything’s ‘better’ now:\n\n@better\ndef goodbye():\n  return('GooDBye world')\n\nprint(goodbye())\n\nGoodbye World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#but",
    "href": "lectures/4.2-Decorators.html#but",
    "title": "Decorators",
    "section": "But…",
    "text": "But…\nBut this:\n\n@better\ndef bad_func():\n  return(2)\n\nprint(bad_func())\n\nWill trigger this:\n      2 def wrapper():\n----&gt; 3   return func().title() + \"!\"\n\nAttributeError: 'int' object has no attribute 'title'"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#chaining-decorators",
    "href": "lectures/4.2-Decorators.html#chaining-decorators",
    "title": "Decorators",
    "section": "Chaining Decorators1",
    "text": "Chaining Decorators1\n\ndef splitter(func):\n  def wrapper():\n    return func().split()\n  return wrapper\n\n@splitter\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())\n\n['Hello', 'World!']\n\n\nDatacamp"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#using-functiontools",
    "href": "lectures/4.2-Decorators.html#using-functiontools",
    "title": "Decorators",
    "section": "Using Func(tion)Tools",
    "text": "Using Func(tion)Tools\nAnd there are decorators for decorators…\n\nfrom functools import wraps\ndef better(func):\n  @wraps(func)\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())\n\nHello World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#unpicking-functools",
    "href": "lectures/4.2-Decorators.html#unpicking-functools",
    "title": "Decorators",
    "section": "Unpicking Functools",
    "text": "Unpicking Functools\n\nfrom functools import wraps\ndef better(func):\n  @wraps(func)\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n..."
  },
  {
    "objectID": "lectures/4.2-Decorators.html#making-use-of-metadata",
    "href": "lectures/4.2-Decorators.html#making-use-of-metadata",
    "title": "Decorators",
    "section": "Making Use of Metadata",
    "text": "Making Use of Metadata\nCompare:\n\n\n\ndef better(func):\n  '''Better formatting of a string'''\n  def wrapper():\n    '''Wraps a function to format it.'''\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  '''Prints hello world'''\n  return('hello world')\n\nprint(hello.__name__)\nprint(hello.__doc__)\n\nwrapper\nWraps a function to format it.\n\n\n\n\nfrom functools import wraps\ndef better(func):\n  @wraps(func)\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  '''Prints hello world'''\n  return('hello world')\n\nprint(hello.__name__)\nprint(hello.__doc__)\n\nhello\nPrints hello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#some-applications-1",
    "href": "lectures/4.2-Decorators.html#some-applications-1",
    "title": "Decorators",
    "section": "Some Applications 11",
    "text": "Some Applications 11\n\ndef simple_logger(func):\n  def wrapper(*args, **kwargs):\n    print(f\"+ Executing '{func.__name__}' with args: {args}\")\n    result = func(*args, **kwargs)\n    print(f\"  + Result is: {result}\")\n    return result\n  return wrapper\n\n@simple_logger\ndef add(a,b):\n  return a+b\n\nadd(2,5)\n\n+ Executing 'add' with args: (2, 5)\n  + Result is: 7\n\n\n7\n\n\nKDNuggets: 8 Built-in Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#some-applications-2",
    "href": "lectures/4.2-Decorators.html#some-applications-2",
    "title": "Decorators",
    "section": "Some Applications 21",
    "text": "Some Applications 21\n\nimport atexit\n\n# Register the exit_handler function\n@atexit.register\ndef exit_handler():\n    print(\"Exiting the program. Cleanup tasks can be performed here.\")\n\n# Rest of the program\ndef main():\n    print(\"Inside the main function.\")\n    # Your program logic goes here.\n\nif __name__ == \"__main__\":\n    main()\n\nThis would output:\nInside the main function.\nExiting the program. Cleanup tasks can be performed here.\nKDNuggets: 8 Built-in Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#benefits-of-decorators",
    "href": "lectures/4.2-Decorators.html#benefits-of-decorators",
    "title": "Decorators",
    "section": "Benefits of Decorators1",
    "text": "Benefits of Decorators1\n\nCode readability: each function is more narrowly focussed on the ‘thing’ it’s supposed to do, without extraneous validation, logging, or authentication ‘cruft’.\nCode reuse: keep your code ‘DRY’ (Don’t Repeat Yourself) by applying the same code across multiple functions (e.g. log the arguments this function received)\nModification without alteration: extending the behaviour of something else (e.g. you can’t/don’t want to modify someone else’s code, but need some additional step to be performend)\nLogging made simple: add/remove debugging and logging functionality quickly, easily, and consistently.\nDevelopment: Python web frameworks (Django, Flask) use decorators to handle requests.\nError handling: manage error-handling ‘centrally’ by placing try and except around every function you want to manage.\n\n5 Key Benefits of Using Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#theres-more",
    "href": "lectures/4.2-Decorators.html#theres-more",
    "title": "Decorators",
    "section": "There’s More…",
    "text": "There’s More…\nWe’ve barely scratched the surface, decorators can:\n\nTake arguments (which might alter the behavour of the wrapped function).\nHelp to make classes and methods more useful (see the Methods and Classes lectures).\nManage common tasks like authorisation and permissions.\n\nThere’s lots, lost more, but using decorators effectively will seriously impress anyone interviewing you for a job while also helping you to understand a lot more about good programming!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#additional-resources",
    "href": "lectures/4.2-Decorators.html#additional-resources",
    "title": "Decorators",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nSpecial /ht to @wraps Explained in 30 seconds\nPython Decorators Explained\nPython Tips: Decorators\nPython Decorators: How to Use it and Why?\n5 Key Benefits of Using Python Decorators for Optimized COding Practices\n\n\n\nPython’s Most Powerful Decorator\nChaining Decorators\nDecorators and Generators in Python\nAvailable Standard Decorators in Python\nAwesome Python Decorators"
  },
  {
    "objectID": "lectures/4.4-Assessments.html#organise-your-group",
    "href": "lectures/4.4-Assessments.html#organise-your-group",
    "title": "Assessments",
    "section": "Organise (Your Group)",
    "text": "Organise (Your Group)\nI am working to allocate everyone to a group of at least four so:\n\nIf you are in a group of less than four you may be partnered with students in another practical session to make a ‘full strength’ group.\nIn this case, you will be reallocated to a mutually convenient practical session so that you are not penalised by being unable to work together.\nI would encourage you to start sitting and working together, and for one of you to set up and share a private GitHub repo for Assessment #2 (more on this in a second).\n\nAnd remember: talk about how you want your group to work! See: Group Working talk.1\nNote: In my experience overly narrow specialisation does not work."
  },
  {
    "objectID": "lectures/4.4-Assessments.html#assessment-1",
    "href": "lectures/4.4-Assessments.html#assessment-1",
    "title": "Assessments",
    "section": "Assessment #1",
    "text": "Assessment #1\n1. What is it?\nTimed, Open Book Exam\n2. What does ‘timed’ mean?\nOnce you start the assessment you will have a fixed amount of time in which to complete it; 1h 20m for most students, but students with relevant SORAs have 2 hours (please email me).\nOk, so when is it?\nFriday, 21 November 2025\nWhat is the window?\n11am-11pm or 1pm-3pm1\nThis is the ‘window’ for doing the assessment. You can start any time after the window opens but must finish before the window closes."
  },
  {
    "objectID": "lectures/4.4-Assessments.html#assessment-2",
    "href": "lectures/4.4-Assessments.html#assessment-2",
    "title": "Assessments",
    "section": "Assessment #2",
    "text": "Assessment #2\nThe reproducible analysis must be a runnable QMD (Quarto Markdown Document) file that addresses the set questions provided in class. The QMD file will be assessed on two components:\n\nIts content (60% of this assessment): do the answers written by the group engage through a mix of literature, critical thinking, and data analysis with the set questions?\nIts reproducibility (40% of this assessment): do the analyses employed, and outputs created by the group run fully and without errors on a different computer, and do they show evidence of thought in relation to the quality of coding and outputs?\n\nA template has been provided. You can see both PDF and HTML output, but please only submit the PDF!"
  },
  {
    "objectID": "lectures/4.4-Assessments.html#assessment-3",
    "href": "lectures/4.4-Assessments.html#assessment-3",
    "title": "Assessments",
    "section": "Assessment #3",
    "text": "Assessment #3\nWe know that people contribute differently to groups or, sometimes, not at all. This self- and peer-assessment seeks to quantify that contribution while also prompting you to reflect on what you contributed to your group."
  },
  {
    "objectID": "lectures/5.1-Methods.html#todays-question",
    "href": "lectures/5.1-Methods.html#todays-question",
    "title": "Methods",
    "section": "Today’s Question",
    "text": "Today’s Question\nWe know that a function looks like this:\n&lt;function name&gt;( &lt;input&gt; )\nAnd we know that a function in a package looks like this:\n&lt;package name&gt;.&lt;function name&gt;( &lt;input&gt; )\nSo is list a package?\nmy_list.append( &lt;value&gt; )"
  },
  {
    "objectID": "lectures/5.1-Methods.html#well",
    "href": "lectures/5.1-Methods.html#well",
    "title": "Methods",
    "section": "Well…",
    "text": "Well…\nmy_list.append( &lt;value&gt; ) is a function.\nmy_list.append( &lt;value&gt; ) is a special type of function called a method."
  },
  {
    "objectID": "lectures/5.1-Methods.html#whats-a-method-then",
    "href": "lectures/5.1-Methods.html#whats-a-method-then",
    "title": "Methods",
    "section": "What’s a Method Then?",
    "text": "What’s a Method Then?\nPackages group useful constants and functions together in one place.\nMethods group constants and functions together in one place with data.\nSo my_list.append(...) is called a list method:\n\nIt only knows how to append things to lists.\nIt is only available as a function when you have an insantiated list (e.g. [] or [1,'dog',3.5]).\nIt is bound to variables (aka. objects) of class list."
  },
  {
    "objectID": "lectures/5.1-Methods.html#proof",
    "href": "lectures/5.1-Methods.html#proof",
    "title": "Methods",
    "section": "Proof!",
    "text": "Proof!\nmy_list = [] # \nhelp(my_list)\nThis will give you:\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  Built-in mutable sequence.\n |  If no argument is given, the constructor creates a new empty list.\n |  Methods defined here:\n | ...\n |  append(self, object, /)\n |      Append object to the end of the list.\n |\n |  clear(self, /)\n |      Remove all items from list.\n |\n |  copy(self, /)\n |      Return a shallow copy of the list.\n | ...\n\nIt’s not obvious here, but you can also create lists by writing list()."
  },
  {
    "objectID": "lectures/5.1-Methods.html#its-all-methods",
    "href": "lectures/5.1-Methods.html#its-all-methods",
    "title": "Methods",
    "section": "It’s all Methods",
    "text": "It’s all Methods\nmsg = 'Hello World'\ndir(msg)\n['__add__', '__class__', ..., 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', ... 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\nAnd then we can inquire about these methods:\nhelp(msg.capitalize)\nHelp on built-in function capitalize:\n\ncapitalize() method of builtins.str instance\n    Return a capitalized version of the string.\n\n    More specifically, make the first character have upper case and the rest lower\n    case."
  },
  {
    "objectID": "lectures/5.1-Methods.html#final-fundamental-concepts",
    "href": "lectures/5.1-Methods.html#final-fundamental-concepts",
    "title": "Methods",
    "section": "Final Fundamental Concepts",
    "text": "Final Fundamental Concepts\nFrom here on out, nearly all of what you learn will be new applications, not new concepts and terminology.\n\n\n\n\n\n\n\n\nTerm\nMeans\nExample\n\n\n\n\nClass\nThe template for a ‘thing’.\nRecipe for a pizza.\n\n\nObject\nThe instantiated ‘thing’.\nA pizza I can eat!\n\n\nMethod\nFunctions defined for the class and available to the object.\nThings I can do with a pizza (eat, cook, make).\n\n\nConstructor\nThe special method that builds new objects of that class.\nHow to start a new pizza!\n\n\nSelf\nA reference to the current object.\nThe pizza in front of me!"
  },
  {
    "objectID": "lectures/5.3-Design.html#its-a-choice",
    "href": "lectures/5.3-Design.html#its-a-choice",
    "title": "Object-Oriented Design",
    "section": "It’s a Choice",
    "text": "It’s a Choice\n\n\nMost things that computers do have both functional and object-oriented representations. It’s a choice set by the language and developer. In Python, for example, there has been a shift from os.path (functional) to pathlib (OO)."
  },
  {
    "objectID": "lectures/5.3-Design.html#tree-of-life",
    "href": "lectures/5.3-Design.html#tree-of-life",
    "title": "Object-Oriented Design",
    "section": "Tree of Life",
    "text": "Tree of Life"
  },
  {
    "objectID": "lectures/5.3-Design.html#tree-of-vehicles",
    "href": "lectures/5.3-Design.html#tree-of-vehicles",
    "title": "Object-Oriented Design",
    "section": "Tree of Vehicles",
    "text": "Tree of Vehicles\nMost people would call this a class hierarchy or diagram.\n\nThere is no natural order here: where do e-bikes, unicycles, and rickshaws go?\n\nIndeed, we could map out vehicles based on the number of axles, their source of power, their driver positioning, etc., etc. The point here is that the designer must make choices that are influenced by (and will influence) the design of the application for which this class hierarchy is buing developed. An automobile manufacturer might make different choices from a government trying to implement a tax policy."
  },
  {
    "objectID": "lectures/5.3-Design.html#classes-vs-packages",
    "href": "lectures/5.3-Design.html#classes-vs-packages",
    "title": "Object-Oriented Design",
    "section": "Classes vs Packages",
    "text": "Classes vs Packages\n\nFunctionally, a class and a package are indistinguishable, but a class produces objects that use methods and instance or class variables, whereas a package is a group of functions and constants that may, or may not, include classes.\n\nUgh, now try to keep this straight in your head."
  },
  {
    "objectID": "lectures/5.3-Design.html#key-takeaways",
    "href": "lectures/5.3-Design.html#key-takeaways",
    "title": "Object-Oriented Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nYou’ve been using Classes and Methods since you started.\nYou can ‘package up’ useful code into functions, and useful functions into packages.\nTogether, packages and classes will turbo-charge your programming skills.\nYou can stand on the shoulders of giants!"
  },
  {
    "objectID": "lectures/5.3-Design.html#additional-resources",
    "href": "lectures/5.3-Design.html#additional-resources",
    "title": "Object-Oriented Design",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhat is object-oriented programming?\nPython object-oriented programming\nObject-oriented programming refresher\nUnderstanding inheritance\nAbstract base classes\nPython - Object-Oriented"
  },
  {
    "objectID": "lectures/6.1-Logic.html#do-you-want",
    "href": "lectures/6.1-Logic.html#do-you-want",
    "title": "Logic",
    "section": "Do You Want?",
    "text": "Do You Want?\n\n\nConsider the following three things I’ve said to my toddler:\n\nDo you want to go to the park and have ice cream?\nDo you want cereal or toast for breakfast?\nDo not touch that again or else!\n\nUnlike my toddler, the computer does listen, but my toddler has more common sense!"
  },
  {
    "objectID": "lectures/6.1-Logic.html#in-code-now",
    "href": "lectures/6.1-Logic.html#in-code-now",
    "title": "Logic",
    "section": "In Code Now",
    "text": "In Code Now\n(x,y) = True,False\nif x:\n  print(\"x\")\n\nif x and y:\n  print(\"x and y\")\n\nif x or y:\n  print(\"x or y\")\n\nif x and not y:\n  print(\"x and not y\")\n\nif not(x and y):\n  print(\"not x and y\")\n\nif x is the simplest: if True then print \"x\". There is no logic, we just do it or we don’t depending on whether x is True.\nif x and y is the ‘park and ice cream question’: there’s no ice cream without going to the park. So both things have to be True for this to work out happily and print(\"x and y\").\nif x or y is the ‘cereal or toast for breakfast’ question: my daughter is having breakfast either way, so if she answers ‘Yes’ (True) to either then it doesn’t matter that the other one is False, she’s still had breakfast, or we’re still printing “x or y”. And note, if she was really hungry and said yes to both then that would also be breakfast. So if they are both True that’s also fine.\nif x and not y: is the negation of one term. So ‘if you behave and don’t touch that again then you will get a treat’ is what we’re looking for here. But you can also negate the entire thing: not(x and y) is also valid logic. So this turns a False on if x and y into a True."
  },
  {
    "objectID": "lectures/6.1-Logic.html#combining-logic-with-operators",
    "href": "lectures/6.1-Logic.html#combining-logic-with-operators",
    "title": "Logic",
    "section": "Combining Logic With Operators",
    "text": "Combining Logic With Operators\nRemember that operators like &lt;= and == also produce True/False answers:\nx = y = 5\nz = 3\nif x==y:\n  print(\"x==y\")\n\nif x==y and x==z:\n  print(\"x==y and x==z\")\n\nif x==y or x==z:\n  print(\"x==y or x==z\")\n\nif x==y and not x==z:\n  print(\"x==y and not x==z\")"
  },
  {
    "objectID": "lectures/6.1-Logic.html#a-special-case",
    "href": "lectures/6.1-Logic.html#a-special-case",
    "title": "Logic",
    "section": "A Special Case",
    "text": "A Special Case\nThere is a second set of logical operators that apply in very specific circumstances. These are called ‘bitwise’ operators and apply to data specified in bits.\n\n\n\nRegular Operator\nBitwise Equivalent\n\n\n\n\nand\n&\n\n\nor\n|\n\n\nnot\n~\n\n\n\nLet’s see (briefly) how these work…"
  },
  {
    "objectID": "lectures/6.1-Logic.html#working-with-bits",
    "href": "lectures/6.1-Logic.html#working-with-bits",
    "title": "Logic",
    "section": "Working With Bits",
    "text": "Working With Bits\nx,y = 38,3\nprint(f\"{x:b}\") # `:b` means byte-format\nprint(f\"{y:b}\")\nThis gives us that x is '100110' and y is '11', so now:\nprint(f\"{x & y:b}\")  # 10\nprint(f\"{x | y:b}\")  # 100111\nprint(f\"{x & ~y:b}\") # 100100"
  },
  {
    "objectID": "lectures/6.1-Logic.html#perhaps-easier-to-see-this-way",
    "href": "lectures/6.1-Logic.html#perhaps-easier-to-see-this-way",
    "title": "Logic",
    "section": "Perhaps Easier to See This Way?",
    "text": "Perhaps Easier to See This Way?\n\n\n\nOperator\n1\n2\n3\n4\n5\n6\n\n\n\n\nx\n1\n0\n0\n1\n1\n0\n\n\ny\n0\n0\n0\n0\n1\n1\n\n\nx & y\n0\n0\n0\n0\n1\n0\n\n\nx | y\n1\n0\n0\n1\n1\n1\n\n\n~y\n1\n1\n1\n1\n0\n0\n\n\nx & ~y\n1\n0\n0\n1\n0\n0\n\n\n\nBitwise operations are very, very fast and so are a good way to, say, find things in large data sets. You’ve been warned.\n\nThey are how pandas and numpy manage indexes and queries against data frames."
  },
  {
    "objectID": "lectures/6.1-Logic.html#nulls-none-vs.-nan",
    "href": "lectures/6.1-Logic.html#nulls-none-vs.-nan",
    "title": "Logic",
    "section": "Nulls: None vs. NaN",
    "text": "Nulls: None vs. NaN\nBeware of using logic with things that are not what they appear:\n\nNone is Python’s way of saying that something has no value at all (not 0 or \"\"… but None). It is a class.\nNaN (Not a Number) is a special numeric data type provided by the numpy package to deal with things like -ve and +ve infinity and similar ‘issues’.\n\nnp.nan should be used whenever you are dealing with data (e.g. see Pandas!)."
  },
  {
    "objectID": "lectures/6.1-Logic.html#none-vs.-nan",
    "href": "lectures/6.1-Logic.html#none-vs.-nan",
    "title": "Logic",
    "section": "None vs. NaN",
    "text": "None vs. NaN\nimport numpy as np\nprint(type(np.nan))     # float\nprint(type(None))       # NoneType\nCritically:\nprint(\"\"==None)         # False\nprint(None==None)       # True\nprint(np.nan==None)     # False\nprint(np.nan==np.nan)   # False!\nprint(np.nan is np.nan) # True\nprint(np.isnan(np.nan)) # True"
  },
  {
    "objectID": "lectures/6.1-Logic.html#in-not-in",
    "href": "lectures/6.1-Logic.html#in-not-in",
    "title": "Logic",
    "section": "In / Not In",
    "text": "In / Not In\nWe’ve touched on these before:\ng = ['Harvey','Rose','Batty','Jefferson']\n\nif 'Batty' in g:\n  print(\"In the group!\")\n\nif 'Marx' not in g:\n  print(\"Not in the group!\")\nThe set data type also supports in, and not in together with all of the set maths (union, intersect, etc.).\n\nThis is a good place for a recap though."
  },
  {
    "objectID": "lectures/6.1-Logic.html#sets",
    "href": "lectures/6.1-Logic.html#sets",
    "title": "Logic",
    "section": "Sets",
    "text": "Sets\nMembership maths:\ns1 = {'cherry','orange','banana','tomato'} # Or s1(...)\ns2 = {'potato','celery','carrot','tomato'} # Or s2(...)\nprint('potato' in s1)      # False\nprint(s1.difference(s2))   # {'banana', ...}\nprint(s1.intersection(s2)) # {'tomato'}\nprint(s1.union(s2))        # {'orange', ...}\n\nThese more advanced functions are only for sets, not lists or dicts."
  },
  {
    "objectID": "lectures/6.1-Logic.html#additional-resources",
    "href": "lectures/6.1-Logic.html#additional-resources",
    "title": "Logic",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nLogical operators: And, or, not\nComparison operators\nBitwise operators\nComparison operators\n\n\n\nBoolean operators\nOperator precedence\nNaN and None in Python\nHandling Missing Data"
  },
  {
    "objectID": "lectures/6.3-Files.html#from-files-to-data",
    "href": "lectures/6.3-Files.html#from-files-to-data",
    "title": "Data Formats",
    "section": "From Files to Data",
    "text": "From Files to Data\nIn order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data set have row or column names? (a.k.a. headers & metadata)\nIs the metadata in a separate file or embedded in the file?"
  },
  {
    "objectID": "lectures/6.3-Files.html#structure-of-a-tabular-data-file",
    "href": "lectures/6.3-Files.html#structure-of-a-tabular-data-file",
    "title": "Data Formats",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names make it a lot easier to find and refer to data (e.g. the ‘East of England row’ or the ‘Total column’) but they are not data and don’t belong in the data set itself.\nUsually, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ’carriage return (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nUsually, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,) which gives rise to CSV (Comma-Separate Values), but it could be tabs (\\t) or anything else too (; or | or EOF).\n\nHow would we choose a good field separator?\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/6.3-Files.html#most-common-formats",
    "href": "lectures/6.3-Files.html#most-common-formats",
    "title": "Data Formats",
    "section": "Most Common Formats",
    "text": "Most Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/6.3-Files.html#mapping-data-types",
    "href": "lectures/6.3-Files.html#mapping-data-types",
    "title": "Data Formats",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often see the term ‘mapping’ used in connection to data that is not spatial, what do they mean? A map is the term used in some programming languages for a dict! So it’s about key : value pairs again.\nHere’s a mapping\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/6.3-Files.html#testing-a-mapping",
    "href": "lectures/6.3-Files.html#testing-a-mapping",
    "title": "Data Formats",
    "section": "Testing a Mapping",
    "text": "Testing a Mapping\nWorking out an appropriate mapping (representation of the data) is hugely time-consuming.\n\nIt’s commonly held that 80% of data science is data cleaning.\n\nThe Unix utilities (grep, awk, tail, head) can be very useful for quickly exploring the data in order to develop a basic understanding of the data and to catch obvious errors.\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/6.3-Files.html#label-these-1",
    "href": "lectures/6.3-Files.html#label-these-1",
    "title": "Data Formats",
    "section": "Label These 1",
    "text": "Label These 1\n\n\nHere’s raw Excel data.\nWhat would we say the row and column names currently are?"
  },
  {
    "objectID": "lectures/6.3-Files.html#label-these-2",
    "href": "lectures/6.3-Files.html#label-these-2",
    "title": "Data Formats",
    "section": "Label These 2",
    "text": "Label These 2\n\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/6.3-Files.html#and-heres-data-for-jaipur-india",
    "href": "lectures/6.3-Files.html#and-heres-data-for-jaipur-india",
    "title": "Data Formats",
    "section": "And Here’s Data for Jaipur, India",
    "text": "And Here’s Data for Jaipur, India\n1\nThis requires the use of hierarchical indexes in pandas."
  },
  {
    "objectID": "lectures/6.3-Files.html#things-that-can-go-wrong",
    "href": "lectures/6.3-Files.html#things-that-can-go-wrong",
    "title": "Data Formats",
    "section": "Things That Can Go Wrong…",
    "text": "Things That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/6.3-Files.html#additional-resources",
    "href": "lectures/6.3-Files.html#additional-resources",
    "title": "Data Formats",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nUnderstanding Directories and Subdirectories\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\n\n\n\nText vs. binary mode\nText files\npetl"
  },
  {
    "objectID": "lectures/7.0-Feedback.html#important-qualification-1",
    "href": "lectures/7.0-Feedback.html#important-qualification-1",
    "title": "Responding to Feedback",
    "section": "Important Qualification 1",
    "text": "Important Qualification 1\nCASA does not get to pick the rooms in which these classes are taught! And we barely have any control at all over timings1.\nFun fact: we have been offered practicals before lectures on occasion."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#important-qualification-2",
    "href": "lectures/7.0-Feedback.html#important-qualification-2",
    "title": "Responding to Feedback",
    "section": "Important Qualification 2",
    "text": "Important Qualification 2\nYou might see your comments here. We are not trying to embarrass anyone. We are trying to improve the course."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#the-good",
    "href": "lectures/7.0-Feedback.html#the-good",
    "title": "Responding to Feedback",
    "section": "The Good",
    "text": "The Good\nI paraphrase:\n\nActivities, reading, and exam relate and the lectures have some fun examples that are easy to follow.\nThere is some reading, followed by questions which makes students engage with the homework.\nPracticals are interactive, with clear tutorials explaining the topic step by step.\nA lot of materials for further learning and organized so that I could practice a lot by myself.\nReally strong PGTAs."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-1",
    "href": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-1",
    "title": "Responding to Feedback",
    "section": "The Bad (and the Ugly) 1",
    "text": "The Bad (and the Ugly) 1\nI paraphrase:\n\nToo much ethics in FSDS so I didn’t have time to practice my coding skills.\nI think it’s not necessary to spend too much time discussing reading materials at class. Instead, we can ask some questions on Slack after class.\nLectures are focused on summarizing the videos and the readings. I would get much more out of them if they were more discussion-based, and we could discuss the readings on a deeper level.\nThe discussion in the lecture is just a very low-level recap of the readings… For those of us that have read and understood the readings, we don’t learn much from the questions being discussed. We could be spending that time on technical elements instead."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-2",
    "href": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-2",
    "title": "Responding to Feedback",
    "section": "The Bad (and the Ugly) 2",
    "text": "The Bad (and the Ugly) 2\nI paraphrase:\n\nThe technical content is very slow and we don’t write a lot of code in the practicals, so I feel like I’m not learning and remembering it very well.\nI’ve tried my best to review and catch up but I don’t know why I still struggle a lot figuring the whole picture. Suddenly the script went from simple into complex really quick.\nI’m quite confused about the content of practicals 1-5. Maybe combining the real data analysis with detailed data learning (objects, function) is better to understand.\nToo much emphasis on Git, Podman/Docker, etc.\nThere was a lot of information about different concepts (Git, Jupyter, Podman/Docker, Python) and it was very overwhelming."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#common-threads",
    "href": "lectures/7.0-Feedback.html#common-threads",
    "title": "Responding to Feedback",
    "section": "Common Threads?",
    "text": "Common Threads?\nStudent feedback is often bimodal and motivated by frustration:\n\nSome students want more technical content. Some find the technical content we have too much.\nSome students want more discussion. Some find the discussion we have too much."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#my-challenge-to-you",
    "href": "lectures/7.0-Feedback.html#my-challenge-to-you",
    "title": "Responding to Feedback",
    "section": "My Challenge to You",
    "text": "My Challenge to You\n\nIf you think the live session is just a recap of the readings, why aren’t you chiming in with your thoughts?\nIf you think that the discussion should ‘go deeper’, why aren’t you asking deeper questions?\nIf you think that Git and Podman/Docker aren’t part of doing data science, why aren’t you pushing us to explain it better?\nIf you are struggling with the practical examples, why aren’t you asking for more examples?"
  },
  {
    "objectID": "lectures/7.0-Feedback.html#my-current-takeaways",
    "href": "lectures/7.0-Feedback.html#my-current-takeaways",
    "title": "Responding to Feedback",
    "section": "My Current Takeaways",
    "text": "My Current Takeaways\nPending additional feedback from you, I am looking at:\n\nResequencing lectures in weeks 1–3 to try to introduce fewer tools at once.\nHaving a chat with Andy to ensure we align Git content and develop understanding across both modules.\nRevisiting weeks 1–5 to see where practicals can be anchored in examples and possibly even introduce a very small version of the InsideAirbnb data set (instead of the city pseudo-data).\nDeveloping additional materials for those who want to go deeper.\nReviewing signposting in lectures and practicals."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reading-writing",
    "href": "lectures/7.2-Geopandas.html#reading-writing",
    "title": "Geopandas",
    "section": "Reading & Writing",
    "text": "Reading & Writing\nSupported file formats:\n\n\n\nType\nExtension\nNotes\n\n\n\n\nShape\n.shp (etc.)\nMaximum compatibility\n\n\nGeoPackage\n.gpkg\nGood default choice\n\n\nGeoJSON\n.geojson\nFor web mapping\n\n\nZip\n.zip\nFor use with Shapefiles\n\n\nWKT\n.txt\nPlain-text & SQL\n\n\nGeoParquet\n.geoparquet\nGood for large data sets & SQL\n\n\n\nAdditionally, it is possible to read only subsets of the data using row, column, geometry, and bbox filters."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reading-remote-files",
    "href": "lectures/7.2-Geopandas.html#reading-remote-files",
    "title": "Geopandas",
    "section": "Reading (Remote Files)",
    "text": "Reading (Remote Files)\nAgain, depending on file size you may want to save these locally, but…\nimport geopandas as gpd\ngpkg_src = 'https://bit.ly/2K4JcsB'\nworld = gpd.read_file(gpkg_src)\n# The ';' suppresses matplotlib output\nworld.plot(facecolor='white', edgecolor='darkblue');"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#writing-local-files",
    "href": "lectures/7.2-Geopandas.html#writing-local-files",
    "title": "Geopandas",
    "section": "Writing (Local Files)",
    "text": "Writing (Local Files)\nWrite any OGR-supported vector drivers.\nworld.to_file('world.gpkg', driver='GPKG')\nworld.to_file('world.shp', driver='ESRI Shapefile')\nworld.to_file('world.geojson', driver='GeoJSON')\n\nIf you forget to specify the driver it writes shapefiles by default. This is mainly an issue if you try to write a GeoPackage or GeoJSON file but then end up writing a shapefile to a directory called world.gpkg!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#data-structures",
    "href": "lectures/7.2-Geopandas.html#data-structures",
    "title": "Geopandas",
    "section": "Data Structures",
    "text": "Data Structures\nGeoPandas does all this by adding just two new classes:\n\nGeoDataFrame\nGeoSeries\n\nIn principle, a GeoSeries can contain multiple geo-data types, but in practice you’ll want to be one of the following shapely classes:\n\nPoints / Multi-Points\nLines / Multi-Lines\nPolygons / Multi-Polygons"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#consider",
    "href": "lectures/7.2-Geopandas.html#consider",
    "title": "Geopandas",
    "section": "Consider",
    "text": "Consider\nRecall that we can ask if a particular object is an instance of any given class:\nimport pandas as pd\nprint(isinstance(world, str))\nprint(isinstance(world, pd.DataFrame))\nprint(isinstance(world, gpd.GeoDataFrame))\nPrints: False, True, True.\nimport pandas as pd\nprint(isinstance(world.geometry, str))\nprint(isinstance(world.geometry, pd.Series))\nprint(isinstance(world.geometry, gpd.GeoSeries))\nAlso prints: False, True, True.\n\nSo converting from Pandas to GeoPandas works well because GeoPandas knows all about Pandas.\nYou can use a GeoDataFrame anywhere you’d use a DataFrame with no loss of functionality! Same for a GeoSeries, though in this case a GeoSeries cannot perform the same statistical operations."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#projections",
    "href": "lectures/7.2-Geopandas.html#projections",
    "title": "Geopandas",
    "section": "Projections",
    "text": "Projections\nDepending on your data source, you may or may not have projection information attached to your GeoDataFrame:\nprint(world.crs)\noutputs epsg:4326, but:\nworld.crs\noutputs:\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#finding-projections",
    "href": "lectures/7.2-Geopandas.html#finding-projections",
    "title": "Geopandas",
    "section": "Finding Projections",
    "text": "Finding Projections\nYou’ll have already covered this in GIS, but you can find nearly any EPSG you might need at epsg.io. By far the most commonly-used here are:\n\nEPSG:4326 for the World Geodetic System 84 used in GPS.\nEPSG:27700 for OSGB 1936/British National Grid used in the UK.\n\nNote: recall that large territories (such as Canada, China and Russia) may well have multiple projections at the state of provincial level."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reprojection",
    "href": "lectures/7.2-Geopandas.html#reprojection",
    "title": "Geopandas",
    "section": "Reprojection",
    "text": "Reprojection\nFor data sets without projection information (i.e. often anything loaded from a shapefile) you must gdf.set_crs(&lt;spec&gt;). For all others you should gdf.to_crs(&lt;spec&gt;).\nworld2 = world.to_crs('ESRI:54030')\nworld2.plot();\n\n\nUnlike a shapefile, you can have more than one geometry column, each with a different projection. However, only one will be plotted (the one named geometry or specified via set_geometry())."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#the-spatial-index",
    "href": "lectures/7.2-Geopandas.html#the-spatial-index",
    "title": "Geopandas",
    "section": "The Spatial Index",
    "text": "The Spatial Index\nWe can use GeoSeries’ spatial index directly to perform simple spatial queries:\nimport matplotlib.pyplot as plt\nwslice = world.cx[-50:50, -20:20] # cx = coordinate index\nax = wslice.plot()\nplt.axvline(-50, linestyle='--', color='red')\nplt.axvline(50, linestyle='--', color='red')\nplt.axhline(-20, linestyle='--', color='red')\nplt.axhline(20, linestyle='--', color='red');"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#attributes",
    "href": "lectures/7.2-Geopandas.html#attributes",
    "title": "Geopandas",
    "section": "Attributes",
    "text": "Attributes\nA GeoSeries has attributes like any other Series, but also includes some spatially-specifc ones:\n\narea — if a polygon\nbounds — for each feature\ntotal_bounds — for each GeoSeries\ngeom_type — if you don’t already know\nis_valid — if you need to test"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#methods",
    "href": "lectures/7.2-Geopandas.html#methods",
    "title": "Geopandas",
    "section": "Methods",
    "text": "Methods\nAdditional GeoSeries methods icnlude:\n\ndistance() — returns Series measuring distances to some other feature (called as: &lt;GeoSeries&gt;.distance(&lt;feature&gt;))\ncentroid — returns GeoSeries of strict centroids (called as: &lt;GeoSeries&gt;.centroid)\nrepresentative_point() — returns GeoSeries of points within features\nto_crs() and plot(), which you’ve already seen.\n\n\nNote that centroid is not called with parentheses. Technically it’s more like an attribute than a method."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#relationship-tests",
    "href": "lectures/7.2-Geopandas.html#relationship-tests",
    "title": "Geopandas",
    "section": "Relationship Tests",
    "text": "Relationship Tests\nSimple geographical tests:\n\ngeom_almost_equals() — tries to deal with rounding issues when comparing two features.\ncontains() — is shape contained within some other features.\nintersects() — does shape intersect some other features."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#converting-non-spatial-data-1",
    "href": "lectures/7.2-Geopandas.html#converting-non-spatial-data-1",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 1",
    "text": "Converting Non-Spatial Data 1\nLat/Long and Northing/Easting benefit from a helper function gpd.points_from_xy():\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url)\n\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.plot()\n\nYou can also use list comprehensions ([x for x in list]) and zip to combine two lists but then need to specify the CRS as a separate step!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#csv-to-points-in-3-lines",
    "href": "lectures/7.2-Geopandas.html#csv-to-points-in-3-lines",
    "title": "Geopandas",
    "section": "CSV to Points in 3 Lines!",
    "text": "CSV to Points in 3 Lines!\n\n\nNotice that the default plot from a GeoDataFrame is… a map!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#converting-non-spatial-data-2",
    "href": "lectures/7.2-Geopandas.html#converting-non-spatial-data-2",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 2",
    "text": "Converting Non-Spatial Data 2\nOther feature types need to be in some kind of regular format such as Well-Known Text (WKT), GeoJSON, or something readable as a Shapely geometry.\nfrom shapely import wkt\n\n# Notice coordinate pairs and last point is same as first one\nbbox = 'POLYGON((5000000.0 2500000.0, 5000000.0 -2500000.0, -5000000.0 -2500000.0, -5000000.0 2500000.0, 5000000.0 2500000.0))'\n\n# Create GeoPandas from dict just like Pandas\nbgdf = gpd.GeoDataFrame({'id':[0], 'coordinates':bbox})\n\n# Turn it into a geometry\nbgdf['geometry'] = bgdf.coordinates.apply(wkt.loads)\nbgdf = bgdf.set_crs('ESRI:54030')\nbgdf.plot() # Not very interesting but...\n\nThese are more rarely used for our purposes but knowing that they exist is useful."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#from-text-to-bounding-box",
    "href": "lectures/7.2-Geopandas.html#from-text-to-bounding-box",
    "title": "Geopandas",
    "section": "From Text to Bounding Box",
    "text": "From Text to Bounding Box\nscale = int(float('1e7'))\nf,ax=plt.subplots(figsize=(8,4))\nworld2.plot(ax=ax)\nbgdf.plot(ax=ax, color='none', edgecolor='r')\nax.set_xlim([-0.75*scale, +0.75*scale])\nax.set_ylim([-3*scale/10, +3*scale/10])"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#additional-resources",
    "href": "lectures/7.2-Geopandas.html#additional-resources",
    "title": "Geopandas",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nI Hate Coordinate Systems\nGeoPandas on ReadTheDocs\nDani’s GDS Course\nDani’s Web Mapping Course"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-is-it",
    "href": "lectures/7.4-Signal_and_Noise.html#what-is-it",
    "title": "Signal & Noise",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-is-it-1",
    "href": "lectures/7.4-Signal_and_Noise.html#what-is-it-1",
    "title": "Signal & Noise",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-is-it-2",
    "href": "lectures/7.4-Signal_and_Noise.html#what-is-it-2",
    "title": "Signal & Noise",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#start-with-a-chart",
    "href": "lectures/7.4-Signal_and_Noise.html#start-with-a-chart",
    "title": "Signal & Noise",
    "section": "Start with a Chart",
    "text": "Start with a Chart\nThe problem of relying on statistics alone was amply illustrated by Anscombe’s Quartet (1973)…\n\nWe are not very good at looking at spreadsheets.\nWe are very good at spotting patterns visually.\n\nSometimes, we are too good; that’s where the stats comes in. Think of it as the ‘tiger in the jungle’ problem.."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#anscombes-quartet",
    "href": "lectures/7.4-Signal_and_Noise.html#anscombes-quartet",
    "title": "Signal & Noise",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\n\nX1\nY1\nX2\nY2\nX3\nY3\nX4\nY4\n\n\n\n\n10.0\n8.04\n10.0\n9.14\n10.0\n7.46\n10.0\n6.58\n\n\n8.0\n6.95\n8.0\n8.14\n8.0\n6.77\n8.0\n5.76\n\n\n13.0\n7.58\n13.0\n8.74\n13.0\n12.74\n13.0\n7.71\n\n\n9.0\n8.81\n9.0\n8.77\n9.0\n7.11\n9.0\n8.84\n\n\n11.0\n8.33\n11.0\n9.26\n11.0\n7.81\n11.0\n8.47\n\n\n14.0\n9.96\n14.0\n8.10\n14.0\n8.84\n14.0\n7.04\n\n\n6.0\n7.24\n6.0\n6.13\n6.0\n6.08\n6.0\n5.25\n\n\n4.0\n4.26\n4.0\n3.10\n4.0\n5.39\n4.0\n12.5\n\n\n12.0\n10.84\n12.0\n9.13\n12.0\n8.15\n12.0\n5.56\n\n\n7.0\n4.82\n7.0\n7.26\n7.0\n6.42\n7.0\n7.91\n\n\n5.0\n5.68\n5.0\n4.74\n5.0\n5.73\n5.0\n6.89"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#summary-statistics-for-the-quartet",
    "href": "lectures/7.4-Signal_and_Noise.html#summary-statistics-for-the-quartet",
    "title": "Signal & Noise",
    "section": "Summary Statistics for the Quartet",
    "text": "Summary Statistics for the Quartet\n\n\n\nProperty\nValue\n\n\n\n\nMean of x\n9.0\n\n\nVariance of x\n11.0\n\n\nMean of y\n7.5\n\n\nVariance of y\n4.12\n\n\nCorrelation between x and y\n0.816\n\n\nLinear Model\ny = 3 + 0.5x"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#but-what-do-they-look-like",
    "href": "lectures/7.4-Signal_and_Noise.html#but-what-do-they-look-like",
    "title": "Signal & Noise",
    "section": "But What do They Look Like?",
    "text": "But What do They Look Like?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#think-it-through",
    "href": "lectures/7.4-Signal_and_Noise.html#think-it-through",
    "title": "Signal & Noise",
    "section": "Think it Through",
    "text": "Think it Through\nYou can make a lot of progress in your research without any advanced statistics!\n\nA ‘picture’ isn’t just worth 1,000 words, it could be a whole dissertation!\nThe right chart makes your case eloquently and succinctly.\n\nAlways ask yourself:\n\nWhat am I trying to say?\nHow can I say it most effectively?\nIs there anything I’m overlooking in the data?\n\nA good chart is a good way to start!"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-makes-a-good-plot",
    "href": "lectures/7.4-Signal_and_Noise.html#what-makes-a-good-plot",
    "title": "Signal & Noise",
    "section": "What Makes a Good Plot?",
    "text": "What Makes a Good Plot?\nA good chart or table:\n\nServes a purpose — it is clear how it advances the argument in a way that could not be done in the text alone.\nContains only what is relevant — zeroes in on what the reader needs and is not needlessly cluttered.\nUses precision that is meaningful — doesn’t clutter the chart with needless numbers.\n\n\nFar too many charts or tables could be easily written up in a single sentence.\nFar too many charts or tables contain redundancy, clutter, and ‘flair’.\nDon’t report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#for-example",
    "href": "lectures/7.4-Signal_and_Noise.html#for-example",
    "title": "Signal & Noise",
    "section": "For Example…",
    "text": "For Example…\nHow much precision is necessary in measuring degrees at the equator?\n\n\n\nDecimal Places\nDegrees\nDistance\n\n\n\n\n0\n1\n111km\n\n\n1\n0.1\n11.1km\n\n\n2\n0.01\n1.11km\n\n\n3\n0.001\n111m\n\n\n4\n0.0001\n11.1m\n\n\n5\n0.00001\n1.11m\n\n\n6\n0.000001\n11.1cm\n\n\n7\n0.0000001\n1.11cm\n\n\n8\n0.00000001\n1.11mm"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final",
    "href": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final",
    "title": "Signal & Noise",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final-1",
    "href": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final-1",
    "title": "Signal & Noise",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#average-goals-by-world-cup-final",
    "href": "lectures/7.4-Signal_and_Noise.html#average-goals-by-world-cup-final",
    "title": "Signal & Noise",
    "section": "Average Goals by World Cup Final",
    "text": "Average Goals by World Cup Final\n\n\nIn 1982 the number of teams went from 16 to 24, and in 1998 it went from 24 to 32!"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality",
    "href": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality",
    "title": "Signal & Noise",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality-1",
    "href": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality-1",
    "title": "Signal & Noise",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#the-purpose-of-a-chart",
    "href": "lectures/7.4-Signal_and_Noise.html#the-purpose-of-a-chart",
    "title": "Signal & Noise",
    "section": "The Purpose of a Chart",
    "text": "The Purpose of a Chart\nThe purpose of a graph is to show that there are relationships within the data set that are not trivial/expected.\nChoose the chart to highlight relationships, or the lack thereof:\n\nThink of a chart or table as part of your ‘argument’ – if you can’t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably don’t need it.\nIdentify & prioritise the relationships in the data.\nChoose a chart type/chart symbology that gives emphasis to the most important relationships."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#real-numbers",
    "href": "lectures/7.4-Signal_and_Noise.html#real-numbers",
    "title": "Signal & Noise",
    "section": "Real Numbers",
    "text": "Real Numbers\nConsider the difference in emphasis between:\n\n11316149\n11,316,149\n11.3 million\n11 x 10\\(^{6}\\)\n22%\n22.2559%\n\nAlways keep in mind the purpose of the number."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#theres-still-a-role-for-tables",
    "href": "lectures/7.4-Signal_and_Noise.html#theres-still-a-role-for-tables",
    "title": "Signal & Noise",
    "section": "There’s Still a Role for Tables",
    "text": "There’s Still a Role for Tables\nWhy a table is sometimes better than a chart:\n\nYou need to present data values with greater detail\nYou need to enable readers to draw comparisons between data values\nYou need to present the same data in multiple ways (e.g. raw number and percentage)\nYou want to show many dimensions for a small number of observations\n\n\ne.g. percentage of people falling into each ethnic or income category for a small number of wards or boroughs."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-failing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-failing-grade",
    "title": "Signal & Noise",
    "section": "Undergraduate Tables (Failing Grade)",
    "text": "Undergraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-passing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-passing-grade",
    "title": "Signal & Noise",
    "section": "Undergraduate Tables (Passing Grade)",
    "text": "Undergraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-failing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-failing-grade",
    "title": "Signal & Noise",
    "section": "Postgraduate Tables (Failing Grade)",
    "text": "Postgraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-passing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-passing-grade",
    "title": "Signal & Noise",
    "section": "Postgraduate Tables (Passing Grade)",
    "text": "Postgraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#design-for-tables",
    "href": "lectures/7.4-Signal_and_Noise.html#design-for-tables",
    "title": "Signal & Noise",
    "section": "Design for Tables",
    "text": "Design for Tables\nPrinciples:\n\nReduce the number of lines to a minimum (and you should almost never need vertical lines).\nUse ‘white-space’ to create visual space between groups of unrelated (or less related) elements.\nRemove redundancy (if you find yourself typing ‘millions’ or ‘GBP’ or ‘Male’ repeatedly then you’ve got redundancy).\nEnsure that meta-data is clearly separate from, but attached to, the graph (i.e. source, title, etc.)."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#additional-resources",
    "href": "lectures/7.4-Signal_and_Noise.html#additional-resources",
    "title": "Signal & Noise",
    "section": "Additional Resources",
    "text": "Additional Resources\nThere’s so much more to find, but:\n\n\n\n26 Ways to Enhance your Tables in Tableau\nTables Aren’t Boring, You Are\n\n\n\nThe Visual Display of Quantitative Information (the Tufte and Graves-Morris 1983 classic)\nWhy Tufte’s Wrong…\nTufte is Dead; Long Live Tufte"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "href": "lectures/8.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "title": "Notebooks to Documents",
    "section": "Recall: Tangled Workflows",
    "text": "Recall: Tangled Workflows\nIt’s not just about mixing code and comment, we also want:\n\nTo separate content from presentation\nTo define mappings between presentation styles\nTo produce the best-quality output for the format chosen\n\n\n\nExamples of this include CSS for web sites, LaTeX templates, and Markdown styles.\nMVC approach to software design."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#pandoc",
    "href": "lectures/8.1-Notebooks_as_Documents.html#pandoc",
    "title": "Notebooks to Documents",
    "section": "Pandoc",
    "text": "Pandoc\nTool for converting documents between formats, including:\n\nPlain Text/YAML\nMarkdown\nLaTeX/PDF\nHTML/Reveal.js\nJupyter Notebook\nXML/ODT/DOCX\nEPUB/DocBook\nBibTeX/JSON"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#latex",
    "href": "lectures/8.1-Notebooks_as_Documents.html#latex",
    "title": "Notebooks to Documents",
    "section": "LaTeX",
    "text": "LaTeX\n\n\n\n\n\n\n\nIntended for type-setting of scientific documents, but has been used for slides, posters, CVs, etc. It is not a word processor, it’s more like a compiler.\nThis format is based on Edward Tufte’s VSQD and can be found on GitHub."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#latex-in-practice",
    "href": "lectures/8.1-Notebooks_as_Documents.html#latex-in-practice",
    "title": "Notebooks to Documents",
    "section": "LaTeX in Practice",
    "text": "LaTeX in Practice\nYou write LaTeX in any text editor, but specialist apps like Texpad or Overleaf make it easier.\n\\documentclass[11pt,article,oneside]{memoir}\n\\newcommand{\\bl}{\\textsc{bl}~\\/}\n\\usepackage{tabularx}\n\n\\begin{document}\n\\maketitle \n\nThis report provides an overview of activities ...\n\n\\section{Applications}\nA primary objective was the submission...\n\nUCL has an institutional license for Overleaf.\nThis document is then compiled (or ‘typeset’) with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.\nCompilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#bibtex",
    "href": "lectures/8.1-Notebooks_as_Documents.html#bibtex",
    "title": "Notebooks to Documents",
    "section": "BibTeX",
    "text": "BibTeX\nProvides bilbiographic support for LaTeX but widely used by other utilities as is also plain-text.\n@article{Lavin:2019,\n        Author = {Lavin, Matthew J.},\n        Doi = {10.46430/phen0082},\n        Journal = {The Programming Historian},\n        Number = {8},\n        Title = {Analyzing Documents with TF-IDF},\n        Year = {2019},\n        Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}\n\n@incollection{Kitchin:2016,\n        Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},\n        Booktitle = {Smart Urbanism},\n        Chapter = {2},\n        Editor = {Marvin, Luque-Ayala, McFarlane},\n        Title = {Smart Cities and the Politics of Urban Data},\n        Year = {2016}}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "href": "lectures/8.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "title": "Notebooks to Documents",
    "section": "BibTeX in Practice",
    "text": "BibTeX in Practice\nTo reference a document we then need to tell LaTeX or Pandoc where to look:\n\\bibliographystyle{apacite} # LaTeX\n\\bibliography{Spatial_Data_Chapter.bib} # LaTeX\nWith citations following formats like:\n\\citep[p.22]{Reades2018} # LaTeX\nOr:\n[@dignazio:2020, chap. 4] # Markdown"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#reveal.js",
    "href": "lectures/8.1-Notebooks_as_Documents.html#reveal.js",
    "title": "Notebooks to Documents",
    "section": "Reveal.js",
    "text": "Reveal.js\nJavaScript-based presentation framework. Can use Markdown to generate portable interactive slides including references/bibliographies.\nHow this presentation was created.\nCompare:\n\nMarkdown\nHTML\nReveal"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#headings",
    "href": "lectures/8.1-Notebooks_as_Documents.html#headings",
    "title": "Notebooks to Documents",
    "section": "Headings",
    "text": "Headings\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n# Heading Level 1\n\\section{Heading Level 1}\n\n\n## Heading Level 2\n\\subsection{Heading Level 2}\n\n\n### Heading Level 3\n\\subsubsection{Heading Level 3}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#inline-elements",
    "href": "lectures/8.1-Notebooks_as_Documents.html#inline-elements",
    "title": "Notebooks to Documents",
    "section": "Inline Elements",
    "text": "Inline Elements\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n1. Numbered item 1\n\\begin{enumerate} \\n \\item ... \\end{enumerate}\n\n\n- Bulleted list item 1\n\\begin{itemize} \\n \\item ... \\n \\end{itemize}\n\n\n_italics_ or *italics*\n\\emph{italics} or \\textit{italics}\n\n\n**bold**\n\\textbf{bold}\n\n\n&gt; blockquote\n\\begin{quote} \\n blockquote \\end{quote}\n\n\nSome `code` is here\nSome \\texttt{code} is here\n\n\n[Link Text](URL)\n\\href{Link Text}{URL}\n\n\n![Alt Text](Image URL)\n\\begin{figure}\\n \\includegraphics[opts]{...}\\n \\end{figure}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#mathematics",
    "href": "lectures/8.1-Notebooks_as_Documents.html#mathematics",
    "title": "Notebooks to Documents",
    "section": "Mathematics",
    "text": "Mathematics\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\nSame, but either 1 or 2 $’s\n$x=5$\n\n\nSame, but either 1 or 2 $’s\n$\\pi$\n\n\nSame, but either 1 or 2 $’s\n$e = mc^{2}$\n\n\n\nWe can show all this directly in the Notebook! \\(\\pi\\); \\(e = mc^{2}\\); \\(\\int_{0}^{\\inf} x^2 \\,dx\\); \\(\\sum_{n=1}^{\\infty} 2^{-n} = 1\\)\n\n\nOverleaf has good documentation for most (basic) applications."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#additional-resources",
    "href": "lectures/8.1-Notebooks_as_Documents.html#additional-resources",
    "title": "Notebooks to Documents",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nJupyter Tips and Tricks\nPandoc Demos\nBeginner’s Guide to Jupyter Notebooks\n7 Essential Tips to Writing With Jupyter Notebooks\nVersion Control with Jupyter\nSustainable Publishing using Pandoc and Markdown\nMaking Pretty PDFs with Quarto\nHOw to use Quarto for Parameterised Reporting"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "href": "lectures/8.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "title": "Cleaning Text",
    "section": "When a Loop Is Not Best",
    "text": "When a Loop Is Not Best\nIf you need to apply the same operation to lots of data why do it sequentially?\n\nYour computer has many cores and can run many threads in parallel.\nThe computer divides the work across the threads it sees fit.\nThe computer reassemble the answer at the end from the threads.\n\nIf you have 4 cores then parallelisation cuts analysis time by 75%."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "href": "lectures/8.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "title": "Cleaning Text",
    "section": "So Do More with Each Clock Cycle",
    "text": "So Do More with Each Clock Cycle\n\nMany libraries/packages implement weak forms of vectorisation or parallelisation, but some libraries do more.\nYou must request it because it requires hardware or other support and it is highly optimsed.\nMultiple separate machines acting as one.\nMultiple GPUs acting as one.\n\n\nConceptually, these get challenging if you can’t clearly separate/parallelise tasks."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "href": "lectures/8.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "title": "Cleaning Text",
    "section": "Pandas.apply() vs. Numpy",
    "text": "Pandas.apply() vs. Numpy\nNumpy is fully vectorised and will almost always out-perform Pandas apply, but both are massive improvements on for loops:\n\nExecute row-wise and column-wise operations.\nApply any arbitrary function to individual elements or whole axes (i.e. row or col).\nCan make use of lambda functions too for ‘one off’ operations (ad-ohoc functions)."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#lambda-functions",
    "href": "lectures/8.3-Cleaning_Text.html#lambda-functions",
    "title": "Cleaning Text",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nFunctional equivalent of list comprehensions: 1-line, anonymous functions.\nFor example:\nx = lambda a : a + 10\nprint(x(5)) # 15\nOr:\nfull_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\nprint(full_name('guido', 'van rossum')) # 'Guido Van Rossum'\nThese are very useful with pandas."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#lets-compare",
    "href": "lectures/8.3-Cleaning_Text.html#lets-compare",
    "title": "Cleaning Text",
    "section": "Let’s Compare",
    "text": "Let’s Compare\nimport time\nimport numpy as np\ndef func(a,b):\n  c = 0\n  for i in range(len(a)): c += a[i]*b[i]\n  return c\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nt1 = time.time()\nprint(func(a,b))\nt2 = time.time()\nprint(np.dot(a,b))\nt3 = time.time()\n\nprint(f\"For loop took {(t2-t1)*1000:.0f} milliseconds\")\nprint(f\"Numpy took {(t3-t2)*1000:.0f} milliseconds\")\nGenerally, I get numpy taking 86ms, while the for loop takes 331ms!\n\n\n/ht to The Last Byte for inspiration."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#beautiful-soup-selenium",
    "href": "lectures/8.3-Cleaning_Text.html#beautiful-soup-selenium",
    "title": "Cleaning Text",
    "section": "Beautiful Soup & Selenium",
    "text": "Beautiful Soup & Selenium\nTwo stages to acquiring web-based documents:\n\nAccessing the document: urllib can deal with many issues (even authentication), but not with dynamic web pages (which are increasingly common); for that, you need Selenium (library + driver).\nProcessing the document: simple data can be extracted from web pages with RegularExpressions, but not with complex (esp. dynamic) content; for that, you need BeautifulSoup4.\n\nThese interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#regular-expressions-breaks",
    "href": "lectures/8.3-Cleaning_Text.html#regular-expressions-breaks",
    "title": "Cleaning Text",
    "section": "Regular Expressions / Breaks",
    "text": "Regular Expressions / Breaks\nNeed to look at how the data is organised:\n\nFor very large corpora, you might want one document at a time (batch).\nFor very large files, you might want one line at a time (streaming).\nFor large files in large corpora, you might want more than one ‘machine’.\n\n\nSee the OpenVirus Project."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#starting-points",
    "href": "lectures/8.3-Cleaning_Text.html#starting-points",
    "title": "Cleaning Text",
    "section": "Starting Points",
    "text": "Starting Points\nThese strategies can be used singly or all-together:\n\nStopwords\nCase\nAccent-stripping\nPunctuation\nNumbers\n\nBut these are just a starting point!\n\nWhat’s the semantic difference between 1,000,000 and 999,999?"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#distributional-pruning",
    "href": "lectures/8.3-Cleaning_Text.html#distributional-pruning",
    "title": "Cleaning Text",
    "section": "Distributional Pruning",
    "text": "Distributional Pruning\nWe can prune from both ends of the distribution:\n\nOverly rare words: what does a word used in one document help us to understand about a corpus?\nOverly common ones: what does a word used in every document help us to understand about a corpus?\n\n\nAgain, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#different-approaches",
    "href": "lectures/8.3-Cleaning_Text.html#different-approaches",
    "title": "Cleaning Text",
    "section": "Different Approaches",
    "text": "Different Approaches\nHumans use a lot of words/concepts1:\n\nStemming: rules-based truncation to a stem (can be augmented by language awareness).\nLemmatisation: usually dictionary-based ‘deduplication’ to a lemma (can be augmented by POS-tagging).\n\nA recent digitsation effort by Harvard and Google estimated 1,022,000 unique word-forms in English alone (Source)."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#different-outcomes",
    "href": "lectures/8.3-Cleaning_Text.html#different-outcomes",
    "title": "Cleaning Text",
    "section": "Different Outcomes",
    "text": "Different Outcomes\n\n\n\nSource\nPorter\nSnowball\nLemmatisation\n\n\n\n\nmonkeys\nmonkey\nmonkey\nmonkey\n\n\ncities\nciti\nciti\ncity\n\n\ncomplexity\ncomplex\ncomplex\ncomplexity\n\n\nReades\nread\nread\nReades\n\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nfor w in ['monkeys','cities','complexity','Reades']:\n    print(f\"Porter: {PorterStemmer().stem(w)}\")\n    print(f\"Snowball: {SnowballStemmer('english').stem(w)}\")\n    print(f\"Lemmatisation: {wnl.lemmatize(w)}\")"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#additional-resources",
    "href": "lectures/8.3-Cleaning_Text.html#additional-resources",
    "title": "Cleaning Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nVectorisation in Python\nLambda Functions\nReal Python Lambda Functions\nStemming words with NLTK\nStemming and Lemmatisation in Python\nKD Nuggets: A Practitioner’s Guide to NLP\nKD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics\nRoadmap to Natural Language Processing (NLP)"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\npd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join",
    "href": "lectures/9.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart\n\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join-result",
    "href": "lectures/9.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\npd.merge(df1, df2, \n         how = 'inner',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#but-what-if",
    "href": "lectures/9.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then:\npd.merge(df1, df2, \n         how      = 'inner',\n         left_on  = 'SensorID',\n         right_on = 'SensorKey')\n \nWe will get an ‘extra’ field:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#outer-join",
    "href": "lectures/9.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\npd.merge(df1, df2,\n         how = 'outer',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#left-join",
    "href": "lectures/9.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\npd.merge(df1, df2, \n        how = 'left',\n        on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append-concat",
    "href": "lectures/9.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#concat",
    "href": "lectures/9.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append",
    "href": "lectures/9.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 0, 'Parameter': 5,  'Humidity', 'Value': 0.45},\n    {'SensorID': 1, 'Parameter': 5,  'Humidity', 'Value': 0.31},\n    {'SensorID': 2, 'Parameter': 4, 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 3, 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#additional-resources",
    "href": "lectures/9.1-Linking_Data.html#additional-resources",
    "title": "Linking Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#the-options",
    "href": "lectures/9.3-Scaling_Data.html#the-options",
    "title": "Scaling Data",
    "section": "The Options",
    "text": "The Options\n\nMake pandas faster.\nMake the storage better.\nMake using more cores easier.\n\n\nWithin each of these options there are several types of solutions:\n\nTo make pandas faster we can look both at how it runs code and how it manages data internally.\nWe could also store data more intelligently so that pandas can access it more efficiently.\nOr we could move away from pandas entirely as a tool for managing and analysing data.\n\nThere are pros and cons to each of these."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#make-pandas-faster-code",
    "href": "lectures/9.3-Scaling_Data.html#make-pandas-faster-code",
    "title": "Scaling Data",
    "section": "Make Pandas Faster: Code",
    "text": "Make Pandas Faster: Code\n\n\nIntroducing: lazy evaluation and filtergraphs.\n\n\n\nThis is the approach taken by Polars and Dask. So while something like strong data typing (which all of the tools I’m talking about today do) is an ‘easy’ win because it allows you to plan make a better plan for handling the data, there are bigger gains to be made from ‘lazy evaluation’."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#make-pandas-faster-storage",
    "href": "lectures/9.3-Scaling_Data.html#make-pandas-faster-storage",
    "title": "Scaling Data",
    "section": "Make Pandas Faster: Storage",
    "text": "Make Pandas Faster: Storage\n\n\nIntroducing: Arrow and Parquet.\n\n\n\n\nArrow is an in-memory columnar format for data. Data is stored in a structured way in RAM making it blazingly fast for operations.\nParquet is a highly-compressed columnar file format for data. Data is stored in a structured way on your hard drive.\nFeather is a raw storage format for Arrow.\n\nTL;DR: for most applications Parquet will give nice, small files on disk and the benefits of columnar file storage; for computationally intensive applications where disk space and interoperability with other systems isn’t an issue then Feather might work."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#make-pandas-faster-scaling",
    "href": "lectures/9.3-Scaling_Data.html#make-pandas-faster-scaling",
    "title": "Scaling Data",
    "section": "Make Pandas Faster: Scaling",
    "text": "Make Pandas Faster: Scaling\n\n\nIntroducing: Parallelisation.\n\n\n\nWe’ve already talked about this a bit, but let’s be explicit: pandas is basically bound to one processor, and if we can find ways to lift that restriction then things are going to run a lot faster. If you build in lazy evaluation, query planning, and better storage, then removing this restriction is a lot easier."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#what-about-polars",
    "href": "lectures/9.3-Scaling_Data.html#what-about-polars",
    "title": "Scaling Data",
    "section": "What About Polars?",
    "text": "What About Polars?\n\n\n\n\nPros:\n\nLooks like pandas code1, doesn’t run like it.\nMulti-threaded execution.\nArrow behind the scenes.\nStreaming data processing possible.\n\nCons:\n\nEasy to miss out on the benefits.\nDesigned for single machines2.\n\n\nThis can be a ‘gotcha’, see migration guidePaid for cloud services gets around this, sort of."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#what-about-the-duck",
    "href": "lectures/9.3-Scaling_Data.html#what-about-the-duck",
    "title": "Scaling Data",
    "section": "What About the Duck?",
    "text": "What About the Duck?\n\n\n\n\nPros:\n\nServerless SQL queries against Parquet files.\nQueries can be returned as Pandas data frames.\nSelect and filter before loading using Arrow.\nPlugins for geospatial and remote parquet files.\n\nCons:\n\nNeed to learn SQL.\nStill constrained by available memory.\nReally still optimised for a single machine."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#what-about-dask",
    "href": "lectures/9.3-Scaling_Data.html#what-about-dask",
    "title": "Scaling Data",
    "section": "What About Dask?",
    "text": "What About Dask?\n\n\n\n\nPros:\n\nDistributed and parallel data processing\nQueries returned as Pandas data frames\nLazy evaluation of code\nBuilt-in scaling\n\nCons:\n\nLots of overhead for small queries.\nMoving a lot of data around.\nNot all problems readily parallelisable."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#postgres",
    "href": "lectures/9.3-Scaling_Data.html#postgres",
    "title": "Scaling Data",
    "section": "Postgres?",
    "text": "Postgres?\n\n\n\n\nPros:\n\nFully-fledged database.\nIndustry-leading handling of geospatial data.\n\nCons:\n\nNeed to learn SQL.\nNeed to learn how to design databases to maximise gains.\nMoving lots of data in/out of the database is slow."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#a-simple-comparison",
    "href": "lectures/9.3-Scaling_Data.html#a-simple-comparison",
    "title": "Scaling Data",
    "section": "A Simple Comparison",
    "text": "A Simple Comparison\n\nimport pandas as pd\nimport polars as pl\nimport duckdb as duck\nimport time\nimport seaborn as sns\nsns.load_dataset('titanic').to_parquet('titanic.pq')\n\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#measuring-performance",
    "href": "lectures/9.3-Scaling_Data.html#measuring-performance",
    "title": "Scaling Data",
    "section": "Measuring Performance",
    "text": "Measuring Performance\nWe can compare performance using profiling1:\n\nimport timeit\n\ndef load_pandas():\n    pd.read_parquet('titanic.pq', columns=['age']).age.mean()\n\ndef load_polars():\n    pl.read_parquet('titanic.pq', columns=['age'])['age'].mean()\n\ndef load_duck():\n    duck.sql(\"SELECT MEAN(age) FROM read_parquet('titanic.pq')\")\n\nnum_reps = 1000\n\nSee especially the timeit module discussed here"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#the-results",
    "href": "lectures/9.3-Scaling_Data.html#the-results",
    "title": "Scaling Data",
    "section": "The Results",
    "text": "The Results\nPandas\n\npd_execution_time = timeit.timeit(load_pandas, number=num_reps)\nprint(f\"Pandas execution time: {pd_execution_time:.6f} seconds\")\n\nPandas execution time: 0.762337 seconds\n\n\nPolars\n\npl_execution_time = timeit.timeit(load_polars, number=num_reps)\nprint(f\"Polars execution time: {pl_execution_time:.6f} seconds\")\n\nPolars execution time: 0.138823 seconds\n\n\n81.79% faster than pandas.\nDuckDB\n\ndb_execution_time = timeit.timeit(load_duck, number=num_reps)\nprint(f\"DuckDB execution time: {db_execution_time:.6f} seconds\")\n\nDuckDB execution time: 0.097314 seconds\n\n\n87.23% faster than pandas and 29.90% faster than polars."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#useful-but-limited",
    "href": "lectures/9.3-Scaling_Data.html#useful-but-limited",
    "title": "Scaling Data",
    "section": "Useful, But Limited?",
    "text": "Useful, But Limited?\n\n\n\nMethod\nAchieves\n\n\n\n\ncount()\nTotal number of items\n\n\nfirst(), last()\nFirst and last item\n\n\nmean(), median()\nMean and median\n\n\nmin(), max()\nMinimum and maximum\n\n\nstd(), var()\nStandard deviation and variance\n\n\nmad()\nMean absolute deviation\n\n\nprod()\nProduct of all items\n\n\nsum()\nSum of all items\n\n\n\n\nHere are a bunch of pandas functions that have to do with aggregating data in some way. Some of these you’ll have seen before, some you may not. However, up to this point if you wanted to to know the median price of each type of Airbnb listing, or the sum of each type of vehicle sold, you’d have had to select out one listing type or vehicle type, call median or sum, and then remember the result. Let’s change that."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#grouping-operations",
    "href": "lectures/9.3-Scaling_Data.html#grouping-operations",
    "title": "Scaling Data",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#in-practice",
    "href": "lectures/9.3-Scaling_Data.html#in-practice",
    "title": "Scaling Data",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(&lt;fields&gt;).&lt;function&gt;\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#grouping-by-arbitrary-mappings",
    "href": "lectures/9.3-Scaling_Data.html#grouping-by-arbitrary-mappings",
    "title": "Scaling Data",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#pivot-tables",
    "href": "lectures/9.3-Scaling_Data.html#pivot-tables",
    "title": "Scaling Data",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(&lt;series&gt;, &lt;bins&gt;) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#in-practice-1",
    "href": "lectures/9.3-Scaling_Data.html#in-practice-1",
    "title": "Scaling Data",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#counts",
    "href": "lectures/9.3-Scaling_Data.html#counts",
    "title": "Scaling Data",
    "section": "Counts",
    "text": "Counts"
  },
  {
    "objectID": "lectures/9.3-Scaling_Data.html#pivots-groups",
    "href": "lectures/9.3-Scaling_Data.html#pivots-groups",
    "title": "Scaling Data",
    "section": "Pivots & Groups",
    "text": "Pivots & Groups"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html",
    "href": "practicals/Practical-01-Getting_Started.html",
    "title": "Practical 1: Getting Started",
    "section": "",
    "text": "This week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: a) get out of the way of doing ‘stuff’; b) support teams in negotating conflicts in code; c) make it easy to share results; and d) make it easy to ensure that everyone is ‘on the same page’."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#learning-outcomes",
    "href": "practicals/Practical-01-Getting_Started.html#learning-outcomes",
    "title": "Practical 1: Getting Started",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou will have finished installing the tools needed to work on Foundations and Quantitative Methods.\nYou will have a GitHub account enabling you to synchronise ‘local’ and ‘remote’ work.\nYou will have seen how code is synchronised using the Command Line.\n\nYou will find things confusing this week, but they will start to make more sense as we move further into the module. The key is to keep trying things out and to ask for help when you get stuck."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#some-terminal-basics",
    "href": "practicals/Practical-01-Getting_Started.html#some-terminal-basics",
    "title": "Practical 1: Getting Started",
    "section": "Some Terminal Basics",
    "text": "Some Terminal Basics\nThroughout Foundations we will make regular use of the ‘Terminal’, also known as the ‘Command Line’ (CLI), on both Mac and PC. There will be a whole lecture on the CLI and why/how we use it, but becoming comfortable with text commands is like gaining a bonus super-power alongside your new programming skills: need to change a line of text or code in thirty files? No problem! Need to start up a new Podman container? Done! Need to see how much free memory your server has? Yep!\nThere are many different types of terminal out there, but we are currently recommending that you use the following:\n\nWindows users: you are very strongly encouraged to use the Windows Terminal instead of the ‘Power Shell’, though the latter will also work in a pinch. The ‘Command Prompt’ (cmd) does not work for our purposes.\nMacOS users: please try to use iTerm2 in preference to the built-in Terminal because it is easy to add additional usability features such as OhMyZSH, but it’s not essential that you do so.\n\nIn fact, you can also run OhMyZSH on Windows but it looks like you’re in for a bit of a long haul and might want to leave it for when you’ve done everything else this week.\nIf you need help understanding how to use the Command Line or want to be able to do much more there are a wide range of tutorials available.\nHere are some starting points for learning more:\n\nI need help understanding: Software Carpentries is your friend! They have an entire tutorial titled The Unix Shell.\nI still need help understanding: the Programming Historian is another good place to look! And they also have an entire tutorial titled An Introduction to Bash.\nI want to do more on the Command Line: O’Reilly has produced an online book called Data Science at the Command Line that will take you much, much further.\n\nTwo more things:\n\nPressing the ‘up’ arrow on your keyboard moves you back one step in your command history. This is massively useful when you want to run the same command again (e.g. the one to run Podman!) since it saves you looking it up or copy+pasting.\nTo close a Terminal window you can run the exit command. If you make changes to the Terminal configuration file you often need to open a new window to see your changes take effect."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#following-convention",
    "href": "practicals/Practical-01-Getting_Started.html#following-convention",
    "title": "Practical 1: Getting Started",
    "section": "Following Convention",
    "text": "Following Convention\nA common convention in programming is to use the &lt;some text or other&gt; to indicate that you should replace the text between the &lt; and &gt; with something that makes sense in your context. For example, if you see cd &lt;directory&gt; (cd means change directory) then if you want to change to the Documents directory you should type cd Documents. If you see &lt;your username here&gt; you should type jreades if that’s your username. And so on. You do not include the &lt; or &gt; characters!\nSo, for example, if you see cd ~&lt;your username&gt; then you should type cd ~jreades (or whatever your username is). If you don’t know your username you can type whoami. After each of these commands you need to hit the Enter/Return key (⏎) in order to run the command."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#starting-up-right",
    "href": "practicals/Practical-01-Getting_Started.html#starting-up-right",
    "title": "Practical 1: Getting Started",
    "section": "Starting Up ‘Right’",
    "text": "Starting Up ‘Right’\nSo before you do anything else please spend a minute in the Terminal (macOS) or Windows Terminal (Windows) learning how to get to your home directory and, within that, to a CASA directory where you can store your work and keep the virtual machine from accessing data that it shouldn’t.\nWhat we are doing is creating a directory on your computer that you can access from the virtual machine. This is where you will store your notebooks, data, and any other files that you need to work with. The ultimate structure we’ll produce this:\n\n\n\n‘Target’ Directory Structure\n\n\nOn both a Mac and a PC you should be able to run the following:\n\ncd $HOME – this will change directory to your home directory (on a Mac it will be /Users/&lt;your username&gt;, on a PC it will be something like C:\\Users\\&lt;your username&gt;). Hint: cd means ‘change directory’!\ncd Documents – this will move you into your ‘Documents’ folder. Note: on Windows this might be called My\\ Documents (the \\ is not a mistake, you need it when there’s a space in the name), in which case it’s cd My\\ Documents! If you have set up your computer in another language this might be called something else, but the Terminal should still ‘know’ which folder contains your documents.\nmkdir CASA – this will create a CASA folder in your home directory.\ncd CASA – you are now changing into the CASA directory.\necho $pwd (PC) or pwd (Mac) – this should show you the ‘full path’ to your new CASA directory (e.g. /Users/&lt;your username&gt;/Documents/CASA or something like that).\n\nLeave the Terminal window open! You will need it in a moment."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#configuring-the-image",
    "href": "practicals/Practical-01-Getting_Started.html#configuring-the-image",
    "title": "Practical 1: Getting Started",
    "section": "Configuring the sds2025 Image",
    "text": "Configuring the sds2025 Image\nDuring the ‘install festival/social’ you should have installed Podman and, time permitting, ‘pulled’ the image appropriate to your system. If you haven’t, then you should do so as a priority now."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#running-podman-1",
    "href": "practicals/Practical-01-Getting_Started.html#running-podman-1",
    "title": "Practical 1: Getting Started",
    "section": "Running Podman",
    "text": "Running Podman\nBy default, the best way to start the sds2025 virtual machine (also called a ‘container’) is from the Terminal or Power Shell.\n\nOn WindowsOn macOS\n\n\nUsing the same Power Shell copy and paste the following all on one line:\npodman run --rm -d --name sds2025 -p 8888:8888 -v \"$(pwd):/home/jovyan/work\" jreades/sds:2025-amd start.sh jupyter lab --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\n\n\n\n\n\n\nWarningWindows Commands\n\n\n\n$(pwd) is actually a command, you are asking the Power Shell to use the current working directory (pwd == print working directory) as the ‘mount point’ for the work directory. The Command Prompt doesn’t support pwd, but the Power Shell should. You can check this by simply typing pwd and hitting enter (⏎) to see if you get an error.\n\n\n\n\nUsing the same Terminal copy and past the following (change the image to jreades/sds:2025-amd if using an older Intel Mac):\ndocker run --rm -d --name sds2025 -p 8888:8888 \\\n   -v \"$(pwd):/home/jovyan/work\" \\\n  jreades/sds:2025-arm start.sh jupyter lab \\\n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#how-do-i-know-it-worked",
    "href": "practicals/Practical-01-Getting_Started.html#how-do-i-know-it-worked",
    "title": "Practical 1: Getting Started",
    "section": "How do I Know it Worked?",
    "text": "How do I Know it Worked?\nWith the virtual machine running, you will mainly interact with Python through your browser. To check if it’s running, we just have to visit the web page and see what happens: http://localhost:8888/lab/tree/work/. We’ll talk more about exactly what is going on over the next few weeks, but this should show you a page that looks something like this (probably with fewer files listed on the left-hand side):\n\n\n\nScreenshot of Jupyter Lab\n\n\n\n\n\n\n\n\nNoteSee the Container Run (and Run)…\n\n\n\nOnce you have started a container, the machine will continue running until you either restart the computer or stop the container. This can consume memory and battery power indefinitely."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#your-new-github-account",
    "href": "practicals/Practical-01-Getting_Started.html#your-new-github-account",
    "title": "Practical 1: Getting Started",
    "section": "Your New GitHub Account",
    "text": "Your New GitHub Account\nIt doesn’t really matter which way you do this, but we recommend that you set up your new GitHub account with both your personal and your UCL email addresses. GitHub ‘knows’ about educational users and will give you access to more features for free if you associate a .ac.uk email address to your account. So choose one email address to start with and then add the other one later.\nFrom a security standpoint you should also enable 2-factor authentication so that you receive a text message when you log in on a new machine and are asked to confirm a code.\nSo in order to complete this task you need to do the following (more detailed explanations below):\n\nCreate a login with GitHub (if you’ve not done so already).\nCreate a new private repository on GitHub (see below).\nEdit the README.md and .gitignore files for your new repository (see below).\nSave the changes (this is called a ‘commit’) and explain in a general way what edits you did (see below).\nWork out how to compare the original and edited versions of any file in your browser."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#creating-a-private-repository",
    "href": "practicals/Practical-01-Getting_Started.html#creating-a-private-repository",
    "title": "Practical 1: Getting Started",
    "section": "Creating a Private Repository",
    "text": "Creating a Private Repository\nTo create a repository, click on the + at the upper-right corner of the GitHub web page and select New Repository. You might as well call your ‘repo’ fsds or foundations since that’s a lot shorter than foundations_of_spatial_data_science.\n\n\n\n\n\n\nImportantYour ‘Repo’ Name\n\n\n\nFor the purposes of this tutorial (and all subsequent tutorials) I will assume that your repository is called fsds. You can call it whatever you like, in which case you will always need to substitute the name that you chose wherever you see me write fsds.\n\n\nIt’s always helpful to provide some basic information about what’s in the project (e.g. your notes and practicals for the Foundations module). And finally, make sure you:\n\nChange the visibility from Public to Private,\nTick Add a README file,\nChange Add .gitignore from None to template: Python.\n\nClick Create Repository and you should end up on a page that looks like this:\n\n\n\nRepository created\n\n\nYour new repository has been created (on GitHub)! The README is a file – written in Markdown – that explains the purpose of a repository and any other notes you care to add."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#exclude-data-files",
    "href": "practicals/Practical-01-Getting_Started.html#exclude-data-files",
    "title": "Practical 1: Getting Started",
    "section": "Exclude Data Files",
    "text": "Exclude Data Files\nWe want to make it hard to accidentally add a large data file to our repository. Git/GitHub isn’t designed for large, binary files (you can’t ‘read’ a Parquet file) and we assume that data is backed up or available elsewhere, but our code is not! So as a first step we want to exclude files that are likely to just be ‘data’:\n\n\n\nFile Type\nExtension\n\n\n\n\nCSV\n.csv\n\n\nExcel\n.xls, .xlsx\n\n\nZip\n.zip\n\n\nGZip\n.gzip\n\n\nParquet\n.parquet, .geoparquet\n\n\n\nMaybe look to see how files are already done to help you figure out how to exlcude .zip, .gz, and .csv files… but as a hint: * means ‘any sequence of characters’ so *.py means any Python script file ending in .py."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#exclude-a-data-directory",
    "href": "practicals/Practical-01-Getting_Started.html#exclude-a-data-directory",
    "title": "Practical 1: Getting Started",
    "section": "Exclude a Data Directory",
    "text": "Exclude a Data Directory\nTo make it even less like that we accidentally include data, let’s also exclude a data directory from our repository. As a clue, nearly everything in the Distribution / packaging section of the .gitignore file is a directory to be excluded from Git.\nSo how would you indicate that data is a directory? Once you’re sure, add the data directory!\nWhen you are done, don’t forget to add a ‘commit message’ (e.g. ‘Added file types to exclude to .gitignore’) at the bottom and then click Commit changes.\n\n\n\n\n\n\nTipQuick Answer\n\n\n\n\n\nI don’t want you to get hung up on this one thing in Practical 1, so if you just can’t make sense of what you’re being asked to do here, have a look at the Answers at the bottom of this page."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#check-your-changes",
    "href": "practicals/Practical-01-Getting_Started.html#check-your-changes",
    "title": "Practical 1: Getting Started",
    "section": "Check Your Changes",
    "text": "Check Your Changes\nOnce you have committed your changes, you should be back to the default view of the .gitignore file but there should be a message to the effect of Latest commit &lt;some hexadecimal number&gt; 10 seconds ago and, next to that, a History button.\nClick on ‘History’ and let’s go back in time!\n\n\n\nThe Gitignore history\n\n\nOn the history page you can browse every edit to your file. Whenever you commit a file, this like taking a snapshot at a point in time. Using the ‘History’ you can compare two different snapshots in order to see what has changed. This would help you to work out how you broke something, check that requested changes have been made, or see how an error might have been introduced.\n\n\n\n\n\n\nTipViewing Your Commit History\n\n\n\nYou can mouseover the buttons to see what they do. Why don’t you try to find See commmit details and check what edits you made to the .gitignore file? You should see at least three plusses in the history view representing three new lines in the .gitignore file."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#working-on-your-markdown",
    "href": "practicals/Practical-01-Getting_Started.html#working-on-your-markdown",
    "title": "Practical 1: Getting Started",
    "section": "Working on Your Markdown",
    "text": "Working on Your Markdown\nWrite your README file using at least the following Markdown features:\n\nA level-1 header (#)\nA level-3 header (###)\nItalic text (_this is italicised_)\nBold text (**this is bold**)\nA link ([link text](url))\nAn image (![Alt text](image_location))\n\nIf you’re unsure how these work, just double-click on this text and you’ll see Markdown in a Jupyter notebook. Here’s some sample text to get you started:\n### Foundations of Spatial Data Science\n\nThis repository contains practicals and notes from the _Foundations_ module. \n\nYou can find the original [here](https://jreades.github.io/fsds/).\nDon’t forget to check out the “Preview” tab!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#commiting-a-change",
    "href": "practicals/Practical-01-Getting_Started.html#commiting-a-change",
    "title": "Practical 1: Getting Started",
    "section": "Commiting a Change",
    "text": "Commiting a Change\nOnce you’re happy with how your text looks and works, it’s time to commit! Scroll down to where you see something like this (you will see your own GitHub username, not mine):\n\n\n\nGitHub Commit\n\n\nYou can just accept the description (e.g. Create README.md) or you can write your own. You can also provide an extended description if you choose. Then click Commit new file and you will see your new README appear."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#configuring-defaults",
    "href": "practicals/Practical-01-Getting_Started.html#configuring-defaults",
    "title": "Practical 1: Getting Started",
    "section": "Configuring Defaults",
    "text": "Configuring Defaults\nThe first thing to do is set up the default username and email for GitHub. These can be changed on a project-by-project basis, but to begin with it’s best to set up the global defaults. Using the Terminal, enter the following (replacing &lt;...&gt; with your details):\ngit config --global user.email '&lt;your GitHub email address&gt;'\ngit config --global user.name '&lt;your GibHub username&gt;'\n\n\n\n\n\n\nTipRecall: Convention!\n\n\n\nAs a reminder, &lt;...&gt; is a convention in programming to indicate that you should replace the text between the &lt; and &gt; with something that makes sense in your context. For example, if you see '&lt;your GitHub email address&gt;' you should type, for example, 'j.reades@ucl.ac.uk'. You do not include the &lt; or &gt; characters!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#creating-a-personal-access-token",
    "href": "practicals/Practical-01-Getting_Started.html#creating-a-personal-access-token",
    "title": "Practical 1: Getting Started",
    "section": "Creating a Personal Access Token",
    "text": "Creating a Personal Access Token\nFor copying changes up to/down from GitHub, you must use a Personal Access Token. This is like issuing special passwords that allow only limited access to parts of your GitHub account.\nTo create a Personal Access Token:\n\nVisit your GitHub User Page (e.g. github.com/jreades)\nClick on your user icon (at the top-right corner of the page) and click the Settings link (near the bottom on the right side of the page).\nScroll down the settings page until you get to Developer settings (near the bottom on the left side of the page).\nClick the Developer settings link to reach the ‘apps’ page and then click on the Personal access tokens link.\nSelect Tokens (classic) from the left-hand menu and then click the Generate new token button.\n\n\n\n\n\n\n\nTipTypes of Personal Tokens\n\n\n\nYou now need to choose the type of token to generate. I personally find the old type of tokens easier to work with because the ‘new’ fine-grained tokens are intended to support complex workflows when all we’re trying to do is allow one computer to push/pull from Git.\n\n\n\nGenerate new token (classic) for general use token and then specify the following:\n\nI’d suggest writing FSDS Token or something similar in the Note section.\nSet the expiration to 90 days\nClick the repo tickbox for ‘Full control of private repositories’.\n\nSave the resulting token somewhere safe as you will need it again! (e.g. a Note on your phone, a password manager, etc.).\n\n\n\n\n\n\n\nWarningKeep your Personal Token Safe\n\n\n\nYou will need it at least twice in this tutorial and may want to use it again on other computers. You can always create a new one, but then you’ll need to update every computer where you access your GitHub repositories."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#cloning-your-repository",
    "href": "practicals/Practical-01-Getting_Started.html#cloning-your-repository",
    "title": "Practical 1: Getting Started",
    "section": "Cloning Your Repository",
    "text": "Cloning Your Repository\nNow we are going to clone (i.e. copy) the repository that you just created on to your own computer. This is surprisingly straightforward provided that you have installed the command line tools.\nOn your private repository page, click on the green button labeled Code visible in the screenshot below:\n\n\n\nScreenshot of cloning interface\n\n\nYou should then copy your HTTPS URL (in my screenshot it’s https://github.com/jreades/i2p.git).\nNow go back to the Terminal or PowerShell that you left open earlier and type the following (replacing &lt;the_url_that_you_copied_from_the_browser&gt; with the URL that you copied from GitHub):\ngit clone &lt;the_url_that_you_copied_from_the_browser&gt;\nThe first time that you do this, you will need to provide login information. Use your GitHub username and the Personal Access Token that you just created. On your computer you should now see a new directory with the same name as your repository. For example: Documents/CASA/fsds."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#storing-credentials-pulling",
    "href": "practicals/Practical-01-Getting_Started.html#storing-credentials-pulling",
    "title": "Practical 1: Getting Started",
    "section": "Storing Credentials & ‘Pulling’",
    "text": "Storing Credentials & ‘Pulling’\nYou can now activate the credtial helper that will store your Personal Access Token (though you should still keep your secure note!):\ncd fsds\ngit config credential.helper store\ngit pull\nWhen you type git pull you should be asked again for your username and password. You should (again) use the Personal Access Token as your password. You should not be asked again for pushing or pulling data into this GitHub repository. If you are not asked for your Personal Access Token then this likely means that your token is already saved and ready to use on all future ‘actions’."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#jupyter---your-computer",
    "href": "practicals/Practical-01-Getting_Started.html#jupyter---your-computer",
    "title": "Practical 1: Getting Started",
    "section": "Jupyter < - > Your Computer",
    "text": "Jupyter &lt; - &gt; Your Computer\nIf you now go back to Jupyter, you should now see that the work directory is actually the same as the CASA directory that you created earlier; you know this because you can now see an fsds directory that wasn’t there before.\nThis is because Podman is ‘mounting’ the work directory in the container to the CASA directory on your computer. This means that you can save files in the work directory in Jupyter and they will be saved in the CASA directory on your computer.\n\n\n\n\n\n\nNoteOrganising Your Work\n\n\n\nThere are any number of ways to organise your CASA work, what’s important is that you are logical about things like names and hierarchy. This will make it much easier to access files and notebooks using Podman, Quarto, and Python."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#adding-a-local-file-to-your-repository",
    "href": "practicals/Practical-01-Getting_Started.html#adding-a-local-file-to-your-repository",
    "title": "Practical 1: Getting Started",
    "section": "Adding a Local File to Your Repository",
    "text": "Adding a Local File to Your Repository\nIn order to tie together the different concepts covered above, we are now going add Practical 1 (this practical) to your GitHub repo. The easiest way to do this is to download the notebook by clicking on the Jupyter link on the right side of this page. So the process is:\n\nClick on the Jupyter link to save this file to your computer as a notebook with the extension .ipynb.\nMove the file to your new repository folder (e.g. $HOME/work/Documents/CASA/fsds/).\n\n\n\n\n\n\n\nTipFile Extensions\n\n\n\nIt is highly likely that your browser automatically added a .txt extension when you saved the Notebook file to your computer. You need to remove that ending to your file name or Jupyter won’t be able to run it. You can rename a file by either doing it directly in the Finder/Windows Explorer, or by moving (mv) the file:\nmv &lt;notebook_name&gt;.ipynb.txt &lt;notebook_name&gt;.ipynb\n\n\nIn the Terminal/PowerShell we now need add this file to Git so that it knows to keep track of it. Unlike Dropbox or OneDrive, just putting a file in a repo directory does not mean that Git will pay attention to it:\n# Assuming that you are 'in' the 'fsds' directory...\ngit add Practical-01-Getting_Started.ipynb\ngit commit -m \"Adding notebook 1 to repo.\"\n\n\n\n\n\n\nWarningAdd, Commit, Push, Repeat\n\n\n\nUnless you have added and committed a file to Git then it is not version controlled.\nUnless you have pushed your committed files to GitHub they are only backed up locally."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#status-check",
    "href": "practicals/Practical-01-Getting_Started.html#status-check",
    "title": "Practical 1: Getting Started",
    "section": "Status Check",
    "text": "Status Check\nWe now want to check that the file has been successfully added to Git. We do this with a status check in the repository directory (i.e. cd $HOME/work/Documents/CASA/fsds/):\ngit status\nYou should see something like:\nOn branch master\nYour branch is ahead of 'origin/master' by 1 commit.\n  (use \"git push\" to publish your local commits)\nThis is telling you that your local computer is 1 commit (the one that you just completed) ahead of the ‘origin’, which is on GitHub. GitHub doesn’t have to be the origin (nor does the repository have to be one that we created in order to be an origin) but conceptually and practically it’s easier to create new repositories on GitHub and clone them to our computer."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#keep-pushing",
    "href": "practicals/Practical-01-Getting_Started.html#keep-pushing",
    "title": "Practical 1: Getting Started",
    "section": "Keep Pushing",
    "text": "Keep Pushing\nTo synchronise the changes we just made, let’s follow Git’s advice:\ngit push\nYou should see something like the below series of messages (the exact details will differ, but the ‘Enumerating, Counting, etc’ messages will be the same):\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 306 bytes | 306.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nremote: This repository moved. Please use the new location:\nremote:   https://github.com/jreades/fsds.git\nTo https://github.com/jreades/fsds.git\n   7410d0e..45aa80a  master -&gt; master\nIf you now go over to your browser and visit your GitHub repo page (e.g. https://jreades.github.io/fsds/) — pressing the Reload button if you had the page open already — then you should see that the file you added on your computer is also showing up on the GitHub site as well! This means it’s now fully version-controlled and backed-up.\n\n\n\n\n\n\nWarningKeep Pushing\n\n\n\nUnless have pushed your commits to GitHub they are only stored on your computer. So your files can be properly version-controlled, but without a push if you lose your computer you still lose everything!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#more-about-git",
    "href": "practicals/Practical-01-Getting_Started.html#more-about-git",
    "title": "Practical 1: Getting Started",
    "section": "More About Git",
    "text": "More About Git\nFrom here on out you can keep changes made either directly on GitHub or locally on your computer (or any other computer to which you clone your repository) in synch by using git push (to push changes from a local computer up to the origin on GitHub) and git pull (to pull changes available on the origin down to the local computer).\nThat said, you can do a lot more than just push/pull to your own repository and this Twitter thread leads to a lot of useful additional resources to do with Git:\n\nIntroduction to Version Control with Git on the Programming Historian web site is written for digital humanities researchers so it’s intended to be accessible.\nOh My Git is an ‘open source game’ to help you learn Git.\nGit Meets Minesweeper? is apparently a ‘thing’.\nVisual Git Reference if you think visually or just want to check your understanding.\nVersion Control with Git is a Software Carpentries lesson that takes you quickly through the important elements of getting set up and started. It would be a good refresher.\nAltassian’s Documentation provides more detailed explanations of the commands and options.\nLearn Git Branching focusses on a key concept for software collaboration.\nGit Immersion provides a ‘guided tour’ of the fundamentals.\n\n\n\n\n\n\n\nTip\n\n\n\nFor the Group Work every member of your group will need to make contributions to a GitHub repository. This will require learning how to invite others to be contributors, how to merge changes, and how to deal with conflicts of the coding kind."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#other-useful-resources",
    "href": "practicals/Practical-01-Getting_Started.html#other-useful-resources",
    "title": "Practical 1: Getting Started",
    "section": "Other Useful Resources",
    "text": "Other Useful Resources\n\nGitHub Markdown Guide\nCommon Mark\nMarkdown Guide, which helpfully includes do’s and don’ts.\n\nFinally, these are a bit overkill but the bits about setting up and installing git, bash/zsh, and so on may come in handy later:\n\nSetting Up a New Mac\nBeginner’s Guide to Setting Up Windows 10\nSetting up Windows without Linux\nMicrosoft Python Setup Guide"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#gitignore",
    "href": "practicals/Practical-01-Getting_Started.html#gitignore",
    "title": "Practical 1: Getting Started",
    "section": ".Gitignore",
    "text": ".Gitignore\nThe main thing you should notice is the pattern: * means ‘anything’, while / at the end of a line implies a directory. So the following four lines should be added to your .gitignore file:\n*.zip\n*.gz\n*.csv\n*.gzip\n*.feather\n*.geofeather\n*.parquet\n*.geoparquet\ndata/\nThat’s it."
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html",
    "href": "practicals/Practical-03-Foundations_2.html",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "",
    "text": "In this notebook we are exploring basic (in the sense of fundamental) data structures so that you understand both how to manage more complex types of data and are prepared for what we will encounter when we start using pandas to perform data analysis. To achieve that, you will need to be ‘fluent’ in nested lists and dictionaries; we will focus primarily on lists-of-lists and dictionaries-of-lists, but note that file formats like JSON can be understood as dictionaries-of-dictionaries-of-lists-of-… so this is just a taster of real-world data structures."
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#list-refresher",
    "href": "practicals/Practical-03-Foundations_2.html#list-refresher",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "List Refresher",
    "text": "List Refresher\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nTo complete these tasks, all of the methods that you need are listed above, so this is about testing yourself on your understanding both of how to read the help and how to index elements in a list.\nThe next line creates a list of (made up) Airbnb property names where each element is a string:\n\nlistings = [\"Sunny 1-Bed\", \"Fantastic Dbl\",\n    \"Home-Away-From-Home\", \"Sunny Single\", \n    \"Whole House\", \"Trendy Terrace\"]\n\n\nList Arithmetic\nReplace the ?? so that it prints Sunny Single.\n\nQuestion\n\nprint(listings[?? + 1])\n\n\n\n\nNegative List Arithmetic\nNow use a negative index to print Whole House:\n\nQuestion\n\nprint(listings[??])\n\n\n\n\nFinding a Position in a List\nReplace the ?? so that it prints the index for Fantastic Dbl in the list.\n\nQuestion\n\nprint(\"The position of 'Fantastic Dbl' in the list is: \" + str( ?? ))"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#looking-across-lists",
    "href": "practicals/Practical-03-Foundations_2.html#looking-across-lists",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Looking Across Lists",
    "text": "Looking Across Lists\n\n\n\n\n\n\nNoteConnections\n\n\n\nThis section draws on the LOLs lecture and you will also find Code Camp’s Loops session useful here.\n\n\n\n\n\n\n\n\nWarningDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNotice that the list of prices is the same length as the list of listings, that’s because these are (made-up) prices for each listing.\n\nlistings = [\"Sunny 1-Bed\", \"Fantastic Dbl\", \"Home-Away-From-Home\", \"Sunny Single\", \"Whole House\", \"Trendy Terrace\"]\nprices = [37.50, 46.00, 125.00, 45.00, 299.99, 175.00]\n\n\nLateral Thinking\nGiven what you know about listings and prices, how do you print:\n\n\"The nightly price for Home-Away-From-Home is £125.0.\"\n\nBut you have to do this without doing any of the following:\n\nUsing a list index directly (i.e. listings[2] and prices[2]) or\nHard-coding the name of the listing?\n\nTo put it another way, neither of these solutions is the answer:\n\nprint(\"The nightly price for `Home-Away-From-Home` is £\" + str(prices[2]) + \".\")\n# ...OR...\nlisting=2\nprint(\"The nightly price for `Home-Away-From-Home` is £\" + str(prices[listings[listing]]) + \".\")\n\n\n\n\n\n\n\nTip\n\n\n\nYou will need to combine some of the ideas above and also think about the fact that the list index is that we need is the same in both lists… Also, remember that you’ll need to wrap a str(...) around your temperature to make it into a string.\n\n\n\nQuestion\n\nlisting=\"Home-Away-From-Home\" # Use this to get the solution...\n\n# This way is perfectly fine\nprint(\"The nightly price of \" + ?? + \" is \" + str(??))\n\n# This way is more Python 3 and a bit easier to read\nprint(f\"The nightly price of {??} is {??}\")\n\n\n\n\nDouble-Checking Your Solution\n\nlisting = 'Sunny Single'\n\nYou’ll know that you got the ‘right’ answer to the question above if you can copy+paste your code and change only one thing in order to print out: “The nightly price of Sunny Single is £45.0”\n\nQuestion\n\nlisting = 'Sunny Single'\nprint(??)\n\n\n\n\nLoops\nNow use a for loop over the listings to print out the price of each. But first, some information about formatting number in Python…\n\n\n\n\n\n\nTipFormatting Numbers\n\n\n\nWe often want to format numbers in a particular way to make the more readable. Commonly, in English we use commas for thousands separators and a full-stop for the decimal. Other countries follow other standards, but by default Python goes the English way. So:\n\nprint(f\"{1234567.25:.0f}\")\nprint(f\"{1234567.25:.1f}\")\nprint(f\"{1234567.25:.2f}\")\nprint(f\"{1234567:.2f}\")\nprint(f\"{1234567:,.2f}\")\n\n1234567\n1234567.2\n1234567.25\n1234567.00\n1,234,567.00\n\n\n\n\nThat should then help you with the output of the following block of code!\n\nQuestion\n\nfor l in listings:\n    ??\n\nThe output should be:\n\nThe nightly price of Sunny 1-Bed is £37.50\nThe nightly price of Fantastic Dbl is £46.00\nThe nightly price of Home-Away-From-Home is £125.00\nThe nightly price of Sunny Single is £45.00\nThe nightly price of Whole House is £299.99\nThe nightly price of Trendy Terrace is £175.00"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#creating-an-atlas",
    "href": "practicals/Practical-03-Foundations_2.html#creating-an-atlas",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Creating an Atlas",
    "text": "Creating an Atlas\nThe code below creates an atlas using a dictionary. The dictionary key is a listing, and the value is the latitude, longitude, and price.\n\nlistings = {\n    'Sunny 1-Bed': [37.77, -122.43, '£37.50'],\n    'Fantastic Dbl': [51.51, -0.08, '£46.00'],\n    'Home-Away-From-Home': [48.86, 2.29, '£125.00'],\n    'Sunny Single': [39.92, 116.40 ,'£45.00'],\n}\n\n\nAdding to a Dict\nAdd a record to the dictionary for “Whole House” following the same format.\n\nQuestion\n\n??\n\n\n\n\nAccessing a Dict\nIn one line of code, print out the price for ‘Whole House’:\n\nQuestion\n\n??"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#dealing-with-errors",
    "href": "practicals/Practical-03-Foundations_2.html#dealing-with-errors",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Dealing With Errors",
    "text": "Dealing With Errors\nCheck you understand the difference between the following two blocks of code by running them.\n\ntry:\n    print(listings['Trendy Terrace'])\nexcept KeyError as e:\n    print(\"Error found\")\n    print(e)\n\n\ntry:\n    print(listings.get('Trendy Terrace','Not Found'))\nexcept KeyError as e:\n    print(\"Error found\")\n    print(e)\n\nNotice that trying to access a non-existent element of a dict triggers a KeyError, while asking the dict to get the same element does not, it simply returns None. Can you think why, depending on the situtation, either of these might be the ‘correct’ answer?"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#thinking-data",
    "href": "practicals/Practical-03-Foundations_2.html#thinking-data",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Thinking Data",
    "text": "Thinking Data\nThis section makes use of both the Dictionaries lecture and the DOLs to Data lecture.\n\n\n\n\n\n\nTip\n\n\n\nIn this section you’ll need to look up (i.e. Google) and make use of a few new functions that apply to dictionaries: &lt;dictionary&gt;.items(), &lt;dictionary&gt;.keys(). Remember: if in doubt, add print(...) statements to see what is going on!\n\n\n\nIterating over a Dict\nAdapting the code below, print out the listing name and price.\n\nQuestion\n\nfor l in listings.keys():\n    print(??)\n\nThe output should look something like this:\n\nSunny 1-Bed -&gt; £37.50&lt;/code&gt;\nFantastic Dbl -&gt; £46.00&lt;/code&gt;\nHome-Away-From-Home -&gt; £125.00&lt;/code&gt;\nSunny Single -&gt; £45.00&lt;/code&gt;\nWhole House -&gt; £299.99&lt;/code&gt;\n\n\n\n\nMore Complex Dicts\nHow would your code need to change to produce the same output from this data structure:\n\nlistings = {\n    'Sunny 1-Bed': {\n        'lat': 37.77, \n        'lon': -122.43, \n        'price': '£37.50'},\n    'Fantastic Dbl': {\n        'lat': 51.51, \n        'lon': -0.08, \n        'price': '£46.00'},\n    'Home-Away-From-Home': {\n        'lat': 48.86, \n        'lon': 2.29, \n        'price': '£125.00'},\n    'Sunny Single': {\n        'lat': 39.92, \n        'lon': 116.40, \n        'price': '£45.00'},\n}\n\n\nQuestion\nSo to print out the below for each listing it’s…\n\nfor l in listings.keys():\n    print(??)\n\nYour output should be:\n\nSunny 1-Bed -&gt; £37.50\nFantastic Dbl -&gt; £46.00\nHome-Away-From-Home -&gt; £125.00\nSunny Single -&gt; £45.00\n\n\n\n\nMore Dictionary Action!\nAnd how would it need to change to print out the name and latitude of every listing?\n\nQuestion\n\nfor l in listings.keys():\n    print(??)\n\nThe output should be something like this:\n\nSunny 1-Bed is at latitude 37.77\nFantastic Dbl is at latitude 51.51\nHome-Away-From-Home is at latitude 48.86\nSunny Single is at latitude 39.92\n\n\n\n\nAnd Another Way to Use a Dict\nNow produce the same output using this new data structure:\n\nlistings_alt = [\n    {'name':     'Sunny 1-Bed',\n     'position': [37.77, -122.43],\n     'price':    '£37.50'},\n    {'name':     'Fantastic Dbl',\n     'position': [51.51, -0.08],\n     'price':    '£46.00'},\n    {'name':     'Home-Away-From-Home',\n     'position': [48.86, 2.29],\n     'price':    '£125.00'},\n    {'name':     'Sunny Single',\n     'position': [39.92, 116.40],\n     'price':    '£45.00'},\n    {'name':     'Whole House', \n     'position': [13.08, 80.28],\n     'price':    '£299.99'}\n]\n\n\nQuestion\n\nfor l in listings_alt:\n    print(??)\n\nThe output should be something like this:\n\nSunny 1-Bed is at latitude 37.77\nFantastic Dbl is at latitude 51.51\nHome-Away-From-Home is at latitude 48.86\nSunny Single is at latitude 39.92\nWhole House is at latitude 13.08\n\n\n\n\nThink Data!\nWhat are some of the main differences that you can think of between these data structures? There is no right answer.\n\nPoint 1 here.\nPoint 2 here.\nPoint 3 here."
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#add-to-gitgithub",
    "href": "practicals/Practical-03-Foundations_2.html#add-to-gitgithub",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Add to Git/GitHub",
    "text": "Add to Git/GitHub\nNow follow the same process that you used last week to ensure that your edited notebook is updated in Git and then synchronised with GitHub."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html",
    "href": "practicals/Practical-05-Objects.html",
    "title": "Practical 5: Objects",
    "section": "",
    "text": "This is a very challenging notebook because it takes you both through the process of building a function incrementally and through a ‘simple’ example of how Python classes actually work. You will need to understand these two very different elements in order to make the most of the remaining 6 weeks of term, because we both improve our code incrementally and make use of objects and their inheritances extensively. You also get an extra chance to revisit the differences between LoLs and DoLs because you will undoubtedly encounter and make use of these data structures even after you become a skillfull Python programmer."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#calculate-mean",
    "href": "practicals/Practical-05-Objects.html#calculate-mean",
    "title": "Practical 5: Objects",
    "section": "Calculate Mean",
    "text": "Calculate Mean\nLet’s start by calculating the sample mean (use Google: Python numpy mean...):\n\nQuestion\n\nimport numpy as ??\n# Use numpy function to calculate mean\nmean = ??.??(myData['Bedrooms'])\nprint(f\"The mean. number of bedrooms is {mean:,.1f}.\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#calculate-standard-deviation",
    "href": "practicals/Practical-05-Objects.html#calculate-standard-deviation",
    "title": "Practical 5: Objects",
    "section": "Calculate Standard Deviation",
    "text": "Calculate Standard Deviation\n\n\n\n\n\n\nTipDifficulty level: Low-ish.\n\n\n\n\n\n\n\n\n\nNow let’s do the standard deviation:\n\nQuestion\n\nimport numpy as np\n# Use a numpy function to calculate the standard deviation\nstd  = np.??(??)\nprint(f\"The standard deviation of bedrooms is {std:,.2f}.\")\n\nSo the numpy package gives us a way to calculate the mean and standard deviation quickly and without having to reinvent the wheel. The other potentially new thing here is {std:,.2f}. This is about string formatting and the main thing to recognise is that this means ‘format this float with commas separating the thousands/millions and 2 digits to the right’. The link I’ve provided uses the slightly older approach of &lt;str&gt;.format() but the formatting approach is the same."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#appending",
    "href": "practicals/Practical-05-Objects.html#appending",
    "title": "Practical 5: Objects",
    "section": "Appending",
    "text": "Appending\n\n\n\n\n\n\nTipDifficulty level: trivial\n\n\n\n\n\n\n\n\n\nAnd now let’s add it to the data set:\n\nmyData['Std. Bedrooms'] = rs\nprint(myData['Std. Bedrooms'])\n\nAnd just to show how everything is in a single data structure:\n\nfor c in myData['Name']:\n    idx = myData['Name'].index(c)\n    print(f\"Listing {c} has {myData['Bedrooms'][idx]:,} bedrooms and standardised score of {myData['Std. Bedrooms'][idx]:.3f}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#downloading-from-a-url",
    "href": "practicals/Practical-05-Objects.html#downloading-from-a-url",
    "title": "Practical 5: Objects",
    "section": "Downloading from a URL",
    "text": "Downloading from a URL\nLet’s focus on the first part first because that’s the precondition for everything else. If we can get the ‘download a file from a URL’ working then the rest will gradually fall into place through iterative improvments!\n\nFinding an Existing Answer\n\n\n\n\n\n\nTipDifficulty level: Low\n\n\n\n\n\n\n\n\n\nFirst, let’s be sensibly lazy–we’ve already written code to read a file from the Internet and save it locally. We’re going to start from this point.\n\nfrom urllib.request import URLError\nfrom urllib.request import urlopen\n\ntry:\n    url = 'https://orca.casa.ucl.ac.uk/~jreades/data/Listings.csv'\n    response = urlopen(url)\n    raw  = response.read()\n    data = raw.decode('utf-8')\n    with open('Listings.csv', 'w') as f:\n        f.writelines(data)\nexcept URLError as e:\n    print(f\"Unable to download data: {e}\")\n\n\n\nGetting Organised\n\n\n\n\n\n\nTipDifficulty level: Low\n\n\n\n\n\n\n\n\n\nLet’s take the code above and modify it so that it is:\n\nA function that takes two arguments: a URL and a destination filename.\nA function that checks if a file exists already before downloading it again.\n\nYou will find that the os module helps here because of the path function. And you will need to Google how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an explanation included of why a particular answer is a ‘good one’. I also look at which answers got the most votes (not always the same as the one that was the ‘accepted answer’). In this particular case, I also found this answer useful.\nI would start by setting my inputs:\n\nimport os\nurl = \"https://orca.casa.ucl.ac.uk/~jreades/data/Listings.csv\"\nout = os.path.join('data','Listings.csv') # Print `out` if you aren't sure what this has done!\n\n\n\nSketching the Function\n\n\n\n\n\n\nTipDifficulty level: Low, if you’ve watched the videos…\n\n\n\n\n\n\n\n\n\nThen I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:\n\nQuestion\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n    \n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(??):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n        \nget_url(url, out)\n\n\n\n\nFleshing Out the Function\n\n\n\n\n\n\nWarningDifficulty level: Medium\n\n\n\n\n\nIf you really explore what’s going on in the function rather than just running it and moving on.\n\n\n\nI would then flesh out the code so that it downloads the file if it isn’t found and then, either way, returns the local file path for our CSV reader to extract:\n\ndef get_url(src, dest):\n    \n    # Check if dest does *not* exist -- that\n    # would mean we had to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found locally!\")\n    else:\n        print(f\"{dest} not found, downloading!\")\n        \n        # Get the data using the urlopen function\n        response = urlopen(src) \n        filedata = response.read().decode('utf-8')\n        \n        # Extract the part of the dest(ination) that is *not*\n        # the actual filename--have a look at how \n        # os.path.split works using `help(os.path.split)`\n        path = list(os.path.split(dest)[:-1])\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how \n        # to use the 'splat' operator! os.makedirs creates missing \n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n        \n        with open(dest, 'w') as f:\n            f.write(filedata)\n            \n        print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#pathlib",
    "href": "practicals/Practical-05-Objects.html#pathlib",
    "title": "Practical 5: Objects",
    "section": "Pathlib",
    "text": "Pathlib\nPathlib is intended to work around the fact that different Operating Systems represent paths on file systems in different ways. Using the os library there is quite a lot of ‘faff’ involved in managing this: whereas Unix, Linux, and the macOS (which is BSD Unix under the hood) use forward slashes in a path (e.g. /Users/&lt;username&gt;/Documents/...)1, Windows usess backslashes (e.g. C:\\Users\\&lt;username&gt;\\My Documents\\...). The differences don’t stop there, but that’s enough to see how life can get complicated; in os this is managed by the bundle of functions and constants under os.path and you can then use os.path.sep.join([&lt;a list representing a path on the computer&gt;]).\nPathlib does away with all of that. Remember that we’ve defined an object as a bundle of data and functions that act on the data. So the Path class is both data about a location on a computer and a set of functions that can manipulate that data. Let’s see what that means in practice:\n\nfrom pathlib import Path\n\nhere = Path('.')\nprint(\"Using a directory: \")\nprint(f\"\\tHere: {here}\")\nprint(f\"\\tParents: {here.parents}\")\nprint(f\"\\tURI: {here.absolute().as_uri()}\")\nprint(f\"\\tIs directory: {here.is_dir()}\")\nprint(f\"\\tIs file: {here.is_file()}\")\nprint(f\"\\tParts: {here.resolve().parts}\")\nprint(f\"\\tDirectory contents: \\n\\t\\t{'\\n\\t\\t'.join([str(x) for x in list(here.glob('*.qmd'))[:3]])}...\")\n\nprint()\n\ntry:\n    fn = here / 'Practical-05-Objects.qmd' # Try changing the ending to html after running\n    print(\"Using a file:\")\n    print(f\"\\tResolved path: {fn.resolve()}\")\n    print(f\"\\tExists: {fn.exists()}\")\n    print(f\"\\tIs file: {fn.is_file()}\")\n    print(f\"\\tFile size (bytes): {fn.stat().st_size:,} bytes\")\n    print(f\"\\tOwner: {fn.owner()}\")\nexcept FileNotFoundError:\n    print(\"File not found!\")\n\nHopefully you can now see how the object-oriented approach of pathlib allows to write neater, more readable code than the older os module? We no longer have to keep passing in a list or string representing a path on a specific operating system, we can just pass around Path objects and let Python take care of turning them into concrete paths on a computer when we need to ‘act’ on the Path in some way (check that it exists, create it, interrogate it, etc.).\n\n\n\n\n\n\nNoteUpdating…\n\n\n\nIn fact, I’m still updating the practicals to make full use of pathlib so you may find places where the older os approach is still in use. If you spot one of these why not submit an Issue or even fix it in a copy of the FSDS repor and then submit a Pull Request on GitHub to correct the code?"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#updating-the-function",
    "href": "practicals/Practical-05-Objects.html#updating-the-function",
    "title": "Practical 5: Objects",
    "section": "Updating the Function",
    "text": "Updating the Function\nSo let’s update the function to move from os to pathlib and see how things change…\n\nfrom functools import wraps\nfrom pathlib import Path\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        src  = Path(args[0])\n        dest = Path(args[1])\n        if dest.exists() and dest.is_file():\n            print(f\"{dest} found locally!\")\n            return(dest)\n        else:\n            print(f\"{dest} not found, downloading!\")\n            return(f(src, dest))\n    return wrapper\n\n@check_cache\ndef get_url(src, dest):    \n    # Get the data using the urlopen function\n    response = urlopen(src) \n    filedata = response.read().decode('utf-8')\n     \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how \n    # to use the 'splat' operator! os.makedirs creates missing \n    # directories in a path automatically.\n    dest.parent.mkdir(parents=True, exist_ok=True)\n     \n    with dest.open(mode='w') as f:\n        f.write(filedata)\n         \n    print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\nprint(f\"{src} is a {type(src)}.\") # &lt;- Note this change!"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#abstract-base-class",
    "href": "practicals/Practical-05-Objects.html#abstract-base-class",
    "title": "Practical 5: Objects",
    "section": "Abstract Base Class",
    "text": "Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the listing_type to the name of the class automatically (so a flat object has shape_type='Flat') and it stores the critical accommodation details.\nIt provides methods (which only raise exceptions) that will allow all listings to be used interchangeably in the right context (e.g. determining availability).\n\n\n# We first need to install a library for managing\n# availability in a highly optimised way.\ntry:\n    from bitarray import bitarray\nexcept:\n    ! pip install bitarray\n    from bitarray import bitarray\n\n\nfrom bitarray import bitarray\n\n# This automatically generates methods and\n# other useful features for child classes\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass Listing(object): # Inherit from base class \n\n    listing_name: str = field(default_factory=str)\n    listing_price: float = field(default_factory=float)\n    listing_bedrooms: int = field(default_factory=int)\n    listing_availability: bitarray = field(default_factory=bitarray)\n    \n    def name(self) -&gt; str:\n        return self.listing_name\n\n    def price(self) -&gt; float:\n        return self.listing_price\n    \n    def bedrooms(self) -&gt; int:\n        return self.listing_bedrooms\n    \n    def is_available(self, requested:bitarray) -&gt; bitarray:\n        return bitarray(self.listing_availability & requested).any()\n\n    def type(self):\n        return self.__class__.__name__\n\nWe can now create a new listing object (an instance of the Listing class) but we can’t do much that is useful with it:\n\nl = Listing('A listing', 25.00, 3, bitarray(14))\n\ntry: \n    print(f\"I am a {l.type()}\")\n    print(f\"My name is {l.name()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#flat",
    "href": "practicals/Practical-05-Objects.html#flat",
    "title": "Practical 5: Objects",
    "section": "Flat",
    "text": "Flat\nImplements a flat:\n\nAdds ‘Flat:’ or ‘Studio:’ to all requests for the name.\nAdds a ‘floor’ method to track what floor the flat is on.\nChecks if the number of ‘bedrooms’ is 0 (in which case it’s a ‘Studio:’) or more (in which case it’s a ‘Flat:’).\n\n\nQuestion\nCan you work out the missing elements that will allow you to create a flat class? We’ll want to know the floor and whether we’re dealing with a Flat or a Studio by looking at the name of the listing.\n\n# Flat class\nclass Flat(Listing): # Inherit from listing \n\n    listing_floor = -99\n\n    def __init__(self, name:str, price:float, bedrooms:int, floor:int, availability:bitarray=bitarray(14)):\n        ??.__init__(name, price, bedrooms, availability)\n        self.listing_floor = ??\n\n# If you've done everything correctly then\n# you will no longer get an error here...\nf = Flat('Room With a View', 73.00, 0, )\n\ntry: \n    print(f\"I am a {f.type()}\")\n    print(f\"My name is {f.name()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nYou should get:\n\nI am a Flat\nMy name is Studio: Room With a View\n\nIf you were to create a second flat listing you can see how things change automatically based on the attributes:\n\nf2 = Flat('Room With a Better View', 123.00, 2, 6, bitarray('10000001000000'))\nprint(f\"I am a `{f2.type()}`\")\nprint(f\"My name is `{f2.name()}`\")\n\n\nI am a Flat\nMy name is Flat: Room With a Better View"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#houseboat",
    "href": "practicals/Practical-05-Objects.html#houseboat",
    "title": "Practical 5: Objects",
    "section": "Houseboat",
    "text": "Houseboat\nImplements a houseboat listing:\n\nAdds an ‘on_water’ boolean attribute.\nAdds ‘Floating Palace:’ to the name if the houseboat is on water, or ‘Grounded!’ if it’s not.\n\n\nQuestion\nCan you work out the missing elements that will allow you to create a houseboat class?\n\n# Houseboat class\nclass Houseboat(Listing): # Inherit from shape\n    def __init__(self, ...):\n        # Something...\n\n    def on_water(self):\n        # Something...\n\n# If you've done everything correctly then\n# you will no longer get an error here...\nh = Houseboat(15)\n\ntry: \n    print(f\"I am a {h.type()}\")\n    print(f\"My name is {h.name()}\")\n    if h.on_water():\n        print(\"Where I expect a houseboat to be.\")\n    else:\n        print(\"Something has gone very wrong!\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#availability",
    "href": "practicals/Practical-05-Objects.html#availability",
    "title": "Practical 5: Objects",
    "section": "Availability",
    "text": "Availability\nThe last thing we want to do is look at how (and why) we’ve recorded availability. Let’s pretend that this can only be measured 2 weeks in advance (i.e. 14 days). Let’s look at how the bitarray can help us here:\n\nb1 = bitarray(14)\nb2 = bitarray('11111111111111')\nb3 = bitarray('01010101010101')\n\nprint(b1 & b2)\nprint(b2 & b3)\nprint(b1 | b2)\nprint(b1 | b3)\nprint(bitarray(b1 & b3).any())\nprint(bitarray(b1 | b3).any())\nprint(bitarray(b1 & b3).count())\nprint(bitarray(b1 | b3).count())\n\nDoes it now make sense why the is_available is set up the way it is?"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#pulling-it-all-together",
    "href": "practicals/Practical-05-Objects.html#pulling-it-all-together",
    "title": "Practical 5: Objects",
    "section": "Pulling It All Together",
    "text": "Pulling It All Together\nLet’s finish with a demonstration of how we can use these classes in some kind of application:\n\nQuestion\n\nf1 = Flat('Room With a View', 73.00, 0, 5, bitarray('11000001100000'))\nf2 = Flat('Room With a Better View', 123.00, 2, 6, bitarray('10000001000000'))\nh1 = Houseboat('Bargemaster', 5.25, 1)\nh2 = Houseboat('Sinking', 15.25, 1, bitarray('11111110000000'))\n\nlistings = [f1, f2, h1, h2]\n\n# You are looking for availability on Mondays...\nsearch = bitarray( ?? )\n\navailable = [x for x in listings if ??]\n\nprint(\"The following options are available: \")\nfor a in available:\n    print(f\"\\t{a.name()} is available for £{a.price():,.2f}\")\n\nYou should get that Studio: Room With a View and Floating Palace: Sinking are available."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#footnotes",
    "href": "practicals/Practical-05-Objects.html#footnotes",
    "title": "Practical 5: Objects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSlight exception to this: the macOS also recognises : as a path separator↩︎"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html",
    "href": "practicals/Practical-07-Spatial_Data.html",
    "title": "Practical 7: Spatial Data",
    "section": "",
    "text": "Last week we did some initial processing on the Inside Airbnb listings data, focussing on its numeric properties. This week we are going to focus on the spatial properties of the data set."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#caching-remote-data",
    "href": "practicals/Practical-07-Spatial_Data.html#caching-remote-data",
    "title": "Practical 7: Spatial Data",
    "section": "Caching Remote Data",
    "text": "Caching Remote Data\nWe don’t want to continually download data over the Internet: 1) because it’s not nice to whoever is hosting the data; and 2) because it requires you be online in order to run your code. So if we can ‘cache’ the data locally so that it’s only downloaded once this makes life much, much easier.\nI’ve used the Numpy-style comments here, but the Google-style also look good in this context and all styles of answer are acceptable so long as they work. See overview of commenting styles on DataCamp.\n\n\n\n\n\n\nTip\n\n\n\nUse this as an opportunity to improve your ability to read code and to learn through documentation.\n\n\n\n\n\n\n\n\nWarningDifficulty level: Moderate\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nfrom pathlib import Path\nfrom requests import get\nfrom functools import wraps\n\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(src:str, dst:str, min_size=100) -&gt; Path:\n        url = Path(src) \n        fn  = url.name  \n        dsn = Path(f\"{dst}/{fn}\") \n        if dsn.is_file() and dsn.stat().st_size &gt; min_size:\n            print(f\"+ {dsn} found locally!\")\n            return(dsn)\n        else:\n            print(f\"+ {dsn} not found, downloading!\")\n            return(f(src, dsn))\n    return wrapper\n\n@check_cache\ndef cache_data(src:Path, dst:Path) -&gt; str:\n    \"\"\"Downloads a remote file.\n    \n    \n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n      \n    # Create...\n    if not dst.parent.exists():\n        dst.parent.mkdir(parents=True, exist_ok=True)\n        \n    # Download and write the file\n    with dst.open(mode='wb') as file:\n        response = get(src)\n        file.write(response.content)\n        \n    print(' + Done downloading...')\n\n    return dst.resolve()\n\nhelp(cache_data) # &lt;- This should show the docstring you've written"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#downloading-the-data",
    "href": "practicals/Practical-07-Spatial_Data.html#downloading-the-data",
    "title": "Practical 7: Spatial Data",
    "section": "Downloading the Data",
    "text": "Downloading the Data\n\n\n\n\n\n\nTipDifficulty level: Low, if your function works!\n\n\n\n\n\n\n\n\n\n\nGeopackages\nUse the function above to download and cache the GeoPackage files found on GitHub for Boroughs, Water, and Greenspace, then pass the output of these to GeoPandas. If you have been having trouble downloading files from GitHub, then use the understanding of the function developed above to download the file manually and place it where this function expects to find it!\n\nQuestion\n\nddir  = Path('data/geo') # destination directory\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\n\nboros = gpd.read_file( ??(spath + 'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( ??(spath + 'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( ??(spath + 'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n\n\n\n\nParquet\nLet’s re-use our cache_data function to download and save the full Inside Airbnb data set. Again, if you have trouble with downloading via code, use your understanding of the function to work out where to save your own copy of this file so that the function works as expected.\n\n\n\n\n\n\nTipDifficulty level: Low\n\n\n\n\n\n\n\n\n\n\nymd  = '20250615'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.parquet'\n\n\nQuestion\n\n# your code here\ndf = pd.read_parquet( ??(??, Path('data/raw').resolve()) )\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nYou should see that the file was ‘not found’ so ‘downloading’ happened and then the size of the data frame was printed out."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#creating-a-geodataframe",
    "href": "practicals/Practical-07-Spatial_Data.html#creating-a-geodataframe",
    "title": "Practical 7: Spatial Data",
    "section": "Creating a GeoDataFrame",
    "text": "Creating a GeoDataFrame\n\n\n\n\n\n\nTipDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nRight, we’re finally there! We need to convert our coordinates into some kind of geo-data. GeoPandas offers two ways to do this: the original way using zip and a new utility method called points_from_xy. Here’s the old way:\n\nfrom shapely.geometry import Point\ngdf = gpd.GeoDataFrame(df, \n                geometry=[Point(x,y) for x, y in zip(df.Longitude,df.Latitude)])\n\nNote, however, that this does not automatically set a projection, unlike the new approach with the ‘helper function’:\n\ngdf = gpd.GeoDataFrame(df,\n      geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))\n\n\nprint(type(gdf))\nprint(type(gdf.geometry))\ngdf.geometry.iloc[1:5]"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#saving-geo-data-locally",
    "href": "practicals/Practical-07-Spatial_Data.html#saving-geo-data-locally",
    "title": "Practical 7: Spatial Data",
    "section": "Saving Geo-Data Locally",
    "text": "Saving Geo-Data Locally\n\nThink Storage Formats\nWe want to save the InsideAirbnb GeoDataFrame to the ‘geo’ directory, but first let’s see what file formats are supported.\n\n\n\nFormat\nAdvantages\nDisadvantages\n\n\n\n\nShapefile\nWide support\nEasy to break via loss of individual file element and not very efficient format\n\n\nGeoJSON\nHuman-readable\nHighly inefficient storage format\n\n\nGeoPackage\nLightweight spatial database with benefits of indexing\nLightweight spatial database with limits on data types supported\n\n\nGeoParquet\nHighly compressed columnar database\nLimited GIS support (except QGIS), and DuckDB has issues.^[You can install the Spatial Extension.\n\n\n\nGeoPackages are really well-supported by QGIS: you can even embed multiple layers with different style information so that your final analysis is easy to distribute; however, they also carry quite a bit of overhead that makes them inefficient for distributing smaller data sets, while also not supporting the full spectrum of Pythonic data structures such as categorical data or lists.\nGeoParquet is an extension of the Parquet format. For working with large data sets this has revolutionised my workflow: getting excited about a columnar database might seem a bit… nerdy… but it’s been transformative for many data scientists. First, because the data set is columnar you only read in the data that you need, so reading Parquet file is blindingly fast. Second, you can stream data from a Parquet file over the Internet, so that means you even gain these advantages reading remote files. Third, you have full support for Python data types, including (up to a point) objects. And, fourth, you can treat multiple Parquet files with the same layout as a single data set or mutiple Parquet files with different layouts as tables in a single database!\n\n\nSpecifying a Driver\nSince there are many formats in which to save geo-data, rather than have multiple to_format_x methods, GeoPandas has tried to have one for local files (to_file). Normally, Geopandas will do the ‘right thing’ based on the extension: so if it sees .shp it will use the ESRI Shapefile driver, if it sees .gpkg it will use the Geopackage dirver, and the same for .geojson, etc.\nThere are two ways you may run into trouble:\n\nIf you are reading/writing a URL (e.g. https://github.com/jreades/…/src/Boroughs.gpkg?raw=true) where there is content after the .gpkg exten sion. If in doubt, specify the driver.\nIf you are reading/writing a Parquet file then you need to use to_parquet() and read_parquet() for compatibility with Pandas.\n\nSo why didn’t we need to do with the cache_data function earlier as well? Well, this was a side-benefit of using the standard URL library: it automatically stripped off the query string (?raw=true) when I asked it for the file name, so we saved the file locally as a GeoPackage with .gpkg extension, which means that GeoPandas could read it without any problems.\nSo the following two bits of code are equivalent:\n# Should be fine for local files but...\nboros.to_file('test.gpkg') \n# This is safer if working across multiple computers/the Internet\nboros.to_file('test.gpkg', driver='GPKG')\nYou may find that saving to a GeoPackage fails with a TypeError because of the presence of Categorical data in the geo-data frame. This behaviour seems to have changed recently so that it may no longer be an issue, but a ‘geoparquet’ file will not have this problem, so that’s another reason to use it!"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#spatial-indexing",
    "href": "practicals/Practical-07-Spatial_Data.html#spatial-indexing",
    "title": "Practical 7: Spatial Data",
    "section": "Spatial Indexing",
    "text": "Spatial Indexing\nIt’s also worth comparing the output of a point with the output of a polygon or multi-polygon because you may well come across data in formats (e.g. WKT) resembling both of these in real data sets and they can be read as well. Notice too that we can use loc and iloc accessor methods to pull individual points and polygons out of a GeoDataFrame!\n\nprint(gdf.geometry.iloc[1]) # Print out the object's contents\ngdf.geometry.iloc[1] # The object knows how to print itself as a point\n\n\n# Object to string then print out first 399 characters\nprint(str(boros.geometry.iloc[1])[:399] + \"...\") \n# So this is a multi-polygon boundary\nboros.geometry.iloc[1] \n\nSo each element of this Series has text indicating the type of shape the geometry applies to (e.g. POLYGON) followed by a bunch of numbers. These numbers are truncated here just to make things a little more legible."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#work-out-the-colour-scheme",
    "href": "practicals/Practical-07-Spatial_Data.html#work-out-the-colour-scheme",
    "title": "Practical 7: Spatial Data",
    "section": "Work Out the Colour Scheme",
    "text": "Work Out the Colour Scheme\n\n\n\n\n\n\nWarningDifficulty level: Moderate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nR and Python take very different approaches to plotting. Do not think of Python’s output as being ‘maps’ in the GIS sense, they are composed of ‘patches’ of color on abstract ‘axes’ that can use any arbitrary coordinate space. So colours are ‘really’ triplet (or quadruplet if you have alpha-blending transparency) values in the range 0.0-1.0. Annotations are then added in similarly abstract fashion.\n\n\nI’d suggest the following colour scheme as a way to test out different ways of specifying colour (though anything you like is fine so long as you manipulate the colours):\n\nThe boroughs can have red edges and white fill with a thick edge.\nThe water should have no edges and XKCD Lightblue fill.\nThe greenspace should have edges and faces specified using different ‘alpha blending’ (i.e. transparency) levels.\n\n\nBoroughs\nBy way of a hint, matplotlib uses edgecolor and facecolor for controlling ‘patches’ (which is what polygons are considered), but the thicker-than-default line-width is specified differently (you’ll need to look this up). So the intention is:\n\nThick red borough borders, and\nWhite fill colour.\n\nJust to drive home how different this is from R, you can find the answer to question 1 on the page for bar plots.\n\nQuestion\n\nboros.plot(??, figsize=(8,6))\n\nYour plot should look similar to this:\n\n\n\n\nWater\nThe process is the same as above, but I’d like you to work out how to specify: 1. No color for an edge, and 2. An XKCD color for the face.\n\nQuestion\n\nwater.plot(??, figsize=(8,6))\n\nYour plot should look similar to this:\n\n\n\n\nGreenspace\nThe process is also the same as above, but I’d like you to work out how to specify colours and transparency using RGBA (red-green-blue-alpha transparency) tuples. So we’re looking for:\n\nNo edge color.\nA partially transparent green specified as a ‘tuple’ (4 numbers in parentheses in the range 0.0-1.0).\n\n\nQuestion\n\ngreen.plot(??, facecolor=(??), figsize=(8,6))\n\nYour plot should look similar to this:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#combining-layers",
    "href": "practicals/Practical-07-Spatial_Data.html#combining-layers",
    "title": "Practical 7: Spatial Data",
    "section": "Combining Layers",
    "text": "Combining Layers\n\n\n\n\n\n\nWarning\n\n\n\nR and Python take very different approaches to plotting. Do not think of Python’s output as being ‘maps’ in the GIS sense, they are composed of ‘patches’ of color on abstract ‘axes’ that can use any arbitrary coordinate space. So colours are ‘really’ numerical triplets (or quadruplets if you have transparency as well) in the range 0.0-1.0. Annotations are then added in similarly abstract fashion.\n\n\nNow that we’ve got our layers looking roughly how we want them, it’s time to combine them. This is also reliant on matplotlib and basically involves plotting items to shared axes which is done by passing in ax=&lt;axis object&gt; to each plot(...). By convention, if you only have a single figure (e.g. a single map) then you create an axis object and name it ax so you will see a lot of ax=ax code in graphing libraries, but =ax is just saying ‘assign to the axis object that I created’.\nSince the axes are how you control what is shown, see if you can find out by Googling how to set the x- and y-limits on the map so that it shows only London and trims out the much larger area of water that is outside of the Greater London Authority. As a rough guideline, this has the Easting range 501,000 to 563,000, and the Northing range 155,000 to 202,000.\nYou can set these limits before or after you start adding layers to the ‘map’, but it’s probably easier conceptually to add them after with the idea of ‘zooming in’ on the features of interest. It’s also easier to debug since you can start by seeing if you can plot the elements at all, and then add the limits to zoom.\nSo the steps are:\n\nWrite the code to plot every image on the same set of axes (I’ve given you something to get started).\nGoogle how to set the limits of the map and then use the ranges I’ve offered above.\nWork out how to change the width of the edges for the boroughs layer.\nSave it somewhere local so that you could, say, load it into a Markdown file!\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a first pass at a map, over the next few weeks we’ll see how to add things like axis labels and titles to make it more ‘map-like’. We don’t have quite the built-in functionality of ggplot alas, but Python is advancing very quickly in this area. There is even an implementation of ggplot in Python, but it’s functionality is more limited. In fact, there’s more than one…\n\n\n\n\n\n\n\n\nCautionDifficulty level: Hard\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n# Creates a new figure with specified number of\n# subplots (we'll see more of this later) and \n# and the specified size (in inches by default).\nfig, ax = plt.subplots(1,1, figsize=(8,6))\n\n# Plot all three GeoPackages to the same axes\nwater.plot(??, ax=ax)\ngreen.??\nboros.??\n\n# Set the x and y limits\n\n\n# Save the image (dpi is 'dots per inch')\nPath('img').mkdir(parents=True, exist_ok=True)\nplt.savefig(Path('img/My_First_Map.png'), dpi=150)\n\nYou may wish for a different look, but here’s one version of the output:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#choropleth-plots",
    "href": "practicals/Practical-07-Spatial_Data.html#choropleth-plots",
    "title": "Practical 7: Spatial Data",
    "section": "Choropleth Plots",
    "text": "Choropleth Plots\n\n\n\n\n\n\nTipDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nNow that we’ve converted the InsideAirbnb data to a GeoDataFrame, we can plot it, reproject it, etc.\nSee if you can work out how to plot the points coloured by their price using the appropriate BNG projection.\n\nQuestion\n\ngdf.to_crs(??).plot(column=??, cmap=??, alpha=0.25, markersize=1, figsize=(8,6));\n\nUsing the Viridis colourmap I get the following:\n\n\n\nWork out the Data Range\n\n\n\n\n\n\nTipDifficulty level: Low\n\n\n\n\n\n\n\n\n\nAs we saw above with the point-plot, in its original form the pricing data will not reveal much of interest because of the range of the data. However, using transformations we can manipulate the data to increase its tractability for analysis. We saw several transformations last week (z-score, natural log, etc.).\nLet’s start by getting a feel for the full data set in terms of the range of prices that it contains:\n\nQuestion\n\nprint(f\"The range of price is ${??:,.2f} to ${??:,.2f}\")\nprint(f\"The mean and median of the price are ${??:,.2f} and ${??:,.2f}\")\n\nNotice the neat little comma-separated thousands in there? That’s fairly easy to do in English, but to use a thousands separator common to another language you would need to do something a little more tricky.\n\n\n\nInheritance!\nWe already know that GeoPandas inherits functionality from Pandas, but let’s formalise this…\nFirst, let’s check what class of object gdf is using the isinstance function:\n\n# Is gdf a GeoDataFrame object?\nif isinstance(gdf, gpd.GeoDataFrame): \n    print(\"\\tI'm a geopandas data frame!\")\n\n# Is gdf *also* a DataFrame object?\nif isinstance(gdf, pd.DataFrame): \n    print(\"\\tI'm a pandas data frame!\")\n\n\n\nBenefiting from Inheritance\nThat result means that we can also investigate the data using, for instance, a pandas histogram and here are two ways to look at the pricing data:\n\n# Oooooh, let's use a *pandas* method here\ngdf.price.plot.hist(bins=200, xlim=(0,1000), figsize=(8,3)); \n\n\n# Oooooh, let's use a *pandas* method here\ngdf[gdf.price &lt; 1000].price.plot.hist(bins=200, xlim=(0,1000), figsize=(8,3)); \n\n\nQuestion\nCan you explain why these two plots look very different?\nNotice how we’ve used our GeoDataFrame as if it’s a plain old DataFrame here? That’s the miracle of Object-Oriented Design: we can do anything we would with a regular Pandas df as we do with a GeoPandas gdf because GeoPandas inherits all the methods of its parent super-class.\nWe can see that there’s very little data above (at a guess) about $1,000, but at this scale it’s hard to tell. We’ve already seen that you can use axes limits to adjust the display of a map, but the same technique applies to plain old plots because they’re fundamentally the same thing in that they’re both matplotlib outputs.\nYou can do the same thing with a boxplot:\n\n\nQuestion\n\nax = gdf.price.plot.??(vert=False, figsize=(8,3))\nax.??\n\nMore complex formatting is also possible if you really know your pandas and your matplotlib:\n\ngdf.price.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#truncate-and-transform",
    "href": "practicals/Practical-07-Spatial_Data.html#truncate-and-transform",
    "title": "Practical 7: Spatial Data",
    "section": "Truncate and Transform",
    "text": "Truncate and Transform\n\n\n\n\n\n\nCautionDifficulty level: Hard\n\n\n\n\n\n\n\n\n\n\nWorking it Out\nAnyway, drawing on everything we’ve seen over the past couple of weeks I’d like you to:\n\nTry to take the natural-log of the price (hint: use numpy) and assign to a new Series called lnprice.\nWork out what the error means.\nWork out how to fix the error and then repeat step 1.\nWork out how many rows were affected.\nReport on the new min/max values.\nWork out if other outliers need to be removed (use code from above).\nRemove outliers and then continue with your work…\n\n\n# Use this as a 'scratch space' to work out what's needed below...\n\n\nQuestion\n\nprint(f\"gdf has {gdf.shape[0]:,.0f} rows.\")\n\n# ---------- Do the processing -------------\n# This will throw an error 'as is'\ngdf['lnprice'] = np.log(gdf.price)\n\n# ---------- Check effects -----------\nprint(f\"The range of ln(price) is {gdf.lnprice.min():,.4f} to {gdf.lnprice.max():,.4f}\")\n\ngdf.lnprice.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);\nplt.title(\"Ln(Price) (Outliers not shown)\")\nplt.show()"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#plot-options",
    "href": "practicals/Practical-07-Spatial_Data.html#plot-options",
    "title": "Practical 7: Spatial Data",
    "section": "Plot Options",
    "text": "Plot Options\nNow plot the ln(price) as a chloropleth using:\n\nA figure size of 9 x 6\nA marker size of 0.25\nThe Viridis colourmap\nA legend\nA legend label of ‘Natural Log of Price per Night ($)’\n\nI’d suggest referring to the documentation.\n\nQuestion\n\nax = gdf[ ?? ].plot(figsize=??, marker='*', markersize=0.25, \n         column=??, cmap=??, \n         legend=??, legend_kwds=??);\nax.set_title(\"Plot of Natural Log of Nightly Price for Airbnb Listings (Outliers Removed)\");\n\nYou should get something like:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#zooming-inout",
    "href": "practicals/Practical-07-Spatial_Data.html#zooming-inout",
    "title": "Practical 7: Spatial Data",
    "section": "Zooming In/Out",
    "text": "Zooming In/Out\n\n\n\n\n\n\nTipDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nThat’s a little hard to see, let’s try zooming in on Central London! Very roughly, let’s call that an Easting range of 525,000 to 535,000 and a Northing range of 178,000 to 185,000.\n\n\n\n\n\n\nTipPlotting\n\n\n\nWe show one way to do this below (f,ax = plt.subplots(...)) because it gives you greater control, but gdf.plot() can return an axis object (ax = gdf.plot(...)) that gives you the same kind of access… but with a bit more ‘faff’.\n\n\n\nQuestion\n\n# Note this new f,ax syntax and that we then\n# pass ax=ax to gdf.plot -- this has to do with\n# where and how things are plotted.\ngdf = gdf[ (gdf.lnprice &gt; 0) & (gdf.lnprice &lt; np.log(1000)) ]\nf,ax = plt.subplots(1,1,figsize=(8,6))\ngdf.plot(ax=ax, marker='*', markersize=0.25, \n         column='lnprice', cmap='viridis', \n         legend=True, legend_kwds={'label':'Natural Log of Price per Night ($)'});\nax.set_title(\"Ln(Price/Night) for Airbnb Listings (Central London Detail)\")\nax.??\nax.??\nf\n\nYour result should look something like this:\n\nThat’s a little better, but ideally we’d do more thinking about outliers…"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#changing-the-classification-scheme",
    "href": "practicals/Practical-07-Spatial_Data.html#changing-the-classification-scheme",
    "title": "Practical 7: Spatial Data",
    "section": "Changing the Classification Scheme",
    "text": "Changing the Classification Scheme\n\n\n\n\n\n\nWarningDifficulty Level: Moderate (mainly computation time)\n\n\n\n\n\n\n\n\n\nLet’s give this one last try using the quantiles classification scheme for Central London!\n\nQuestion\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax = gdf.plot(marker='*', markersize=0.25, \n         column='lnprice', cmap='viridis', ??, k=5, \n         legend=True, ax=ax); \n         # Note that the legend *label* had to go -- \n         # there are other ways to add it.\n\nax.set_xlim([525000,535000])\nax.set_ylim([178000,185000])\n\nYour answer should look like:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#credits",
    "href": "practicals/Practical-07-Spatial_Data.html#credits",
    "title": "Practical 7: Spatial Data",
    "section": "Credits!",
    "text": "Credits!\n\nContributors:\nThe following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk), James Millington (james.millington@kcl.ac.uk)\n\n\nLicense\nThese teaching materials are licensed under a mix of The MIT License and the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license.\n\n\nAcknowledgements:\nSupported by the Royal Geographical Society (with the Institute of British Geographers) with a Ray Y Gildea Jr Award.\n\n\nPotential Dependencies:\nThis notebook may depend on the following libraries: geopandas, pandas, matplotlib, seaborn"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html",
    "href": "practicals/Practical-08-Textual_Data.html",
    "title": "Practical 8: Working with Text",
    "section": "",
    "text": "Working with text is unquestionably hard. In fact, conceptually this is probaly the most challenging practical of the term! But data scientists are always dealing with text because so much of the data that we collect (even more so thanks to the web) is not only text-based (URLs are text!) but, increasingly, unstructured (social media posts, tags, etc.). So while getting to grips with text is a challenge, it also uniquely positions you with respect to the skills and knowledge that other graduates are offering to employers."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#required-modules",
    "href": "practicals/Practical-08-Textual_Data.html#required-modules",
    "title": "Practical 8: Working with Text",
    "section": "Required Modules",
    "text": "Required Modules\n\n\n\n\n\n\nNote\n\n\n\nNotice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: sklearn, nltk, and spacy.\n\n\nStandard libraries we’ve seen before.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport re\nimport math\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\n\nThis next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:\n\nfrom IPython.display import display_markdown\n\ndef as_markdown(head='', body='Some body text'):\n    if head != '':\n        display_markdown(f\"##### {head}\\n\\n&gt;{body}\\n\", raw=True)\n    else:\n        display_markdown(f\"&gt;{body}\\n\", raw=True)\n\nas_markdown('Result!', \"Here's my output...\")\n\nResult!\n\nHere’s my output…"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#loading-data",
    "href": "practicals/Practical-08-Textual_Data.html#loading-data",
    "title": "Practical 8: Working with Text",
    "section": "Loading Data",
    "text": "Loading Data\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nBecause I generally want each practical to stand on its own (unless I’m trying to make a point), I’ve not moved this to a separate Python file (e.g. utils.py, but in line with what we covered back in the lectures on [Functions and Packages][  ]sessions/week3.html#lectures), this sort of thing is a good candidate for being split out to a separate file to simplify re-use.\n\n\nRemember this function from last week? We use it to save downloading files that we already have stored locally. But notice I’ve made some small changes… what do these do to help the user?\n\nfrom pathlib import Path\nfrom requests import get\nfrom functools import wraps\n\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(src:str, dst:str, min_size=100) -&gt; Path:\n        if src.find('?') == -1:\n            url = Path(src)\n        else:\n            url = Path(src[:src.find('?')])\n        fn  = url.name  # Extract the filename\n        dsn = Path(f\"{dst}/{fn}\") # Destination filename\n        if dsn.is_file() and dsn.stat().st_size &gt; min_size:\n            print(f\"+ {dsn} found locally!\")\n            return(dsn)\n        else:\n            print(f\"+ {dsn} not found, downloading!\")\n            return(f(src, dsn))\n    return wrapper\n\n@check_cache\ndef cache_data(src:Path, dst:Path) -&gt; str:\n    \"\"\"Downloads a remote file.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dst : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n      \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how\n    # to use the 'splat' operator! os.makedirs creates missing\n    # directories in a path automatically.\n    if not dst.parent.exists():\n        dst.parent.mkdir(parents=True, exist_ok=True)\n        \n    # Download and write the file\n    with dst.open(mode='wb') as file:\n        response = get(src)\n        file.write(response.content)\n        \n    print(' + Done downloading...')\n\n    return dst.resolve()\n\n\n\n\n\n\n\nTip\n\n\n\nFor very large non-geographic data sets, remember that you can use_cols (or columns depending on the file type) to specify a subset of columns to load.\n\n\nLoad the main data set:\n\nymd  = '20250615'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.geoparquet'\n\n\ngdf = gpd.read_parquet( cache_data(url, Path('data/geo')), \n          columns=['geometry', 'listing_url', 'name', \n                   'description', 'amenities', 'price']).to_crs('epsg:27700')\n\nprint(f\"gdf has {gdf.shape[0]:,} rows and CRS is {gdf.crs.name}.\")\n\n+ data/geo/20250615-London-listings.geoparquet found locally!\ngdf has 82,856 rows and CRS is OSGB36 / British National Grid.\n\n\nLoad supporting Geopackages:\n\nddir  = Path('data/geo') # destination directory\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\n\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n\n+ data/geo/Boroughs.gpkg found locally!\n+ data/geo/Water.gpkg found locally!\n+ data/geo/Greenspace.gpkg found locally!\nDone."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#the-description-field",
    "href": "practicals/Practical-08-Textual_Data.html#the-description-field",
    "title": "Practical 8: Working with Text",
    "section": "The Description Field",
    "text": "The Description Field\n\n\n\n\n\n\nWarningDifficulty level: Moderate, because of the questions.\n\n\n\n\n\n\n\n\n\nTo explore the description field properly you’ll need to filter out any NA/NaN descriptions before sampling the result. Hint: you’ll need to think about boolean NOT (~) applied to a field where the only test we have is is NA.\n\nQuestion\n\ngdf[???].sample(5, random_state=42)[['description']]\n\nWhat do you notice about the above? Are they simple text? Are there patterns or problems? Are there characters that represent things other than words and simple punctuation?\n\n\nQuestions\n\nWhat patterns can you see that might need ‘dealing with’ for text-mining to work?\nWhat non-text characters can you see? (Things other than A-Z, a-z, and simple punctuation!)"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#the-amenities-field",
    "href": "practicals/Practical-08-Textual_Data.html#the-amenities-field",
    "title": "Practical 8: Working with Text",
    "section": "The Amenities Field",
    "text": "The Amenities Field\n\n\n\n\n\n\nWarningDifficulty level: Moderate, because of the questions.\n\n\n\n\n\n\n\n\n\nThis field presents a subtle issue that might not be obvious here:\n\ngdf.amenities.sample(5, random_state=42)\n\n56034    [\"Dining table\", \"Paid high chair - available upon request\", \"Bathtub\", \"Cooking basics\", \"First aid kit\", \"Air conditioning\", \"Clothing storage: closet\", \"Bed linens\", \"Carbon monoxide alarm\", \"Kitchen\", \"Shampoo\", \"Coffee maker: Nespresso\", \"Sh...\n25066              [\"Kitchen\", \"Elevator\", \"Bathtub\", \"Refrigerator\", \"Microwave\", \"Stove\", \"Cooking basics\", \"Hair dryer\", \"Dishes and silverware\", \"Dedicated workspace\", \"Heating\", \"Hot water kettle\", \"Iron\", \"Wifi\", \"Hot water\", \"Hangers\", \"TV\", \"Washer\"]\n12382    [\"Dining table\", \"Free parking on premises\", \"Cooking basics\", \"Carbon monoxide alarm\", \"Bed linens\", \"Clothing storage: closet and dresser\", \"Free dryer \\u2013 In unit\", \"Kitchen\", \"Shampoo\", \"Host greets you\", \"Shower gel\", \"Dedicated workspace...\n72986                                                                                                                      [\"Hangers\", \"Pets allowed\", \"Heating\", \"Essentials\", \"Shampoo\", \"Elevator\", \"Smoke alarm\", \"Kitchen\", \"TV with standard cable\", \"Wifi\"]\n23137                                           [\"Kitchen\", \"Refrigerator\", \"Free parking on premises\", \"Self check-in\", \"City skyline view\", \"Hair dryer\", \"Exercise equipment\", \"Dedicated workspace\", \"Smoke alarm\", \"Iron\", \"Wifi\", \"TV\", \"Washer\", \"Lockbox\"]\nName: amenities, dtype: object\n\n\nBut look what happens now, can you see the issue a little more easily?\n\ngdf.amenities.iloc[0][:25]\n\n'[\"Kitchen\", \"Free parking'\n\n\n\nQuestions\n\nWhat’s the implicit format of the Amenities columns?\nHow could you represent the data contained in the column?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#remove-nan-values",
    "href": "practicals/Practical-08-Textual_Data.html#remove-nan-values",
    "title": "Practical 8: Working with Text",
    "section": "Remove NaN Values",
    "text": "Remove NaN Values\nI would be wary of doing what I do below in a ‘proper’ application without some careful research, but to make our lives easier, we’re going to drop rows where one of the two key columns (Description, Amenities) for this practical is NaN because it will simplify the steps below. In reality, I would spend quite a bit more time investigating which values are NaN and why before simply dropping them.\n\nQuestion\nHow do you drop all rows where either the description or amenities (or both) are NaN:\n\ngdf = gdf.dropna(???)\nprint(f\"Now gdf has {gdf.shape[0]:,} rows.\")\n\nYou should get that there are 80,917 rows."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#using-strings-in-pandas",
    "href": "practicals/Practical-08-Textual_Data.html#using-strings-in-pandas",
    "title": "Practical 8: Working with Text",
    "section": "Using Strings in Pandas",
    "text": "Using Strings in Pandas\nWe’ve not done much formally with text data in Pandas, so let’s have a quick look at what’s possible. Most of these operations are accessed by calling on the .str accessor and you can read allllllll about it.\n\nTrimming\n\n# From the pandas documentation\nidx = pd.Index([\" jack\", \"jill \", \" jesse \", \"frank\"])\n\nprint(idx)\nprint(idx.str.strip())\nprint(idx.str.lstrip())\nprint(idx.str.rstrip())\n\nIndex([' jack', 'jill ', ' jesse ', 'frank'], dtype='object')\nIndex(['jack', 'jill', 'jesse', 'frank'], dtype='object')\nIndex(['jack', 'jill ', 'jesse ', 'frank'], dtype='object')\nIndex([' jack', 'jill', ' jesse', 'frank'], dtype='object')\n\n\n\n\nLower/Upper Case\n\ngdf.sample(3, random_state=42).description.str.lower()\n\n31167    welcome to the stylish 1br 1bath apartment located just few minutes from the bustling streets of central london. whether shopping along oxford street, catching a west end show or sampling local cuisine, it will be the perfect starting point for y...\n79360    ✔ lovely house w/ garden nicely nestled in marylebone &lt;br /&gt;✔ 1345 ft2 | 125 m2 &lt;br /&gt;✔ 3 bedrooms &lt;br /&gt;✔ 2 bathrooms &lt;br /&gt;✔ well-equipped kitchen &lt;br /&gt;✔ backyard patio &lt;br /&gt;✔ next to regent's park &lt;br /&gt;✔ 10-minute walk to the sherlock holme...\n25654    enjoy a stylish experience at this centrally-located flat. supermarkets, a plethora of bars, restaurants and cafes on your doorstep and only a 7 minutes walk to angel station (1 stop from kingcross station) which will get you anywhere you need to...\nName: description, dtype: object\n\n\n\ngdf.sample(3, random_state=42).description.str.upper()\n\n31167    WELCOME TO THE STYLISH 1BR 1BATH APARTMENT LOCATED JUST FEW MINUTES FROM THE BUSTLING STREETS OF CENTRAL LONDON. WHETHER SHOPPING ALONG OXFORD STREET, CATCHING A WEST END SHOW OR SAMPLING LOCAL CUISINE, IT WILL BE THE PERFECT STARTING POINT FOR Y...\n79360    ✔ LOVELY HOUSE W/ GARDEN NICELY NESTLED IN MARYLEBONE &lt;BR /&gt;✔ 1345 FT2 | 125 M2 &lt;BR /&gt;✔ 3 BEDROOMS &lt;BR /&gt;✔ 2 BATHROOMS &lt;BR /&gt;✔ WELL-EQUIPPED KITCHEN &lt;BR /&gt;✔ BACKYARD PATIO &lt;BR /&gt;✔ NEXT TO REGENT'S PARK &lt;BR /&gt;✔ 10-MINUTE WALK TO THE SHERLOCK HOLME...\n25654    ENJOY A STYLISH EXPERIENCE AT THIS CENTRALLY-LOCATED FLAT. SUPERMARKETS, A PLETHORA OF BARS, RESTAURANTS AND CAFES ON YOUR DOORSTEP AND ONLY A 7 MINUTES WALK TO ANGEL STATION (1 STOP FROM KINGCROSS STATION) WHICH WILL GET YOU ANYWHERE YOU NEED TO...\nName: description, dtype: object\n\n\n\n\nReplacing\n\ngdf.sample(3, random_state=42).description.str.replace(r' ','_',regex=False)\n\n31167    Welcome_to_the_stylish_1BR_1Bath_apartment_located_just_few_minutes_from_the_bustling_streets_of_central_London._Whether_shopping_along_Oxford_Street,_catching_a_West_End_show_or_sampling_local_cuisine,_it_will_be_the_perfect_starting_point_for_y...\n79360    ✔_Lovely_house_w/_garden_nicely_nestled_in_Marylebone_&lt;br_/&gt;✔_1345_ft2_|_125_m2_&lt;br_/&gt;✔_3_bedrooms_&lt;br_/&gt;✔_2_bathrooms_&lt;br_/&gt;✔_Well-equipped_kitchen_&lt;br_/&gt;✔_Backyard_patio_&lt;br_/&gt;✔_Next_to_Regent's_Park_&lt;br_/&gt;✔_10-minute_walk_to_The_Sherlock_Holme...\n25654    Enjoy_a_stylish_experience_at_this_centrally-located_flat._Supermarkets,_a_plethora_of_bars,_restaurants_and_cafes_on_your_doorstep_and_only_a_7_minutes_walk_to_Angel_station_(1_stop_from_Kingcross_station)_which_will_get_you_anywhere_you_need_to...\nName: description, dtype: object\n\n\n\ngdf.sample(3, random_state=42).description.str.replace(r'\\W','',regex=True)\n\n31167                                     Welcometothestylish1BR1BathapartmentlocatedjustfewminutesfromthebustlingstreetsofcentralLondonWhethershoppingalongOxfordStreetcatchingaWestEndshoworsamplinglocalcuisineitwillbetheperfectstartingpointforyouradventures\n79360    LovelyhousewgardennicelynestledinMarylebonebr1345ft2125m2br3bedroomsbr2bathroomsbrWellequippedkitchenbrBackyardpatiobrNexttoRegentsParkbr10minutewalktoTheSherlockHolmesMuseumbrBakerStreetStationRegentsParkStationarebothundera10minutewalkawaybrAne...\n25654                                                   EnjoyastylishexperienceatthiscentrallylocatedflatSupermarketsaplethoraofbarsrestaurantsandcafesonyourdoorstepandonlya7minuteswalktoAngelstation1stopfromKingcrossstationwhichwillgetyouanywhereyouneedtogo\nName: description, dtype: object\n\n\n\n\nSplitting\n\ngdf.sample(3, random_state=42).amenities.str.split(r',')\n\n31167    [[\"Microwave\",  \"Hot water kettle\",  \"Body soap\",  \"Cleaning products\",  \"Kitchen\",  \"Bathtub\",  \"Dishes and silverware\",  \"Shower gel\",  \"Dining table\",  \"Heating\",  \"Self check-in\",  \"Long term stays allowed\",  \"Washer\",  \"Smoke alarm\",  \"Oven\"...\n79360    [[\"Bathtub\",  \"Cooking basics\",  \"First aid kit\",  \"Carbon monoxide alarm\",  \"Bed linens\",  \"Kitchen\",  \"Shampoo\",  \"Host greets you\",  \"Backyard\",  \"Coffee maker\",  \"Stove\",  \"Room-darkening shades\",  \"Hair dryer\",  \"Private entrance\",  \"Oven\", ...\n25654    [[\"Bathtub\",  \"Dishes and silverware\",  \"Carbon monoxide alarm\",  \"Central heating\",  \"Dedicated workspace\",  \"Essentials\",  \"Lockbox\",  \"Self check-in\",  \"Iron\",  \"Hot water\",  \"Washer\",  \"Smoke alarm\",  \"Kitchen\",  \"Hair dryer\",  \"Portable fans...\nName: amenities, dtype: object\n\n\n\ngdf.sample(3, random_state=42).amenities.str.split(r',', expand=True)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n\n\n\n\n31167\n[\"Microwave\"\n\"Hot water kettle\"\n\"Body soap\"\n\"Cleaning products\"\n\"Kitchen\"\n\"Bathtub\"\n\"Dishes and silverware\"\n\"Shower gel\"\n\"Dining table\"\n\"Heating\"\n...\n\"Bed linens\"\n\"Lockbox\"\n\"Shampoo\"\n\"Iron\"\n\"Hot water\"\n\"Refrigerator\"\n\"Stove\"\n\"Clothing storage: closet\"\n\"Hair dryer\"\n\"Freezer\"]\n\n\n79360\n[\"Bathtub\"\n\"Cooking basics\"\n\"First aid kit\"\n\"Carbon monoxide alarm\"\n\"Bed linens\"\n\"Kitchen\"\n\"Shampoo\"\n\"Host greets you\"\n\"Backyard\"\n\"Coffee maker\"\n...\n\"Fire extinguisher\"\n\"Iron\"\n\"Wifi\"\n\"Hot water\"\n\"Hangers\"\n\"TV\"\n\"Washer\"]\nNone\nNone\nNone\n\n\n25654\n[\"Bathtub\"\n\"Dishes and silverware\"\n\"Carbon monoxide alarm\"\n\"Central heating\"\n\"Dedicated workspace\"\n\"Essentials\"\n\"Lockbox\"\n\"Self check-in\"\n\"Iron\"\n\"Hot water\"\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\n\n3 rows × 34 columns\n\n\n\n\n\nIndexing\n\ngdf.sample(3, random_state=42).description.str[:25]\n\n31167    Welcome to the stylish 1B\n79360    ✔ Lovely house w/ garden \n25654    Enjoy a stylish experienc\nName: description, dtype: object\n\n\n\n\nExtraction\n\n# For some reason this doesn't take regex=True\ngdf.sample(3, random_state=42).description.str.extract(r'stylish (?P&lt;detail&gt;\\w+)')\n\n\n\nPutting it Together\n\ngdf.sample(3, random_state=42).amenities.str.upper().str.replace(r'(?:\\\"|\\[|\\])', '', regex=True).str.split(r'\\s*,\\s*', regex=True, expand=True)\n\nOr you could also tackle it this way:\n\ngdf.sample(3, random_state=42).amenities.str.upper().str.replace(r'(?:\\\"|\\[|\\])', '', regex=True).str.get_dummies(sep=r',')\n\n\n\n\n\n\n\nWarningCheck Your Data!\n\n\n\nThe get_dummies method works better than I expected! I assumed it would struggle with the [ and ] at the start and end of the string; however, I think what’s happening is that these are not allowed characters in a column name so they’re simply dropped from the expansion.\nWhat’s nice about the get_dummies approach from an analytical standpoint is that it deals with the issue that the amenities are not all listed in the same order in every listing. So some might lead with the size of the TV, while other listings (with equally large televisions) lead with the views. Converting all of these to dummies automatically ‘sorts’ them so that we can ask questions like ‘which listings have a TV and a great view?’"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#selecting-data",
    "href": "practicals/Practical-08-Textual_Data.html#selecting-data",
    "title": "Practical 8: Working with Text",
    "section": "Selecting Data",
    "text": "Selecting Data\nIn the same way that we can use operators like &lt; and == to select rows from a data frame, we also have the output of operations like contains available. So here’s an example:\n\ngdf[\n    gdf.description.str.contains(r'Flat', regex=False, flags=re.IGNORECASE)\n].sample(3, random_state=42)[['description']]\n\n\n\n\n\n\n\n\ndescription\n\n\n\n\n62750\nThis is my spacious 1 bed flat (approx 50 sqm), kitchen equipped with all appliances you need), desk and WiFi. Flat is on the 1st floor (1 flight of stairs with no lift - sorry)&lt;br /&gt;&lt;br /&gt;Just 15 min walk to central Ldn. A view of the Emirates ...\n\n\n35981\nFlat is mins walk to tower bridge and many tourist places.. confirm you like dogs when you enquiry and ask as many questions you want, i will be happy to help\n\n\n23645\nThis three bedroom intrior designed flat on the edge of the idyliic Battersea park and moments from the new Battersea Power Station. &lt;br /&gt;&lt;br /&gt;Very central location - just a walk across the park to get into Chelsea, or 2 minute walk to Batterse..."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html",
    "href": "practicals/Practical-10-Presenting_Data.html",
    "title": "Practical 10: Presenting Data",
    "section": "",
    "text": "WarningImportant\n\n\n\nThis practical focusses on the final topic we want to cover in Foundations: visualisation! You will have seen and learned quite a bit of this across the preceding weeks, but it was done in an ad-hoc way, here we try to systematise things a bit."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#anatomy-of-a-figure",
    "href": "practicals/Practical-10-Presenting_Data.html#anatomy-of-a-figure",
    "title": "Practical 10: Presenting Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\n\n\n\n\n\nTip\n\n\n\nYou might want to bookmark the ‘Anatomy of a Figure’ image so that you can easily find and refer to it in the future. This structure is why matplotlib is so much nastier than ggplot, but it does also give you greater control over the output if you really dig into the guts of things.\n\n\nOne of the reasons that Matplotlib is so much more complex than ggplot is that it can actually do a lot more than ggplot, including image manipulation, axis translation, and even 3D. You can get a sense of this by looking at the tutorials since the Users guide can be a bit overwhelming.\nNevertheless, the core components of all matplotlib figures can be seen here:"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#finding-fonts",
    "href": "practicals/Practical-10-Presenting_Data.html#finding-fonts",
    "title": "Practical 10: Presenting Data",
    "section": "Finding Fonts",
    "text": "Finding Fonts\nI find matplotlib’s use of fonts to be profoundly weird. If you use conda and install directly on to the computer then you might have access to all of your computer’s fonts (though there are different types of fonts as well, not all of which will show up), but for most users it will be those that were installed into Podman.\n\nListing Fonts\n\n\n\n\n\n\nWarning\n\n\n\nDepending on how you are running the notebook, you may not be able to see all of the fonts available on your system. If the notebook is running in a Podman container it only has access to the fonts that are installed in the container. In other words: don’t panic if you don’t see the font(s) you want..\n\n\n\nfrom matplotlib import font_manager\nfrom IPython.core.display import HTML\n\nflist = font_manager.findSystemFonts()\nnames = []\nfor fname in flist:\n    try:\n        names.append(font_manager.FontProperties(fname=fname).get_name())\n    except RuntimeError:\n        pass # Think the issue is emoji-support/colour fonts\n\nprint(f\"Found {len(set(names))} fonts.\")\n\ndef make_html(fontname):\n    return f\"&lt;p&gt;&lt;span style='font-family: {fontname}; font-size: 18px;'&gt;{fontname}&lt;/p&gt;\"\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set(names)) if not font.startswith('.')])\n\nHTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code))\n\nFound 42 fonts.\n\n\nBig Shoulders Display\nCousine\nDejaVu Sans\nEB Garamond\nFont Awesome 5 Brands\nFont Awesome 5 Free\nFont Awesome 6 Brands\nFont Awesome 6 Free\nFont Awesome v4 Compatibility\nHanken Grotesk\nInconsolata\nInconsolata Condensed\nInconsolata Expanded\nInconsolata ExtraCondensed\nInconsolata ExtraExpanded\nInconsolata SemiCondensed\nInconsolata SemiExpanded\nInconsolata UltraCondensed\nInconsolata UltraExpanded\nInter Tight\nLeague Gothic\nLiberation Mono\nLiberation Sans\nLiberation Serif\nLobster\nMicro 5\nRoboto Flex\nRoboto Mono\nRoboto Slab\nSilkscreen\nSource Code Pro\nSource Sans 3\nSource Sans Pro\nSource Serif 4\nSource Serif 4 18pt\nSource Serif 4 36pt\nSource Serif 4 48pt\nSpectral\nSpectral SC\nUbuntu\nUbuntu Condensed\nUbuntu Mono\n\n\n\n\nUsing Fontconfig\nfontconfig is the base Linux utility for managing fonts. We can list font using fc-list and then a set of ‘switches’ determining the kind of information we want back. Since fontconfig doesn’t exist on OSX or Windows, you’ll need to do some more investigating and poking around to get these details on a conda install (I’ll show an option further down)…\nHere we ask fontconfig to format the output so that we only get the first part of the family name, and then we pipe (recall | sends output from one utility to another!) the output of that to sort, which sorts the output, and uniq which removes duplicates (which there will be because there are bold, italic, small-caps, etc. versions of each font). To make better sense of this you can always try playing around with all three steps in the output below!\n\nfonts = ! fc-list --format=\"%{family[0]}\\n\" | sort | uniq\nprint(fonts[:5])\n\n['Big Shoulders Display', 'Bitstream Charter', 'Courier', 'Cousine', 'DejaVu Sans']\n\n\n\n\n\n\n\n\nTipCapturing output\n\n\n\nNotice that we’re able to capture the output of an external application (called via the Terminal) with fonts = ! .... This can be useful when something is easy to do on the command line but hard to do in Python.\n\n\nThe below option also pipes output from fonctconfig, but to the grep utility which checks each line for the character sequence Liberation. Now we’re asking fontconfig to include style details which will relate to both weight (regular, bold, extra bold, light, etc.) and italic, bold, small caps, etc.\n\nfonts = ! fc-list : family style | grep \"Liberation\"\nprint(sorted(fonts)[:5])\n\n['Liberation Mono:style=Bold', 'Liberation Mono:style=Bold Italic', 'Liberation Mono:style=Italic', 'Liberation Mono:style=Regular', 'Liberation Sans:style=Bold']\n\n\nYou can find more examples here, a more detailed set of instructions here, and even information about (for example) supported languages based on RFC 3066.\nHere are the languages supported by the Ubuntu Light font:\n\nlangs = ! fc-list \"Liberation Mono\" : lang\nprint(sorted(langs)[:5], \"...\")\n\n[':lang=aa|ab|af|ast|av|ay|az-az|ba|bm|be|bg|bi|bin|br|bs|bua|ca|ce|ch|chm|co|cs|cu|cv|cy|da|de|el|en|eo|es|et|eu|fi|fj|fo|fr|ff|fur|fy|ga|gd|gl|gn|gv|ha|haw|he|ho|hr|hu|ia|ig|id|ie|ik|io|is|it|kaa|ki|kk|kl|ku-am|kum|kv|kw|ky|la|lb|lez|ln|lt|lv|mg|mh|mi|mk|mo|mt|nb|nds|nl|nn|no|nr|nso|ny|oc|om|os|pl|pt|rm|ro|ru|sah|sco|se|sel|sh|shs|sk|sl|sm|sma|smj|smn|sms|so|sq|sr|ss|st|sv|sw|tg|tk|tl|tn|to|tr|ts|tt|tw|tyv|uk|uz|ve|vi|vo|vot|wa|wen|wo|xh|yap|yi|yo|zu|ak|an|ber-dz|crh|csb|ee|fat|fil|hsb|ht|hz|jv|kab|kj|kr|ku-tr|kwm|lg|li|mn-mn|ms|na|ng|nv|pap-an|pap-aw|qu|quz|rn|rw|sc|sg|sn|su|ty|za|agr|ayc|bem|dsb|lij|mfe|mhr|miq|mjw|nhn|niu|rif|sgs|szl|tpi|unm|wae|yuw'] ...\n\n\nHere are the monospace fonts installed:\n\nmonos = ! fc-list :spacing=mono : family | sort | uniq\nprint(sorted(monos)[:5], \"...\")\n\n['Courier', 'Cousine', 'Inconsolata', 'Inconsolata Condensed', 'Inconsolata Condensed,Inconsolata Condensed Black'] ...\n\n\n\n\nFontdicts\nNow that we know what’s available, the next step is to set up some useful defaults that we can re-use across multiple plots to ensure consistency of output. The format for specifying fonts on a per-figure basis is a dictionary, so where you see fontdict in the matplotlib documentation the following should work:\nHere’s the example:\n\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n\n\nff='Liberation Sans'\ntfont = {'fontname':ff}\nbfont = {'fontname':ff, 'weight':'bold', 'horizontalalignment':'left'}\nafont = {'fontname':ff}\n\nI am setting the ‘title font’ (tfont) and ‘body copy font’ (bfont) and ‘axis font’ (afont) here to use in the output below. You can pick another font and see what happens.\n\n\n2.3: Using Fonts\nAt this point we’re going to work towards a kind of ‘atlas’ that would make it easy to compare some features for different London boroughs. I basically implemented a the basic matplotlib version of QGIS’ Atlas functionality.\n\n# This will be whatever LA you processed last week\nLA = 'Waltham Forest'\n\n\nmsoa_gdf = gpd.read_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n\n\nmedian_gdf  = msoa_gdf[['MSOA11CD','median_price','geometry']]\nlisting_gdf = msoa_gdf[['MSOA11CD','listing_count','geometry']]\n\n\nimport matplotlib.pyplot as plt\n\n\n\nThe Defaults\nHere is a demonstration of some of the ways you can adjust features in a Python matplotlib plot. I’m not suggesting either of these is a good output, but that’s not the point! The idea is to see the various ways you can tweak a plot… And notice that we’ve not yet changed any fonts. And it shows.\n\n# Set up a 1 x 2 plot (you can also leave off the nrows= and ncols=)\nf,axes = plt.subplots(nrows=1, ncols=2, figsize=(8,6))\n# ax1 will be the first plot on the left, ax2 will be on the right;\n# a 2 (or more) *row* plot will return a list of lists... 1 list/row.\nax1 = axes[0]\nax2 = axes[1]\n\n# Left plot is the median price\nmedian_gdf.plot(column='median_price', ax=ax1, legend=True, cmap='viridis')\nax1.set_title(\"Median Price per MSOA\");\n# Turn off the frame, one side of the plat at a time\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n# Set the labels\nax1.set_xlabel(\"Easting\");\nax1.set_ylabel(\"Northing\");\n\n# Right plot is the number of listings; note\n# here the use of both zorder (which is the \n# 'stacking order' of elements on the plot, and\n# the legend_kwds (keywords) to change the \n# orientation of the plot to horizontal\nlisting_gdf.plot(column='listing_count', ax=ax2, legend=True, cmap='plasma', zorder=1, \n                 legend_kwds={\"orientation\": \"horizontal\"})\nax2.set_title(\"Count of Listings per MSOA\");\n# Set a background colour for the plot\nax2.set_facecolor((.4, .4, .4, .2))\n# Add grid lines and set their zorder to\n# below that of the data on the plot\nplt.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\nax2.set_axisbelow(True)\n\n# This is equivalent to the ax1.spines... \n# above, but if you use it here you lose\n# the background to the plot as well!\n#plt.gca().set(frame_on=False)\n\n# Remove the labels on the ticks of the \n# axes (meaning: remove the numbers on \n# x- and y-axes).\nax2.set_xticklabels([])\nax2.set_yticklabels([])\n\n# Set the labels\nax2.set_xlabel(\"Easting\");\nax2.set_ylabel(\"Northing\");\n\n\n\n\n\n\n\n\n\n\nImproving on Defaults\n\nf,axes = plt.subplots(1,2,figsize=(8,6))\n\n# Set up the plots\nmedian_gdf.plot(column='median_price', ax=axes[0], legend=True, cmap='viridis')\nlisting_gdf.plot(column='listing_count', ax=axes[1], legend=True, cmap='plasma')\nfor ax in axes:\n    ax.axis('off')\n    # Note that here, set_facebolor doesn't work,\n    # presumably because the axis is 'off'\n    ax.set_facecolor((.4, .4, .4, .2))\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expading the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\naxes[0].set_title('Median Price', size=20, **afont)\naxes[1].set_title('Count', size=20, **afont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.88, y=0.957, linespacing=1.4, va='top', ha='right', size=12, \n            s=f\"Total listings: {listing_gdf['listing_count'].sum():,.0f}  Median price: ${median_gdf['median_price'].median():,.2f}\", **bfont);\nf"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#adding-picture-in-picture",
    "href": "practicals/Practical-10-Presenting_Data.html#adding-picture-in-picture",
    "title": "Practical 10: Presenting Data",
    "section": "Adding Picture-in-Picture",
    "text": "Adding Picture-in-Picture\nWe’re now going to emulate a bit of QGIS’ Atlas function by creating two subplots and then adding a third plot afterwards that shows where the borough is.\n\nf,axes = plt.subplots(1,3,gridspec_kw={'width_ratios':[1,4,4]}, figsize=(8,6))\n\n# Plot 0 is basically being used as a 'spacer' \n# as you'll see below\naxes[0].axis('off')\n\n# Plot 1 is the median price\nmedian_gdf.plot(column='median_price', ax=axes[1], legend=True, cmap='viridis')\naxes[1].set_title('Median Price', size=20, **afont)\n\n# Plot 2 is the count of listings\nlisting_gdf.plot(column='listing_count', ax=axes[2], legend=True, cmap='plasma')\naxes[2].set_title('Count', size=20, **afont)\n\n# For plots 1 and 2... if you were doing this a lot it could be a function!\nfor ax in axes[1:]:\n    ax.set_facecolor((.9, .9, .9, .5))\n    ax.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\n    ax.set_axisbelow(True)\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.tick_params(axis='both', which='both', length=0)\n\n# Add a *third* chart that we use as a kind of 'PiP'\n# to show which borough we're talking about. The \n# add_axes call is here taking information about the\n# positioning and size of the additional figure.\n# Disable ax2.axis('off') if you want to see the\n# figure in full.\nax3 = f.add_axes([0.015, 0.7, 0.2, 0.2])\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nboros.plot(facecolor='lightgrey', edgecolor='w', linewidth=1, ax=ax3)\nboros[boros.NAME==LA].plot(facecolor='r', edgecolor='none', hatch='///', ax=ax3)\nax3.axis('off')\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expanding the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.65, s=f\"Total listings: {listing_gdf['listing_count'].sum():,.0f}\", size=12, **bfont);\n\n+ data/geo/Boroughs.gpkg found locally!"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#bonus-achievement-unlocked",
    "href": "practicals/Practical-10-Presenting_Data.html#bonus-achievement-unlocked",
    "title": "Practical 10: Presenting Data",
    "section": "Bonus Achievement Unlocked!",
    "text": "Bonus Achievement Unlocked!\nIf you have the time and inclination, see if you can convert the above to an actual atlas output:\n\nYou’ll want to turn this plot into a function so as to be able to produce (and save) the map for every borough.\nYou’ll even need to parameterise the filename so that you save to different PNG files as well as going back to see how we generated the listing and pricing data frames for the Local Authority…\nAnd you’ll also need to make sure that you ensure a consistent colour bar (for all of London, because the median price and number of listings will vary rather a lot by LA)\nThen there’s the placement of the PiP for some boroughs with long names\nAnd finally, you might consider adding some more text to atlas–maybe pull some content from Wikipedia using Beautiful Soup (bs4)?"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#for-a-chart",
    "href": "practicals/Practical-10-Presenting_Data.html#for-a-chart",
    "title": "Practical 10: Presenting Data",
    "section": "For a Chart",
    "text": "For a Chart\nGroup the listings by Borough and Room Type, and aggregate by median price, also producing a count variable for the number of listings of each type in each Borough.\n\nla_tots = gdf_la[gdf_la.NAME==LA].groupby(by='room_type', observed=False).agg(\n                            {'price':'median', 'listing_url':'count'}\n                        ).reset_index().rename(columns={'listing_url':'count'})\nla_tots\n\n\n\n\n\n\n\n\nroom_type\nprice\ncount\n\n\n\n\n0\nEntire home/apt\n135.0\n965\n\n\n1\nHotel room\nNaN\n0\n\n\n2\nPrivate room\n53.0\n704\n\n\n3\nShared room\n45.5\n7\n\n\n\n\n\n\n\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.palettes import Spectral4\nfrom bokeh.models import CustomJS, Dropdown\n\noutput_notebook()\n\nroom_types = la_tots.room_type.to_list()\nprices     = la_tots.price.to_list()\ncounts     = la_tots['count'].to_list()\n\n# Add hover tooltip\nsource = ColumnDataSource(data=dict(\n    rt=room_types,\n    count=counts,\n    price=prices,\n))\n\nTOOLTIPS = [\n    (\"Room Type\", \"@rt\"),\n    (\"Number of Listings\", \"@count{,}\"),\n    (\"Median Price\", \"$@price{,}/night\")\n]\n\np = figure(x_range=room_types, height=300, tooltips=TOOLTIPS,\n           title=f\"Median Price by Room Type in {LA}\",\n           toolbar_location=None, tools=\"\")\n\np.vbar(x='rt', top='count', width=0.9, source=source)\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nshow(p)\n\n    \n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#for-a-map",
    "href": "practicals/Practical-10-Presenting_Data.html#for-a-map",
    "title": "Practical 10: Presenting Data",
    "section": "For a Map",
    "text": "For a Map\nThis is not the prettiest code, but it should work…\n\nfrom bokeh.plotting import figure\n\nfrom bokeh.io import output_file, show, output_notebook, push_notebook, export_png\nfrom bokeh.models import ColumnDataSource, GeoJSONDataSource, LinearColorMapper, ColorBar, HoverTool\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import brewer\n\n#output_notebook()\n\n\nmsoadf = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            msoas[msoas.Borough==LA].drop(columns=['index_right']), predicate='within')\n\n\nmsoagrdf = msoadf.groupby('MSOA11NM').agg({'price':['median','count']}).reset_index()\nmsoagrdf.columns=['msoa11nm','median','count']\n\nI cobbled the mapping functions below together from two tutorials I found online (this one and this one). As you can see, this is a very different approach to mapping data, but it has clear benefits for exploratory purposes and produces fast, interactive maps… and I’ve not even added selection and filtering tools!\n\nimport json\n\ndef get_geodatasource(gdf):    \n    \"\"\"Get getjsondatasource from geopandas object\"\"\"\n    json_data = json.dumps(json.loads(gdf.to_json()))\n    return GeoJSONDataSource(geojson = json_data)\n\ndef bokeh_plot_map(gdf, column=None, title=''):\n    \"\"\"Plot bokeh map from GeoJSONDataSource \"\"\"\n\n    geosource = get_geodatasource(gdf)\n    palette = brewer['OrRd'][8]\n    palette = palette[::-1]\n    vals = gdf[column]\n    \n    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n    color_mapper = LinearColorMapper(palette=palette, low=vals.min(), high=vals.max())\n    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8, width=500, height=10,\n                         location=(0,0), orientation='horizontal')\n\n    tools = 'wheel_zoom,pan,reset,hover'\n    \n    p = figure(title = title, height=700, width=850, toolbar_location='right', tools=tools)\n    p.add_tile(\"CartoDB Positron\", retina=True)\n    p.xgrid.grid_line_color = None\n    p.ygrid.grid_line_color = None\n    \n    # Add patch renderer to figure\n    p.patches('xs','ys', source=geosource, fill_alpha=0.5, line_width=0.5, line_color='white',  \n              fill_color={'field' :column , 'transform': color_mapper})\n    \n    # Specify figure layout.\n    p.add_layout(color_bar, 'below')\n    \n    # Add hover\n    hover = p.select_one(HoverTool)\n    hover.point_policy = \"follow_mouse\"\n    hover.tooltips = [(\"Borough\", \"@Borough\"),\n                      (\"Neighbourhood\", \"@msoa11hclnm\"),\n                      (\"Count of Listings\", \"@count\"),\n                      (\"Median Price\", \"$@median\")]\n    \n    return p\n\nReproject to Web Mercator:\n\nmsoa_gdf = pd.merge(msoagrdf, msoas, left_on='msoa11nm', right_on='MSOA11NM', how='inner')\nmsoa_gdf = msoa_gdf.set_geometry('geometry').set_crs('epsg:27700')\n\n\nmsoageo = msoa_gdf.to_crs('epsg:3785')\nmsoageo.total_bounds\n\narray([-6.74542047e+03,  6.71906611e+06,  3.04361304e+03,  6.73637453e+06])\n\n\nAnd map it!\n\n# Need to drop the right geometry column\n# as Bokeh doesn't know how to handle two\n# and tries to 'serialise' the second geom.\np = bokeh_plot_map(msoageo.drop(columns=['geometry_right','index_right']), 'median', title=f'MSOA-Level Activity in {LA}')\n\nhandle = show(p, notebook_handle=True)\npush_notebook(handle=handle)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNote🔗 Connections\n\n\n\nAnd that’s it. That’s all she wrote! You’ve now covered in &lt;10 weeks what many people might take 10 months to cover. So do not feel like either: 1) you know it all; or 2) you know nothing. You have learned a lot, but it’s probably just enough to see how much you don’t know. And that is the start of wisdom. Good luck, young Python-master!"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html",
    "href": "practicals/Practical-12-Grouping_Data.html",
    "title": "Practical 12: Grouping Data",
    "section": "",
    "text": "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: this group of observations are similar to one another, that group is dissimilar to this group. Sometimes we have a label that we can use as part of the process (in which case we’re doing classification), and somtimes we don’t (in which case we’re doing clustering). But what defines similarity and difference? There is no one answer to that question and so there are many different ways to cluster or classify data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#london-data-layers",
    "href": "practicals/Practical-12-Grouping_Data.html#london-data-layers",
    "title": "Practical 12: Grouping Data",
    "section": "London Data Layers",
    "text": "London Data Layers\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nmsoas = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', ddir) )\nmsoas = msoas.to_crs(epsg=27700)\n\n# I don't use this in this practical, but it's a\n# really useful data set that gives you 'names'\n# for MSOAs that broadly correspond to what most\n# Londoners would think of as a 'neighbourhood'.\nmsoa_nms = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011-Names.gpkg', ddir) )\nmsoa_nms = msoa_nms.to_crs(epsg=27700)\nprint(\"Done.\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#reduced-dimensionality-msoa-data",
    "href": "practicals/Practical-12-Grouping_Data.html#reduced-dimensionality-msoa-data",
    "title": "Practical 12: Grouping Data",
    "section": "Reduced Dimensionality MSOA Data",
    "text": "Reduced Dimensionality MSOA Data\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nYou should have this locally from last week, but just in case…\n\nhost = 'http://orca.casa.ucl.ac.uk'\npath = '~jreades/data'\nrddf = gpd.read_parquet( cache_data(f'{host}/{path}/Reduced_Dimension_Data.geoparquet', ddir) )\nprint(f\"Data frame is {rddf.shape[0]:,} x {rddf.shape[1]}\")\n\nYou should have: Data frame is 983 x 93.\nAnd below you should see both the components and the dimensions from last week’s processing.\n\nrddf.iloc[0:3, -7:]\n\nI get the results below, but note that the Dimension values may be slightly different:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent 5\nComponent 6\nComponent 7\nBorough\nDimension 1\nDimension 2\nSubregion\n\n\n\n\nE02000001\n1.44\n3.95\n-1.52\nCity of London\n7.74\n3.36\nInner West\n\n\nE02000002\n-0.28\n0.89\n0.26\nBarking and Dagenham\n2.04\n7.59\nOuter East and North East\n\n\nE02000003\n-0.11\n1.12\n0.83\nBarking and Dagenham\n2.20\n6.87\nOuter East and North East"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#listings-data",
    "href": "practicals/Practical-12-Grouping_Data.html#listings-data",
    "title": "Practical 12: Grouping Data",
    "section": "Listings Data",
    "text": "Listings Data\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nLet’s also get the listings data from a few weeks back:\n\nymd  = '20250615'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.geoparquet'\n\n\nlistings = gpd.read_parquet( cache_data(url, ddir) )\nlistings = listings.to_crs(epsg=27700)\nprint(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")\n\nYou should have: Data frame is 85,134 x 31.\nAnd a quick plot of the price to check:\n\nlistings.plot(???, cmap='plasma', scheme='quantiles', k=10, \n              markersize=.5, alpha=0.15, figsize=(10,7));"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#join-listings-to-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#join-listings-to-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Join Listings to MSOA",
    "text": "Join Listings to MSOA\n\n\n\n\n\n\nWarningDifficulty: Medium-to-hard.\n\n\n\n\n\n\n\n\n\nFirst, let’s link all this using the MSOA Geography that we created last week and a mix or merge and sjoin!\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: Notice a few things going on here! We are calling `gpd.sjoin` because pandas (`pd`) doesn't know about spatial joins, only geopandas (`gpd`) does. More on this [next week](https://jreades.github.io/fsds/sessions/week10.html#lectures). Also see how we drop some columns _at the point where we do the join_ by taking advantage of the fact that most pandas/geopandas operations return a _copy_ of the (Geo)DataFrame. That allows us to get back from the spatial join a neat, tidy data frame ready for further analysis. If you're struggling to make sense of this, try removing the `drop` operations and see what your data frame looks like afterwards. This should all be old hat, but in case you need a refresher there's always [Week 5](https://jreades.github.io/fsds/sessions/week5.html) on pandas.\n\n\n\n# Before the spatial join\nlistings.columns\n\n\nmsoa_listings = gpd.sjoin(???, msoas.drop(\n                        columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM',\n                                 'USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', \n                                 'AVHHOLDSZ']), predicate='???').drop(\n                        columns=['latitude','longitude','index_right']\n                )\n\n\n# All we've added is the MSOA11CD\nmsoa_listings.columns\n\nAll being well you should now have:\nIndex(['listing_url', 'last_scraped', 'name', 'description', 'host_id',\n       'host_name', 'host_since', 'host_location', 'host_is_superhost',\n       'host_listings_count', 'host_total_listings_count',\n       'host_verifications', 'property_type', 'room_type', 'accommodates',\n       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n       'minimum_nights', 'maximum_nights', 'availability_365',\n       'number_of_reviews', 'first_review', 'last_review',\n       'review_scores_rating', 'reviews_per_month', 'geometry', 'MSOA11CD'],\n      dtype='object')"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#price-by-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#price-by-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Price by MSOA",
    "text": "Price by MSOA\n\n\n\n\n\n\nWarningDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nLet’s calculate the median price by MSOA… Notice that we have to specify the column we want after the groupby so the we don’t get the median of every column returned\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: I find `groupby` to be a complex operation and often need a couple of gos before I get back waht I want. The thing to take away is that: 1) anything in the `groupby` will become part of the `index` afterwards (so if you group on multiple things you get a multi-part index); 2) aggregating functions apply to _all_ columns unless you filter them some way. Here we filter by selecting only the `price` column to aggregate. You can also filter for `numeric only`.\n\n\n\n# *m*soa *l*istings *g*rouped by *p*rice\nmlgp = msoa_listings.groupby('???')['price'].agg('???') \nmlgp.head()\n\nYou should get something like:\nMSOA11CD\nE02000001   170.00\nE02000002    97.00\nE02000003    80.00\nE02000004    54.00\nE02000005   100.00\nName: price, dtype: float64"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#room-type-by-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#room-type-by-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Room Type by MSOA",
    "text": "Room Type by MSOA\n\n\n\n\n\n\nWarningDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow let’s calculate the count of room types by MSOA and compare the effects of reset_index on the outputs below. And notice too that we can assign the aggregated value to a column name!\n\n# *m*soa *l*istings *g*rouped *c*ount\nmlgc = msoa_listings.groupby(['???','???'], observed=False).listing_url.agg(Count='???')\nmlgc.head()\n\nYou should get something resembling this:\n\n\n\nMSOA11CD\nroom_type\nCount  \n\n\n\n\nE02000001\nEntire home/apt\n466\n\n\n\nHotel room\n0\n\n\n\nPrivate room\n61\n\n\n\nShared room\n1\n\n\nE02000002\nEntire home/apt\n4\n\n\n\n\n# *m*soa *l*istings *g*rouped *c*ount *r*eset index\nmlgcr = msoa_listings.groupby(['???','???'], observed=False).listing_url.agg(Count='???').reset_index() # msoa listings grouped counts\nmlgcr.head()\n\nYou should get something like:\n\n\n\n\nMSOA11CD\nroom_type\nCount\n\n\n\n\n0\nE02000001\nEntire home/apt\n466\n\n\n1\nE02000001\nHotel room\n0\n\n\n2\nE02000001\nPrivate room\n61\n\n\n3\nE02000001\nShared room\n1\n\n\n4\nE02000002\nEntire home/apt\n4"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#price-by-room-type",
    "href": "practicals/Practical-12-Grouping_Data.html#price-by-room-type",
    "title": "Practical 12: Grouping Data",
    "section": "Price by Room Type",
    "text": "Price by Room Type\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nBut perhaps median price/room type would make more sense? And do we want to retain values where there are no listings? For example, there are no hotel rooms listed for E02000001, how do we ensure that these NAs are dropped?\n\n# *m*soa *l*istings *g*rouped *r*oom *p*rice\nmlgrp = msoa_listings.???(???, observed=True\n                      )['price'].agg('???').reset_index()\nmlgrp.head()\n\nYou should get something like:\n\n\n\n\nMSOA11CD\nroom type\nprice\n\n\n\n\n0\nE02000001\nEntire home/apt\n177.00\n\n\n2\nE02000001\nPrivate room\n100.00\n\n\n3\nE02000001\nShared room\n120.00\n\n\n4\nE02000002\nEntire home/apt\n117.00\n\n\n6\nE02000002\nPrivate room\n42.00"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#explore-outlier-per-msoa-prices",
    "href": "practicals/Practical-12-Grouping_Data.html#explore-outlier-per-msoa-prices",
    "title": "Practical 12: Grouping Data",
    "section": "Explore Outlier Per-MSOA Prices",
    "text": "Explore Outlier Per-MSOA Prices\n\n\n\n\n\n\nWarningDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nAre there MSOAs what look like they might contain erroneous data?\n\nPlot MSOA Median Prices\n\nmlgp.hist(bins=200);\n\n\n\nExamine Listings from High-Priced MSOAs\nCareful, this is showing the listings from MSOAs whose median price is above $300/night:\n\nmsoa_listings[\n    msoa_listings.MSOA11CD.isin(mlgp[mlgp &gt; 300].index)\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','name','description']\n]\n\nSome of these look legi (4, 5, and… 8 bedroom ‘villas’?), though not every one…\nAnd how about these?\n\nmsoa_listings[\n    (msoa_listings.MSOA11CD.isin(mlgp[mlgp &gt; 300].index)) & (msoa_listings.room_type!='Entire home/apt')\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','property_type','name','description']\n]\n\nIf we wanted to be rigorous then we’d have to investigate further: properties in Mayfair and Westminster are going to be expensive, but are these plausible nightly prices? In some cases, yes. In others…\n\nmsoa_listings[\n    (msoa_listings.MSOA11CD.isin(mlgp[mlgp &lt; 100].index)) & (msoa_listings.room_type!='Entire home/apt')\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','name','description']\n]\n\nOn the whole, let’s take a guess that there are a small number of implausibly high prices for individual units that aren’t in very expensive neighbourhoods and that these are either erroneous/deliberately incorrect, or represent a price that is not per-night.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: What's the right answer here? There isn't one. You could probably spend _months_ figuring out what's a real, available-to-let listing and waht isn't. I would argue that assuming all listings are 'legit' without doing some additional EDA and ESDA is negligent. You could also look at how some of the methods of standardisation/normalisation work and use those to identify improbable listings (but remember that a £10,000 in Mayfair _might_ be legit, while a $5,000 listing in Barking _probably_ isn't!). Or you could look at the inter-decile range (or just define your own range: 1%-99%?).\n\n\n\n\nFilter Unlikely Listings\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nSee if you can filter out these less likely listings on the following criteria:\n\nListings are priced above $300/night AND\nRoom type is not 'Entire home/apt' AND\nListings do not contain the words: suite, luxury, loft, stunning, prime, historic, or deluxe.\n\nI found 901 rows to drop this way.\n\ntarget_regex = r'(?:suite|luxury|loft|stunning|prime|historic|deluxe|boutique)'\nto_drop = msoa_listings[\n            (???) & \n            (???) &\n            ~(???(target_regex, flags=re.IGNORECASE, regex=True, na=True))]\nprint(f\"Have found {to_drop.shape[0]:,} rows to drop on the basis of unlikely per night prices.\")\n\n\nto_drop.sort_values(by='price', ascending=False)[['price','room_type','name','description']]\n\n\n\nPlot Unlikely Listings\nHere we use the plt_ldn function – notice how it’s designed to return f,ax in the same way that plt.subplots (which we’re already familiar with) does!\n\nf,ax = plt_ldn(???, ???)\nto_drop.plot(column='price', markersize=10, alpha=0.7, cmap='viridis', ax=ax);\n\n\n\n… And Drop\nSome might be legitimate, but I’m feeling broadly ok with the remainder.\n\ncleaned = msoa_listings.drop(index=to_drop.???)\nprint(f\"Cleaned data has {cleaned.shape[0]:,} rows.\")\n\nAfter this I had 84,308 rows.\nI would normally, at this point, spend quite a bit of time validating this cleaning approach, but right now we’re going to take a rough-and-ready approach.\n\n\nQuestions\n\nWhat data type did Task 2.2 return?\n\n\n\n\n\nWhat is the function of reset_index() in Task 2.3 and when might you choose to reset (or not)?"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#create-pivot-table",
    "href": "practicals/Practical-12-Grouping_Data.html#create-pivot-table",
    "title": "Practical 12: Grouping Data",
    "section": "Create Pivot Table",
    "text": "Create Pivot Table\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe can make use of the pivot table function to generate counts by MSOA in a ‘wide’ format.\n\npivot = cleaned.groupby(\n                ['MSOA11CD','room_type'], observed=False\n        ).listing_url.agg(Count='count').reset_index().pivot(\n                index='???', columns=['???'], values=['???'])\npivot.head(3)\n\nThe formatting will look a tiny bit different, but you should get something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCount\n\n\n\n\nroom_type\nEntire home/apt\nHotel room\nPrivate room\nShared room\n\n\n\nMSOA11CD\n\n\n\n\n\n\n\nE02000001\n466\n0\n55\n1\n\n\n\nE02000002\n4\n0\n2\n0\n\n\n\nE02000003\n12\n0\n13\n0"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#check-counts",
    "href": "practicals/Practical-12-Grouping_Data.html#check-counts",
    "title": "Practical 12: Grouping Data",
    "section": "Check Counts",
    "text": "Check Counts\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\npivot.sum()\n\nJust to reassure you that the pivot results ‘make sense’:\n\nprint(cleaned[cleaned.room_type=='Entire home/apt'].listing_url.count())\nprint(cleaned[cleaned.room_type=='Private room'].listing_url.count())"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#tidy-normalise",
    "href": "practicals/Practical-12-Grouping_Data.html#tidy-normalise",
    "title": "Practical 12: Grouping Data",
    "section": "Tidy & Normalise",
    "text": "Tidy & Normalise\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nMy instinct at this point is that, looking at the pivot table, we see quite different levels of Airbnb penetration and it is hard to know how handle this difference: share would be unstable because of the low counts in some places and high counts in others; a derived variable that tells us something about density or mix could be interesting (e.g. HHI or LQ) but wouldn’t quite capture the pattern of mixing.\n\nTidy\nPersonally, based on the room type counts above I think we can drop Hotel Rooms and Shared Rooms from this since the other two categories are so dominant.\n\n# Flatten the column index\npivot.columns = ['Entire home/apt','Hotel room','Private room','Shared room']\n# Drop the columns\npivot.drop(???, inplace=True)\npivot.head()\n\nYou should have only the Entire home/apt and Private room columns now.\n\n\nNormalise\n\npivot_norm = pd.DataFrame(index=pivot.index)\nfor c in pivot.columns.to_list():\n    # Power Transform\n    pivot_norm[c] = pts.???(pivot[c].to_numpy().reshape(???,???))\n\npivot_norm.head()\n\nYou should have something like:\n\n\n\n\nEntire home/apt\nPrivate room\n\n\n\n\nMSOA11CD\n\n\n\n\nE02000001\n2.20\n1.06\n\n\nE02000002\n-1.29\n-1.85\n\n\n\n\n\nPlot\n\npnm = pd.merge(msoas.set_index('MSOA11CD'), pivot_norm, left_index=True, right_index=True)\npnm.plot(column='Entire home/apt', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#pca",
    "href": "practicals/Practical-12-Grouping_Data.html#pca",
    "title": "Practical 12: Grouping Data",
    "section": "PCA",
    "text": "PCA\n\n\n\n\n\n\nWarningDifficulty: Moderate, though you might find the questions hard.\n\n\n\n\n\n\n\n\n\nYou can merge the output of this next step back on to the rddf data frame as part of a clustering process, though we’d really want to do some more thinking about what this data means and what transformations we’d need to do in order to make them meaningful.\nFor instance, if we went back to last week’s code, we could have appended this InsideAirbnb data before doing the dimensionality reduction, or we could apply it now to create a new measure that could be used as a separate part of the clustering process together with the reduced dimensionality of the demographic data.\n\nPerform Reduction\n\npcomp = PCA(n_components=???, random_state=42)\nrd    = pcomp.???(pivot_norm)\nprint(f\"The explained variance of each component is: {', '.join([f'{x*100:.2f}%' for x in pcomp.explained_variance_ratio_])}\")\n\nTake the first component and convert to a series to enable the merge:\n\nairbnb_pca = pd.DataFrame(\n                {'Airbnb Component 1': mms.fit_transform(rd[:,1].reshape(-1,1)).reshape(1,-1)[0]}, \n                index=pivot.index)\n\nairbnb_pca.head()\n\nYou should have something like: | | Airbnb Component 1 | | :—- | —-: | | MSOA11CD | | | E02000001 | 0.47 | | E02000002 | 0.19\n\npcanm = pd.merge(msoas.set_index('MSOA11CD'), airbnb_pca, left_index=True, right_index=True)\npcanm.plot(column='Airbnb Component 1', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));\n\n\n\nWrite to Data Frame\n\n# Result Set from merge\nrs = pd.merge(rddf, airbnb_pca, left_index=True, right_index=True)\n\nGrab the PCA, UMAP, and Airbnb outputs for clustering and append rescaled price:\n\n# Merge the reducded dimensionality data frame with the PCA-reduced Airbnb data\n# to create the *cl*uster *d*ata *f*rame\ncldf = pd.merge(rddf.loc[:,'Component 1':], airbnb_pca, \n                left_index=True, right_index=True)\n\n# Append median price from cleaned listings grouped by MSOA too!\ns1 = cleaned.groupby(by='MSOA11CD').price.agg('median')\ncldf['median_price'] = pd.Series(np.squeeze(mms.fit_transform(s1.values.reshape(-1,1))), index=s1.index)\n\n# Append mean price from cleaned listings grouped by MSOA too!\ns2 = cleaned.groupby(by='MSOA11CD').price.agg('mean')\ncldf['mean_price'] = pd.Series(np.squeeze(mms.fit_transform(s2.values.reshape(-1,1))), index=s2.index)\n\ncldf.drop(columns=['Subregion','Borough'], inplace=True)\n\ncldf.head()\n\n\n\nQuestions\n\nHave a think about why you might want to keep the Airbnb data separate from the MSOA data when doing PCA (or any other kind of dimensionality reduction)!\n\n\n\n\n\nWhy might it be interesting to add both mean and median MSOA prices to the clustering process? Here’s a hint (but it’s very subtle): sns.jointplot(x=s1, y=s2, s=15, alpha=0.6)"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#perform-clustering",
    "href": "practicals/Practical-12-Grouping_Data.html#perform-clustering",
    "title": "Practical 12: Grouping Data",
    "section": "Perform Clustering",
    "text": "Perform Clustering\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nc_nm   = 'KMeans' # Clustering name\nk_pref = ??? # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf.drop(columns=['Dimension 1','Dimension 2'])) # The process\n\nHere are the results:\n\nprint(kmeans.labels_) # The results"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#save-clusters-to-data-frame",
    "href": "practicals/Practical-12-Grouping_Data.html#save-clusters-to-data-frame",
    "title": "Practical 12: Grouping Data",
    "section": "Save Clusters to Data Frame",
    "text": "Save Clusters to Data Frame\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nWrite Series and Assign\nNow capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (rs):\n\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n\n\n\nHistogram of Cluster Members\nHow are the clusters distributed?\n\nsns.histplot(data=???, x=c_nm, bins=k_pref);\n\n\n\nMap Clusters\nAnd here’s a map!\n\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=???, ax=ax, linewidth=0, zorder=0, categorical=???, legend=True);\n\n\n\nQuestions\n\nWhat critical assumption did we make when running this analysis?\n\n\n\n\n\nWhy did I not use the UMAP dimensions here?\n\n\n\n\n\nWhy do we have the c_nm='kMeans' when we know what kind of clustering we’re doing?\n\n\n\n\n\nDoes this look like a good clustering?"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#whats-the-right-number-of-clusters",
    "href": "practicals/Practical-12-Grouping_Data.html#whats-the-right-number-of-clusters",
    "title": "Practical 12: Grouping Data",
    "section": "What’s the ‘Right’ Number of Clusters?",
    "text": "What’s the ‘Right’ Number of Clusters?\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nThere’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\n\nkcldf = cldf.drop(columns=['Dimension 1','Dimension 2'])\n\n\nRepeated Clustering\nLet’s try clustering across a wider range. Because we repeatedly re-run the clustering code (unlike with Hierarchical Clustering) this can take a few minutes. I got nearly 5 minutes on a M2 Mac.\n\n%%time \n\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n\nx = []\ny = []\n\n# For resolutions of 'k' in the range 2..40\nfor k in range(2,41):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(kcldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\n\n\nPlot Silhouette Scores\n\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gca().xaxis.grid(True);\nplt.gcf().suptitle(\"Average Silhouette Scores\");\n\n\n\n\n\n\n\nWarning\n\n\n\n**&#9888; Note**: Had we used the UMAP dimensions here you'd likely see more instability in the silhouette plot because the distribution is not remotely Gaussian, though a lot depends on the magnitude of the columns and the number of UMAP vs. PCA components.\n\n\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#final-clustering",
    "href": "practicals/Practical-12-Grouping_Data.html#final-clustering",
    "title": "Practical 12: Grouping Data",
    "section": "Final Clustering",
    "text": "Final Clustering\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=3 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\n\nPerform Clustering\n\nk_pref=???\n\n#############\n# Do the clustering using the main columns\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(kcldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=kcldf.index, name=c_nm)\n\n# We do this for plotting\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(kcldf, kmeans.labels_)\n\n\n\nPlot Diagnostics\n\n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, kcldf.shape[0] + (k_pref + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k_pref)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k_pref)\n    ax2.scatter(kcldf[kcldf.columns[0]], kcldf[kcldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\n\n\n\n\n\n\nWarning\n\n\n\n**&#9888; Stop**: Make sure that you understand how the silhouette plot and value work, and why your results _may_ diverge from mine.\n\n\n\n\nMap Clusters\n\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#representative-centroids",
    "href": "practicals/Practical-12-Grouping_Data.html#representative-centroids",
    "title": "Practical 12: Grouping Data",
    "section": "‘Representative’ Centroids",
    "text": "‘Representative’ Centroids\n\n\n\n\n\n\nWarningDifficulty: Moderate since, conceptually, there’s a lot going on.\n\n\n\n\n\n\n\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\nThese are columns that we want to suppress from our sample:\n\nto_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n             'Airbnb Component 1']\n\nTake a sample of the full range of numeric columns:\n\ncols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\nprint(cols)\n\nCalculate the mean of these columns for each cluster:\n\n# Empty data frame with the columns we'll need\ncentroids = pd.DataFrame(columns=cols)\n\n# For each cluster...\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n    \n    # Select rows where the cluster name matches the cluster number\n    clust = rs[rs[c_nm]==k]\n    \n    # Append the means to the centroids data frame\n    centroids.loc[k] = clust[cols].mean()\n\n\ncentroids\n\n\ncentroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    for r in row.index:\n        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n        centroids_long = pd.concat([centroids_long, d], ignore_index=True)\n\n\ng = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.bar, \"Cluster\", \"Std. Value\")\n\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: The above centroid outputs are a way to think about how each cluster is 'loaded' on to the data. We can't show all of the variables in the data, so we've randomly selected a subset and can then look at how different clusters are more (or less) associated with the standardised value of a particular column/variable."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#work-out-the-neighbour-distance",
    "href": "practicals/Practical-12-Grouping_Data.html#work-out-the-neighbour-distance",
    "title": "Practical 12: Grouping Data",
    "section": "Work out the Neighbour Distance",
    "text": "Work out the Neighbour Distance\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\n\nnbrs = NearestNeighbors(n_neighbors=6).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#derive-approximate-knee",
    "href": "practicals/Practical-12-Grouping_Data.html#derive-approximate-knee",
    "title": "Practical 12: Grouping Data",
    "section": "Derive Approximate Knee",
    "text": "Derive Approximate Knee\n\n\n\n\n\n\nTipDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nfrom kneed import knee_locator\n\nkn = knee_locator.KneeLocator(np.arange(distances.shape[0]), distances, S=12,\n                              curve='convex', direction='increasing')\nprint(f\"Knee detected at: {kn.knee}\")\nkn.plot_knee()\nkn.plot_knee_normalized()\n\n\nprint(f\"Best guess at epsilon for DBSCAN is {distances[kn.knee]:0.4f}\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#explore-epsilons",
    "href": "practicals/Practical-12-Grouping_Data.html#explore-epsilons",
    "title": "Practical 12: Grouping Data",
    "section": "Explore Epsilons",
    "text": "Explore Epsilons\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error, though we can use the above result as a target. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nIterate Over Range\n\n\n\n\n\n\nCaution\n\n\n\n**&#9888; Warning**: Depending on the data volume, this next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn't) the graph above.\n\n\n\n%%time \n\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.76, 0.01): # &lt;- You might want to adjust these!\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\n\n\n\nExamine Clusters\n\nedf.head() # Notice the -1 cluster for small epsilons\n\n\nepsilon_long = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = pd.DataFrame({'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}, index=[1])\n            epsilon_long = pd.concat([epsilon_long, d], ignore_index=True)\n\nepsilon_long['Count'] = epsilon_long.Count.astype(float)\n\n\n\nPlot Cluster Sizes\nOne of the really big problems with DBSCAN and this kind of data is that you have no practical way of specifying epsilon (whereas if you were doing walkability analysis then you could cluster on walking distance!). So you can look at the data (as above) to get a reasoanble value, but look what the output below shows about the stability of the clusters for different values of epsilon!\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=epsilon_long, x='Epsilon', y='Count', hue='Cluster');\nplt.vlines(x=distances[kn.knee], ymin=0, ymax=epsilon_long.Count.max(), color=(1, .7, .7, .8), linestyles='dashed')\nplt.gcf().suptitle(f\"Cluster sizes for various realisations of Epsilon\");\nplt.tight_layout()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#final-clustering-1",
    "href": "practicals/Practical-12-Grouping_Data.html#final-clustering-1",
    "title": "Practical 12: Grouping Data",
    "section": "Final Clustering",
    "text": "Final Clustering\n\n\n\n\n\n\nWarningDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\n###: Perform Clustering\nUse the value from kneed…\n\ndbs = DBSCAN(eps=distances[kn.knee], min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n\n###: Map Clusters\n\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n‘Representative’ Centroids\n\nto_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n             'Airbnb Component 1']\n\nTake a sample of the full range of numeric columns:\n\ncols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\nprint(cols)\n\nCalculate the mean of these columns for each cluster:\n\n# Empty data frame with the columns we'll need\ncentroids = pd.DataFrame(columns=cols)\n\n# For each cluster...\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n    \n    # Select rows where the cluster name matches the cluster number\n    clust = rs[rs[c_nm]==k]\n    \n    # Append the means to the centroids data frame\n    centroids.loc[k] = clust[cols].mean()\n\n\n# Drop the unclustered records (-1)\ncentroids.drop(labels=[-1], axis=0, inplace=True)\ncentroids\n\n\ncentroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    for r in row.index:\n        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n        centroids_long = pd.concat([centroids_long, d], ignore_index=True)\n\n\ng = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.bar, \"Cluster\", \"Std. Value\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#additional-setup",
    "href": "practicals/Practical-12-Grouping_Data.html#additional-setup",
    "title": "Practical 12: Grouping Data",
    "section": "Additional Setup",
    "text": "Additional Setup\n\n\n\n\n\n\nCautionDifficulty: Hard, as I’ve left out quite a bit of code.\n\n\n\n\n\n\n\n\n\n\nImport Libraries\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.inspection import permutation_importance\n\n\n\nSet Up Data\nI’m taking a fairly brutal approach here: anything that is not inherently numeric is gone (bye, bye, text), and I’m not bothering to convert implicitly numeric values either: dates could be converted to ‘months since last review’, for instance, while amenities could be One-Hot Encoded after some pruning of rare amenities. This leaves us with a much smaller number of columns to feed in to the classifier.\n\nprint(f\"Cleaned columns: {', '.join(cleaned.columns.to_list())}.\")\nclassifier_in = cleaned.drop(columns=['listing_url','last_scraped','name','description',\n                                      'host_name', 'host_location', 'property_type', \n                                      'bathrooms_text', 'amenities', 'geometry', 'MSOA11CD',\n                                      'host_since', 'first_review', 'last_review',\n                                      'host_verifications', 'review_scores_rating',\n                                      'reviews_per_month'])\n\n\n\nRemove NAs\nNot all classifiers have this issue, but some will struggle to make predictions (or not be able to do so at all) if there are NAs in the data set. The classifier we’re using can’t deal with NAs, so we have to strip these out, but before we do let’s check the effect:\n\nclassifier_in.isna().sum()\n\nWe can safely drop these now, and you should end up with about 54,000 rows to work with.\n\nclassifier_in = classifier_in.dropna(axis=0, how='any')\nprint(f\"Now have {classifier_in.shape[0]:,} rows of data to work with (down from {cleaned.shape[0]:,}).\")\n\n\nprint()\nprint(f\"Classifier training columns: {', '.join(classifier_in.columns.to_list())}.\")\nclassifier_in.head()\n\n\n\nRemap Non-Numeric Columns\nWe do still have a couple of non-numeric columns to deal with: booleans and the thing we’re actually trying to predict (the room type)!\n\nclassifier_in['host_is_superhost'] = classifier_in.host_is_superhost.replace({True:1, False:0}).astype('int')\n\n\nle = LabelEncoder()\nclassifier_in['room_class'] = le.fit_transform(classifier_in.room_type)\n\nA quick check: we should only have one type per class and vice versa.\n\nclassifier_in.groupby(by=['room_type','room_class']).host_id.agg('count').reset_index()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#random-forest-classification",
    "href": "practicals/Practical-12-Grouping_Data.html#random-forest-classification",
    "title": "Practical 12: Grouping Data",
    "section": "Random Forest Classification",
    "text": "Random Forest Classification\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe’re going to use a Random Forest Classifier but the nice thing about sklearn is that you can quite easily swap in other classifiers if you’d like to explore further. This is one big advantage of Python over R in my book: whereas R tends to get new algorithms first, they are often implemented independently by many people and you can end up with incompatible data structures that require a lot of faff to reorganise for a different algorithm. Python is a bit more ‘managed’ and the dominance of numpy and sklearn and pandas means that people have an incentive to contribute to this library or, if it’s genuinely novel, to create an implementation that works like it would if it were part of sklearn!\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: So here's an _actual_ Machine Learning implementation, but you'll have seen a lot of parts of the code before! \n\n\n\nTrain/Test Split\n\ntrain, test = train_test_split(classifier_in, test_size=0.2, random_state=42)\nprint(f\"Train contains {train.shape[0]:,} records.\")\nprint(f\"Test contains {test.shape[0]:,} records.\")\n\n\ny_train = train.room_class\nX_train = train.drop(columns=['room_class','room_type'])\n\ny_test  = test.room_class\nX_test  = test.drop(columns=['room_class','room_type'])\n\n\n\nClassifier Setup\n\nrfc = RandomForestClassifier(\n    max_depth=8,\n    min_samples_split=7,\n    n_jobs=4,\n    random_state=42\n)\n\n\n\nFit and Predict\n\nrfc.fit(X_train, y_train)\n\n\ny_hat = rfc.predict(X_test)"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#validate",
    "href": "practicals/Practical-12-Grouping_Data.html#validate",
    "title": "Practical 12: Grouping Data",
    "section": "Validate",
    "text": "Validate\n\n\n\n\n\n\nCautionDifficulty: Hard.\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\nc_matrix = pd.DataFrame(confusion_matrix(y_test, y_hat))\nc_matrix.index = le.inverse_transform(c_matrix.index)\nc_matrix.columns = le.inverse_transform(c_matrix.columns)\nc_matrix\n\n\n\nFeature Importance\nCompare the Random Forest’s built-in ‘feature importance’ with Permutation Feature Importance as documented here.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: This next section is the reason you shouldn't blindly run ML algorithms on your data. It turns out that the Random Forest is seriously affected by the scale of different variables, and of binary variables in particular. You will tend to get erroneous feature importance values back from `sklearn`'s RF implementation and should _normally_ look at Permutation Feature Importance values instead. But this little demonstration also shows (above) a more subtle issue: imbalanced data. There are far fewer hotels than there are private rooms, and far fewer of _those_ than there are entire home/apt listings in the sample. So you'll see that the RF has trouble predicting the classes correctly: that's because with a data set like this it's hard to to _better_ than just predicting entire home/apt _Every Single Time_.\n\n\n\nmdi_importances = pd.Series(\n    rfc.feature_importances_, index=rfc.feature_names_in_\n).sort_values(ascending=True)\n\nax = mdi_importances.plot.barh()\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nax.figure.tight_layout()\n\n\n\nPermutation Feature Importance\n\nresult = permutation_importance(\n    rfc, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\n\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X_test.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (Test Set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#shapely-values",
    "href": "practicals/Practical-12-Grouping_Data.html#shapely-values",
    "title": "Practical 12: Grouping Data",
    "section": "Shapely Values",
    "text": "Shapely Values\nShapely values are a big part of explainable AI and they work (very broadly) by permuting the data to explore how sensitive the predictions made by the model are to the results that you see. For these we need to install two libraries: shap (to do the heavy lifting) and slicer to deal with the data.\n\nInstall Libraries\nWe should now have this already available in Podman, but just in case…\n\ntry:\n    import shap\nexcept ModuleNotFoundError:\n    ! pip install slicer shap\n    import shap\n\n\n\nCheck for Data Types\nYou are looking for anything other than int64 or float64 for the most part. Boolean should be fine, but pandas’ internal, nullable integer type will give you a ufunc error.\n\nX_test.info() \n\n\nX_test['beds'] = X_test.beds.astype('int')\n\n\n\nPlot Partial Dependence\n\nshap.partial_dependence_plot(\n    \"price\", rfc.predict, X_test, ice=False,\n    model_expected_value=True, feature_expected_value=True\n)\n\n\n\nCalculate Shapely Values\nThis can take a long time: 4-5 hours (!!!) without developing a strategy for tackling it. See the long discussion here. I’ve taken the approach of subsetting the data substantially (the model is already trained so it won’t impact the model’s predictions) with a 20% fraction of the test data and an explainer sample of 5%. On my laptop the ‘Permutation explainer’ stage took about 14 minutes, but your results may obviously be rather different.\n\nXsample = shap.utils.sample(X_test.sample(frac=0.2, random_state=41), 10)\nexplainer = shap.Explainer(rfc.predict, Xsample)\n\nNow we calculate the shap values for the 5% sample from X_test.\n\n\n\n\n\n\nCaution\n\n\n\n**&#9888; Warning**: This next block is the one that takes a long time to run. I got between 3mn and 4mn.\n\n\n\n%%time\nshap_values = explainer(X_test.sample(frac=.05, random_state=42))\n\n\n\nSingle Observation\nNow you can take any random record (sample_ind) and produce a shap plot to show the role that each attribute played in its classification. Note that getting these plots to save required some searching on GitHub.\n\nsample_ind=250\nshap.plots.waterfall(shap_values[sample_ind], max_display=14, show=False);\nplt.title(f\"Shapely values for observation #{sample_ind} ({X_test.sample(frac=.05, random_state=42).iloc[sample_ind].name})\")\nplt.tight_layout()\n#plt.savefig('practical-09-waterfall.png', dpi=150)\n\n\n\n\nShapely Feature Plot for Feature 250\n\n\n\n\nAll Observations\n\nshap.plots.beeswarm(shap_values, show=False)\nplt.title(f\"Shapely Swarm Plot for Sample\")\nplt.tight_layout()\nplt.savefig('practical-09-swarm.png', dpi=150)\n\n\n\n\nShapely Swarm Plot"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#wrap-up",
    "href": "practicals/Practical-12-Grouping_Data.html#wrap-up",
    "title": "Practical 12: Grouping Data",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "ref/Font_Test.html",
    "href": "ref/Font_Test.html",
    "title": "Installed Font Test",
    "section": "",
    "text": "This file shows the fonts installed and accessible to Quarto/Matplotlib. It’s used partly to audit the latest Docker/Podman image release and partly to allow students to explore what is available in a reasonably intuitive fashion.\n\nRaw List\nTaken directly from Matplotlib’s font cache:\n\n! grep '\"name\"' ~/.cache/matplotlib/fontlist-v390.json | sort | uniq\n\n      \"name\": \"Big Shoulders Display\",\n      \"name\": \"Bitstream Charter\",\n      \"name\": \"Computer Modern\",\n      \"name\": \"Courier\",\n      \"name\": \"Cousine\",\n      \"name\": \"DejaVu Sans Display\",\n      \"name\": \"DejaVu Sans Mono\",\n      \"name\": \"DejaVu Sans\",\n      \"name\": \"DejaVu Serif Display\",\n      \"name\": \"DejaVu Serif\",\n      \"name\": \"EB Garamond\",\n      \"name\": \"Font Awesome 5 Brands\",\n      \"name\": \"Font Awesome 5 Free\",\n      \"name\": \"Font Awesome 6 Brands\",\n      \"name\": \"Font Awesome 6 Free\",\n      \"name\": \"Hanken Grotesk\",\n      \"name\": \"Helvetica\",\n      \"name\": \"ITC Avant Garde Gothic\",\n      \"name\": \"ITC Bookman\",\n      \"name\": \"ITC Zapf Chancery\",\n      \"name\": \"ITC Zapf Dingbats\",\n      \"name\": \"Inconsolata Condensed\",\n      \"name\": \"Inconsolata Expanded\",\n      \"name\": \"Inconsolata ExtraCondensed\",\n      \"name\": \"Inconsolata ExtraExpanded\",\n      \"name\": \"Inconsolata SemiCondensed\",\n      \"name\": \"Inconsolata SemiExpanded\",\n      \"name\": \"Inconsolata UltraCondensed\",\n      \"name\": \"Inconsolata UltraExpanded\",\n      \"name\": \"Inconsolata\",\n      \"name\": \"Inter Tight\",\n      \"name\": \"LMMono10\",\n      \"name\": \"LMMonoCaps10\",\n      \"name\": \"LMMonoLt10\",\n      \"name\": \"LMMonoLtCond10\",\n      \"name\": \"LMMonoProp10\",\n      \"name\": \"LMMonoPropLt10\",\n      \"name\": \"LMMonoSlant10\",\n      \"name\": \"LMRoman10\",\n      \"name\": \"LMRomanCaps10\",\n      \"name\": \"LMRomanDemi10\",\n      \"name\": \"LMRomanDunh10\",\n      \"name\": \"LMRomanSlant10\",\n      \"name\": \"LMRomanUnsl10\",\n      \"name\": \"LMSans10\",\n      \"name\": \"LMSansDemiCond10\",\n      \"name\": \"LMSansQuot8\",\n      \"name\": \"Liberation Mono\",\n      \"name\": \"Liberation Sans\",\n      \"name\": \"Liberation Serif\",\n      \"name\": \"Lobster\",\n      \"name\": \"Micro 5\",\n      \"name\": \"New Century Schoolbook\",\n      \"name\": \"Palatino\",\n      \"name\": \"Roboto Flex\",\n      \"name\": \"Roboto Mono\",\n      \"name\": \"Roboto Slab\",\n      \"name\": \"STIXGeneral\",\n      \"name\": \"STIXNonUnicode\",\n      \"name\": \"STIXSizeFiveSym\",\n      \"name\": \"STIXSizeFourSym\",\n      \"name\": \"STIXSizeOneSym\",\n      \"name\": \"STIXSizeThreeSym\",\n      \"name\": \"STIXSizeTwoSym\",\n      \"name\": \"Silkscreen\",\n      \"name\": \"Source Code Pro\",\n      \"name\": \"Source Sans 3\",\n      \"name\": \"Source Serif 4 18pt\",\n      \"name\": \"Source Serif 4 36pt\",\n      \"name\": \"Source Serif 4 48pt\",\n      \"name\": \"Source Serif 4\",\n      \"name\": \"Spectral SC\",\n      \"name\": \"Spectral\",\n      \"name\": \"Symbol\",\n      \"name\": \"Times\",\n      \"name\": \"Ubuntu Condensed\",\n      \"name\": \"Ubuntu Mono\",\n      \"name\": \"Ubuntu\",\n      \"name\": \"Utopia\",\n      \"name\": \"ZapfDingbats\",\n      \"name\": \"cmb10\",\n      \"name\": \"cmex10\",\n      \"name\": \"cmmi10\",\n      \"name\": \"cmr10\",\n      \"name\": \"cmss10\",\n      \"name\": \"cmsy10\",\n      \"name\": \"cmtt10\",\n\n\n\n\nFormatted List\nTaken from Matplotlib’s font manager tool.\nPlease note that fonts may not render correctly in Safari, but will do so in Google Chrome and other browsers. The list is correct and these fonts can be used to render PDFs from Quarto.\n\nfrom matplotlib import font_manager\nfrom IPython.core.display import HTML\n\nflist = font_manager.findSystemFonts()\nnames = []\nfor fname in flist:\n    try:\n        names.append(font_manager.FontProperties(fname=fname).get_name())\n    except RuntimeError:\n        print(f\"- Problem detected with {fname}, skipping...\") # Think the issue is emoji-support/colour fonts\n\nprint(f\"Found {len(set(names))} valid fonts.\")\n\ndef make_html(fontname):\n    return \"&lt;p&gt;&lt;span style='font-family:{font}; font-size: 20px;'&gt;{font}&lt;/span&gt;&lt;/p&gt;\".format(font=fontname)\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set(names))])\n\nHTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code))\n\nFound 42 valid fonts.\n\n\nBig Shoulders Display\nCousine\nDejaVu Sans\nEB Garamond\nFont Awesome 5 Brands\nFont Awesome 5 Free\nFont Awesome 6 Brands\nFont Awesome 6 Free\nFont Awesome v4 Compatibility\nHanken Grotesk\nInconsolata\nInconsolata Condensed\nInconsolata Expanded\nInconsolata ExtraCondensed\nInconsolata ExtraExpanded\nInconsolata SemiCondensed\nInconsolata SemiExpanded\nInconsolata UltraCondensed\nInconsolata UltraExpanded\nInter Tight\nLeague Gothic\nLiberation Mono\nLiberation Sans\nLiberation Serif\nLobster\nMicro 5\nRoboto Flex\nRoboto Mono\nRoboto Slab\nSilkscreen\nSource Code Pro\nSource Sans 3\nSource Sans Pro\nSource Serif 4\nSource Serif 4 18pt\nSource Serif 4 36pt\nSource Serif 4 48pt\nSpectral\nSpectral SC\nUbuntu\nUbuntu Condensed\nUbuntu Mono\n\n\n\n\nPlotting Test\nA quick test to ensure that a font is rendering correctly.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.random.random(20)\ny = np.random.random(20)\n\nff = 'SPectral SC'\ntfont = {'fontname':ff}\nbfont = {'fontname':ff, 'weight':'bold', 'horizontalalignment':'left'}\nafont = {'fontname':ff}\n\nf,ax = plt.subplots(1,1,figsize=(7,5))\nplt.scatter(x,y)\nf.suptitle(\"Testing Font Spec\", x=0.025, ha='left', size=24, **tfont)\nax.set_ylabel('Median Price', size=20, **afont)\nax.set_xlabel('Count', size=20, **afont)\n\nText(0.5, 0, 'Count')"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "This page contains all articles from the weekly readings and a few more that may be useful.\n\n\nAbhinav. 2025. “Docker’s Gone — Here’s Why It’s Time to Move on | by Abhinav | Medium.” Online; Medium.\n\n\nAlsudais, Abdulkareem. 2021. “Incorrect Data in the Widely Used Inside Airbnb Dataset.” Decision Support Systems 141:113453. https://doi.org/10.1016/j.dss.2020.113453.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/10.1177/0263276419851846.\n\n\nAnalyst Uttam. 2025. “‘Data Science Is a Dead Career’ — the Truth Behind the Trend No One Wants to Say Out Loud.” https://medium.com/ai-analytics-diaries/https-analystuttam-substack-com-p-data-science-is-a-dead-career-87ee2d8bd338.\n\n\nAnderson, C. 2008. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete.” Wired. https://www.wired.com/2008/06/pb-theory/.\n\n\nArribas-Bel, Daniel. 2014. “Accidental, Open and Everywhere: Emerging Data Sources for the Understanding of Cities.” Applied Geography 49. Elsevier:45–53. https://doi.org/10.1016/j.apgeog.2013.09.012.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBergstrom, C. T., and J. D. West. 2025. “Modern-Day Oracles or Bullshit Machines.” https://thebullshitmachines.com.\n\n\nBivens, J. 2019. “The Economic Costs and Benefits of Airbnb.” Economic Policy Institute. The economic costs and benefits of Airbnb.\n\n\nBrockes, E. 2023. “Airbnb was wild, disruptive and cheap: we loved it. But it wasn’t a love strong enough to last.” The Guardian. https://www.theguardian.com/commentisfree/2023/mar/08/airbnb-wild-disruptive-cheap-lettings-agency.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nClark, J. 2023. “Bidding wars: inside the super-charged fight for rental properties.” The Guardian. https://www.theguardian.com/money/2023/apr/08/bidding-wars-inside-the-super-charged-fight-for-rental-properties.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020d. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020c. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020e. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020b. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020a. Data Feminism. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\nDark Matter Labs. 2019. “A Smart Commons: A New Model for INvesting in the Commons.” Medium. https://provocations.darkmatterlabs.org/a-smart-commons-528f4e53cec2.\n\n\nDavenport, T. H., and D. J. Patil. 2012. “Data Scientist: The Sexiest Job of the 21st Century.” Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\nDelmelle, Elizabeth C, and Isabelle Nilsson. 2021. “The Language of Neighborhoods: A Predictive-Analytical Framework Based on Property Advertisement Text and Mortgage Lending Data.” Computers, Environment and Urban Systems 88. Elsevier:101658. https://doi.org/10.1016/j.compenvurbsys.2021.101658.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics‘.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching introductory GIS programming to geographers using an open source Python approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30. https://doi.org/10.1080/03098265.2015.1086981.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nFitzpatrick, B., and B. Collins-Sussman. n.d. “The Myth of the ’Genius Programmer’.” Google. https://www.youtube.com/watch?v=0SARbwvhupQ.\n\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGoel, N. 2025. “New Junior Developers Can’t Actually Code.” https://nmn.gl/blog/ai-and-learning.\n\n\nGraham, P. 2022. “Putting Ideas into Words.” 2022. https://paulgraham.com/words.html.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHägerstrand, Torsten. 1967. “The Computer and the Geographer.” Transactions of the Institute of British Geographers. JSTOR, 1–19.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHarvey, D. 2008 [1972]. “Revolutionary and Counter-Revolutionary Theory in Geography and the Problem of Ghetto Formation.” In Geographic Thought, edited by G. Henderson and M. Waterstone, 1st Edition. Routledge. https://doi.org/10.4324/9780203893074.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nHuck, G. 2025. “The Developer Skill You Might Be Neglecting.” https://stackoverflow.blog/2025/01/17/the-developer-skill-you-might-be-neglecting/.\n\n\nIqbal, N., and A. Chakrabortty. 2023. “Why are London’s inner-city schools disappearing?” Edited by A. Bransbury. The Guardian. 2023. https://www.theguardian.com/news/audio/2023/apr/26/why-are-london-schools-disappearing-podcast.\n\n\nJolly, J. 2023. “Owners of 100,000 properties held by foreign shell companies unknown despite new UK laws.” The Guardian. https://www.theguardian.com/business/2023/sep/03/owners-of-100000-properties-held-by-foreign-shell-companies-unknown-despite-new-uk-laws.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nKlaas, B. 2025. “The Death of the Student Essay–and the Future of Cognition.” https://www.forkingpaths.co/p/the-death-of-the-student-essayand.\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2). Oxford University Press:97–111.\n\n\n———. 1996. Selected Papers on Computer Science. Cambridge University Press.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLansley, Guy. 2016. “Cars and Socio-Economics: Understanding Neighbourhood Variations in Car Characteristics from Administrative Data.” Regional Studies, Regional Science 3 (1). Taylor & Francis:264–85. https://doi.org/10.1080/21681376.2016.1177466.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are statisticians cold-blooded bosses? a new perspective on the ’old’ concept of statistical population.” Teaching Statistics 35 (1). Wiley Online Library:66–71. https://doi.org/10.1111/j.1467-9639.2012.00524.x.\n\n\nLund, C. 2025. “The Incuriosity Engine.” https://cooperlund.medium.com/the-incuriosity-engine-16bdf41e229d.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458.\n\n\nMattern, Shannon. 2015. “Mission control: A history of the urban dashboard.” Places Journal. https://doi.org/10.22269/150309.\n\n\n———. 2017. “A City Is Not a Computer.” Places Journal. https://doi.org/10.22269/170207.\n\n\nMcCullough, D. 2002. “Interview with NEH chairman Bruce Cole.” Humanities Magazine.\n\n\nMiller, Harvey J, and Michael F Goodchild. 2015. “Data-Driven Geography.” GeoJournal 80. Springer:449–61. https://doi.org/10.1007/s10708-014-9602-6.\n\n\nMinton, A. 2023. “New York is breaking free of Airbnb’s clutches. This is how the rest of the world can follow suit.” The Guardian. https://www.theguardian.com/commentisfree/2023/sep/27/new-york-airbnb-renters-cities-law-ban-properties.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nNeate, R. 2023. “‘This is where people with staggering wealth end up’: who will buy Britain’s most expensive house?” The Guardian. https://www.theguardian.com/money/2023/apr/08/britain-most-expensive-house-rutland-gate-mansion-london-super-rich-buyer.\n\n\nO’Sullivan, David, and Steven M Manson. 2015. “Do physicists have geography envy? And what can geographers learn from it?” Annals of the Association of American Geographers 105 (4). Taylor & Francis:704–22. https://doi.org/10.1080/00045608.2015.1039105.\n\n\nOpen Data Institute. n.d. “SOD0009 - Evidence on Statistics and Open Data.” https://committees.parliament.uk/writtenevidence/45220/pdf/.\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nReades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian. https://doi.org/10.46430/phen0111.\n\n\nReades, J., H. Yingjie, Emmanouil Tranos, and E. Delmelle. 2025. “The City as Text.” Nature Cities. https://doi.org/https://doi.org/10.1038/s44284-025-00314-x.\n\n\nRichmond, T. 2025. “EducAItion, educAItion, educAItion: Could Generative Artificial Intelligence pose a risk to educational standards?” Social Market Foundation. 2025. https://www.smf.co.uk/publications/ai-and-learning/.\n\n\nRose, Gillian. 1997. “Situating Knowledges: Positionality, Reflexivities and Other Tactics.” Progress in Human Geography 21 (3):305–20. https://doi.org/10.1191/030913297673302122.\n\n\nScheider, Simon, Enkhbold Nyamsuren, Han Kruiger, and Haiqi Xu. 2020. “Why Geographic Data Science Is Not a Science.” Geography Compass 14 (11). Wiley Online Library:e12537.\n\n\nSchneider, A. 2013. “How to Become a Data Scientist Before You Graduate.” Berkeley Science Review. http://berkeleysciencereview.com/how-to-become-a-data-scientist-before-you-graduate/.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2P) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSmith, D. 2010. “Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London.” GLA Economics. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nStrauß, Stefan. 2015. “Datafication and the Seductive Power of Uncertainty–a Critical Exploration of Big Data Enthusiasm.” Information 6 (4). MDPI:836–47.\n\n\nSusnjara, S., and I. Smalley. 2025. “What Is Virtualization?” 2025. https://www.ibm.com/think/topics/virtualization.\n\n\nTravers, Tony, Sam Sims, and Nicolas Bosetti. 2016. “Housing and Inequality in London.” Centre for London. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nTufte, Edward R, and Peter R Graves-Morris. 1983. The Visual Display of Quantitative Information. Vol. 2. 9. Graphics press Cheshire, CT.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nVanderPlas, Jake. 2014. “Is Seattle Really Seeing an Uptick in Cycling?” http://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1):160018. https://doi.org/10.1038/sdata.2016.18.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608. https://doi.org/10.1177/0309132520924722.\n\n\nXiao, Ningchuan. 2016. GIS Algorithms: Theory and Applications for Geographic Information Science & Technology. Research Methods. SAGE. https://doi.org/https://dx.doi.org/10.4135/9781473921498.\n\n\nXie, Tessa. 2024a. “How to Better Communicate as a Data Scientist.” Towards Data Science. https://www.divingintodata.com/p/how-to-better-communicate-as-a-data-scientist-6fc5428d3143.\n\n\n———. 2024b. “One Mindset Shift That Will Make You a Better Data Scientist.” Diving Into Data. https://www.divingintodata.com/p/one-mindset-shift-that-will-make-you-a-better-data-scientist-a015f8000ad7.\n\n\n———. 2024c. “The Most Undervalued Skill for Data Scientists.” Towards Data Science. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321.\n\n\nZemanek, H. 1983. “Algorithmic Perfection.” Annals of the History of Computing. AMER FED INFORM PROCESSING SOC.\n\n\nZervas, Georgios, Davide Proserpio, and John W Byers. 2021. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” Marketing Letters 32. Springer:1–16.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "sessions/reading_week.html",
    "href": "sessions/reading_week.html",
    "title": "Reading Week",
    "section": "",
    "text": "Important\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any missed reading(s).\nStart to work on the first few questions of the group assessment.\nStart to test your ability to work with the Inside Airbnb data (Week 5 Practical).\n\n\n\nPast student performance strongly suggests that this is a good week to:\n\nCatch up on readings, particularly the more critical ones and the ones focussing on the impact of Airbnb.\nGo back over the first five notebooks in order to self-test and check your understanding. This doesn’t mean re-doing the full notebook, but (for example) seeing if you can use pandas to analyse the simpler data we looked at in weeks 3 and 4.\n\nLooking ahead to the Group Work, I’d also strongly suggest that you browse the Full Reference List for ideas. The bibliography is a working document and I will add more items as and when I come across them or new works are published, but this is a good time to start reading about the ethical and practical issues arising from Airbnb’s operations and the data to which we have access.",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "sessions/reading_week.html#overview",
    "href": "sessions/reading_week.html#overview",
    "title": "Reading Week",
    "section": "",
    "text": "Important\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any missed reading(s).\nStart to work on the first few questions of the group assessment.\nStart to test your ability to work with the Inside Airbnb data (Week 5 Practical).\n\n\n\nPast student performance strongly suggests that this is a good week to:\n\nCatch up on readings, particularly the more critical ones and the ones focussing on the impact of Airbnb.\nGo back over the first five notebooks in order to self-test and check your understanding. This doesn’t mean re-doing the full notebook, but (for example) seeing if you can use pandas to analyse the simpler data we looked at in weeks 3 and 4.\n\nLooking ahead to the Group Work, I’d also strongly suggest that you browse the Full Reference List for ideas. The bibliography is a working document and I will add more items as and when I come across them or new works are published, but this is a good time to start reading about the ethical and practical issues arising from Airbnb’s operations and the data to which we have access.",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "sessions/reading_week.html#readings",
    "href": "sessions/reading_week.html#readings",
    "title": "Reading Week",
    "section": "Readings",
    "text": "Readings\nWe’ve deliberately refrained from more academic readings this week, so the following content was selected to be easily-digested and interesting in terms of the practical limitations of ‘AI’ tools and the ways in which they reproduce errors in our own thinking rather that offering neutral, ‘objective’ insight:\n\nAI translation jeopardises Afghan asylum claims\nAn Iowa school district is using ChatGPT to decide which books to ban\nSingapore’s tech-utopia dream is turning into a surveillance state nightmare\n\nAnd here’s a nice example of why it’s not about the algorithm:\n\n\n\nAlgorithmic Perfection\n\n\nSource: Zemanek (1983)\n\nHow I’m fighting bias in algorithms\n\n    \n        \n    \n\n\n\nEthics, Politics & Data-driven Research and Technology",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Presenting Data",
    "section": "",
    "text": "This week is focussed on formalising our understanding of how to tune matplotlib plots to ‘publication quality’ by tweaking fine-grained elements. So this week we face off with the monster that most Python visualisation libraries ultimately depend on: matplotlib.\nAs a tool it is both incredibly powerful and intensely counter-intuitive if you are used to R’s ggplot. We will also be looking more widely to the future of quantitative methods, the potential of a geographic data science, and the ways in which we can move between spatial and non-spatial paradigms of analysis within the same piece of work.\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nYou have broadened your thinking about the purpose of data visualisation.\nYou are working intensively on the group project.",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#overview",
    "href": "sessions/week10.html#overview",
    "title": "Presenting Data",
    "section": "",
    "text": "This week is focussed on formalising our understanding of how to tune matplotlib plots to ‘publication quality’ by tweaking fine-grained elements. So this week we face off with the monster that most Python visualisation libraries ultimately depend on: matplotlib.\nAs a tool it is both incredibly powerful and intensely counter-intuitive if you are used to R’s ggplot. We will also be looking more widely to the future of quantitative methods, the potential of a geographic data science, and the ways in which we can move between spatial and non-spatial paradigms of analysis within the same piece of work.\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nYou have broadened your thinking about the purpose of data visualisation.\nYou are working intensively on the group project.",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#readings",
    "href": "sessions/week10.html#readings",
    "title": "Presenting Data",
    "section": "Readings",
    "text": "Readings\nWe will briefly touch on the following readings, but they are primarily to signpost other readings on ethics and related topics that might help you with the assessment or with your reflection on the module as a whole:\n\n\n\nCitation\nArticle\n\n\n\n\nElwood and Leszczynski (2018)\nURL\n\n\nCrawford and Finn (2015)\nURL\n\n\nBemt et al. (2018)\nURL\n\n\n\n\nStudy Guide\n\n\nAdditional Resources\nYou might want to look at the following reports / profiles with a view to thinking about employability and how the skills acquired in this module can be applied beyond the end of your MSc:\n\nCARTO The State of Data Science Report &lt;URL&gt;\nGeospatial Skills Report &lt;URL&gt;\nAAG Profile of Nicolas Saravia &lt;URL&gt;\nWolf et al. (2021) &lt;URL&gt;\n\n\n\n\n\n\n\nTipConnections\n\n\n\nWhile I expect most of you will be focussed on assessments, you should at least skim-read these as they may help you to reflect on what you’ve learned this term in this module and across the programme as a whole, as well as work greater reflection into the group assessment. The other Additional Resources might be useful in terms of looking at the direction of the field, the opportunities in industry, and the kinds of work that people with (sptial) data science skills can do.",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#pre-recorded-lectures",
    "href": "sessions/week10.html#pre-recorded-lectures",
    "title": "Presenting Data",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nYou will find this session a useful overview of how matplotlib works and introduction to wider concerns with designing a good chart or table. Since you have already been making adjustments to figures in the practical sessions, this lecture is intended to build on prior experience:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nData Visualisation\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#practical",
    "href": "sessions/week10.html#practical",
    "title": "Presenting Data",
    "section": "Practical",
    "text": "Practical\nThe practical will lead you through the fine-tuning of data visualisations in Matplotlib/Seaborn. In many ways, this should be seen as largely a recap of material encountered in previous sessions. However, you should see this as an important step in the production of outputs and analyses needed for the final project. That said, you would be better off spending time on the substance of the report first and only turning to the fine-tuning of the visualisations if time permits.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nAutomating the production of map outputs in Python to create an ‘atlas’.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week12.html",
    "href": "sessions/week12.html",
    "title": "Grouping Data",
    "section": "",
    "text": "This session is ‘supplemental’, meaning that it is here to help you integrate ideas seen across Term 1 (and which will be encountered again in Term 2) in a way that supports long-term learning. It is not essential to passing the course and there are no ‘bonus points’ for using methods found in this session.\n\nThis week we will be looking at various ways of grouping data, whether it is by variable or by algorithm. So we begin by covering how data can be aggregated in Python using Pandas before turning to the practical challenges of classification (labeled data) and clustering (unlabeled data).\nWe hare now ‘completing’ the pipeline begun in Week 5 using the concepts introduced in Weeks 1–4, but if you remember your ‘epicycles of analysis’ then you’ll realise that this is, at best, a first pass through the data science process and there are multiple places where insights derived from the practicals (on outliers/problematic records, on data quality issues, on data selection, etc.) could be fed back through the pipeline to adjust and improve the analytical outputs.\nWe will also be shifting our focus in the live session to the final parts of the group submission, but you should also be looking at how this module connects and integrates ideas covered in CASA0001 (UST), CASA0005 (GIS), and CASA0007 (QM). So there will be only a minimal live-coding session in order to leave as much time as possible for the groups to meet and start working on their final projects.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nAn understanding of the differences between aggregation, classification, and clustering.\nAn appreciation of the utility of deriving grouped variables and proxies from raw data.\nAn appreciation of how clustering as part of an analytical pipeline differs from the material covered in CASA0007 and so enhances our understanding of ‘paradigms’ in CASA0001.\nA general appreciation of how different clustering algorithms work and how this differs from classifcation.",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#overview",
    "href": "sessions/week12.html#overview",
    "title": "Grouping Data",
    "section": "",
    "text": "This session is ‘supplemental’, meaning that it is here to help you integrate ideas seen across Term 1 (and which will be encountered again in Term 2) in a way that supports long-term learning. It is not essential to passing the course and there are no ‘bonus points’ for using methods found in this session.\n\nThis week we will be looking at various ways of grouping data, whether it is by variable or by algorithm. So we begin by covering how data can be aggregated in Python using Pandas before turning to the practical challenges of classification (labeled data) and clustering (unlabeled data).\nWe hare now ‘completing’ the pipeline begun in Week 5 using the concepts introduced in Weeks 1–4, but if you remember your ‘epicycles of analysis’ then you’ll realise that this is, at best, a first pass through the data science process and there are multiple places where insights derived from the practicals (on outliers/problematic records, on data quality issues, on data selection, etc.) could be fed back through the pipeline to adjust and improve the analytical outputs.\nWe will also be shifting our focus in the live session to the final parts of the group submission, but you should also be looking at how this module connects and integrates ideas covered in CASA0001 (UST), CASA0005 (GIS), and CASA0007 (QM). So there will be only a minimal live-coding session in order to leave as much time as possible for the groups to meet and start working on their final projects.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nAn understanding of the differences between aggregation, classification, and clustering.\nAn appreciation of the utility of deriving grouped variables and proxies from raw data.\nAn appreciation of how clustering as part of an analytical pipeline differs from the material covered in CASA0007 and so enhances our understanding of ‘paradigms’ in CASA0001.\nA general appreciation of how different clustering algorithms work and how this differs from classifcation.",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#preparatory-lectures",
    "href": "sessions/week12.html#preparatory-lectures",
    "title": "Grouping Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nYou should, by now, be familiar with the concept of how to cluster data from the QM module (CASA0007), so this week is actually focussed on how to move beyond k-means. The point is to contextualise these approaches as part of a data science ‘pipeline’ and to contrast to them with the more theoretical aspects covered elsewhere. We are less interested in the mathematical and technical aspects, and more interested in how one might go about selecting the appropriate algorithm for a particular problem.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nDiverse Data\nVideo\nSlides\n\n\nClassification\nVideo\nSlides\n\n\nClustering\nVideo\nSlides\n\n\nClustering and Geography\nVideo\nSlides",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#other-preparation",
    "href": "sessions/week12.html#other-preparation",
    "title": "Grouping Data",
    "section": "Other Preparation",
    "text": "Other Preparation\n\n\n\n\n\n\nTipConnections\n\n\n\nWe’re trying to move between technical and critical representations of data and methods – showing (again) how all data analysis represents a series of choices about what matters. Ultimately, it’s up to us whether we make these consciously or unconsciously: being a ‘critical’ (spatial) data scientist positions us to question the data constructively to ensure that it is ‘fit for purpose’ – that it is appropriate and adequate to the the processes or behaviours that we wish to study – be it for profit, policy, and public engagement.\n\n\n\nThe follow readings may be useful for reflecting on the topics covered in this session:\n\nBadger, Bui, and Gebeloff (2019) &lt;URL&gt;\nShapiro and Yavuz (2017) &lt;URL&gt;",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#practical",
    "href": "sessions/week12.html#practical",
    "title": "Grouping Data",
    "section": "Practical",
    "text": "Practical\nThe previous week has set up nicely for approaching aggregation, classification, and clustering as functions of the (transformed and reduced) data space. With this, you have essentially covered a full data science analytical pipeline from start (setting up) to finish (cluster/classification analysis) and can hopefully see how these pieces fit together to support one another, and how there is no such thing as a ‘right’ way to approach an analysis… but that there are better and worse ways.\nNote that, while you should be trying to advance your understanding of clustering and classification in Python, these final practicals are also a very good time to be working on your group project. So look at whether the techniques covered this week can help (or distract) you on this work and adjust the time given accordingly.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nHow to group and aggregate data.\nThe connections between classification and clustering.\nThe use of classification as a predictive process with labeled data.\nThe choice of k in k-means and extraction of representative centroids.\nThe use of alternative clustering algorithms (DBSCAN, OPTICS, Self-Organising Maps, and ADBSCAN).\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Foundations (Pt. 2)",
    "section": "",
    "text": "This week we will dig into data (lists and dictionaries) in greater detail so that you understand how we design structures to store and organise data to simplify our analysis. We will also be looking to the Unix Shell/Terminal as a ‘power user feature’ that is often overlooked by novice data scientists.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nTo see how ‘simple’ concepts can be (re)combined to tackle complex problems.\nAn introduction to making use of Git+GitHub.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#overview",
    "href": "sessions/week3.html#overview",
    "title": "Foundations (Pt. 2)",
    "section": "",
    "text": "This week we will dig into data (lists and dictionaries) in greater detail so that you understand how we design structures to store and organise data to simplify our analysis. We will also be looking to the Unix Shell/Terminal as a ‘power user feature’ that is often overlooked by novice data scientists.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nTo see how ‘simple’ concepts can be (re)combined to tackle complex problems.\nAn introduction to making use of Git+GitHub.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#readings",
    "href": "sessions/week3.html#readings",
    "title": "Foundations (Pt. 2)",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nD’Ignazio and Klein (2020) Ch.6\nURL\n\n\nMattern (2017)\nURL\n\n\n\n\nStudy Guide\nThe following questions will help guide your reading and prepare you for class discussions:\n\nDrawing on D’Ignazio and Klein (2020) (The Numbers Don’t Speak for Themselves) consider:\n\n\nWhat critical attribute is missing from ‘big data’ found in the wild and why does it matter?\nHow do the risks this creates differ for insiders, outsiders, and minortised groups?\nHow can the case of VAWG (Violence Against Women and Girls) help you understand the feminist critique of these kinds of data?\nWhat does it mean to say that data emerges ‘fully cooked’?\nWhy is it fundamentally unethical to ‘let the numbers speak for themselves’?\n\n\nUsing Mattern (2017), critically analyze the “city as computer” metaphor:\n\n\nWhat are its strengths and weaknesses?\nHow does this metaphor shape our understanding of urban environments and the solutions we prioritize for urban challenges?\nWhat aspects of cities does it overlook or oversimplify?\nHow does Mattern propose we move beyond this metaphor towards a richer understanding of urban intelligence?",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#pre-recorded-lectures",
    "href": "sessions/week3.html#pre-recorded-lectures",
    "title": "Foundations (Pt. 2)",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nDictionaries\nVideo\nSlides\n\n\nLOLs\nVideo\nNotes\n\n\nDOLs to Data\nVideo\nSlides\n\n\nGetting Stuck into Git\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#practical",
    "href": "sessions/week3.html#practical",
    "title": "Foundations (Pt. 2)",
    "section": "Practical",
    "text": "Practical\nThis week’s practical will take you through the use of dictionaries and introduce the concept of ‘nested’ data structures. We’ll also be looking at how functions (and variables) can be collected into resuable packages that we can either make ourselves or draw on a worldwide bank of experts – I know who I’d rather depend on when the opportunity arises! However, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling the following sessions:\n\nDictionaries\nLoops\nRecap 2\n\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nComparing the use of Python lists and dictionaries to store tabular data.\nExtending lists and dictionaries into nested data structures.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Objects",
    "section": "",
    "text": "This week we will see how Python actually works by looking beyond simple functions and into methods, classes, and the outlines of Object Oriented Design and Programming (OOD/OOP). We’ll also look at what to do when ‘things go wrong’, because they will, but sometimes we want that to blow up the application while other times we want Python to handle the ‘exception’ gracefully. Learning to read Exceptions is essential to debugging code: the one thing that almost never works when you get an exception is ignoring it.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nDevelop enough of an understanding of classes and inheritance that you can make effective use of Python.\nDevelop an understanding of how to read and write Exceptions so as to be able to create robust code.\nUnderstand the process of moving code from in-line scripting, to functions, to packages of functions.\nBegin developing an appreciation of the substantive concerns of the module (data, ethics, bias, and the risks of ‘experts’ who ‘know it all’).\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#overview",
    "href": "sessions/week5.html#overview",
    "title": "Objects",
    "section": "",
    "text": "This week we will see how Python actually works by looking beyond simple functions and into methods, classes, and the outlines of Object Oriented Design and Programming (OOD/OOP). We’ll also look at what to do when ‘things go wrong’, because they will, but sometimes we want that to blow up the application while other times we want Python to handle the ‘exception’ gracefully. Learning to read Exceptions is essential to debugging code: the one thing that almost never works when you get an exception is ignoring it.\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\n\nDevelop enough of an understanding of classes and inheritance that you can make effective use of Python.\nDevelop an understanding of how to read and write Exceptions so as to be able to create robust code.\nUnderstand the process of moving code from in-line scripting, to functions, to packages of functions.\nBegin developing an appreciation of the substantive concerns of the module (data, ethics, bias, and the risks of ‘experts’ who ‘know it all’).\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#readings",
    "href": "sessions/week5.html#readings",
    "title": "Objects",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nD’Ignazio and Klein (2020a) Ch.5\nURL\n\n\nEtherington (2016)\nURL\n\n\n\n\nStudy Guide\n\nReading D’Ignazio and Klein (2020a) (Ch.5) reflect on:\n\n\nThe authors of “Data Feminism” propose a shift from “data for good” to “data for co-liberation.” Explain the core differences between these two frameworks using examples from the sources.\nCompare and contrast the data science methodologies employed by the Anti-Eviction Mapping Project (AEMP) and the Eviction Lab. How do their approaches to data collection, cleaning, and analysis reflect different values and priorities? What are the ethical implications of these choices, and how do they relate to the concept of “epistemic violence”?\nThe dominant metaphors used to describe data scientists, such as “unicorns,” “wizards,” and “rock stars,” can reinforce power imbalances in data work. Analyze the implications of these metaphors, and discuss how alternative framings of data science labor could promote more collaborative and equitable practices.\n\n\nReading Etherington (2016) reflect on:\n\n\nWhy does Etherington advocate for teaching GIS programming to geographers using an open-source Python approach?\nDescribe the six core spatial concepts that Etherington uses to structure his GIS programming course and reflect on whether they improve our understanding of both. \n\n\nCollectively:\n\n\nThe sources highlight a tension between the need for “clean” data in analysis and the potential for data cleaning to obscure diversity. Consider specific examples from the sources that illustrate this tension, and analyze how different approaches to data cleaning might impact the representation of ‘outliers’ in the data (be they marginalised groups, or extreme values).\n\n\n\n\n\n\n\nTipConnections\n\n\n\nD’Ignazio and Klein (2020b, Ch.5) will hopefully help you to reorient your thoughts on what it is that data scientists do (are they rock stars or janitors or something else?), while Etherington (2016) will help you to see how spatial and programming concepts can be connected to one another to improve our understanding of both. For a particularly good example of this see the GIS Algorithms text (Xiao 2016).\n\n\nWe’re now going to shift our focus slightly towards more critical takes on data and data science. These are not by people who can’t ‘do’ data science, rather they point to ways in which neither the data, nor the people who process them, are neutral or objective in the way that we often like to think they are.",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#pre-recorded-lectures",
    "href": "sessions/week5.html#pre-recorded-lectures",
    "title": "Objects",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMethods\nVideo\nSlides\n\n\nClasses\nVideo\nSlides\n\n\nDesign\nVideo\nSlides\n\n\nExceptions\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#practical",
    "href": "sessions/week5.html#practical",
    "title": "Objects",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nWarning\n\n\n\nThis practical will lead you through the process of converting inline scripts into functions and, ultimately, into a simple package. The last parts of the practical are optional – creating classes in hierarchies is integral to how Python works, but many data scientists will rarely need to write their own classes… they just make use of classes written by others (which is why understanding what they are is important, but being able to write your own is a little less so).\n\n\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nBedding in the ‘data thinking’ from last week’s practical.\nCreating functions to perform repetitive tasks.\nPackaging these functions up and accessing via the appropriate namespace.\nImplementing a few simple classes so that you understand the basics of how they work.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week7.html",
    "href": "sessions/week7.html",
    "title": "Spatial Data",
    "section": "",
    "text": "This week we will be focussing on the use of the geopandas library for spatial data analysis and management through a focus on spatial data and its distribution(s). Geopandas will help to clarify how Object-Oriented design and inheritance processes work, while also allowing to interrogate and map the assigned data set(s). A critical concept that should be emerging here is that spatial and numerical data analyses are, fundamentally, just two different views of the same data.\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nYou develop better judgement about interpreting and representing data.\nYou understand how GeoPandas extends Pandas with spatial functionality.\nYou build on material covered in Week 1-3, and 5 of CASA0005 to extend your understanding of mapping and spatial data.\nYou develop better practices for (spatial) data exploration.\n\n\n\nSo we’re going to be looking at both how to work geo-data in Python and how to explore a real-world data set using Exploratory Data Analysis (EDA) and Exploratory Spatial Data Analysis (ESDA) approaches to mapping distributions, testing for NaNs, and so on.",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#overview",
    "href": "sessions/week7.html#overview",
    "title": "Spatial Data",
    "section": "",
    "text": "This week we will be focussing on the use of the geopandas library for spatial data analysis and management through a focus on spatial data and its distribution(s). Geopandas will help to clarify how Object-Oriented design and inheritance processes work, while also allowing to interrogate and map the assigned data set(s). A critical concept that should be emerging here is that spatial and numerical data analyses are, fundamentally, just two different views of the same data.\n\n\n\n\n\n\nImportantLearning Outcomes\n\n\n\n\nYou develop better judgement about interpreting and representing data.\nYou understand how GeoPandas extends Pandas with spatial functionality.\nYou build on material covered in Week 1-3, and 5 of CASA0005 to extend your understanding of mapping and spatial data.\nYou develop better practices for (spatial) data exploration.\n\n\n\nSo we’re going to be looking at both how to work geo-data in Python and how to explore a real-world data set using Exploratory Data Analysis (EDA) and Exploratory Spatial Data Analysis (ESDA) approaches to mapping distributions, testing for NaNs, and so on.",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#readings",
    "href": "sessions/week7.html#readings",
    "title": "Spatial Data",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nLu and Henning (2013)\nURL\n\n\nBunday (n.d.)\nURL\n\n\n\n\nStudy Guide\nThinking about Bunday (n.d.):\n\nThe professors in Bundy’s article seem to be searching for a data transformation that will reveal the “true” ranking of the students. How does this relate to the concept of a “data-generating process” discussed in Lu and Henning?\nBundy’s tale suggests that any data transformation can be used to justify a particular conclusion. How does this relate to D’Ignazio and Klein (2020, Ch.4) and warnings about the potential for bias in data analysis? Are there specific examples that resonate?\n\nReflecting on Lu and Henning (2013):\n\nLu and Henning use the example of retail cashier salaries to illustrate the limitations of traditional population-based thinking. How does their example help us to understand how the concept of a “population” is used and potentially misused?\nWhat are the implications of Lu and Henning’s argument for the use of data in policy-making, and how can we connect this back to D’Ignazio and Klein (2020, Ch.4) as part of a larger debate around ‘the numbers’?\n\n\n\n\n\n\n\nTipConnections\n\n\n\nWe’re focussing this week on the links between the data you’re working with and the process you’re trying to study! You might (quite reasonably) assume that these line up nicely, but in the era of big data that isn’t the case. ‘Accidental’ data (Arribas-Bel 2014) such as smartcard, mobile, web traffic, etc. are only ever partial accounts of messy human reality, so we want you to think about the gap between what you have and what you want to study.",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#pre-recorded-lectures",
    "href": "sessions/week7.html#pre-recorded-lectures",
    "title": "Spatial Data",
    "section": "Pre-Recorded Lectures",
    "text": "Pre-Recorded Lectures\nCome to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMapping\nVideo\nSlides\n\n\nGeoPandas\nVideo\nSlides\n\n\nEDA\nVideo\nSlides\n\n\nSignal and Noise\nVideo\nSlides\n\n\nESDA\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#practical",
    "href": "sessions/week7.html#practical",
    "title": "Spatial Data",
    "section": "Practical",
    "text": "Practical\nIn the practical we will continue to work with the InsideAirbnb data, here focussing on the second ‘class’ of data in the data set: geography. We will see how to use GoePandas and PySAL for (geo)visualisation and analysis.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nCreating/working with geo-data in Python.\nMaking maps with Python.\nExploring the data visually.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Selecting Data",
    "section": "",
    "text": "ImportantLearning Outcomes\n\n\n\n\nYou have formalised your understanding of how to link data in Python.\nYou are working on the group project.",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#overview",
    "href": "sessions/week9.html#overview",
    "title": "Selecting Data",
    "section": "",
    "text": "ImportantLearning Outcomes\n\n\n\n\nYou have formalised your understanding of how to link data in Python.\nYou are working on the group project.",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#readings",
    "href": "sessions/week9.html#readings",
    "title": "Selecting Data",
    "section": "Readings",
    "text": "Readings\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\n\n\n\n\nElwood and Wilson (2017)\nURL\n\n\nO’Sullivan and Manson (2015)\nURL\n\n\nMattern (2015)\nURL\n\n\n\n\nStudy Guide\n\nHow do the concepts of “physics envy” and “geography envy” relate to the evolution of GIScience and the increasing use of urban dashboards?\nCompare and contrast the “Week 10: Ethics” approach to critical GIS with the integrated approach advocated by Elwood and Wilson (2017). What are the strengths and weaknesses of each approach?\nMattern (2015) argues that urban dashboards can obscure the complexity of cities by “bracketing out” certain variables and simplifying representations. How does this critique connect to the concerns raised by Elwood and Wilson (2017)?\nHow does the emphasis on generalization in physics-based approaches to social phenomena highlighted by O’Sullivan and Manson (2015) challenge traditional geographical perspectives that prioritize local and particular knowledge?\nWhat are the shared concerns and potential synergies between the arguments of Elwood and Wilson (2017) and O’Sullivan and Manson (2015)?\n\n\n\n\n\n\n\nTipConnections\n\n\n\nHere we focus on what you can now bring to the table that might help you to dinstinguish yourself from someone who did a ‘data science degree’; through what we study here (and in your other modules) you have been exposed to ways of thinking about data critically and ethically that are rarely part of an Informatics or Machine Learning degree. But as we hope you’re now conviced: these things matter. It’s not just that being critical and ethical is a good way to do your job (whatever that might end up being), it’s that being critical and ethical is a good way to do your job better. You will writing better code. You will write better assessments. You will draw better conclusions.",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#preparatory-lectures",
    "href": "sessions/week9.html#preparatory-lectures",
    "title": "Selecting Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nYou should by now be familiar with the concept of how to join data from the GIS module (CASA0005), so this focusses on two things: 1) how to do this in Python (with a bit of SQL thrown in); and 2) how to approach this process with large or mismatched data sets.\nSo come to class having watched:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLinking Data\nVideo\nSlides\n\n\nLinking Spatial Data\nVideo\nSlides\n\n\nScaling Data\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#practical",
    "href": "sessions/week9.html#practical",
    "title": "Selecting Data",
    "section": "Practical",
    "text": "Practical\nThe practical will lead you through the selection of data in pandas and the equivalent using SQL via DuckDB.\n\n\n\n\n\n\nTipConnections\n\n\n\nThe practical focusses on:\n\nComparing different approaches to data linkage\nLinking data as part of a visualisation process.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  }
]