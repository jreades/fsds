[
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html",
    "href": "practicals/Practical-06-Numeric_Data.html",
    "title": "Practical 6: Numeric Data",
    "section": "",
    "text": "This session is a tour-de-pandas; since this is Python’s equivalent of the tidyverse meets data.tables it is fundamental to the data science ecosystem and is probably one of the most-widely used libraries in the language as a whole. I get more than 286,000 questions tagged with pandas on StackOverflow.\nThis week we are also going to start looking at the InsideAirbnb data which forms the core of the work that we do over the rest of the term. The focus of this notebook is simple numeric data: no mapping or text data… yet… and direct manipulation of data types, derivation of summary statistics, and simple plotting.\nWe hope that you will be able to draw on the past few practical sessions to develop a more intuitive understanding of how to interact with pandas since it supports both a ‘dictionary-of-lists’ style of interaction and a methods-based style of interaction with the ‘Data Frame’."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#reading-remote-data",
    "href": "practicals/Practical-06-Numeric_Data.html#reading-remote-data",
    "title": "Practical 6: Numeric Data",
    "section": "Reading Remote Data",
    "text": "Reading Remote Data\n\n\n\n\n\n\nDifficulty: Low (this time around).\n\n\n\n\n\n\n\n\n\nYou will need to do several things here to read the remote, compressed CSV file specified by url into a data frame called df. Setting low_memory=False ensures that pandas will try to load the entire data set before guessing the data format! Obviously, with very large files this is probably a bad idea and it’s possible to force a particular column type while readng in the data as well. For larger data sets there are platforms like Dask (see, eg, this), and beyond that are other options.\n\n# Set download URL\nymd  = '20240614'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.csv.gz'\n\n\nQuestion\n\n# your code here\ndf = pd.read_csv(??, compression='gzip', low_memory=False)\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nYou should get a data frame containing 75 columns and 87,953 rows of data."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#inspecting-the-data-frame",
    "href": "practicals/Practical-06-Numeric_Data.html#inspecting-the-data-frame",
    "title": "Practical 6: Numeric Data",
    "section": "Inspecting the Data Frame",
    "text": "Inspecting the Data Frame\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nLet’s get a general sense of the data by printing out information about the data frame. There are several ways to do this (and we’ll see another futher on):\n\ndf.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False) – descriptive stats for all numeric columns\ndf.info(verbose=None, buf=None, max_cols=None, memory_usage=None, show_counts=None) – summarises all columns, but without distribution information\ndf.memory_usage(index=True, deep=True) – memory usage details about each column (can be quite slow as it’s doing a lot of digging)\n\n\nQuestion\nWhat is another term for the 0.5 percentile?\n\n\nDescribing\nDescribing a data frame provides general information about numeric columns, such as the median, IQR, or number of discrete values.\nSo to show the 5th and 95th percentiles you need to pass an argument to describe to override the default report from pandas:\n\nQuestion\n\ndf.describe(percentiles=[??])\n\n\n\n\nInfo\nThe info method provides a more system-oriented view of the data frame, helping you to understand what each column is composed of, how many NAs there might be, and some high-level (but often incomplete) data on performance.\n\n\ndf.info(verbose=True)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 87953 entries, 0 to 87952\nData columns (total 75 columns):\n #   Column                                        Non-Null Count  Dtype  \n---  ------                                        --------------  -----  \n 0   id                                            87946 non-null  float64\n 1   listing_url                                   87950 non-null  object \n 2   scrape_id                                     87950 non-null  object \n 3   last_scraped                                  87951 non-null  object \n 4   source                                        87953 non-null  object \n 5   name                                          87953 non-null  object \n 6   description                                   86686 non-null  object \n 7   neighborhood_overview                         47194 non-null  object \n 8   picture_url                                   87950 non-null  object \n 9   host_id                                       87953 non-null  float64\n 10  host_url                                      87953 non-null  object \n 11  host_name                                     87948 non-null  object \n 12  host_since                                    87948 non-null  object \n 13  host_location                                 69172 non-null  object \n 14  host_about                                    45913 non-null  object \n 15  host_response_time                            59024 non-null  object \n 16  host_response_rate                            59031 non-null  object \n 17  host_acceptance_rate                          62760 non-null  object \n 18  host_is_superhost                             87014 non-null  object \n 19  host_thumbnail_url                            87941 non-null  object \n 20  host_picture_url                              87941 non-null  object \n 21  host_neighbourhood                            48075 non-null  object \n 22  host_listings_count                           87941 non-null  object \n 23  host_total_listings_count                     87941 non-null  float64\n 24  host_verifications                            87941 non-null  object \n 25  host_has_profile_pic                          87941 non-null  object \n 26  host_identity_verified                        87941 non-null  object \n 27  neighbourhood                                 47194 non-null  object \n 28  neighbourhood_cleansed                        87946 non-null  object \n 29  neighbourhood_group_cleansed                  7 non-null      float64\n 30  latitude                                      87946 non-null  float64\n 31  longitude                                     87946 non-null  float64\n 32  property_type                                 87946 non-null  object \n 33  room_type                                     87946 non-null  object \n 34  accommodates                                  87946 non-null  float64\n 35  bathrooms                                     0 non-null      float64\n 36  bathrooms_text                                87843 non-null  object \n 37  bedrooms                                      55172 non-null  float64\n 38  beds                                          86812 non-null  float64\n 39  amenities                                     87946 non-null  object \n 40  price                                         87946 non-null  object \n 41  minimum_nights                                87946 non-null  object \n 42  maximum_nights                                87946 non-null  float64\n 43  minimum_minimum_nights                        87945 non-null  float64\n 44  maximum_minimum_nights                        87945 non-null  float64\n 45  minimum_maximum_nights                        87944 non-null  object \n 46  maximum_maximum_nights                        87944 non-null  object \n 47  minimum_nights_avg_ntm                        87944 non-null  float64\n 48  maximum_nights_avg_ntm                        87944 non-null  float64\n 49  calendar_updated                              6 non-null      float64\n 50  has_availability                              87945 non-null  object \n 51  availability_30                               87945 non-null  float64\n 52  availability_60                               87945 non-null  float64\n 53  availability_90                               87945 non-null  float64\n 54  availability_365                              87939 non-null  float64\n 55  calendar_last_scraped                         87946 non-null  object \n 56  number_of_reviews                             87946 non-null  float64\n 57  number_of_reviews_ltm                         87946 non-null  float64\n 58  number_of_reviews_l30d                        87946 non-null  float64\n 59  first_review                                  65789 non-null  object \n 60  last_review                                   65788 non-null  object \n 61  review_scores_rating                          65782 non-null  float64\n 62  review_scores_accuracy                        64847 non-null  float64\n 63  review_scores_cleanliness                     64859 non-null  float64\n 64  review_scores_checkin                         64815 non-null  float64\n 65  review_scores_communication                   64845 non-null  float64\n 66  review_scores_location                        64815 non-null  float64\n 67  review_scores_value                           64814 non-null  float64\n 68  license                                       1 non-null      object \n 69  instant_bookable                              87939 non-null  object \n 70  calculated_host_listings_count                87939 non-null  float64\n 71  calculated_host_listings_count_entire_homes   87939 non-null  float64\n 72  calculated_host_listings_count_private_rooms  87939 non-null  float64\n 73  calculated_host_listings_count_shared_rooms   87939 non-null  float64\n 74  reviews_per_month                             65782 non-null  float64\ndtypes: float64(35), object(40)\nmemory usage: 50.3+ MB\n\n\n\nYou should get that the data frame has a mix of float64, int, and object (text) columns and that some columns contain many nulls. You will also get an estimate of memory usage that may differ substantially from the more complete picture provided below, which suggests a ‘true’ value of 396MB.\n\n\nMemory Usage\nIf you really need to get into the ‘weeds’ and profile your data frame because you are crashing Python and seeing messages about ‘core dumped’, or seeing appallingly poor performance, then memory_usage is the way to go:\ndf.memory_usage(index=True, deep=True)\n\n\n\nIndex                                               132\nid                                               703624\nlisting_url                                     7940958\nscrape_id                                       5540904\nlast_scraped                                    5189137\n                                                 ...   \ncalculated_host_listings_count                   703624\ncalculated_host_listings_count_entire_homes      703624\ncalculated_host_listings_count_private_rooms     703624\ncalculated_host_listings_count_shared_rooms      703624\nreviews_per_month                                703624\nLength: 76, dtype: int64\n\n\n\nYou should see that the data frame uses 415,086,430 bytes of memory, but the really important thing to note here is the difference between string and other types of data: keeping data as raw strings (instead of converting to categories, for instance) uses up a lot more memory and this can have a huge impact on the performance of your code.\n\n\nPrinting the Columns\nFinally, I find it very useful to be able to quickly print out a list of the columns without all of the details shown above. You just need to print the columns as a list:\n\nprint(df.columns.to_list())\n\n['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id', 'host_url', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability', 'availability_30', 'availability_60', 'availability_90', 'availability_365', 'calendar_last_scraped', 'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'license', 'instant_bookable', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'reviews_per_month']\n\n\nYou should get a list showing every single column. If you get Index(['id', 'listing_url',...], dtype='object') then you have printed the column index object and you to need to tell the object to convert its output to a list (hint: Google)."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#saving-the-file-locally",
    "href": "practicals/Practical-06-Numeric_Data.html#saving-the-file-locally",
    "title": "Practical 6: Numeric Data",
    "section": "Saving the File Locally",
    "text": "Saving the File Locally\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nNow save the file somewhere local so that you don’t have to keep downloading 40MB of compressed data every time you want to start the practical. We’ll be using this data for the rest of term, so you might as well save yourself some time and bandwidth! We’ll talk more about data processing pipelines over the course of the term, but I’d suggest putting this data set into a data/raw folder because then you can have directories like data/clean and data/analytical as you move through the process of cleaning and prepping your data for analysis.\n\npath = os.path.join('data','raw') # A default location to save raw data\nfn   = url.split('/')[-1]         # What does this do?\nprint(f\"Writing to: {fn}\")\n\nWriting to: 20240614-London-listings.csv.gz\n\n\n\nif not os.path.exists(path):      # And what does *this* do?\n    print(f\"Creating {path} under {os.getcwd()}\")\n    os.makedirs(path)\n\nif not os.path.exists(os.path.join(path,fn)):  \n    df.to_csv(os.path.join(path,fn), index=False)\n    print(\"Done.\")"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#file-names",
    "href": "practicals/Practical-06-Numeric_Data.html#file-names",
    "title": "Practical 6: Numeric Data",
    "section": "File Names",
    "text": "File Names\nYou should always be looking for ways to avoid hard-coding values that might change over time, especially those linked to the date of the data file.\nIn this case you might try to work out how to make it easy to update the code to download the latest file. For instance, if the file looks like 2022-09-10-listings.csv.gz then I might well specify the url as {date}-listings.csv.gz or {year}-{month}-{day}-listings.csv.gz and set up the variables that I need beforehand or in a separate file.\nUsing parameters makes it easier to write robust code that doesn’t have unwanted side-effects. Here’s a common one: you write code to download and process a file named 20221111-data.csv.gz. After doing all the steps in Tasks 2 and 3 below you save it to clean-data.csv.gz.\n\nQuestion\nWhat happens when your boss asks you to process 20221211-data.csv.gz?"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#file-loading",
    "href": "practicals/Practical-06-Numeric_Data.html#file-loading",
    "title": "Practical 6: Numeric Data",
    "section": "File Loading",
    "text": "File Loading\nNow let’s write something that will allow us to more quickly write our code and validate the results in exploratory phase. For simplicity I’ve called this ‘testing’, but you could also think of it as ‘dev’ mode. What we want is to be able to easily swap between testing and operational contexts using a ‘switch’ (typically, a Boolean value) and limit the data load in testing mode.\nTo achieve this you could set pandas to:\n\nLoad only the first 10,000 rows using nrows if we are testing\nUse the columns specified in cols\nAllow pandas to load the entire data set before deciding on the column type by setting low_memory appropriately.\n\n\nRow Subsetting\nLet’s tackle the rows problem first:\n\nQuestion\n\ntesting = True\n\nif testing:\n    df = pd.read_csv(os.path.join(path,fn), \n                low_memory=??, ??)\nelse:\n    df = pd.read_csv(os.path.join(path,fn), \n                low_memory=??)\n\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nSo notice how this code deliberately works the same for either testing or operational execution – we just flip between the option by changing the testing variable from True to False!\nTo make this more robust and useful we could use this testing variable throughout our code if we wanted to change other behaviours based on development/deployment context. The state of the switch could then be set globally using an external configuration file (usually just called a ‘conf file’). The easiest way to do this is to have a conf.py which contains your global parameters and then every script or notebook file reads in the configuration and sets these variables.\nSomething like:\ntesting = False\nAnd:\nfrom conf import *\n\n\n\nColumn Subsetting\nNow let’s tackle the column problem… In order to avoid having to load lots of data that we aren’t sure we need yet, we can restrict the columns that we load. We got cols below by copying the output of (df.columns.to_list() and then removing the fields that we thought we weren’t interested in.\n\ncols = ['id', 'listing_url', 'last_scraped', 'name', 'description', 'host_id', \n        'host_name', 'host_since', 'host_location', 'host_about', 'host_is_superhost', \n        'host_listings_count', 'host_total_listings_count', 'host_verifications', \n        'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', \n        'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price', \n        'minimum_nights', 'maximum_nights', 'availability_365', 'number_of_reviews', \n        'first_review', 'last_review', 'review_scores_rating', 'license', \n        'reviews_per_month']\nprint(f\"Cols contains {len(cols)} columns.\")\n\nCols contains 34 columns.\n\n\nSo let’s extend our previous answer\n\nQuestion\n\ntesting = True\n\nif testing:\n    df = pd.read_csv(os.path.join(path,fn), \n                low_memory=False, nrows=10000, ??)\nelse:\n    df = pd.read_csv(os.path.join(path,fn), \n                low_memory=False, ??)\n\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#releasing-memory",
    "href": "practicals/Practical-06-Numeric_Data.html#releasing-memory",
    "title": "Practical 6: Numeric Data",
    "section": "Releasing Memory",
    "text": "Releasing Memory\nA particular risk when working with Jupyter notebooks is that you either: a) have run code in an order other than the order shown in the notebook; or b) have made edits to code but not re-run the changed code. So you’re still working from code that is no longer visible!\nWhen that happens you can get very confusing issues because what you see doesn’t square with what the computer has executed. To resolve this without having to re-run the entire notebook (though that can also be a good choice!) you might want to ‘delete’ the current object and re-load or re-run the relevant data or code.\n\ndel(df)\n\nSo we use del(df) to ensure that we aren’t accidentally using the ‘old’ data frame. But another good reason to delete data you’re no longer using is to free up memory."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-rows",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-rows",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting Rows",
    "text": "Selecting Rows\n\n\n\n\n\n\n🔗 Connections\n\n\n\nYou will want to refer to the Randomness lecture to understand how we can select the same random sample each time and to the session on Logic lecture to cover NaNs and NAs.\n\n\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nI often like to start my EDA by simply printing out randomly-selected rows to get a feel for what’s in the data. Does what I see square with what I read in the documentation? What does the name look like? What do I see in last_scraped and is it a sensible? What’s the id field for?\n\ndf.sample(3)\n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_about\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nlicense\nreviews_per_month\n\n\n\n\n69393\n8.506050e+17\nhttps://www.airbnb.com/rooms/850605049432826320\n2023-09-06\nRental unit in Greater London · ★4.92 · 1 bedr...\nYou'll have a great time at this double room i...\n227306886.0\nSebastian\n2018-11-24\nLondon, United Kingdom\nI'm Seb. Originally I come from Hong Kong, but...\n...\n$56.00\n1\n365.0\n242.0\n24.0\n2023-04-29\n2023-08-28\n4.92\nNaN\n5.50\n\n\n66496\n8.214627e+17\nhttps://www.airbnb.com/rooms/821462691922154093\n2023-09-07\nRental unit in Greater London · 2 bedrooms · 1...\nStunning spacious one bedroom apartment in one...\n258154594.0\nAtanas\n2019-04-26\nLondon, United Kingdom\nHi I'm Atanas from Staykeepers Homes. \\nWe are...\n...\n$1,000.00\n2\n365.0\n76.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n40851\n4.407031e+07\nhttps://www.airbnb.com/rooms/44070306\n2023-09-07\nHome in Greater London · 1 bedroom · 2 beds · ...\nA clean welcoming family home 10 minutes walk ...\n127741798.0\nStella\n2017-04-26\nLondon, United Kingdom\nI am a primary school teacher and often visit ...\n...\n$75.00\n3\n31.0\n0.0\n1.0\n2020-08-24\n2020-08-24\n5.00\nNaN\n0.03\n\n\n\n\n3 rows × 34 columns\n\n\n\nSee if you can work out from the documentation (Google search time!) how to get the same ‘random’ sample every time you re-run this code block:\n\nQuestion\n\ndf.sample(3, ??)"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-columns",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-columns",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nIf you look very closely, you’ll see that pandas isn’t showing you the full range of columns since there are 34! If you’d like to only look at specific columns then you can specify them after the sample method call using what looks like a nested list: [[&lt;column names as strings&gt;]].\nI’d like you to sample 3 random rows, selecting the ‘latitude’, ‘longitude’, ‘license’, ‘property_type’, ‘room_type’ and ‘price’ columns only.\n\nQuestion\n\ndf.sample(??)[??]\n\nYour answer should look like this:\n\n\n\n\n\n\nlatitude\nlongitude\nlicense\nproperty_type\nroom_type\nprice\n\n\n\n\n76609\n51.468913\n-0.024023\nNaN\nShared room in home\nShared room\n$53.00\n\n\n50088\n51.536740\n-0.240320\nNaN\nEntire rental unit\nEntire home/apt\n$190.00\n\n\n87872\n51.464072\n-0.181207\nNaN\nEntire rental unit\nEntire home/apt\n$104.00\n\n\n\n\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nlicense\nproperty_type\nroom_type\nprice\n\n\n\n\n76609\n51.468913\n-0.024023\nNaN\nShared room in home\nShared room\n$53.00\n\n\n50088\n51.536740\n-0.240320\nNaN\nEntire rental unit\nEntire home/apt\n$190.00\n\n\n87872\n51.464072\n-0.181207\nNaN\nEntire rental unit\nEntire home/apt\n$104.00"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dealing-with-nans-and-nulls",
    "href": "practicals/Practical-06-Numeric_Data.html#dealing-with-nans-and-nulls",
    "title": "Practical 6: Numeric Data",
    "section": "Dealing with NaNs and Nulls",
    "text": "Dealing with NaNs and Nulls\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\nThere is a lot going on here and you should be paying close attention.\n\n\n\nIf you really dig into the data you will see that a number of data types that aren’t ‘appropriate’ for their contents: the id columns are floats; the dates aren’t dates; there’s a boolean that’s not a boolean… It would be nice to fix these!\n# Add some columns here...\n\n\n\n\n\n\nNote\n\n\n\nI had intended to ask you to fix these by combining code from previous weeks with information provided in the lecture, but it turns out that the InsideAirbnb data set is dirty. There are a lot of NaN values and some of these are deeply problematic for some of the column types in pandas. There are also a number of challenges with other columns so, instead, I’ve opted to show you how I would clean this data as a first pass to get it into a format where it’s tractable for further cleaning.\n\n\n\nIdentifying Problem Rows\nThe reason I’m not asking you to do this part yourselves is that it took me nearly an hour just to work out why I couldn’t convert some of the columns to the right data types; then I started finding rows like these:\n\ndf[df.price.isna()]\n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_about\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nlicense\nreviews_per_month\n\n\n\n\n8668\n1.012818e+07\nhttps://www.airbnb.com/rooms/10128178\n2023-09-06\nLoft in London · ★4.15 · 2 bedrooms · 2 beds ·...\nThis is a lived in apartment with two medium s...\n233649.0\nMichelle\n2010-09-13\nLondon, United Kingdom\nMichelle McLaughlin - A professional who work...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11814\n1.359431e+07\nhttps://www.airbnb.com/rooms/13594306\n2023-09-06\nRental unit in London · ★5.0 · 2 bedrooms · 2 ...\nBeautiful quiet flat with high ceilings and lo...\n18380563.0\nCécile And Maarten\n2014-07-19\nLondon, United Kingdom\nHello! We live in London with our two daughter...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n21912\n2.206322e+07\nhttps://www.airbnb.com/rooms/22063217\n2023-09-07\nRental unit in Greater London · 1 bedroom · 1 ...\n&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Parejas\n1824036.0\nArya\n2012-02-28\nLondon, United Kingdom\nCitizen of the world.\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n48138\n5.342282e+07\nhttps://www.airbnb.com/rooms/53422816\n2023-09-06\nCondo in London · ★4.93 · 1 bedroom · 1 bed · ...\nWe've newly refurbished our 1-bed apartment fo...\n30626999.0\nSitara\n2015-04-05\nLondon, United Kingdom\nLove travelling and meeting people!\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n48765\n5.389986e+07\nhttps://www.airbnb.com/rooms/53899858\n2023-09-06\nLoft in Greater London · ★4.0 · 1 bedroom · 1 ...\nEnjoy a stylish experience at this centrally-l...\n233649.0\nMichelle\n2010-09-13\nLondon, United Kingdom\nMichelle McLaughlin - A professional who work...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n68884\n8.446589e+17\nhttps://www.airbnb.com/rooms/844658929307006668\n2023-09-06\nRental unit in Greater London · 1 bedroom · 1 ...\nStylish apartment Located in sought after neig...\n10573878.0\nLiz\n2013-12-11\nLondon, United Kingdom\nI love hosting and meeting worldly guests trav...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n74896\n8.971145e+17\nhttps://www.airbnb.com/rooms/897114471991989638\n2023-09-06\nRental unit in Greater London · 1 bedroom · 1 ...\nTucked away right off the high street, near 50...\n11520835.0\nBarry\n2014-01-21\nLevittown, NY\nHey there! born and raised in New York. Airbnb...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n7 rows × 34 columns\n\n\n\n\ndf[df.room_type.isna()]\n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_about\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nlicense\nreviews_per_month\n\n\n\n\n8668\n1.012818e+07\nhttps://www.airbnb.com/rooms/10128178\n2023-09-06\nLoft in London · ★4.15 · 2 bedrooms · 2 beds ·...\nThis is a lived in apartment with two medium s...\n233649.0\nMichelle\n2010-09-13\nLondon, United Kingdom\nMichelle McLaughlin - A professional who work...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11814\n1.359431e+07\nhttps://www.airbnb.com/rooms/13594306\n2023-09-06\nRental unit in London · ★5.0 · 2 bedrooms · 2 ...\nBeautiful quiet flat with high ceilings and lo...\n18380563.0\nCécile And Maarten\n2014-07-19\nLondon, United Kingdom\nHello! We live in London with our two daughter...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n21912\n2.206322e+07\nhttps://www.airbnb.com/rooms/22063217\n2023-09-07\nRental unit in Greater London · 1 bedroom · 1 ...\n&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Parejas\n1824036.0\nArya\n2012-02-28\nLondon, United Kingdom\nCitizen of the world.\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n48138\n5.342282e+07\nhttps://www.airbnb.com/rooms/53422816\n2023-09-06\nCondo in London · ★4.93 · 1 bedroom · 1 bed · ...\nWe've newly refurbished our 1-bed apartment fo...\n30626999.0\nSitara\n2015-04-05\nLondon, United Kingdom\nLove travelling and meeting people!\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n48765\n5.389986e+07\nhttps://www.airbnb.com/rooms/53899858\n2023-09-06\nLoft in Greater London · ★4.0 · 1 bedroom · 1 ...\nEnjoy a stylish experience at this centrally-l...\n233649.0\nMichelle\n2010-09-13\nLondon, United Kingdom\nMichelle McLaughlin - A professional who work...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n68884\n8.446589e+17\nhttps://www.airbnb.com/rooms/844658929307006668\n2023-09-06\nRental unit in Greater London · 1 bedroom · 1 ...\nStylish apartment Located in sought after neig...\n10573878.0\nLiz\n2013-12-11\nLondon, United Kingdom\nI love hosting and meeting worldly guests trav...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n74896\n8.971145e+17\nhttps://www.airbnb.com/rooms/897114471991989638\n2023-09-06\nRental unit in Greater London · 1 bedroom · 1 ...\nTucked away right off the high street, near 50...\n11520835.0\nBarry\n2014-01-21\nLevittown, NY\nHey there! born and raised in New York. Airbnb...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n7 rows × 34 columns\n\n\n\n\ndf[~(df.price.str.startswith('$', na=False))]\n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_about\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nlicense\nreviews_per_month\n\n\n\n\n8668\n1.012818e+07\nhttps://www.airbnb.com/rooms/10128178\n2023-09-06\nLoft in London · ★4.15 · 2 bedrooms · 2 beds ·...\nThis is a lived in apartment with two medium s...\n233649.0\nMichelle\n2010-09-13\nLondon, United Kingdom\nMichelle McLaughlin - A professional who work...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8669\nNaN\nwithin an hour\n71%\nhttps://a0.muscache.com/im/pictures/user/652bf...\nhttps://a0.muscache.com/im/pictures/user/652bf...\n3.0\nt\nt\nLondon, England, United Kingdom\nHackney\n...\n170\n2023-09-06\n14.0\nNaN\n2.0\n0\n0.19\nNaN\nNaN\nNaN\n\n\n11814\n1.359431e+07\nhttps://www.airbnb.com/rooms/13594306\n2023-09-06\nRental unit in London · ★5.0 · 2 bedrooms · 2 ...\nBeautiful quiet flat with high ceilings and lo...\n18380563.0\nCécile And Maarten\n2014-07-19\nLondon, United Kingdom\nHello! We live in London with our two daughter...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11815\nNaN\nNaN\nNaN\nhttps://a0.muscache.com/im/pictures/user/User-...\nhttps://a0.muscache.com/im/pictures/user/User-...\n1.0\nt\nt\nLondon, United Kingdom\nBrent\n...\n0\n2023-09-06\n6.0\nNaN\n1.0\n0\n0.07\nNaN\nNaN\nNaN\n\n\n21912\n2.206322e+07\nhttps://www.airbnb.com/rooms/22063217\n2023-09-07\nRental unit in Greater London · 1 bedroom · 1 ...\n&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Parejas\n1824036.0\nArya\n2012-02-28\nLondon, United Kingdom\nCitizen of the world.\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n21913\nNaN\nNaN\nNaN\nhttps://a0.muscache.com/defaults/user_pic-50x5...\nhttps://a0.muscache.com/defaults/user_pic-225x...\n1.0\nf\nt\nNaN\nTower Hamlets\n...\n0\n2023-09-07\n2.0\nNaN\n1.0\n0\n0.03\nNaN\nNaN\nNaN\n\n\n48138\n5.342282e+07\nhttps://www.airbnb.com/rooms/53422816\n2023-09-06\nCondo in London · ★4.93 · 1 bedroom · 1 bed · ...\nWe've newly refurbished our 1-bed apartment fo...\n30626999.0\nSitara\n2015-04-05\nLondon, United Kingdom\nLove travelling and meeting people!\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n48139\nNaN\nNaN\n83%\nhttps://a0.muscache.com/im/pictures/user/3a51c...\nhttps://a0.muscache.com/im/pictures/user/3a51c...\n2.0\nt\nt\nLondon, United Kingdom\nTower Hamlets\n...\n179\n2023-09-06\n15.0\nNaN\n1.0\n0\n0.75\nNaN\nNaN\nNaN\n\n\n48765\n5.389986e+07\nhttps://www.airbnb.com/rooms/53899858\n2023-09-06\nLoft in Greater London · ★4.0 · 1 bedroom · 1 ...\nEnjoy a stylish experience at this centrally-l...\n233649.0\nMichelle\n2010-09-13\nLondon, United Kingdom\nMichelle McLaughlin - A professional who work...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n48766\nNaN\nwithin an hour\n71%\nhttps://a0.muscache.com/im/pictures/user/652bf...\nhttps://a0.muscache.com/im/pictures/user/652bf...\n3.0\nt\nt\nNaN\nHackney\n...\n78\n2023-09-06\n5.0\nNaN\n2.0\n0\n0.28\nNaN\nNaN\nNaN\n\n\n68884\n8.446589e+17\nhttps://www.airbnb.com/rooms/844658929307006668\n2023-09-06\nRental unit in Greater London · 1 bedroom · 1 ...\nStylish apartment Located in sought after neig...\n10573878.0\nLiz\n2013-12-11\nLondon, United Kingdom\nI love hosting and meeting worldly guests trav...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n68885\nNaN\na few days or more\n6%\nhttps://a0.muscache.com/im/pictures/user/7d57d...\nhttps://a0.muscache.com/im/pictures/user/7d57d...\n3.0\nt\nt\nNaN\nKensington and Chelsea\n...\n359\n2023-09-06\n1.0\nNaN\n1.0\n0\n0.17\nNaN\nNaN\nNaN\n\n\n74896\n8.971145e+17\nhttps://www.airbnb.com/rooms/897114471991989638\n2023-09-06\nRental unit in Greater London · 1 bedroom · 1 ...\nTucked away right off the high street, near 50...\n11520835.0\nBarry\n2014-01-21\nLevittown, NY\nHey there! born and raised in New York. Airbnb...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n74897\nNaN\nwithin an hour\n80%\nhttps://a0.muscache.com/im/users/11520835/prof...\nhttps://a0.muscache.com/im/users/11520835/prof...\n3.0\nt\nt\nGreater London, England, United Kingdom\nEaling\n...\n157\n2023-09-06\n0.0\nNaN\n1.0\n0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n14 rows × 34 columns\n\n\n\nIf I had to guess, I’d say that it’s some kind of partial extract/write process because there are elements in some of the problem row(s) that look right but they are in the wrong columns. So we can probably drop some of these rows, but one thing to do is look at the frequency of NaNs across the data frame first. So we need to look for NaNs and Nulls, but it’s quite obvious that a NaN in the listing id is a basic problem and we should drop these.\n\ndf[df.id.isna()][['id','listing_url','name','description','host_id','host_name','price']]\n\n\n\n\n\n\n\n\nid\nlisting_url\nname\ndescription\nhost_id\nhost_name\nprice\n\n\n\n\n8669\nNaN\nwithin an hour\nhttps://a0.muscache.com/im/pictures/user/652bf...\nhttps://a0.muscache.com/im/pictures/user/652bf...\n3.0\nt\n170\n\n\n11815\nNaN\nNaN\nhttps://a0.muscache.com/im/pictures/user/User-...\nhttps://a0.muscache.com/im/pictures/user/User-...\n1.0\nt\n0\n\n\n21913\nNaN\nNaN\nhttps://a0.muscache.com/defaults/user_pic-50x5...\nhttps://a0.muscache.com/defaults/user_pic-225x...\n1.0\nf\n0\n\n\n48139\nNaN\nNaN\nhttps://a0.muscache.com/im/pictures/user/3a51c...\nhttps://a0.muscache.com/im/pictures/user/3a51c...\n2.0\nt\n179\n\n\n48766\nNaN\nwithin an hour\nhttps://a0.muscache.com/im/pictures/user/652bf...\nhttps://a0.muscache.com/im/pictures/user/652bf...\n3.0\nt\n78\n\n\n68885\nNaN\na few days or more\nhttps://a0.muscache.com/im/pictures/user/7d57d...\nhttps://a0.muscache.com/im/pictures/user/7d57d...\n3.0\nt\n359\n\n\n74897\nNaN\nwithin an hour\nhttps://a0.muscache.com/im/users/11520835/prof...\nhttps://a0.muscache.com/im/users/11520835/prof...\n3.0\nt\n157\n\n\n\n\n\n\n\nAs always, if you don’t know that’s going on, break it down:\n\nYou have seen how column works ([[&lt;column names&gt;]]), so that’s just selecting the columns that we want to show;\nYou know how row selection works (df[&lt;selection criteria&gt;]), so that isn’t anything really new either;\nSo the only really new part is df.id.isna(): df.id is the id column (we could have written this df['id'] if we wanted) and isna() is a test for whether or not a value is NaN.\n\nSo this shows that only one row in the 10,000 row sub-sample has a NaN for its id.\nIf you’re not sure what the next line does, try breaking it down by running the inner bits before you run the drop command; and also try looking online for examples of how to use df.drop (e.g. just up above):\n\ndf.drop(df[df.id.isna()].index.array, axis=0, inplace=True)\n\nWith that really troublesome data out of the way, you can now turn to counting NaNs or Nulls in the remaining data with a view to identifying other rows that can probably be dropped.\n\n\nCounting Nulls by Column\nAs a starting point I would look to drop the columns that contain only NaNs. Remember that we’ve dropped a row from the data frame so our maximum is now \\(n-1\\))! Notice how this next command works:\n# returns a data frame with all values set to True/False according to Null status\ndf.isnull() \n# counts these values by column (we'll see another option in a moment)\ndf.isnull.sum(axis=0) \n# Sort results in descending order\ndf.isnull.sum(axis=0).sort_values(ascending=False) \n\ndf.isnull().sum(axis=0).sort_values(ascending=False)[:12]\n\nbathrooms               87946\nlicense                 87945\nhost_about              42040\nbedrooms                32781\nfirst_review            22164\nreviews_per_month       22164\nlast_review             22164\nreview_scores_rating    22164\nhost_location           18778\ndescription              1267\nbeds                     1141\nhost_is_superhost         939\ndtype: int64\n\n\nThe most obvious ones here are: bathrooms, license, and host_about.\n\ndf.drop(columns=['bathrooms','license','host_about'], inplace=True)\n\nBecause we have dropped everything inplace the code simply runs and doesn’t return anything.\n\n\nCounting Nulls by Row\nWe now know that there are still quite a few problems, but we do still need a way to identify the rows that are causing most of the problems.\nNotice here that the change from axis=0 to axis=1 changes the ‘direction’ of the sum from columns to rows. And we are getting back a data series because the summing operation reduces it to just one column.\n\ndf.isnull().sum(axis=1).sort_values(ascending=False).head(20)\n\n48765    22\n8668     22\n21912    22\n68884    22\n48138    22\n11814    22\n74896    22\n7003     11\n6042     11\n5353     11\n4274     11\n6694     11\n2412      9\n39141     8\n39082     8\n40686     8\n27778     8\n39023     8\n1134      8\n611       8\ndtype: int64\n\n\nSo that is Series showing how many NaN values there are by index value. You should see two columns of numbers: the first is the row id, the second is the number of Nulls in that row.\nIf we save the results to a variable called probs (i.e. problems) then we can decide what to do next.\n\n\n\n\n\n\nWarning\n\n\n\nThere’s a chance that Python will complain why you try to run the third line of code. This is particularly likely if you are using Anaconda Python directly (i.e. not Docker). In that case you need to add the code listed at the start of Task 5.\n\n\n\nprobs = df.isnull().sum(axis=1)\nprint(type(probs))       # Note that this has returned a series!\nprobs.plot.hist(bins=30) # Oooooooh, check out what we can do with a series!\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\nLooking at this histogram, these look like two groups in the data so I would start there. I would take values greater than 3–5 as being ones that are most likely be problematic. We can use the index from probs to select out the rows we want to inspect from the main data frame.\nHere’s another bit of code that bears unpacking:\n\ncutoff = 5\ndf.drop(probs[probs &gt; cutoff].index, inplace=True)\n\n\nprobs &gt; 5: this selects only those rows in the ‘probs’ series whose value is greater than 5\nprobs[...].index returns the index values from the Series, which we will then pass to the drop command.\ndf.drop(..., inplace=True) will then drop the rows selected by probs[probs&gt;5].index.\n\n\nprint(f\"Have reduced data frame to: {df.shape[0]:,} rows and {df.shape[1]:,} columns\")\n\nHave reduced data frame to: 85,127 rows and 31 columns"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#label-and-numeric-indexing",
    "href": "practicals/Practical-06-Numeric_Data.html#label-and-numeric-indexing",
    "title": "Practical 6: Numeric Data",
    "section": "Label and Numeric Indexing",
    "text": "Label and Numeric Indexing\nPerhaps this will (eventually) help to make it more clear:\n\ndf.loc[\n    [4552, 4554, 4556, 4557],\n    ['latitude','longitude','property_type','room_type','price']\n]\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nproperty_type\nroom_type\nprice\n\n\n\n\n4552\n51.531070\n-0.186060\nPrivate room in rental unit\nPrivate room\n$95.00\n\n\n4554\n51.476640\n-0.215200\nEntire rental unit\nEntire home/apt\n$250.00\n\n\n4556\n51.400162\n-0.076788\nPrivate room in home\nPrivate room\n$56.00\n\n\n4557\n51.408920\n-0.180910\nEntire rental unit\nEntire home/apt\n$150.00\n\n\n\n\n\n\n\nAnd compare that with:\n\ndf.iloc[\n    4552:4557,\n    14:19\n]\n\n\n\n\n\n\n\n\nlongitude\nproperty_type\nroom_type\naccommodates\nbathrooms_text\n\n\n\n\n4655\n-0.15610\nPrivate room in home\nPrivate room\n1.0\n1 shared bath\n\n\n4656\n-0.11470\nPrivate room in rental unit\nPrivate room\n1.0\n1 bath\n\n\n4657\n-0.06745\nPrivate room in rental unit\nPrivate room\n1.0\n1 shared bath\n\n\n4658\n-0.24050\nEntire rental unit\nEntire home/apt\n4.0\n1 bath\n\n\n4659\n-0.18325\nPrivate room in home\nPrivate room\n2.0\n1 bath\n\n\n\n\n\n\n\nThis code seems similar, but what are iloc and loc? The way I remember it is that iloc means integer location (as you would with list indexing), while loc means label location (as you would with dictionary keys or labels). I guess that should therefore be lloc, but you get the idea)."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#numeric-indexes",
    "href": "practicals/Practical-06-Numeric_Data.html#numeric-indexes",
    "title": "Practical 6: Numeric Data",
    "section": "Numeric Indexes",
    "text": "Numeric Indexes\nIn this case, the index (the numbers down the left-hand side in bold) is numeric, so we can treat it as a label (which allows us to use df.loc) or a list-type index (which allows us to use df.iloc). So with loc we refer to the columns by label, whereas with iloc we refer to them by location; as well, loc allows us to access rows and columns non-sequentially/randomly by label, while iloc allows us to access them as a numeric range."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#non-numeric-indexes",
    "href": "practicals/Practical-06-Numeric_Data.html#non-numeric-indexes",
    "title": "Practical 6: Numeric Data",
    "section": "Non-numeric Indexes",
    "text": "Non-numeric Indexes\nNotice how this works differently if we specify a non-numeric index:\n\ndf.set_index('listing_url')[\n    ['latitude','longitude','property_type','room_type','price']\n].sample(3)\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nproperty_type\nroom_type\nprice\n\n\nlisting_url\n\n\n\n\n\n\n\n\n\nhttps://www.airbnb.com/rooms/2862816\n51.558840\n-0.150470\nPrivate room in condo\nPrivate room\n$90.00\n\n\nhttps://www.airbnb.com/rooms/928259532672943243\n51.440358\n-0.055779\nEntire rental unit\nEntire home/apt\n$210.00\n\n\nhttps://www.airbnb.com/rooms/52538426\n51.551740\n-0.114140\nEntire serviced apartment\nEntire home/apt\n$178.00\n\n\n\n\n\n\n\nNotice change in indexing because ‘listing_url’ is no longer a column, it’s the index now!\n\ndf.set_index('listing_url').iloc[0:3,13:18] \n\n\n\n\n\n\n\n\nlongitude\nproperty_type\nroom_type\naccommodates\nbathrooms_text\n\n\nlisting_url\n\n\n\n\n\n\n\n\n\nhttps://www.airbnb.com/rooms/92644\n-0.18739\nPrivate room in rental unit\nPrivate room\n2.0\n1.5 shared baths\n\n\nhttps://www.airbnb.com/rooms/93015\n-0.21707\nEntire rental unit\nEntire home/apt\n5.0\n1 bath\n\n\nhttps://www.airbnb.com/rooms/13913\n-0.11270\nPrivate room in rental unit\nPrivate room\n1.0\n1 shared bath\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt’s vital that you understand how this code works. By which I mean why it does something at all, not exactly how to use loc and iloc (though that is also useful).\ndf.set_index(...) changes the index from the default row number to another field in the data frame. This operation returns a new data frame with listing_url as its index. Because set index returned a data frame, we can simply add another method call (iloc or loc) on to the end of that line and it returns a new data frame in turn!\nThe fact that each operation returns a new data frame (or data series) is why you can even do this:\n\n    df.set_index('listing_url').iloc[0:3].latitude.mean()\n\n51.50351666666666"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#profiling-optional",
    "href": "practicals/Practical-06-Numeric_Data.html#profiling-optional",
    "title": "Practical 6: Numeric Data",
    "section": "Profiling (Optional)",
    "text": "Profiling (Optional)\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nThe Pandas Profiling tool (rebranded a year or so back as ydata-profiling) offers an alternative way of understanding what’s going on in your data. The output looks rather nice and you might be tempted to ask why we didn’t use this straight away on the full data set – well, if you really want to know, see what happens when you profile all 70,000-odd rows and 70-odd columns in the raw data frame… in effect: while it’s ‘nice to have’, the likelihood of crashing your computer increases significantly and it’s a bit of a tangent, so that’s why it’s no longer included in the Docker image.\nIf you do want to explore this then you’ll need to install the library, and this is a good chance to look at a quite sophisiticated way to install software on another machine:\nfrom ydata_profiling import ProfileReport\n\nSpecify the Profiling Columns\nLooking back over earlier code see if you can work out how to profile latitude, longitude,and review_scores_rating together.\n\nQuestion\n\nprofile = ProfileReport(??, title=\"Pandas Profiling Report\")\n\n\n\n\nProfiling Targets\nYou can write the profile either directly into the Jupyter notebook (this file) or into a separate HTML (i.e. Web) page.\nprofile.to_notebook_iframe()\n# You can also write this profile to a web page:\n# profile.to_file(\"your_report.html\")"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#managing-memory",
    "href": "practicals/Practical-06-Numeric_Data.html#managing-memory",
    "title": "Practical 6: Numeric Data",
    "section": "Managing Memory",
    "text": "Managing Memory\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nAs to why you’d want to fix your data types, there are two reasons: 1) to ensure that you can make the most of your data; 2) to ensure that it takes up as little space as possible in memory. Some simple examples:\n\nA column containing only 'True' (4 bytes) and 'False' (5 bytes) will take up much more space than a column containing only True and False (1 bit each).\nA column containing only 'Red', 'Green', and 'Blue' (3, 5, and 4 bytes each respectively) will take up much more space that a column where we use the numbers 1, 2, 3 to represent these values and have a map that tells us 1==Red, 2==Blue, and 3==Green.\n\nLet’s test this idea out:\n\nrtm = df.room_type.memory_usage(deep=True) # Room Type Memory\nctm = df.room_type.astype('category').memory_usage(deep=True) # Categorical Type Memory\n\nprint(f\"The raw memory usage of `room_type` is {rtm/1024:,.0f} Kb.\")\nprint(f\"The categorical memory usage of `room_type` is {ctm/1024:,.0f} Kb.\")\nprint(f\"That's {(ctm/rtm)*100:.0f}% of the original!\")\n\nThe raw memory usage of `room_type` is 7,958 Kb.\nThe categorical memory usage of `room_type` is 2,813 Kb.\nThat's 35% of the original!\n\n\n\nshm = df.host_is_superhost.memory_usage(deep=True) # Super Host Memory\nbhm = df.host_is_superhost.replace({'f':False, 't':True}).astype('bool').memory_usage(deep=True) # Boolean Host Memory\n\nprint(f\"The raw memory usage of `host_is_superhost` is {shm/1024:,.0f} Kb.\")\nprint(f\"The boolean memory usage of `host_is_superhost` is {bhm/1024:,.0f} Kb.\")\nprint(f\"That's {(bhm/shm)*100:.0f}% of the original!\")\n\nThe raw memory usage of `host_is_superhost` is 6,870 Kb.\nThe boolean memory usage of `host_is_superhost` is 2,812 Kb.\nThat's 41% of the original!"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#boolean-values",
    "href": "practicals/Practical-06-Numeric_Data.html#boolean-values",
    "title": "Practical 6: Numeric Data",
    "section": "Boolean Values",
    "text": "Boolean Values\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nLet’s start with columns that are likely to be boolean:\n\nbools = ['host_is_superhost']\ndf.sample(5, random_state=43)[bools]\n\n\n\n\n\n\n\n\nhost_is_superhost\n\n\n\n\n55197\nf\n\n\n24505\nf\n\n\n44546\nf\n\n\n16312\nt\n\n\n66564\nf\n\n\n\n\n\n\n\nHere we have to map ‘t’ to True and ‘f’ to False before converting the column to a boolean type. If you simply tried to replace them with the strings ‘True’ and ‘False’, then the conversion would run into the same problem as Week 3: any string that is not None will convert a True boolean.\n\n# This approach requires us to map 't' \n# and 'f' to True and False\nfor b in bools:\n    print(f\"Converting {b}\")\n    df[b] = df[b].replace({'f':False, 't':True}).astype('bool')\n\nConverting host_is_superhost\n\n\n\ndf.sample(5, random_state=43)[bools]\n\n\n\n\n\n\n\n\nhost_is_superhost\n\n\n\n\n55197\nFalse\n\n\n24505\nFalse\n\n\n44546\nFalse\n\n\n16312\nTrue\n\n\n66564\nFalse"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dates",
    "href": "practicals/Practical-06-Numeric_Data.html#dates",
    "title": "Practical 6: Numeric Data",
    "section": "Dates",
    "text": "Dates\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nI’ve found dates to be particularly challenging, though pandas has tried to make this process less painful than it was a few years ago. What can be particularly frustrating is if one row has a non-sensical date value (e.g. a t, as happened in 2019/20) then the entire type conversion will fail. When that happens, pandas is not great about communicating where the problem occurred and I had to work it out by trying to convert parts of each series (using .iloc) to the datetime type until I had a block that failed. I then knew that I could narrow this down further using integer location indexing.\n\ndates = ['last_scraped','host_since','first_review','last_review']\n\nprint(f\"Currently {dates[1]} is of type '{df[dates[1]].dtype}'\", \"\\n\")\ndf.sample(5, random_state=43)[dates]\n\nCurrently host_since is of type 'object' \n\n\n\n\n\n\n\n\n\n\nlast_scraped\nhost_since\nfirst_review\nlast_review\n\n\n\n\n55197\n2023-09-07\n2015-02-11\nNaN\nNaN\n\n\n24505\n2023-09-06\n2012-12-18\n2019-01-05\n2020-01-05\n\n\n44546\n2023-09-06\n2016-05-26\n2021-08-15\n2023-01-10\n\n\n16312\n2023-09-07\n2017-04-02\n2017-04-04\n2023-08-26\n\n\n66564\n2023-09-06\n2017-05-07\nNaN\nNaN\n\n\n\n\n\n\n\n\nfor d in dates:\n    print(\"Converting \" + d)\n    df[d] = pd.to_datetime(df[d])\n\nConverting last_scraped\nConverting host_since\nConverting first_review\nConverting last_review\n\n\n\ndf.sample(5, random_state=43)[dates]\n\n\n\n\n\n\n\n\nlast_scraped\nhost_since\nfirst_review\nlast_review\n\n\n\n\n55197\n2023-09-07\n2015-02-11\nNaT\nNaT\n\n\n24505\n2023-09-06\n2012-12-18\n2019-01-05\n2020-01-05\n\n\n44546\n2023-09-06\n2016-05-26\n2021-08-15\n2023-01-10\n\n\n16312\n2023-09-07\n2017-04-02\n2017-04-04\n2023-08-26\n\n\n66564\n2023-09-06\n2017-05-07\nNaT\nNaT\n\n\n\n\n\n\n\nOf course, it’s not actually clear there what has changed! But if you dig a little more deeply:\n\nprint(f\"Now {dates[1]} is of type '{df[dates[1]].dtype}'\", \"\\n\")\ndf.sample(5, random_state=45)[dates[1]].dt.strftime('%A %B %d, %Y')\n# Try some other formats!\n\nNow host_since is of type 'datetime64[ns]' \n\n\n\n45006    Sunday December 15, 2013\n11882       Monday April 13, 2015\n74675     Wednesday July 16, 2014\n80039         Friday May 06, 2022\n84399       Friday April 22, 2022\nName: host_since, dtype: object\n\n\nIn that line of code we:\n\nTook a random sample (setting the state to 45),\nTook the second column from the dates list (dates[1]),\nUsed the date ‘accessor method’ (.dt),\nAnd called string format time with the format %A %B %d, %Y (Full Day of Week, Month Name, Date, 4-digit Year)"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#categories",
    "href": "practicals/Practical-06-Numeric_Data.html#categories",
    "title": "Practical 6: Numeric Data",
    "section": "Categories",
    "text": "Categories\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nWe know that these are likely to be categories because there’d be no other way to allow users to effectively search Airbnb.\n\ncats = ['property_type','room_type']\n\nprint(f\"Currently {cats[1]} is of type '{df[cats[1]].dtype}'\", \"\\n\")\ndf.sample(5, random_state=42)[cats]\n\nCurrently room_type is of type 'object' \n\n\n\n\n\n\n\n\n\n\nproperty_type\nroom_type\n\n\n\n\n18050\nPrivate room in home\nPrivate room\n\n\n86868\nPrivate room in home\nPrivate room\n\n\n54798\nPrivate room in bed and breakfast\nPrivate room\n\n\n74233\nEntire rental unit\nEntire home/apt\n\n\n87242\nEntire rental unit\nEntire home/apt\n\n\n\n\n\n\n\nThis next piece of code is quite useful for grouping and counting operations: we are counting the occurences of each unique value in part particular column or combination of columns:\n\n\ndf[cats[0]].value_counts()\n\nproperty_type\nEntire rental unit                   33450\nPrivate room in rental unit          13278\nPrivate room in home                  9795\nEntire condo                          8656\nEntire home                           7530\n                                     ...  \nYurt                                     1\nPrivate room in island                   1\nShared room in villa                     1\nShared room in serviced apartment        1\nTreehouse                                1\nName: count, Length: 99, dtype: int64\n\n\n\n\n\ndf[cats[1]].value_counts()\n\nroom_type\nEntire home/apt    54203\nPrivate room       30366\nShared room          340\nHotel room           218\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne column has many different values (including Campers/RVs and Yurts!), the other has just four. If I were looking to conduct research I’d probably start with the room_type column since I may not care about hotels and therefore never even need to decide whether I care about boutique ones!\n\n\n\nfor c in cats:\n    print(f\"Converting {c}\")\n    df[c] = df[c].astype('category')\n\nConverting property_type\nConverting room_type\n\n\n\nprint(f\"Now {cats[1]} is of type '{df[cats[1]].dtype}'\", \"\\n\")\nprint(df[cats[1]].cat.categories.values)\n\nNow room_type is of type 'category' \n\n['Entire home/apt' 'Hotel room' 'Private room' 'Shared room']\n\n\n\ndf.sample(5, random_state=42)[cats]\n\n\n\n\n\n\n\n\nproperty_type\nroom_type\n\n\n\n\n18050\nPrivate room in home\nPrivate room\n\n\n86868\nPrivate room in home\nPrivate room\n\n\n54798\nPrivate room in bed and breakfast\nPrivate room\n\n\n74233\nEntire rental unit\nEntire home/apt\n\n\n87242\nEntire rental unit\nEntire home/apt"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dealing-with-strings",
    "href": "practicals/Practical-06-Numeric_Data.html#dealing-with-strings",
    "title": "Practical 6: Numeric Data",
    "section": "Dealing with Strings",
    "text": "Dealing with Strings\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe’ll have to put some more work into deal with the description and other more free-from text fields later in the term, but for now let’s just deal with a straightforward one: price!\n\nmoney = ['price']\ndf.sample(5, random_state=42)[money]\n\n\n\n\n\n\n\n\nprice\n\n\n\n\n18050\n$51.00\n\n\n86868\n$56.00\n\n\n54798\n$45.00\n\n\n74233\n$104.00\n\n\n87242\n$126.00\n\n\n\n\n\n\n\nYou will get an error when you run the next code block, that’s because I want you to do a little thinking about how to extend the code to fix the data. You’ve already got the code you need to fix it, you just need to do a bit of thinking about ‘method chaining’!\n\nfor m in money:\n    print(f\"Converting {m}\")\n    try:\n        df[m] = df[m].str.replace('$','', regex=False).astype('float')\n    except ValueError as e:\n        print(f\"    xxxx Unable to convert {m} to float xxxx\")\n        print(e)\n\nConverting price\n    xxxx Unable to convert price to float xxxx\ncould not convert string to float: '1,000.00'\n\n\nLook closely at the error and then think about what you need to add to the code below:\n\n\n\n\n\n\nNote\n\n\n\nFor now don’t worry about what regex=False means. It will all make sense when we get to dealing with text.\n\n\n\nQuestion\n\nfor m in money:\n    print(f\"Converting {m}\")\n    df[m] = df[m].str.replace('$','', regex=False).str.replace(??).astype('float')\n\n\ndf.sample(5, random_state=42)[money]\n\n\n\n\n\n\n\n\nprice\n\n\n\n\n18050\n51.0\n\n\n86868\n56.0\n\n\n54798\n45.0\n\n\n74233\n104.0\n\n\n87242\n126.0\n\n\n\n\n\n\n\n\ndf.sort_values(by='price', ascending=False).head(3)[['id','name','price','minimum_nights']]\n\n\n\n\n\n\n\n\nid\nname\nprice\nminimum_nights\n\n\n\n\n36168\n3.845268e+07\nGuesthouse in Dagenham · ★5.0 · 1 bedroom · 1 ...\n80100.0\n2\n\n\n11249\n1.325477e+07\nRental unit in London · ★4.85 · 1 bedroom · 1 ...\n53588.0\n3\n\n\n53708\n6.454631e+17\nServiced apartment in Greater London · 4 bedro...\n36000.0\n28"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#dealing-with-integers",
    "href": "practicals/Practical-06-Numeric_Data.html#dealing-with-integers",
    "title": "Practical 6: Numeric Data",
    "section": "Dealing with Integers",
    "text": "Dealing with Integers\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nThis is the issue that made me abandon the idea of making you clean the data yourselves. Although floats have no issues with np.nan in the Series, by default there are no numpy integer arrays that can cope with NaNs. This was such a major issue for Pandas that they’ve actually created their own data type that does support NaN values in integer columns. There are a lot of integer columns, but only one of them seems to be a problem.\n\nints  = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',\n         'beds','minimum_nights','maximum_nights','availability_365']\nfor i in ints:\n    print(f\"Converting {i}\")\n    try:\n        df[i] = df[i].astype('float').astype('int')\n    except ValueError as e:\n        print(\"  - !!!Converting to unsigned 16-bit integer!!!\")\n        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())\n\nConverting id\nConverting host_id\nConverting host_listings_count\nConverting host_total_listings_count\nConverting accommodates\nConverting beds\n  - !!!Converting to unsigned 16-bit integer!!!\nConverting minimum_nights\nConverting maximum_nights\nConverting availability_365\n\n\nSo we convert the column but using a try / except approach that allows to trap ValueError exceptions triggered by the presence of NaNs in the column. The following code tells us that there are just eight of these in the 10k sample, but they’re enough to cause the code to fail if you don’t trap them. The alternatives would be to: a) drop those rows; or b) leave the data as floats. For some reason the latter offends my sense of order, and the former feels like avoiding the problem rather than dealing with it.\n\ndf.beds.isna().value_counts()\n\nbeds\nFalse    84326\nTrue       801\nName: count, dtype: int64"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#validation",
    "href": "practicals/Practical-06-Numeric_Data.html#validation",
    "title": "Practical 6: Numeric Data",
    "section": "Validation",
    "text": "Validation\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nOrdinarily, at this point I would then output information to confirm that all of the opeations I think I’ve undertaken were correctly applied.\ndf.info()"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#saving",
    "href": "practicals/Practical-06-Numeric_Data.html#saving",
    "title": "Practical 6: Numeric Data",
    "section": "Saving",
    "text": "Saving\nAlso at this point I would save a copy of the cleaned data, though I would only consider this data partially cleaned since we’ve not made it any further than just ensuring that each column is in an appropriate format and that some particularly problematic rows have been dropped!\n\npath = os.path.join('data','clean')\n\nif not os.path.exists(path):\n    print(f\"Creating {path} under {os.getcwd()}\")\n    os.makedirs(path)\n    \ndf.to_csv(os.path.join(path,fn), index=False)\nprint(\"Done.\")\n\nDone.\n\n\nFeather is an alternative format (gradually being replaced by parquet, which is more widely supported) for data interchange between R and Python: it’s fast, it preserves data types, it’s compressed, and it will avoid the kinds of the problems that come up when you move to/from CSV as a default."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-using-data-types",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-using-data-types",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting using Data Types",
    "text": "Selecting using Data Types\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nIf we wanted to filter in/out certain columns pandas can do that! Let’s try for floats and ints (hint: these are 64-bit data types).\n\nQuestion\n\ndf.select_dtypes(include=[??])"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selecting-using-conditions",
    "href": "practicals/Practical-06-Numeric_Data.html#selecting-using-conditions",
    "title": "Practical 6: Numeric Data",
    "section": "Selecting using Conditions",
    "text": "Selecting using Conditions\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWhat if we wanted to find whole homes listings for more than $100/night?\nTo do this we use a combination of the selection approaches above in combination with conditionals, but first we need to see what sort of properties there are in the data set! groupby is a really useful function that we’ll come back to later in the term, but for now notice that it helps us to group the analysis by room_type so that subsequently asking for the property_type value counts allows the same property_type to appear in more than once place if it’s associated with more than one room_type.\n\ndf.groupby('room_type', observed=True)['property_type'].value_counts()\n\nroom_type        property_type            \nEntire home/apt  Entire rental unit           33450\n                 Entire condo                  8656\n                 Entire home                   7530\n                 Entire serviced apartment     2021\n                 Entire townhouse              1153\n                                              ...  \nShared room      Tent                             0\n                 Tiny home                        0\n                 Tower                            0\n                 Treehouse                        0\n                 Yurt                             0\nName: count, Length: 396, dtype: int64\n\n\nNow try to select only the Entire home/apt room type:\n\nQuestion\n\ndf[df.??=='??']['property_type'].value_counts().head(10)\n\nYour output should be:\n\nproperty_type\nEntire rental unit           33450\nEntire condo                  8656\nEntire home                   7530\nEntire serviced apartment     2021\nEntire townhouse              1153\nEntire loft                    389\nEntire guesthouse              205\nEntire guest suite             178\nEntire vacation home           102\nBoat                            68\nName: count, dtype: int64"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#arbitrary-selection-criteria",
    "href": "practicals/Practical-06-Numeric_Data.html#arbitrary-selection-criteria",
    "title": "Practical 6: Numeric Data",
    "section": "Arbitrary Selection Criteria",
    "text": "Arbitrary Selection Criteria\n\n\n\n\n\n\nDifficulty: Moderate, if the previous section made sense to you.\n\n\n\n\n\n\n\n\n\nOK, now let’s look for the Entire home/apt listings that are more expected than average… to do that let’s get a sense of where the mean and median value fall:\n\nQuestion\n\nprint(f\"The mean price is ${df.price.??():0.2f}\")\nprint(f\"The median price is ${df.price.??():0.2f}\")\n\nYou should see that the mean is higher than the median price but both are very roughly plausible values. Given your understanding of distributions from, say, Quantitative Methods, what can you say about the pricing distribution of Airbnb units?\nYou might want to have a look at the documentation: it’s rather a long list, but most of your descriptive stats are on that page in the Cumulative / Descriptive Stats section, and there’s also lots of information about methods for strings and categorical data.\n\n\nFiltering: it’s ‘logical’\nSo we want to take Entire home/apt and filter the data set together with the price per night from the price column. For that, let’s use the mean price/night of $183.63 (note: this is totally arbitrary)?\n\nQuestion\nSo here we want to filter on two values in the data set using &:\n\npricey = df[(??) & (df.price&gt;df.price.??)]\nprint(f\"Selected {pricey.shape[0]:,} rows\")\n\nIn the code above we see two things:\n\nThe use of the bitwise & (it’s not the same as and).\nThe fact that you need parentheses around the selection in order to make the the & work."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selection-with-an-aggregate",
    "href": "practicals/Practical-06-Numeric_Data.html#selection-with-an-aggregate",
    "title": "Practical 6: Numeric Data",
    "section": "Selection with an Aggregate",
    "text": "Selection with an Aggregate\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nLet’s find the cheapest and most expensive listings using min and max methods:\n\nQuestion\nLeast expensive:\n\ndf[df.price==df.price.??()][['price','id','listing_url','room_type','description']]\n\nMost expensive:\n\ndf[df.price==df.price.??()][['price','id','listing_url','room_type','description']]\n\nYou should see one or more units priced at exceedingly high levels… and here’s a way to see a few more of these budget-busting options.\n\ndf.sort_values(by='price', ascending=False).head(3)[\n    ['price','listing_url','room_type','description']\n]\n\n\n\n\n\n\n\n\nprice\nlisting_url\nroom_type\ndescription\n\n\n\n\n36168\n80100.0\nhttps://www.airbnb.com/rooms/38452677\nEntire home/apt\nBouquet design open plan house. &lt;br /&gt;Beautifu...\n\n\n11249\n53588.0\nhttps://www.airbnb.com/rooms/13254774\nPrivate room\nPLEASE NOTE THIS IS NO LONGER AVAILABLE! Cosy ...\n\n\n53708\n36000.0\nhttps://www.airbnb.com/rooms/645463113262447532\nEntire home/apt\nEnjoy your stay in London in a modern, archite...\n\n\n\n\n\n\n\n\n\n\n\n\n\nStop: Ask yourself if the result is plausible.\n\n\n\n\n\n\n\n\nQuestion\nWhat do you make of this result?"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#selection-with-a-range",
    "href": "practicals/Practical-06-Numeric_Data.html#selection-with-a-range",
    "title": "Practical 6: Numeric Data",
    "section": "Selection with a Range",
    "text": "Selection with a Range\n\n\n\n\n\n\nDifficulty: Moderate\n\n\n\n\n\n\n\n\n\nPerhaps we aren’t just looking for extremes… how about all of the properties falling within the middle of the distribution? We can ask for any abitrary quantile we like, so let’s go with the 25th and 75th percentile to get the middle 50% of the data. Google how to get percentiles from pandas.\n\nQuestion\n\ndfr = df[ \n            (df.price &gt; df.price.quantile(??)) & \n            (df.price &lt; df.price.quantile(??)) ]\n\nprint(f\"Lower Quartile: {df.price.quantile(??):&gt;6.2f}\")\nprint(f\"Upper Quartile: {df.price.quantile(??):&gt;6.2f}\")\nprint()\nprint(f\"Range selected contains {dfr.shape[0]:,} rows.\")\nprint(f\"Minimum price: {dfr.price.??():&gt;6.2f}\")\nprint(f\"Maximum price: {dfr.price.??():&gt;6.2f}\")\n\nThat example contains a few things to which you need to pay attention:\n\nAgain you can see that, with mutiple selections, we had to put parentheses around each one – this forces Python to…\nProcess the & (bit-wise AND) that asks pandas to “Find all the rows where condition 1 and condition 2 are both True”. So it calculates the True/False for the left side and the True/False for the right side of the &, and then combines them."
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#deriving-a-new-variable",
    "href": "practicals/Practical-06-Numeric_Data.html#deriving-a-new-variable",
    "title": "Practical 6: Numeric Data",
    "section": "Deriving a New Variable",
    "text": "Deriving a New Variable\n\n\n\n\n\n\nDifficulty: 🤯\n\n\n\n\n\n\n\n\n\nLet’s try calculating several derived measures of distribution for the price… these deliberately demonstrate different ways of handling this process (and notice also the little call to apply that can perform additional tasks).\n\nThe z-Score\nThe z-score is given by \\(z = (x - \\bar{x})/\\sigma\\).\n\nQuestion\n\ndf['z'] = (df.?? - df.??.??()) / df.??.??()\ndf.z.describe().apply(lambda x: f\"{x:5.5f}\")\n\n\n\n\nInter-Quartile Standardisation\nThe IQR-standardised score is given by \\(i = (x - Q_{1})/(Q_{3} - Q_{1})\\)\n\nQuestion\n\ndf['iqs'] = (df.price - ??)/(??-??)\ndf.iqs.describe().apply(lambda x: f\"{x:5.5f}\")\n\n\n\n\nLog-Normalisation\nThe natural log of the price is gven by \\(ln(x)\\)\n\nQuestion\n\ndf['lnprice'] = np.log(??)\ndf.lnprice.describe().apply(lambda x: f\"{x:5.5f}\")"
  },
  {
    "objectID": "practicals/Practical-06-Numeric_Data.html#quick-and-dirty-plotting",
    "href": "practicals/Practical-06-Numeric_Data.html#quick-and-dirty-plotting",
    "title": "Practical 6: Numeric Data",
    "section": "Quick (and Dirty) Plotting",
    "text": "Quick (and Dirty) Plotting\nOne of the first things we should do when exploring a new dataset is plot (aka graph) the data. We’ve left plotting until a little later in this practical so that we could see some other basic attributes of how pandas stores data. We’ll look at plotting and exploratory data analyses in much more detail next week, including using packages other than pandas.\nFor now, let’s look at the basic plotting functionality pandas provides - in conjunctions with the online documentation for both DataFrames and Series. There are also examples of all the different types of plots pandas can produce.\n\n\n\n\n\n\nMacOS plotting without Docker\n\n\n\n\n\nMacOS users who are not using Docker will need to do certain things in a specific order at the start of any notebook in order to show maps or graphs. Please make a copy of the following code for any notebook that you create and make it the first code that you run in the notebook…\n# Needed on a Mac\nimport matplotlib as mpl\nmpl.use('TkAgg')\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\n\n\nHistograms\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nFirst, let’s see some of the ways we could visualise the distribution of the Series in the dataset:\n\ndf.price.plot.hist() # histogram\n\n\n\n\n\n\n\n\nIf the code worked properly you should have just created a standard histogram plot (if you can’t see one, ask for help). However, a basic problem here may be the range of the data: if your maximum price is much more than £5,000 then you’ll find the majority of your data plotted in one bar, which isn’t very helpful.\nYou can filter the data and pass in some simple options to improve the plotting:\n\n# Notice the ';' here to suppress `&lt;AxesSubplot...&gt;`\n# That information doesn't *always* appear, but whenever\n# you have unwanted textual output above your plot just\n# add a ';' on the end of the line of code!\ndf[df.price &lt; 1000].price.plot.hist(bins=50); \n\n\n\n\n\n\n\n\n\n\nKDE Plots\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nSimilarly, we can produce a Kernel Density Estimate plot. This time, instead of dropping data just before calling plot we’re going to modify the limits of the x-axis using xlim:\n\nQuestion\nLook for information about using xlim:\n\ndf.price.plot.kde(xlim=(??)); #kernel density estimate plot\n\nKind of handy, no? These aren’t the best looking plots, but they are all being generated on-the-fly for you by pandas with no more than a cheery DataFrame.Series.plot.&lt;plot type&gt;! Since those plots are all just method calls, many of them take optional parameters to change the colour, the notation (scientific or not), and other options. For example, many of the documentation pages linked to above are rather brief, but include a link to the general options that can be applied to all Series.plot calls.\nThis is why we like pandas: it allows us to be constructively lazy. We don’t need to know how a draw a KDE plot (though it always helps if you don’t see what you expected), we just need to know that pandas provides a method that will do it for you. And that is why it’s always worth having a look at the documentation.\n\n\n\nA Slight Case of Over-Plotting\nGenerally, Jupyter is clever enough to overlay plots one on top of the other if you call them all in the same cell. We’ll see ways to gain more control later, but this is still a good start! Note that here we also need to get rid of the -inf values from rows that had a price of £0.\n\n\n\n\n\n\nBug Alert \n\n\n\nThe more we use pandas to sort and filter data the more you will start to see a SettingWithCopyWarning. This happens because of an interaction between how Pandas works and how Python works: when you are working with a very large data set you don’t want to make a ‘deep copy’ of the data structure every time you make a change to the data. Instead, you get a ‘view’ into the data using a reference, which is a just a lightweight shortcut. So what happens when you try to modify that lightweight copy? Well, if you want to drop rows or columns then you either want to make a copy() at that point, or you will have to accept the warning and the computational risks that go with it.\n\n\n\n# Calling copy() ensures the index is updated\n# and note that all subsequent plots will have\n# these £0 rows removed!\ndf = df[df.price &gt; 0].copy() \ndf.z.plot.kde(xlim=[-2, 10])\ndf.iqs.plot.kde(xlim=[-2, 10])\ndf.lnprice.plot.kde();\n\n\n\n\n\n\n\n\n\n\nBoxplots\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nA standard boxplot:\n\ndf.lnprice.plot.box(figsize=(4, 8));\n\n\n\n\n\n\n\n\n\n\nScatterplots\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nWe can also plot two variables in a scatter plot by applying a plot method to the DataFrame (not an individual Series):\n\ndf.plot.scatter(x='longitude', y='latitude', c='price', s=2, cmap='viridis', figsize=(15,10))\n\n\n\n\n\n\n\n\nNote how the code above has the form DataFrame.plot.&lt;plot type&gt;, not DataFrame.Series.plot.&lt;plot type&gt; as in the prior plots. Think about why this then means we need the x and y arguments.\nLooking at the plot produced, it’s hard to see where the high values are, so we might want to think about ways that we could make it easier to spot the big numbers… We could, for instance, also vary the size of the point in a plot by some variable, but why does the following not really work?\n\ndf.plot.scatter(x='longitude', y='latitude', c='price', s=(df.price/df.price.min()), cmap='viridis', figsize=(15,10))\n\n\n\n\n\n\n\n\nAnd we can plot subsets of our data without creating a new object. See if you can work out what the following code is doing that is different from the last plot:\n\ndf[df.price &gt; df.price.quantile(0.90)].plot.scatter(x='longitude', y='latitude', c='price', cmap='viridis', s=8)\n\n\n\n\n\n\n\n\n\n\nHex Bin Plots\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nAnd pandas allows us to create ‘less standard’ plots, like a hex bin plot:\n\ndf.plot.hexbin(x='longitude', y='latitude', gridsize=50, figsize=(10,7))\n\n\n\n\n\n\n\n\nThat’s just a taste of what the basic plotting functionality of pandas can do. Feel free to explore more yourself and we’ll also see the seaborn package later."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "",
    "text": "Part 2 of Practical 8 is optional and should only be attempted if Part 1 made sense to you.\nThe later parts are largely complete and ready to run; however, that doesn’t mean you should just skip over them and think you’ve grasped what’s happening and it will be easy to apply in your own analyses. I would not pay as much attention to LDA topic mining since I don’t think it’s results are that good, but I’ve included it here as it’s still commonly-used in the Digital Humanities and by Marketing folks. Word2Vec is much more powerful and forms the basis of the kinds of advances seen in ChatGPT and other LLMs."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#required-modules",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#required-modules",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Required Modules",
    "text": "Required Modules\n\n\n\n\n\n\nNote\n\n\n\nNotice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: sklearn, nltk, and spacy.\n\n\nStandard libraries we’ve seen before.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport re\nimport math\nimport matplotlib.pyplot as plt\n\nVectorisers we will use from the ‘big beast’ of Python machine learning: Sci-Kit Learn.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# We don't use this but I point out where you *could*\nfrom sklearn.preprocessing import OneHotEncoder \n\nNLP-specific libraries that we will use for tokenisation, lemmatisation, and frequency analysis.\n\nimport nltk\nimport spacy\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\ntry:\n    from nltk.corpus import stopwords\nexcept:\n    nltk.download('stopwords')\n    from nltk.corpus import stopwords\nstopword_list = set(stopwords.words('english'))\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom nltk import ngrams, FreqDist\n\nlemmatizer = WordNetLemmatizer()\ntokenizer = ToktokTokenizer()\n\nRemaining libraries that we’ll use for processing and display text data. Most of this relates to dealing with the various ways that text data cleaning is hard because of the myriad formats it comes in.\n\nimport string\nimport unicodedata\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud, STOPWORDS\n\nThis next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:\n\nfrom IPython.display import display_markdown\n\ndef as_markdown(head='', body='Some body text'):\n    if head != '':\n        display_markdown(f\"##### {head}\\n\\n&gt;{body}\\n\", raw=True)\n    else:\n        display_markdown(f\"&gt;{body}\\n\", raw=True)\n\nas_markdown('Result!', \"Here's my output...\")\n\nResult!\n\nHere’s my output…"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#loading-data",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#loading-data",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Loading Data",
    "text": "Loading Data\n\n\n\n\n\n\n🔗 Connections\n\n\n\nBecause I generally want each practical to stand on its own (unless I’m trying to make a point), I’ve not moved this to a separate Python file (e.g. utils.py, but in line with what we covered back in the lectures on Functions and Packages, this sort of thing is a good candidate for being split out to a separate file to simplify re-use.\n\n\nRemember this function from last week? We use it to save downloading files that we already have stored locally. But notice I’ve made some small changes… what do these do to help the user?\n\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -&gt; str:\n    \"\"\"Downloads and caches a remote file locally.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dest : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[-1] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    # Check if dest+filename does *not* exist -- \n    # that would mean we have to download it!\n    if not os.path.isfile(dfn) or os.path.getsize(dfn) &lt; 1:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        # Convert the path back into a list (without)\n        # the filename -- we need to check that directories\n        # exist first.\n        path = os.path.split(dest)\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how\n        # to use the 'splat' operator! os.makedirs creates missing\n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        # Download and write the file\n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(response.content)\n            \n        print('Done downloading...')\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n    return dfn\n\n\n\n\n\n\n\nTip\n\n\n\nFor very large non-geographic data sets, remember that you can use_cols (or columns depending on the file type) to specify a subset of columns to load.\n\n\nLoad the main data set:\n\n# Load the data sets created in the previous practical\nlux    = gpd.read_parquet(os.path.join('data','clean','luxury.geopackage'))\naff    = gpd.read_parquet(os.path.join('data','clean','affordable.geopackage'))\nbluesp = gpd.read_parquet(os.path.join('data','clean','bluespace.geopackage'))"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#downloading-a-web-page",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#downloading-a-web-page",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Downloading a Web Page",
    "text": "Downloading a Web Page\n\n\n\n\n\n\nDifficulty Level: Low.\n\n\n\n\n\n\n\n\n\nThere is plenty of good economic geography research being done using web pages. Try using Google Scholar to look for work using the British Library’s copy of the Internet Archive.\n\nfrom urllib.request import urlopen, Request\n\n# We need this so that the Bartlett web site 'knows'\n# what kind of browser it is deasling with. Otherwise\n# you get a Permission Error (403 Forbidden) because\n# the site doesn't know what to do.\nhdrs = {\n    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n}\nurl = 'https://www.ucl.ac.uk/bartlett/casa/about-0'\n\n\nQuestionAnswer\n\n\n# Notice that here we have to assemble a request and\n# then 'open' it so that the request is properly issued\n# to the web server. Normally, we'd just use `urlopen`, \n# but that doesn't give you the ability to set the headers.\nrequest  = Request(url, None, hdrs) #The assembled request\nresponse = urlopen(request)\nhtml     = response.???.decode('utf-8') # The data you need\n\nprint(html[:1000])\n\n\n\n# Notice that here we have to assemble a request and\n# then 'open' it so that the request is properly issued\n# to the web server. Normally, we'd just use `urlopen`, \n# but that doesn't give you the ability to set the headers.\nrequest  = Request(url, None, hdrs) #The assembled request\nresponse = urlopen(request)\nhtml     = response.read().decode('utf-8') # The data you need\n\nprint(html[:1000])\n\n&lt;!DOCTYPE html&gt;\n&lt;!--[if IE 7]&gt;\n&lt;html lang=\"en\" class=\"lt-ie9 lt-ie8 no-js\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 8]&gt;\n&lt;html lang=\"en\" class=\"lt-ie9 no-js\"&gt; &lt;![endif]--&gt;\n&lt;!--[if gt IE 8]&gt;&lt;!--&gt;\n&lt;html lang=\"en\" class=\"no-js\"&gt; &lt;!--&lt;![endif]--&gt;\n&lt;head&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/&gt;\n  &lt;meta name=\"author\" content=\"UCL\"/&gt;\n  &lt;meta property=\"og:profile_id\" content=\"uclofficial\"/&gt;\n  &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" /&gt;\n&lt;link rel=\"shortcut icon\" href=\"https://www.ucl.ac.uk/bartlett/casa/sites/all/themes/indigo/favicon.ico\" type=\"image/vnd.microsoft.icon\" /&gt;\n&lt;meta name=\"description\" content=\"The Centre for Advanced Spatial Analysis (CASA) is an interdisciplinary research institute focusing on the science of cities within The Bartlett Faculty of the Built Environment at UCL.\" /&gt;\n&lt;link rel=\"canonical\" href=\"https://www.ucl.ac.uk/bartlett/casa/about-0\" /&gt;\n&lt;meta name=\"ucl:faculty\" content=\"Bartlett\" /&gt;\n&lt;meta name=\"ucl:org_unit\" content=\"Cent"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#removing-html",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#removing-html",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Removing HTML",
    "text": "Removing HTML\n\n\n\n\n\n\nDifficulty level: Moderate\n\n\n\n\n\nBecause what we’re doing will seem really strange and uses some previously unseen libraries that you’ll have to google.\n\n\n\nHint: you need to need to get the text out of the each returned &lt;p&gt; and &lt;div&gt; element! I’d suggest also commenting this up since there is a lot going on on some of these lines of code!\n\nQuestionAnswer\n\n\ncleaned = []\n\nsoup = BeautifulSoup(html)\nbody = soup.find('body')\n\nfor c in body.findChildren(recursive=False):\n    if c.name in ['div','p'] and c.???.strip() != '': \n        # \\xa0 is a non-breaking space in Unicode (&nbsp; in HTML)\n        txt = [re.sub(r'(?:\\u202f|\\xa0|\\u200b)',' ',x.strip()) for x in c.get_text(separator=\" \").split('\\n') if x.strip() != '']\n        cleaned += txt\n\ncleaned\n\n\n\ncleaned = []\n\nsoup = BeautifulSoup(html)\nbody = soup.find('body')\n\nfor c in body.findChildren(recursive=False):\n    if c.name in ['div','p'] and c.get_text().strip() != '':\n        # \\xa0 is a non-breaking space in Unicode (&nbsp; in HTML)\n        txt = [re.sub(r'(?:\\u202f|\\xa0|\\u200b)',' ',x.strip()) for x in c.get_text(separator=\" \").split('\\n') if x.strip() != '']\n        cleaned += txt\n\ncleaned\n\n['UCL Home The Bartlett Centre for Advanced Spatial Analysis About',\n 'About',\n 'The Centre for Advanced Spatial Analysis (CASA) is an interdisciplinary research institute focusing on the science of cities within The Bartlett Faculty of the Built Environment at UCL.',\n 'The Centre for Advanced Spatial Analysis (CASA) was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science, sensing the urban environment, visualisation and computation.  Today, CASA’s research is still pushing boundaries to create better cities for everyone, both leading the intellectual agenda and working closely with government and industry partners to make real-world impact.  Our teaching reflects this, making the most of our cutting-edge research, tools, new forms of data, and long-standing non-academic partnerships to train the next generation of urban scientists with the skills and ideas they’ll need to have an impact in industry, government, academia, and the third sector.  The CASA community is closely connected, but strongly interdisciplinary. We bring together people from around the world with a unique variety of backgrounds – including physicists, planners, geographers, economists, data scientists, architects, mathematicians and computer scientists – united by our mission to tackle the biggest challenges facing cities and societies around the world.  We work across multiple scales: from the hyper-local environment of the low-powered sensor all the way up to satellite remote sensing of whole countries and regions.  Studying at CASA brings lifelong value, with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science.  By studying with us you will become part of our active and engaged alumni community, with access to job listings, networking and social activities, as well as continued contact with our outstanding teachers and researchers.   Location The UCL Centre for Advanced Spatial Analysis is located at 90 Tottenham Court Road, London, W1T 4TJ.',\n 'View Map',\n 'Contact Address: UCL Centre for Advanced Spatial Analysis First Floor, 90 Tottenham Court Road London W1T 4TJ Telephone:  +44 (0)20 3108 3877 Email:   casa@ucl.ac.uk']"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#lower-case",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#lower-case",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Lower Case",
    "text": "Lower Case\n\n\n\n\n\n\nDifficulty Level: Low.\n\n\n\n\n\n\n\n\n\n\nQuestionAnswer\n\n\nlower = [c.???() for ??? in cleaned]\nlower\n\n\n\nlower = [s.lower() for s in cleaned]\nlower\n\n['ucl home the bartlett centre for advanced spatial analysis about',\n 'about',\n 'the centre for advanced spatial analysis (casa) is an interdisciplinary research institute focusing on the science of cities within the bartlett faculty of the built environment at ucl.',\n 'the centre for advanced spatial analysis (casa) was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science, sensing the urban environment, visualisation and computation.  today, casa’s research is still pushing boundaries to create better cities for everyone, both leading the intellectual agenda and working closely with government and industry partners to make real-world impact.  our teaching reflects this, making the most of our cutting-edge research, tools, new forms of data, and long-standing non-academic partnerships to train the next generation of urban scientists with the skills and ideas they’ll need to have an impact in industry, government, academia, and the third sector.  the casa community is closely connected, but strongly interdisciplinary. we bring together people from around the world with a unique variety of backgrounds – including physicists, planners, geographers, economists, data scientists, architects, mathematicians and computer scientists – united by our mission to tackle the biggest challenges facing cities and societies around the world.  we work across multiple scales: from the hyper-local environment of the low-powered sensor all the way up to satellite remote sensing of whole countries and regions.  studying at casa brings lifelong value, with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science.  by studying with us you will become part of our active and engaged alumni community, with access to job listings, networking and social activities, as well as continued contact with our outstanding teachers and researchers.   location the ucl centre for advanced spatial analysis is located at 90 tottenham court road, london, w1t 4tj.',\n 'view map',\n 'contact address: ucl centre for advanced spatial analysis first floor, 90 tottenham court road london w1t 4tj telephone:  +44 (0)20 3108 3877 email:   casa@ucl.ac.uk']"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#stripping-punctuation",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#stripping-punctuation",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Stripping ‘Punctuation’",
    "text": "Stripping ‘Punctuation’\n\n\n\n\n\n\nDifficulty level: Hard\n\n\n\n\n\nThis is because you need to understand: 1) why we’re compiling the regular expression and how to use character classes; and 2) how the NLTK tokenizer differs in approach to the regex.\n\n\n\n\nRegular Expression Approach\nWe want to clear out punctuation using a regex that takes advantage of the [...] (character class) syntax. The really tricky part is remembering how to specify the ‘punctuation’ when some of that punctuation has ‘special’ meanings in a regular expression context. For instance, . means ‘any character’, while [ and ] mean ‘character class’. So this is another escaping problem and it works the same way it did when we were dealing with the Terminal…\nHints: some other factors…\n\nYou will want to match more than one piece of punctuation at a time, so I’d suggest add a + to your pattern.\nYou will need to look into metacharacters for creating a kind of ‘any of the characters in this class’ bag of possible matches.\n\n\nQuestionAnswer\n\n\npattern = re.compile(r'[???]+')\nprint(pattern)\n\n\n\npattern = re.compile(r'[,\\.!\\-&gt;&lt;=\\(\\)\\[\\]\\/&\\'\\\"’;\\+\\–\\—]+')\nprint(pattern)\n\nre.compile('[,\\\\.!\\\\-&gt;&lt;=\\\\(\\\\)\\\\[\\\\]\\\\/&\\\\\\'\\\\\"’;\\\\+\\\\–\\\\—]+')\n\n\n\n\n\n\n\nTokenizer\nThe other way to do this, which is probably easier but produces more complex output, is to draw on the tokenizers already provided by NLTK. For our purposes word_tokenize is probably fine, but depending on your needs there are other options and you can also write your own.\n\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nprint(word_tokenize)\n\n&lt;function word_tokenize at 0x3230ee160&gt;\n\n\n[nltk_data] Downloading package punkt to /Users/jreades/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/jreades/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\n\nCompare\nLook at how these outputs differ in subtle ways:\n\nsubbed = []\ntokens = []\nfor l in lower:\n    subbed.append(re.sub(pattern, ' ', l))\n    tokens.append(word_tokenize(l))\n\nfor s in subbed:\n    as_markdown(\"Substituted\", s)\n\nfor t in tokens:\n    as_markdown(\"Tokenised\", t)\n\nSubstituted\n\nucl home the bartlett centre for advanced spatial analysis about\n\n\n\nSubstituted\n\nabout\n\n\n\nSubstituted\n\nthe centre for advanced spatial analysis casa is an interdisciplinary research institute focusing on the science of cities within the bartlett faculty of the built environment at ucl\n\n\n\nSubstituted\n\nthe centre for advanced spatial analysis casa was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science sensing the urban environment visualisation and computation today casa s research is still pushing boundaries to create better cities for everyone both leading the intellectual agenda and working closely with government and industry partners to make real world impact our teaching reflects this making the most of our cutting edge research tools new forms of data and long standing non academic partnerships to train the next generation of urban scientists with the skills and ideas they ll need to have an impact in industry government academia and the third sector the casa community is closely connected but strongly interdisciplinary we bring together people from around the world with a unique variety of backgrounds including physicists planners geographers economists data scientists architects mathematicians and computer scientists united by our mission to tackle the biggest challenges facing cities and societies around the world we work across multiple scales: from the hyper local environment of the low powered sensor all the way up to satellite remote sensing of whole countries and regions studying at casa brings lifelong value with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science by studying with us you will become part of our active and engaged alumni community with access to job listings networking and social activities as well as continued contact with our outstanding teachers and researchers location the ucl centre for advanced spatial analysis is located at 90 tottenham court road london w1t 4tj\n\n\n\nSubstituted\n\nview map\n\n\n\nSubstituted\n\ncontact address: ucl centre for advanced spatial analysis first floor 90 tottenham court road london w1t 4tj telephone: 44 0 20 3108 3877 email: casa@ucl ac uk\n\n\n\nTokenised\n\n[‘ucl’, ‘home’, ‘the’, ‘bartlett’, ‘centre’, ‘for’, ‘advanced’, ‘spatial’, ‘analysis’, ‘about’]\n\n\n\nTokenised\n\n[‘about’]\n\n\n\nTokenised\n\n[‘the’, ‘centre’, ‘for’, ‘advanced’, ‘spatial’, ‘analysis’, ‘(’, ‘casa’, ‘)’, ‘is’, ‘an’, ‘interdisciplinary’, ‘research’, ‘institute’, ‘focusing’, ‘on’, ‘the’, ‘science’, ‘of’, ‘cities’, ‘within’, ‘the’, ‘bartlett’, ‘faculty’, ‘of’, ‘the’, ‘built’, ‘environment’, ‘at’, ‘ucl’, ‘.’]\n\n\n\nTokenised\n\n[‘the’, ‘centre’, ‘for’, ‘advanced’, ‘spatial’, ‘analysis’, ‘(’, ‘casa’, ‘)’, ‘was’, ‘established’, ‘in’, ‘1995’, ‘to’, ‘lead’, ‘the’, ‘development’, ‘of’, ‘a’, ‘science’, ‘of’, ‘cities’, ‘drawing’, ‘upon’, ‘methods’, ‘and’, ‘ideas’, ‘in’, ‘modelling’, ‘and’, ‘data’, ‘science’, ‘,’, ‘sensing’, ‘the’, ‘urban’, ‘environment’, ‘,’, ‘visualisation’, ‘and’, ‘computation’, ‘.’, ‘today’, ‘,’, ‘casa’, ’’‘, ’s’, ‘research’, ‘is’, ‘still’, ‘pushing’, ‘boundaries’, ‘to’, ‘create’, ‘better’, ‘cities’, ‘for’, ‘everyone’, ‘,’, ‘both’, ‘leading’, ‘the’, ‘intellectual’, ‘agenda’, ‘and’, ‘working’, ‘closely’, ‘with’, ‘government’, ‘and’, ‘industry’, ‘partners’, ‘to’, ‘make’, ‘real-world’, ‘impact’, ‘.’, ‘our’, ‘teaching’, ‘reflects’, ‘this’, ‘,’, ‘making’, ‘the’, ‘most’, ‘of’, ‘our’, ‘cutting-edge’, ‘research’, ‘,’, ‘tools’, ‘,’, ‘new’, ‘forms’, ‘of’, ‘data’, ‘,’, ‘and’, ‘long-standing’, ‘non-academic’, ‘partnerships’, ‘to’, ‘train’, ‘the’, ‘next’, ‘generation’, ‘of’, ‘urban’, ‘scientists’, ‘with’, ‘the’, ‘skills’, ‘and’, ‘ideas’, ‘they’, ’’‘, ’ll’, ‘need’, ‘to’, ‘have’, ‘an’, ‘impact’, ‘in’, ‘industry’, ‘,’, ‘government’, ‘,’, ‘academia’, ‘,’, ‘and’, ‘the’, ‘third’, ‘sector’, ‘.’, ‘the’, ‘casa’, ‘community’, ‘is’, ‘closely’, ‘connected’, ‘,’, ‘but’, ‘strongly’, ‘interdisciplinary’, ‘.’, ‘we’, ‘bring’, ‘together’, ‘people’, ‘from’, ‘around’, ‘the’, ‘world’, ‘with’, ‘a’, ‘unique’, ‘variety’, ‘of’, ‘backgrounds’, ‘–’, ‘including’, ‘physicists’, ‘,’, ‘planners’, ‘,’, ‘geographers’, ‘,’, ‘economists’, ‘,’, ‘data’, ‘scientists’, ‘,’, ‘architects’, ‘,’, ‘mathematicians’, ‘and’, ‘computer’, ‘scientists’, ‘–’, ‘united’, ‘by’, ‘our’, ‘mission’, ‘to’, ‘tackle’, ‘the’, ‘biggest’, ‘challenges’, ‘facing’, ‘cities’, ‘and’, ‘societies’, ‘around’, ‘the’, ‘world’, ‘.’, ‘we’, ‘work’, ‘across’, ‘multiple’, ‘scales’, ‘:’, ‘from’, ‘the’, ‘hyper-local’, ‘environment’, ‘of’, ‘the’, ‘low-powered’, ‘sensor’, ‘all’, ‘the’, ‘way’, ‘up’, ‘to’, ‘satellite’, ‘remote’, ‘sensing’, ‘of’, ‘whole’, ‘countries’, ‘and’, ‘regions’, ‘.’, ‘studying’, ‘at’, ‘casa’, ‘brings’, ‘lifelong’, ‘value’, ‘,’, ‘with’, ‘our’, ‘students’, ‘poised’, ‘to’, ‘take’, ‘on’, ‘leadership’, ‘and’, ‘integration’, ‘roles’, ‘at’, ‘the’, ‘forefront’, ‘of’, ‘urban’, ‘and’, ‘spatial’, ‘data’, ‘science’, ‘.’, ‘by’, ‘studying’, ‘with’, ‘us’, ‘you’, ‘will’, ‘become’, ‘part’, ‘of’, ‘our’, ‘active’, ‘and’, ‘engaged’, ‘alumni’, ‘community’, ‘,’, ‘with’, ‘access’, ‘to’, ‘job’, ‘listings’, ‘,’, ‘networking’, ‘and’, ‘social’, ‘activities’, ‘,’, ‘as’, ‘well’, ‘as’, ‘continued’, ‘contact’, ‘with’, ‘our’, ‘outstanding’, ‘teachers’, ‘and’, ‘researchers’, ‘.’, ‘location’, ‘the’, ‘ucl’, ‘centre’, ‘for’, ‘advanced’, ‘spatial’, ‘analysis’, ‘is’, ‘located’, ‘at’, ‘90’, ‘tottenham’, ‘court’, ‘road’, ‘,’, ‘london’, ‘,’, ‘w1t’, ‘4tj’, ‘.’]\n\n\n\nTokenised\n\n[‘view’, ‘map’]\n\n\n\nTokenised\n\n[‘contact’, ‘address’, ‘:’, ‘ucl’, ‘centre’, ‘for’, ‘advanced’, ‘spatial’, ‘analysis’, ‘first’, ‘floor’, ‘,’, ‘90’, ‘tottenham’, ‘court’, ‘road’, ‘london’, ‘w1t’, ‘4tj’, ‘telephone’, ‘:’, ‘+44’, ‘(’, ‘0’, ‘)’, ‘20’, ‘3108’, ‘3877’, ‘email’, ‘:’, ‘casa’, ‘@’, ‘ucl.ac.uk’]"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#stopword-removal",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#stopword-removal",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Stopword Removal",
    "text": "Stopword Removal\n\n\n\n\n\n\nDifficulty Level: Moderate\n\n\n\n\n\nYou need to remember how list comprehensions work to use the stopword_list.\n\n\n\n\nstopword_list = set(stopwords.words('english'))\nprint(stopword_list)\n\n{\"shan't\", 'under', \"haven't\", \"she's\", 'been', 'over', 'it', 'himself', 'they', 'does', \"wasn't\", \"that'll\", 'their', 'them', \"you're\", 'or', 'should', 'all', 're', 'itself', 'who', 'haven', 'ma', 'once', 'against', 'yourselves', 'shouldn', 'until', 'then', 'shan', 'wouldn', 'nor', 'myself', 'weren', \"you've\", 'won', \"hasn't\", 'm', 'most', \"mightn't\", 'your', 'll', 'that', 'into', 'hers', 'below', \"aren't\", 'own', \"it's\", 'off', 'needn', 'hadn', 'which', \"you'd\", 'to', 'o', \"mustn't\", 'this', 'he', 'whom', 'do', 'why', \"didn't\", 'themselves', 'my', 'she', 'such', \"you'll\", 'had', \"wouldn't\", 'are', 'during', 'so', 'did', 'him', 'if', \"needn't\", 'here', 'me', 'how', 'didn', 'where', \"don't\", 'but', 'again', 'were', 'have', 'too', 't', 'couldn', 'his', 'after', \"isn't\", 'any', 's', 'when', 'was', 'doing', \"couldn't\", \"doesn't\", 'isn', 'mightn', 'for', 'these', 'between', 'there', 'more', 'no', 'now', 'down', 'in', 'we', 'the', 'out', 'will', \"won't\", 'her', 'just', 'herself', 'don', 'ain', 'than', 'our', 'doesn', 'what', 'having', 'am', 'aren', 'not', 'you', 'from', 'be', \"hadn't\", 'above', \"shouldn't\", \"weren't\", 'is', 'of', 'other', 'd', 'wasn', 'hasn', 'an', 'about', 'each', 'being', 'with', 'because', 'its', 'some', 'mustn', 'theirs', 'a', 'at', 'has', 'before', 'by', 'ours', 'can', 've', 'ourselves', 'y', \"should've\", 'and', 'both', 'i', 'on', 'same', 'very', 'through', 'yourself', 'further', 'those', 'only', 'up', 'as', 'few', 'while', 'yours'}\n\n\n\nQuestionAnswer\n\n\nstopped = []\nfor p in tokens[2:4]: # &lt;-- why do I just take these items from the list?\n    stopped.append([x for x in p if x not in ??? and len(x) &gt; 1])\n\nfor s in stopped:\n    as_markdown(\"Line\", s)\n\n\n\nstopped = []\nfor p in tokens[2:4]: # &lt;-- why do I just take these items from the list?\n    stopped.append([x for x in p if x not in stopword_list and len(x) &gt; 1])\n\nfor s in stopped:\n    as_markdown(\"Line\", s)\n\nLine\n\n[‘centre’, ‘advanced’, ‘spatial’, ‘analysis’, ‘casa’, ‘interdisciplinary’, ‘research’, ‘institute’, ‘focusing’, ‘science’, ‘cities’, ‘within’, ‘bartlett’, ‘faculty’, ‘built’, ‘environment’, ‘ucl’]\n\n\n\nLine\n\n[‘centre’, ‘advanced’, ‘spatial’, ‘analysis’, ‘casa’, ‘established’, ‘1995’, ‘lead’, ‘development’, ‘science’, ‘cities’, ‘drawing’, ‘upon’, ‘methods’, ‘ideas’, ‘modelling’, ‘data’, ‘science’, ‘sensing’, ‘urban’, ‘environment’, ‘visualisation’, ‘computation’, ‘today’, ‘casa’, ‘research’, ‘still’, ‘pushing’, ‘boundaries’, ‘create’, ‘better’, ‘cities’, ‘everyone’, ‘leading’, ‘intellectual’, ‘agenda’, ‘working’, ‘closely’, ‘government’, ‘industry’, ‘partners’, ‘make’, ‘real-world’, ‘impact’, ‘teaching’, ‘reflects’, ‘making’, ‘cutting-edge’, ‘research’, ‘tools’, ‘new’, ‘forms’, ‘data’, ‘long-standing’, ‘non-academic’, ‘partnerships’, ‘train’, ‘next’, ‘generation’, ‘urban’, ‘scientists’, ‘skills’, ‘ideas’, ‘need’, ‘impact’, ‘industry’, ‘government’, ‘academia’, ‘third’, ‘sector’, ‘casa’, ‘community’, ‘closely’, ‘connected’, ‘strongly’, ‘interdisciplinary’, ‘bring’, ‘together’, ‘people’, ‘around’, ‘world’, ‘unique’, ‘variety’, ‘backgrounds’, ‘including’, ‘physicists’, ‘planners’, ‘geographers’, ‘economists’, ‘data’, ‘scientists’, ‘architects’, ‘mathematicians’, ‘computer’, ‘scientists’, ‘united’, ‘mission’, ‘tackle’, ‘biggest’, ‘challenges’, ‘facing’, ‘cities’, ‘societies’, ‘around’, ‘world’, ‘work’, ‘across’, ‘multiple’, ‘scales’, ‘hyper-local’, ‘environment’, ‘low-powered’, ‘sensor’, ‘way’, ‘satellite’, ‘remote’, ‘sensing’, ‘whole’, ‘countries’, ‘regions’, ‘studying’, ‘casa’, ‘brings’, ‘lifelong’, ‘value’, ‘students’, ‘poised’, ‘take’, ‘leadership’, ‘integration’, ‘roles’, ‘forefront’, ‘urban’, ‘spatial’, ‘data’, ‘science’, ‘studying’, ‘us’, ‘become’, ‘part’, ‘active’, ‘engaged’, ‘alumni’, ‘community’, ‘access’, ‘job’, ‘listings’, ‘networking’, ‘social’, ‘activities’, ‘well’, ‘continued’, ‘contact’, ‘outstanding’, ‘teachers’, ‘researchers’, ‘location’, ‘ucl’, ‘centre’, ‘advanced’, ‘spatial’, ‘analysis’, ‘located’, ‘90’, ‘tottenham’, ‘court’, ‘road’, ‘london’, ‘w1t’, ‘4tj’]"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#lemmatisation-vs-stemming",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#lemmatisation-vs-stemming",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Lemmatisation vs Stemming",
    "text": "Lemmatisation vs Stemming\n\n\n\n\n\n\nDifficulty level: Low.\n\n\n\n\n\n\n\n\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer \n\n\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('monkeys'))\nprint(lemmatizer.lemmatize('cities'))\nprint(lemmatizer.lemmatize('complexity'))\nprint(lemmatizer.lemmatize('Reades'))\n\nmonkey\ncity\ncomplexity\nReades\n\n\n\nstemmer = PorterStemmer()\nprint(stemmer.stem('monkeys'))\nprint(stemmer.stem('cities'))\nprint(stemmer.stem('complexity'))\nprint(stemmer.stem('Reades'))\n\nmonkey\nciti\ncomplex\nread\n\n\n\nstemmer = SnowballStemmer(language='english')\nprint(stemmer.stem('monkeys'))\nprint(stemmer.stem('cities'))\nprint(stemmer.stem('complexity'))\nprint(stemmer.stem('Reades'))\n\nmonkey\nciti\ncomplex\nread\n\n\n\nlemmatizer = WordNetLemmatizer()\nlemmas  = []\nstemmed = []\n\n# This would be better if we passed in a PoS (Part of Speech) tag as well,\n# but processing text for parts of speech is *expensive* and for the purposes\n# of this tutorial, not necessary.\nfor s in stopped:\n    lemmas.append([lemmatizer.lemmatize(x) for x in s])\n\nfor s in stopped:\n    stemmed.append([stemmer.stem(x) for x in s])\n\nfor l in lemmas:\n    as_markdown('Lemmatised',l)\n\nfor s in stemmed:\n    as_markdown('Stemmed',s)\n\nLemmatised\n\n[‘centre’, ‘advanced’, ‘spatial’, ‘analysis’, ‘casa’, ‘interdisciplinary’, ‘research’, ‘institute’, ‘focusing’, ‘science’, ‘city’, ‘within’, ‘bartlett’, ‘faculty’, ‘built’, ‘environment’, ‘ucl’]\n\n\n\nLemmatised\n\n[‘centre’, ‘advanced’, ‘spatial’, ‘analysis’, ‘casa’, ‘established’, ‘1995’, ‘lead’, ‘development’, ‘science’, ‘city’, ‘drawing’, ‘upon’, ‘method’, ‘idea’, ‘modelling’, ‘data’, ‘science’, ‘sensing’, ‘urban’, ‘environment’, ‘visualisation’, ‘computation’, ‘today’, ‘casa’, ‘research’, ‘still’, ‘pushing’, ‘boundary’, ‘create’, ‘better’, ‘city’, ‘everyone’, ‘leading’, ‘intellectual’, ‘agenda’, ‘working’, ‘closely’, ‘government’, ‘industry’, ‘partner’, ‘make’, ‘real-world’, ‘impact’, ‘teaching’, ‘reflects’, ‘making’, ‘cutting-edge’, ‘research’, ‘tool’, ‘new’, ‘form’, ‘data’, ‘long-standing’, ‘non-academic’, ‘partnership’, ‘train’, ‘next’, ‘generation’, ‘urban’, ‘scientist’, ‘skill’, ‘idea’, ‘need’, ‘impact’, ‘industry’, ‘government’, ‘academia’, ‘third’, ‘sector’, ‘casa’, ‘community’, ‘closely’, ‘connected’, ‘strongly’, ‘interdisciplinary’, ‘bring’, ‘together’, ‘people’, ‘around’, ‘world’, ‘unique’, ‘variety’, ‘background’, ‘including’, ‘physicist’, ‘planner’, ‘geographer’, ‘economist’, ‘data’, ‘scientist’, ‘architect’, ‘mathematician’, ‘computer’, ‘scientist’, ‘united’, ‘mission’, ‘tackle’, ‘biggest’, ‘challenge’, ‘facing’, ‘city’, ‘society’, ‘around’, ‘world’, ‘work’, ‘across’, ‘multiple’, ‘scale’, ‘hyper-local’, ‘environment’, ‘low-powered’, ‘sensor’, ‘way’, ‘satellite’, ‘remote’, ‘sensing’, ‘whole’, ‘country’, ‘region’, ‘studying’, ‘casa’, ‘brings’, ‘lifelong’, ‘value’, ‘student’, ‘poised’, ‘take’, ‘leadership’, ‘integration’, ‘role’, ‘forefront’, ‘urban’, ‘spatial’, ‘data’, ‘science’, ‘studying’, ‘u’, ‘become’, ‘part’, ‘active’, ‘engaged’, ‘alumnus’, ‘community’, ‘access’, ‘job’, ‘listing’, ‘networking’, ‘social’, ‘activity’, ‘well’, ‘continued’, ‘contact’, ‘outstanding’, ‘teacher’, ‘researcher’, ‘location’, ‘ucl’, ‘centre’, ‘advanced’, ‘spatial’, ‘analysis’, ‘located’, ‘90’, ‘tottenham’, ‘court’, ‘road’, ‘london’, ‘w1t’, ‘4tj’]\n\n\n\nStemmed\n\n[‘centr’, ‘advanc’, ‘spatial’, ‘analysi’, ‘casa’, ‘interdisciplinari’, ‘research’, ‘institut’, ‘focus’, ‘scienc’, ‘citi’, ‘within’, ‘bartlett’, ‘faculti’, ‘built’, ‘environ’, ‘ucl’]\n\n\n\nStemmed\n\n[‘centr’, ‘advanc’, ‘spatial’, ‘analysi’, ‘casa’, ‘establish’, ‘1995’, ‘lead’, ‘develop’, ‘scienc’, ‘citi’, ‘draw’, ‘upon’, ‘method’, ‘idea’, ‘model’, ‘data’, ‘scienc’, ‘sens’, ‘urban’, ‘environ’, ‘visualis’, ‘comput’, ‘today’, ‘casa’, ‘research’, ‘still’, ‘push’, ‘boundari’, ‘creat’, ‘better’, ‘citi’, ‘everyon’, ‘lead’, ‘intellectu’, ‘agenda’, ‘work’, ‘close’, ‘govern’, ‘industri’, ‘partner’, ‘make’, ‘real-world’, ‘impact’, ‘teach’, ‘reflect’, ‘make’, ‘cutting-edg’, ‘research’, ‘tool’, ‘new’, ‘form’, ‘data’, ‘long-stand’, ‘non-academ’, ‘partnership’, ‘train’, ‘next’, ‘generat’, ‘urban’, ‘scientist’, ‘skill’, ‘idea’, ‘need’, ‘impact’, ‘industri’, ‘govern’, ‘academia’, ‘third’, ‘sector’, ‘casa’, ‘communiti’, ‘close’, ‘connect’, ‘strong’, ‘interdisciplinari’, ‘bring’, ‘togeth’, ‘peopl’, ‘around’, ‘world’, ‘uniqu’, ‘varieti’, ‘background’, ‘includ’, ‘physicist’, ‘planner’, ‘geograph’, ‘economist’, ‘data’, ‘scientist’, ‘architect’, ‘mathematician’, ‘comput’, ‘scientist’, ‘unit’, ‘mission’, ‘tackl’, ‘biggest’, ‘challeng’, ‘face’, ‘citi’, ‘societi’, ‘around’, ‘world’, ‘work’, ‘across’, ‘multipl’, ‘scale’, ‘hyper-loc’, ‘environ’, ‘low-pow’, ‘sensor’, ‘way’, ‘satellit’, ‘remot’, ‘sens’, ‘whole’, ‘countri’, ‘region’, ‘studi’, ‘casa’, ‘bring’, ‘lifelong’, ‘valu’, ‘student’, ‘pois’, ‘take’, ‘leadership’, ‘integr’, ‘role’, ‘forefront’, ‘urban’, ‘spatial’, ‘data’, ‘scienc’, ‘studi’, ‘us’, ‘becom’, ‘part’, ‘activ’, ‘engag’, ‘alumni’, ‘communiti’, ‘access’, ‘job’, ‘list’, ‘network’, ‘social’, ‘activ’, ‘well’, ‘continu’, ‘contact’, ‘outstand’, ‘teacher’, ‘research’, ‘locat’, ‘ucl’, ‘centr’, ‘advanc’, ‘spatial’, ‘analysi’, ‘locat’, ‘90’, ‘tottenham’, ‘court’, ‘road’, ‘london’, ‘w1t’, ‘4tj’]\n\n\n\n\n# What are we doing here?\nfor ix, p in enumerate(stopped):\n    stopped_set = set(stopped[ix])\n    lemma_set   = set(lemmas[ix])\n    print(sorted(stopped_set.symmetric_difference(lemma_set)))\n\n['cities', 'city']\n['activities', 'activity', 'alumni', 'alumnus', 'architect', 'architects', 'background', 'backgrounds', 'boundaries', 'boundary', 'challenge', 'challenges', 'cities', 'city', 'countries', 'country', 'economist', 'economists', 'form', 'forms', 'geographer', 'geographers', 'idea', 'ideas', 'listing', 'listings', 'mathematician', 'mathematicians', 'method', 'methods', 'partner', 'partners', 'partnership', 'partnerships', 'physicist', 'physicists', 'planner', 'planners', 'region', 'regions', 'researcher', 'researchers', 'role', 'roles', 'scale', 'scales', 'scientist', 'scientists', 'skill', 'skills', 'societies', 'society', 'student', 'students', 'teacher', 'teachers', 'tool', 'tools', 'u', 'us']"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#downloading-the-custom-module",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#downloading-the-custom-module",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Downloading the Custom Module",
    "text": "Downloading the Custom Module\n\n\n\n\n\n\nDifficulty level: Low.\n\n\n\n\n\n\n\n\n\nThis custom module is not perfect, but it gets the job done… mostly and has some additional features that you could play around with for a final project (e.g. detect_entities and detect_acronyms).\n\nimport urllib.request\nhost  = 'https://orca.casa.ucl.ac.uk'\nturl  = f'{host}/~jreades/__textual__.py'\ntdirs = os.path.join('textual')\ntpath = os.path.join(tdirs,'__init__.py')\n\nif not os.path.exists(tpath):\n    os.makedirs(tdirs, exist_ok=True)\n    urllib.request.urlretrieve(turl, tpath)"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#importing-the-custom-module",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#importing-the-custom-module",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Importing the Custom Module",
    "text": "Importing the Custom Module\n\n\n\n\n\n\nDifficulty Level: Low.\n\n\n\n\n\nBut only because you didn’t have to write the module! However, the questions could be hard…\n\n\n\nIn a Jupyter notebook, this code allows us to edit and reload the library dynamically:\n%load_ext autoreload\n%autoreload 2\nNow let’s import it.\n\nfrom textual import *\n\nAll NLTK libraries installed...\n\n\n\nas_markdown('Input', cleaned)\n\nInput\n\n[‘UCL Home The Bartlett Centre for Advanced Spatial Analysis About’, ‘About’, ‘The Centre for Advanced Spatial Analysis (CASA) is an interdisciplinary research institute focusing on the science of cities within The Bartlett Faculty of the Built Environment at UCL.’, ‘The Centre for Advanced Spatial Analysis (CASA) was established in 1995 to lead the development of a science of cities drawing upon methods and ideas in modelling and data science, sensing the urban environment, visualisation and computation. Today, CASA’s research is still pushing boundaries to create better cities for everyone, both leading the intellectual agenda and working closely with government and industry partners to make real-world impact. Our teaching reflects this, making the most of our cutting-edge research, tools, new forms of data, and long-standing non-academic partnerships to train the next generation of urban scientists with the skills and ideas they’ll need to have an impact in industry, government, academia, and the third sector. The CASA community is closely connected, but strongly interdisciplinary. We bring together people from around the world with a unique variety of backgrounds – including physicists, planners, geographers, economists, data scientists, architects, mathematicians and computer scientists – united by our mission to tackle the biggest challenges facing cities and societies around the world. We work across multiple scales: from the hyper-local environment of the low-powered sensor all the way up to satellite remote sensing of whole countries and regions. Studying at CASA brings lifelong value, with our students poised to take on leadership and integration roles at the forefront of urban and spatial data science. By studying with us you will become part of our active and engaged alumni community, with access to job listings, networking and social activities, as well as continued contact with our outstanding teachers and researchers. Location The UCL Centre for Advanced Spatial Analysis is located at 90 Tottenham Court Road, London, W1T 4TJ.’, ‘View Map’, ‘Contact Address: UCL Centre for Advanced Spatial Analysis First Floor, 90 Tottenham Court Road London W1T 4TJ Telephone: +44 (0)20 3108 3877 Email: casa@ucl.ac.uk’]\n\n\n\n\nas_markdown('Normalised', [normalise_document(x, remove_digits=True) for x in cleaned])\n\nNormalised\n\n[‘home bartlett centre advanced spatial analysis’, ’‘, ’centre advanced spatial analysis . casa . interdisciplinary research institute focus science city within bartlett faculty built environment .’, ‘centre advanced spatial analysis . casa . establish lead development science city draw upon method idea modelling data science sense urban environment visualisation computation . today research still push boundary create good city everyone lead intellectual agenda work closely government industry partner make realworld impact . teaching reflect make cuttingedge research tool form data longstanding nonacademic partnership train next generation urban scientist skill idea need impact industry government academia third sector . casa community closely connect strongly interdisciplinary . bring together people around world unique variety background include physicist planner geographer economist data scientist architect mathematician computer scientist unite mission tackle challenge face city society around world . work across multiple scale hyperlocal environment lowpowered sensor satellite remote sensing whole country region . studying casa bring lifelong value student poise take leadership integration role forefront urban spatial data science . study become part active engage alumnus community access listing network social activity well continue contact outstanding teacher researcher . location centre advanced spatial analysis locate tottenham court road london .’, ‘view’, ‘contact address centre advanced spatial analysis first floor tottenham court road london telephone . email casa ucl.ac.uk’]\n\n\n\n\nhelp(normalise_document)\n\nHelp on function normalise_document in module textual:\n\nnormalise_document(doc: str, html_stripping=True, contraction_expansion=True, accented_char_removal=True, text_lower_case=True, text_lemmatization=True, special_char_removal=False, punctuation_removal=True, keep_sentences=True, stopword_removal=True, remove_digits=False, infer_numbers=True, shortest_word=3) -&gt; str\n    Apply all of the functions above to a document using their\n    default values so as to demonstrate the NLP process.\n\n    doc: a document to clean.\n\n\n\n\nQuestions\nLet’s assume that you want to analyse web page content…\n\nBased on the above output, what stopwords do you think are missing?\nBased on the above output, what should be removed but isn’t?\nBased on the above output, how do you think a computer can work with this text?\n\n\n\n\n\n\n\nStop!\n\n\n\nBeyond this point, we are moving into Natural Language Processing. If you are already struggling with regular expressions, I would recommend stopping here. You can come back to revisit the NLP components and creation of word clouds later."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#process-the-selected-listings",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#process-the-selected-listings",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Process the Selected Listings",
    "text": "Process the Selected Listings\n\n\n\n\n\n\nDifficulty level: Low, but you’ll need to be patient!\n\n\n\n\n\n\n\n\n\nNotice the use of %%time here – this will tell you how long each block of code takes to complete. It’s a really useful technique for reminding yourself and others of how long something might take to run. I find that with NLP this is particularly important since you have to do a lot of processing on each document in order to normalise it.\n\n\n\n\n\n\nTip\n\n\n\nNotice how we can change the default parameters for normalise_document even when using apply, but that the syntax is different. So whereas we’d use normalise_document(doc, remove_digits=True) if calling the function directly, here it’s .apply(normalise_document, remove_digits=True)!\n\n\n\nQuestionAnswer\n\n\n%%time \n# I get about 1 minute on a M2 Mac\nlux['description_norm'] = lux.???.apply(???, remove_digits=True)\n%%time \n# I get about 1 minute on a M2 Mac\naff['description_norm'] = aff.???.apply(???, remove_digits=True)\n%%time  \n# I get about 2 seconds on a M2 Mac\nbluesp['description_norm'] = bluesp.???.apply(???, remove_digits=True)\n\n\n\n%%time\n# I get about 1 minute on a M2 Mac\nlux['description_norm'] = lux.description.apply(normalise_document, remove_digits=True)\n\n/Users/jreades/Documents/git/fsds/practicals/textual/__init__.py:606: MarkupResemblesLocatorWarning:\n\nThe input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n\n\n\nCPU times: user 43.8 s, sys: 1.08 s, total: 44.9 s\nWall time: 44.9 s\n\n\n\n%%time\n# I get about 1 minute on a M2 Mac\naff['description_norm'] = aff.description.apply(normalise_document, remove_digits=True)\n\n/Users/jreades/Documents/git/fsds/practicals/textual/__init__.py:606: MarkupResemblesLocatorWarning:\n\nThe input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n\n\n\nCPU times: user 37.4 s, sys: 926 ms, total: 38.4 s\nWall time: 38.4 s\n\n\n\n%%time\n# I get about 1 seconds on a M2 Mac\nbluesp['description_norm'] = bluesp.description.apply(normalise_document, remove_digits=True)\n\nCPU times: user 1.66 s, sys: 32.3 ms, total: 1.69 s\nWall time: 1.69 s"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#select-and-tokenise",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#select-and-tokenise",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Select and Tokenise",
    "text": "Select and Tokenise\n\n\n\n\n\n\nDifficulty level: Low, except for the double list-comprehension.\n\n\n\n\n\n\n\n\n\n\nSelect and Extract Corpus\nSee useful tutorial here. Although we shouldn’t have any empty descriptions, by the time we’ve finished normalising the textual data we may have created some empty values and we need to ensure that we don’t accidentally pass a NaN to the vectorisers and frequency distribution functions.\n\nsrcdf = bluesp \n\n\n\n\n\n\n\nCoding Tip\n\n\n\nNotice how you only need to change the value of the variable here to try any of the different selections we did above? This is a simple kind of parameterisation somewhere between a function and hard-coding everything.\n\n\n\ncorpus = srcdf.description_norm.fillna(' ').values\nprint(corpus[0:3])\n\n['house garden close thames river . walk  private road river nearby .  district line underground . walk . direct access central london  near gardens . kids playground walk distance along thames path . space residential neighborhood  english corporate expat family . house culdesac private road river thames . river foot away . walking distance subway . central london underground   district line . gardens stop walk zone . addition  overground stratford also stop gardens underground station . gardens stop walk .  overland railway station bridge . walk . take waterloo railway station minute . bicycle follow towpath hammersmith bridge continue putney bridge . lastly  several stree'\n 'space appartment upper floor modernised secure building near canary wharf  fantastic view river thames london . offer home home experience  light breafast fresh juice coffee . double room plenty storage  fresh towels linen . shared bathroom step shower  hairdryer essentials . wifi service . appartment  minutes walk nearest . south quay . london underground . canary wharf . network central london right next stop . close asda tesco supermarkets chains  plenty local shop restaurants . walking distance city farm  greenwich foot tunnel fabulous walkway along river front . also hour medical center close appartment  questions'\n 'newly renovate  totally equipped furnished modern apartment heart london . easily accessible kind transport . walk waterloo station  london  . space newly renovate  totally equipped furnished modern apartment heart london . easily accessible kind transport . walk waterloo station  london  . apartment flat   broadband  wash machine  microwave  iron  nespresso coffee machine  . large live room easily divide space . modern comfortable bathroom whirlpool japanese . bedroom modern design closet . shared terrace view picturesque vivid lower marsh street   london . lower marsh london best hidden jewel  special alive place bridget jones  bourne identit']\n\n\n\n\nTokenise\nThere are different forms of tokenisation and different algorithms will expect differing inputs. Here are two:\n\nsentences = [nltk.sent_tokenize(text) for text in corpus]\nwords     = [[nltk.tokenize.word_tokenize(sentence) \n                  for sentence in nltk.sent_tokenize(text)] \n                  for text in corpus]\n\nNotice how this has turned every sentence into an array and each document into an array of arrays:\n\nprint(f\"Sentences 0: {sentences[0]}\")\nprint()\nprint(f\"Words 0: {words[0]}\")\n\nSentences 0: ['house garden close thames river .', 'walk  private road river nearby .', 'district line underground .', 'walk .', 'direct access central london  near gardens .', 'kids playground walk distance along thames path .', 'space residential neighborhood  english corporate expat family .', 'house culdesac private road river thames .', 'river foot away .', 'walking distance subway .', 'central london underground   district line .', 'gardens stop walk zone .', 'addition  overground stratford also stop gardens underground station .', 'gardens stop walk .', 'overland railway station bridge .', 'walk .', 'take waterloo railway station minute .', 'bicycle follow towpath hammersmith bridge continue putney bridge .', 'lastly  several stree']\n\nWords 0: [['house', 'garden', 'close', 'thames', 'river', '.'], ['walk', 'private', 'road', 'river', 'nearby', '.'], ['district', 'line', 'underground', '.'], ['walk', '.'], ['direct', 'access', 'central', 'london', 'near', 'gardens', '.'], ['kids', 'playground', 'walk', 'distance', 'along', 'thames', 'path', '.'], ['space', 'residential', 'neighborhood', 'english', 'corporate', 'expat', 'family', '.'], ['house', 'culdesac', 'private', 'road', 'river', 'thames', '.'], ['river', 'foot', 'away', '.'], ['walking', 'distance', 'subway', '.'], ['central', 'london', 'underground', 'district', 'line', '.'], ['gardens', 'stop', 'walk', 'zone', '.'], ['addition', 'overground', 'stratford', 'also', 'stop', 'gardens', 'underground', 'station', '.'], ['gardens', 'stop', 'walk', '.'], ['overland', 'railway', 'station', 'bridge', '.'], ['walk', '.'], ['take', 'waterloo', 'railway', 'station', 'minute', '.'], ['bicycle', 'follow', 'towpath', 'hammersmith', 'bridge', 'continue', 'putney', 'bridge', '.'], ['lastly', 'several', 'stree']]"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#frequencies-and-ngrams",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#frequencies-and-ngrams",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Frequencies and Ngrams",
    "text": "Frequencies and Ngrams\n\n\n\n\n\n\nDifficulty level: Moderate.\n\n\n\n\n\n\n\n\n\nOne new thing you’ll see here is the ngram: ngrams are ‘simply’ pairs, or triplets, or quadruplets of words. You may come across the terms unigram (ngram(1,1)), bigram (ngram(2,2)), trigram (ngram(3,3))… typically, you will rarely find anything beyond trigrams, and these present real issues for text2vec algorithms because the embedding for geographical, information, and systems is not the same as for geographical information systetms.\n\nBuild Frequency Distribution\nBuild counts for ngram range 1..3:\n\nfcounts = dict()\n\n# Here we replace all full-stops... can you think why we might do this?\ndata = nltk.tokenize.word_tokenize(' '.join([text.replace('.','') for text in corpus]))\n\nfor size in 1, 2, 3:\n    fdist = FreqDist(ngrams(data, size))\n    print(fdist)\n    # If you only need one note this: https://stackoverflow.com/a/52193485/4041902\n    fcounts[size] = pd.DataFrame.from_dict({f'Ngram Size {size}': fdist})\n\n&lt;FreqDist with 2692 samples and 26245 outcomes&gt;\n&lt;FreqDist with 14173 samples and 26244 outcomes&gt;\n&lt;FreqDist with 19540 samples and 26243 outcomes&gt;\n\n\n\n\nOutput Top-n Ngrams\nAnd output the most common ones for each ngram range:\n\nfor dfs in fcounts.values():\n    print(dfs.sort_values(by=dfs.columns.values[0], ascending=False).head(10))\n    print()\n\n           Ngram Size 1\nwalk                594\nroom                472\nlondon              469\nriver               418\nbedroom             412\nspace               391\nminute              382\napartment           373\nstation             307\nflat                303\n\n                 Ngram Size 2\nminute  walk              252\nriver   thames            138\n        view              138\nliving  room              134\ncanary  wharf             112\nguest   access            110\ncentral london            107\nfully   equip              76\nequip   kitchen            71\nthames  river              65\n\n                        Ngram Size 3\nfully  equip  kitchen             68\nwalk   river  thames              37\nclose  river  thames              35\nwalk   thames river               27\nminute walk   river               23\nopen   plan   kitchen             20\nthames river  view                20\nsofa   living room                19\nwithin walk   distance            19\nopen   plan   live                18\n\n\n\n\n\nQuestions\n\nCan you think why we don’t care about punctuation for frequency distributions and n-grams?\nDo you understand what n-grams are?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#count-vectoriser",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#count-vectoriser",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Count Vectoriser",
    "text": "Count Vectoriser\n\n\n\n\n\n\nDifficulty level: Low, but the output needs some thought!\n\n\n\n\n\n\n\n\n\nThis is a big foray into sklearn (sci-kit learn) which is the main machine learning and clustering module for Python. For processing text we use vectorisers to convert terms to a vector representation. We’re doing this on the smallest of the derived data sets because these processes can take a while to run and generate huge matrices (remember: one row and one column for each term!).\n\nFit the Vectoriser\n\ncvectorizer = CountVectorizer(ngram_range=(1,3))\ncvectorizer.fit(corpus)\n\nCountVectorizer(ngram_range=(1, 3))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  CountVectorizer?Documentation for CountVectorizeriFittedCountVectorizer(ngram_range=(1, 3)) \n\n\n\n\nBrief Demonstration\nFind the number associated with a word in the vocabulary and how many times it occurs in the original corpus:\n\nterm = 'stratford'\npd.options.display.max_colwidth=750\n# Find the vocabulary mapping for the term\nprint(f\"Vocabulary mapping for {term} is {cvectorizer.vocabulary_[term]}\")\n# How many times is it in the data\nprint(f\"Found {srcdf.description_norm.str.contains(term).sum():,} rows containing {term}\")\n# Print the descriptions containing the term\nfor x in srcdf[srcdf.description_norm.str.contains(term)].description_norm:\n    as_markdown('Stratford',x)\n\nVocabulary mapping for stratford is 29373\nFound 10 rows containing stratford\n\n\nStratford\n\nhouse garden close thames river . walk private road river nearby . district line underground . walk . direct access central london near gardens . kids playground walk distance along thames path . space residential neighborhood english corporate expat family . house culdesac private road river thames . river foot away . walking distance subway . central london underground district line . gardens stop walk zone . addition overground stratford also stop gardens underground station . gardens stop walk . overland railway station bridge . walk . take waterloo railway station minute . bicycle follow towpath hammersmith bridge continue putney bridge . lastly several stree\n\n\n\nStratford\n\nplease read things note comfortable clean bright brand flat east london minute central london tube quite central great transport link major london attraction minute walk river park undergroundtubedlr station supermarket docklands stratford olympic stadium westfield shopping centre . enjoy brick lane indian restaurant spitalfields market colombian flower market historical whitechapel . space please read things note nice clean fresh bright airy . space perfect professional single person couple . make feel like home choice anything like wake relaxing cooking . guest access please read things note entire flat . please treat home away . please treat .\n\n\n\nStratford\n\ncomfortable fairly flat east london travel zone minute central london quite central great transport link major london attraction minute walk river park undergroundtube station minute supermarket minute docklands stratford olympic stadium westfield shopping centre . enjoy brick lane indian restaurant spitalfields market colombian flower market historical whitechapel . space spacious comfortable tidy clean airy relaxing . live flat sleep open plan lounge balcony . guest access bathroom share . welcome microwave ready meal toaster make drink till fridge store food . please make sure clean clear immediately . dining table . stay present morning evening weekend . also\n\n\n\nStratford\n\nentire appartment double bedroom large living area.the apartment feature kitchen come free wifi flat screen tv.to make exicting luxurious even free free sauna well . space stunning apartment london docklands bank thames river close thames barrier park canary wharf . apartment floor spacious living room balcony . nearest station pontoon dock walkable distance direct train stratford . mins . bank . excel centre . walk . arena canary wharf mins train mins central london . world heritage sitethames barrier thames barrier park walk appartment . london city airport train station away . fully kitchen bathroom broadband internet underground secure parking onsite . attract\n\n\n\nStratford\n\nluxurious bedroom apartment zone love hidden secret part town minute away everywhere river view slow pace main artery town right doorstep well hidden beauty park waterway . easy . walk tube route center town well stratford olympic park canary wharf much much right doorstep space welcome home place love bedroom hold personal belonging bedroom give guest idea size . bedroom large double accommodate comfortably . sofa chair accommodate guest extra extra charge . welcome guest personally wish know . therefore important check time convenient . midnight arrival . time ehich discuss good\n\n\n\nStratford\n\nplace close mile tube station brick lane shoreditch queen mary university london stratford westfield minute tube central london . love place newly renovate flat amazing canal view guest bedroom clean friendly environment . place good couple solo adventurer business traveller .\n\n\n\nStratford\n\nlocate high street give amazing water view stadium sight amazing architectural structure walk pudding mill lane walk abba walk stratford westfield walk stratfordstratford international station mins walk mins train ride central london\n\n\n\nStratford\n\nmodern spacious bedroom suite apartment close river thames wimbledon . situate wandsworth district london england lawn tennis club centre court .km clapham junction . stratford bridge chelsea . city view free wifi throughout property . apartment feature bedroom kitchen fridge oven wash machine flat screen seating area bathroom shower . eventim .km away .\n\n\n\nStratford\n\nperfect group trip . modern spacious suite apartment close river thames wimbledon . situated wandsworth district london england lawn tennis club centre court .km clapham junction . stratford bridge chelsea . city view free wifi throughout property . apartment feature bedroom kitchen wfridge oven wash machine flat screen seating area bathroom wshower . eventim .km away .\n\n\n\nStratford\n\nflat locate zone east london near canary wharf . nice quiet residential area canal . flat amazing canal view balcony . enjoy morning coffee swan goose everyday . huge park opposite flat picnic . canary wharf shop mall . mins bank stratford westfield . mins central oxford circus tube . locate convenient transportation link .\n\n\n\n\n\nTransform the Corpus\nYou can only tranform the entire corpus after the vectoriser has been fitted. There is an option to fit_transform in one go, but I wanted to demonstrate a few things here and some vectorisers are don’t support the one-shot fit-and-transform approach. Note the type of the transformed corpus:\n\ncvtcorpus = cvectorizer.transform(corpus)\ncvtcorpus # cvtcorpus for count-vectorised transformed corpus\n\n&lt;408x35278 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 71420 stored elements in Compressed Sparse Row format&gt;\n\n\n\n\nSingle Document\nHere is the first document from the corpus:\n\ndoc_df = pd.DataFrame(cvtcorpus[0].T.todense(), \n                      index=cvectorizer.get_feature_names_out(), columns=[\"Counts\"]\n                     ).sort_values('Counts', ascending=False)\ndoc_df.head(10)\n\n\n\n\n\n\n\n\nCounts\n\n\n\n\nwalk\n6\n\n\ngardens\n4\n\n\nriver\n4\n\n\nbridge\n3\n\n\nstop\n3\n\n\nthames\n3\n\n\nstation\n3\n\n\nunderground\n3\n\n\nrailway\n2\n\n\ncentral london\n2\n\n\n\n\n\n\n\n\n\nTransformed Corpus\n\ncvdf = pd.DataFrame(data=cvtcorpus.toarray(),\n                        columns=cvectorizer.get_feature_names_out())\nprint(f\"Raw count vectorised data frame has {cvdf.shape[0]:,} rows and {cvdf.shape[1]:,} columns.\")\ncvdf.iloc[0:5,0:10]\n\nRaw count vectorised data frame has 408 rows and 35,278 columns.\n\n\n\n\n\n\n\n\n\naaathe\naaathe apartment\naaathe apartment quiet\naand\naand comfy\naand comfy sofa\nabba\nabba arena\nabba arena entertainmentconcerts\nabba walk\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFilter Low-Frequency Words\nThese are likely to be artefacts of text-cleaning or human input error. As well, if we’re trying to look across an entire corpus then we might not want to retain words that only appear in a couple of documents.\nLet’s start by getting the column sums:\n\nsums = cvdf.sum(axis=0)\nprint(f\"There are {len(sums):,} terms in the data set.\")\nsums.head()\n\nThere are 35,278 terms in the data set.\n\n\naaathe                    1\naaathe apartment          1\naaathe apartment quiet    1\naand                      1\naand comfy                1\ndtype: int64\n\n\nRemove columns (i.e. terms) appearing in less than 1% of documents. You can do this by thinking about what the shape of the data frame means (rows and/or columns) and how you’d get 1% of that!\n\nQuestionAnswer\n\n\nfilter_terms = sums &gt;= cvdf.shape[0] * ???\n\n\n\nfilter_terms = sums &gt;= cvdf.shape[0] * 0.01\n\n\n\n\nNow see how we can use this to strip out the columns corresponding to low-frequency terms:\n\nfcvdf = cvdf.drop(columns=cvdf.columns[~filter_terms].values)\nprint(f\"Filtered count vectorised data frame has {fcvdf.shape[0]:,} rows and {fcvdf.shape[1]:,} columns.\")\nfcvdf.iloc[0:5,0:10]\n\nFiltered count vectorised data frame has 408 rows and 2,043 columns.\n\n\n\n\n\n\n\n\n\nable\naccess\naccess access\naccess bathroom\naccess central\naccess central london\naccess entire\naccess everything\naccess full\naccess guest\n\n\n\n\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nfcvdf.sum(axis=0)\n\nable                         8\naccess                     242\naccess access                7\naccess bathroom              5\naccess central               8\n                          ... \nzone comfortable cosy        7\nzone near                    5\nzone near underground        5\nzone recently               10\nzone recently refurbish     10\nLength: 2043, dtype: int64\n\n\nWe’re going to pick this up again in Task 7.\n\n\nQuestions\n\nCan you explain what doc_df contains?\nWhat does cvdf contain? Explain the rows and columns.\nWhat is the function of filter_terms?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#tfidf-vectoriser",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#tfidf-vectoriser",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "TF/IDF Vectoriser",
    "text": "TF/IDF Vectoriser\n\n\n\n\n\n\nDifficulty level: Moderate\n\n\n\n\n\nBut only if you want to understand how max_df and min_df work!\n\n\n\n\nFit and Transform\n\ntfvectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3), \n                               max_df=0.75, min_df=0.01) # &lt;-- these matter!\ntftcorpus    = tfvectorizer.fit_transform(corpus) # TF-transformed corpus\n\n\n\nSingle Document\n\ndoc_df = pd.DataFrame(tftcorpus[0].T.todense(), index=tfvectorizer.get_feature_names_out(), columns=[\"Weights\"])\ndoc_df.sort_values('Weights', ascending=False).head(10)\n\n\n\n\n\n\n\n\nWeights\n\n\n\n\ngardens\n0.414885\n\n\nstop\n0.241659\n\n\ndistrict line\n0.239192\n\n\nrailway\n0.232131\n\n\nunderground\n0.201738\n\n\ndistrict\n0.197221\n\n\nbridge\n0.191983\n\n\nwalk\n0.189485\n\n\nroad\n0.151163\n\n\ndistance\n0.142999\n\n\n\n\n\n\n\n\n\nTransformed Corpus\n\ntfidf = pd.DataFrame(data=tftcorpus.toarray(),\n                        columns=tfvectorizer.get_feature_names_out())\nprint(f\"TF/IDF data frame has {tfidf.shape[0]:,} rows and {tfidf.shape[1]:,} columns.\")\ntfidf.head()\n\nTF/IDF data frame has 408 rows and 1,911 columns.\n\n\n\n\n\n\n\n\n\nable\naccess\naccess access\naccess bathroom\naccess central\naccess central london\naccess entire\naccess everything\naccess full\naccess guest\n...\nyoung\nzone\nzone central\nzone central city\nzone comfortable\nzone comfortable cosy\nzone near\nzone near underground\nzone recently\nzone recently refurbish\n\n\n\n\n0\n0.0\n0.043972\n0.0\n0.0\n0.11031\n0.11031\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.069702\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.000000\n0.0\n0.0\n0.00000\n0.00000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.000000\n0.0\n0.0\n0.00000\n0.00000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.000000\n0.0\n0.0\n0.00000\n0.00000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.044127\n0.0\n0.0\n0.00000\n0.00000\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 1911 columns\n\n\n\n\n\nQuestions\n\nWhat does the TF/IDF score represent?\nWhat is the role of max_df and min_df?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#for-counts",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#for-counts",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "For Counts",
    "text": "For Counts\n\n\n\n\n\n\nDifficulty level: Easy!\n\n\n\n\n\n\n\n\n\n\nfcvdf.sum().sort_values(ascending=False)\n\nwalk                      595\nroom                      472\nlondon                    471\nriver                     418\nbedroom                   412\n                         ... \nchic                        5\nterm                        5\nchoice                      5\nteddington                  5\nlondon aquarium minute      5\nLength: 2043, dtype: int64\n\n\n\nff = 'RobotoMono-VariableFont_wght.ttf'\ndp = '/home/jovyan/.local/share/fonts/'\ntp = os.path.join(os.path.expanduser('~'),'Library','Fonts')\nif os.path.exists(tp):\n    fp = os.path.join(tp,ff)\nelse:\n    fp = os.path.join(dp,ff)\n\n\nf,ax = plt.subplots(1,1,figsize=(8, 8))\nplt.gcf().set_dpi(150)\nCloud = WordCloud(\n    background_color=\"white\", \n    max_words=75,\n    font_path=fp\n).generate_from_frequencies(fcvdf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n#plt.savefig(\"Wordcloud 1.png\")"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#for-tfidf-weighting",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#for-tfidf-weighting",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "For TF/IDF Weighting",
    "text": "For TF/IDF Weighting\n\n\n\n\n\n\nDifficulty level: Low, but you’ll need to be patient!\n\n\n\n\n\n\n\n\n\n\ntfidf.sum().sort_values(ascending=False)\n\nwalk                     23.037285\nroom                     19.135852\nlondon                   18.744519\nminute                   18.650909\napartment                18.082855\n                           ...    \nstation apartment one     0.401426\nstation also close        0.401426\napartment one benefit     0.401426\napartment one             0.401426\nalso close station        0.401426\nLength: 1911, dtype: float64\n\n\n\nf,ax = plt.subplots(1,1,figsize=(8, 8))\nplt.gcf().set_dpi(150)\nCloud = WordCloud(\n    background_color=\"white\", \n    max_words=100,\n    font_path=fp\n).generate_from_frequencies(tfidf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n#plt.savefig(\"Wordcloud 2.png\")\n\n\n\n\n\n\n\n\n\nQuestions\n\nWhat does the sum represent for the count vectoriser?\nWhat does the sum represent for the TF/IDF vectoriser?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#calculate-topics",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#calculate-topics",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Calculate Topics",
    "text": "Calculate Topics\n\nvectorizer.fit(corpus) \ntcorpus = vectorizer.transform(corpus) # tcorpus for transformed corpus\n\nLDA = LatentDirichletAllocation(n_components=3, random_state=42) # Might want to experiment with n_components too\nLDA.fit(tcorpus)\n\nLatentDirichletAllocation(n_components=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LatentDirichletAllocation?Documentation for LatentDirichletAllocationiFittedLatentDirichletAllocation(n_components=3, random_state=42) \n\n\n\nfirst_topic = LDA.components_[0]\ntop_words = first_topic.argsort()[-25:]\n\nfor i in top_words:\n    print(vectorizer.get_feature_names_out()[i])\n\nriver thames\nmodern\narea\nwharf\nflat\naccess\nbathroom\nguest\nhouse\nminute\nprivate\nkitchen\nlarge\nthames\nliving\nview\nstation\nfloor\nbedroom\napartment\nwalk\nlondon\nriver\nroom\nspace\n\n\n\nfor i,topic in enumerate(LDA.components_):\n    as_markdown(f'Top 10 words for topic #{i}', ', '.join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]]))\n\nTop 10 words for topic #0\n\nriver thames, modern, area, wharf, flat, access, bathroom, guest, house, minute, private, kitchen, large, thames, living, view, station, floor, bedroom, apartment, walk, london, river, room, space\n\n\n\nTop 10 words for topic #1\n\npark, fully, modern, private, living, guest, view, double, close, access, area, flat, thames, bathroom, station, apartment, kitchen, space, minute walk, room, london, river, bedroom, minute, walk\n\n\n\nTop 10 words for topic #2\n\nliving, river view, canary wharf, canary, central, wharf, stay, close, bathroom, guest, kitchen, double, minute, thames, access, station, flat, space, view, bedroom, apartment, river, walk, london, room"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#maximum-likelihood-topic",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#maximum-likelihood-topic",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Maximum Likelihood Topic",
    "text": "Maximum Likelihood Topic\n\ntopic_values = LDA.transform(tcorpus)\ntopic_values.shape\n\n(408, 3)\n\n\n\npd.options.display.max_colwidth=20\nsrcdf['Topic'] = topic_values.argmax(axis=1)\nsrcdf.head()\n\n\n\n\n\n\n\n\ngeometry\nlisting_url\nname\ndescription\namenities\nprice\ndescription_norm\nTopic\n\n\n\n\n19\nPOINT (519474.79...\nhttps://www.airb...\nTownhouse in Ric...\n3 Bed House with...\n[\"Bathtub\", \"Sha...\n245.0\nhouse garden clo...\n1\n\n\n71\nPOINT (537104.16...\nhttps://www.airb...\nRental unit in L...\n&lt;b&gt;The space&lt;/b&gt;...\n[\"Heating\", \"Ele...\n60.0\nspace appartment...\n2\n\n\n713\nPOINT (530945.88...\nhttps://www.airb...\nRental unit in G...\nNewly renovated,...\n[\"Bathtub\", \"Sha...\n169.0\nnewly renovate ...\n2\n\n\n928\nPOINT (539808.31...\nhttps://www.airb...\nRental unit in L...\nBrand new rivers...\n[\"Waterfront\", \"...\n191.0\nbrand riverside ...\n1\n\n\n1397\nPOINT (538098.29...\nhttps://www.airb...\nRental unit in L...\nPLEASE READ OTHE...\n[\"Heating\", \"Ele...\n79.0\nplease read thin...\n2\n\n\n\n\n\n\n\n\npd.options.display.max_colwidth=75\nsrcdf[srcdf.Topic==1].description_norm.head(10)\n\n19      house garden close thames river . walk  private road river nearby .  di...\n928     brand riverside apartment greenwich peninsula . perfect explore london ...\n1424    mint walk thames river  mint tower bridge  mint greenwich bus mint walk...\n1644    bright  spacious chill bedroom cosy apartment level friendly quiet bloc...\n3887    fabulous bathshower flat modern development east putney  london literal...\n3966    large double room river view available large spacious townhouse hampton...\n4664    private quiet bright  large double room ensuite bathroom . location stu...\n4890    bright  airy bedroom  ground floor apartment quiet street . modernly fu...\n5815    perfect claphambattersea modern decor . brand kitchenbathroom minute wa...\n6699    room front house overlook balcony  road river . bright sunny house view...\nName: description_norm, dtype: object\n\n\n\nvectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', analyzer='word', max_df=0.7, min_df=0.05)\ntopic_corpus = vectorizer.fit_transform(srcdf[srcdf.Topic==1].description.values) # tcorpus for transformed corpus\n\n\ntopicdf = pd.DataFrame(data=topic_corpus.toarray(),\n                        columns=vectorizer.get_feature_names_out())\n\n\nf,ax = plt.subplots(1,1,figsize=(8,8))\nplt.gcf().set_dpi(150)\nCloud = WordCloud(\n            background_color=\"white\", \n            max_words=75).generate_from_frequencies(topicdf.sum())\nax.imshow(Cloud) \nax.axis(\"off\");\n# plt.savefig('Wordcloud 3.png')"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#configure",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#configure",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Configure",
    "text": "Configure\n\nfrom gensim.models.word2vec import Word2Vec\n\n\ndims = 100\nprint(f\"You've chosen {dims} dimensions.\")\n\nwindow = 4\nprint(f\"You've chosen a window of size {window}.\")\n\nmin_v_freq  = 0.01 # Don't keep words appearing less than 1% frequency\nmin_v_count = math.ceil(min_v_freq * srcdf.shape[0])\nprint(f\"With a minimum frequency of {min_v_freq} and {srcdf.shape[0]:,} documents, minimum vocab frequency is {min_v_count:,}.\")\n\nYou've chosen 100 dimensions.\nYou've chosen a window of size 4.\nWith a minimum frequency of 0.01 and 408 documents, minimum vocab frequency is 5."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#train",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#train",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Train",
    "text": "Train\n\n%%time \n\ncorpus      = srcdf.description_norm.fillna(' ').values\n#corpus_sent = [nltk.sent_tokenize(text) for text in corpus] # &lt;-- with more formal writing this would work well\ncorpus_sent = [d.replace('.',' ').split(' ') for d in corpus] # &lt;-- deals better with many short sentences though context may end up... weird\nmodel       = Word2Vec(sentences=corpus_sent, vector_size=dims, window=window, epochs=200, \n                 min_count=min_v_count, seed=42, workers=1)\n\n#model.save(f\"word2vec-d{dims}-w{window}.model\") # &lt;-- You can then Word2Vec.load(...) which is useful with large corpora\n\nCPU times: user 3.94 s, sys: 97 ms, total: 4.04 s\nWall time: 4.02 s"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#explore-similarities",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#explore-similarities",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Explore Similarities",
    "text": "Explore Similarities\nThis next bit of code only runs if you have calculated the frequencies above in the Frequencies and Ngrams section.\n\npd.set_option('display.max_colwidth',150)\n\ndf = fcounts[1] # &lt;-- copy out only the unigrams as we haven't trained anything else\n\nn     = 14 # number of words\ntopn  = 7  # number of most similar words\n\nselected_words = df[df['Ngram Size 1'] &gt; 5].reset_index().level_0.sample(n, random_state=42).tolist()\n\nwords = []\nv1    = []\nv2    = []\nv3    = []\nsims  = []\n\nfor w in selected_words:\n    try: \n        vector = model.wv[w]  # get numpy vector of a word\n        #print(f\"Word vector for '{w}' starts: {vector[:5]}...\")\n    \n        sim = model.wv.most_similar(w, topn=topn)\n        #print(f\"Similar words to '{w}' include: {sim}.\")\n    \n        words.append(w)\n        v1.append(vector[0])\n        v2.append(vector[1])\n        v3.append(vector[2])\n        sims.append(\", \".join([x[0] for x in sim]))\n    except KeyError:\n        print(f\"Didn't find {w} in model. Can happen with low-frequency terms.\")\n    \nvecs = pd.DataFrame({\n    'Term':words,\n    'V1':v1, \n    'V2':v2, \n    'V3':v3,\n    f'Top {topn} Similar':sims\n})\n\nvecs\n\n\n\n\n\n\n\n\nTerm\nV1\nV2\nV3\nTop 7 Similar\n\n\n\n\n0\ncomplimentary\n1.524467\n1.134316\n0.237703\nessentials, mbps, toiletry, excel, workspace, morning, baby\n\n\n1\nequip\n-1.171649\n0.706236\n0.676694\nequipped, utensil, flatscreen, plan, sofabed, lock, bedlinen\n\n\n2\nshower\n-0.272218\n-2.609626\n1.885777\nseparate, corridor, additional, bathroom, toilet, toiletry, plus\n\n\n3\nsmart\n1.758183\n-0.123735\n1.295561\nflatscreen, inch, comfy, netflix, kitchen, shared, streaming\n\n\n4\ndesign\n1.657050\n1.763350\n-1.095689\nchic, beautifully, interior, decor, standard, equipped, comfort\n\n\n5\nappliance\n-0.066985\n0.668519\n1.304341\nessential, necessary, kitchenette, utensil, wellequipped, disposal, cook\n\n\n6\nliving\n0.795928\n-1.091040\n0.090184\nlive, separate, lounge, double, come, modern, bathroom\n\n\n7\nthe\n0.678621\n0.764269\n-1.814383\ncastle, zone, guarantee, cosy, equip, nicely, elephant\n\n\n8\ndirectly\n-0.485994\n-1.765136\n-0.389416\ndocklands, excel, cathedral, undergroundtube, barrier, direct, canary\n\n\n9\nfridgefreezer\n-1.023075\n-0.408912\n-0.379294\ncutlery, kettle, toaster, freezer, washer, oven, microwave\n\n\n10\nbathtub\n-1.153225\n-0.845233\n1.582382\nreception, ensuite, bath, walkin, toilet, corridor, shared\n\n\n11\ntrain\n1.615595\n-0.275208\n-1.623259\nkingston, jubilee, taxi, mudchute, india, airport, transportation\n\n\n12\npalace\n-2.354299\n-1.162588\n-0.131212\nbuckingham, hyde, min, parliament, sloane, houses, stamford\n\n\n13\nshard\n0.661091\n0.241524\n-0.174146\nglobe, cathedral, shoredich, borough, tower, mudchute, southbank\n\n\n\n\n\n\n\n#print(model.wv.index_to_key) # &lt;-- the full vocabulary that has been trained"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#apply",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#apply",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Apply",
    "text": "Apply\nWe’re going to make use of this further next week…\n\nQuestions\n\nWhat happens when dims is very small (e.g. 25) or very large (e.g. 300)?\nWhat happens when window is very small (e.g. 2) or very large (e.g. 8)?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#applications",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#applications",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Applications",
    "text": "Applications\nThe above is still only the results for the one of the subsets of apartments alone. At this point, you would probably want to think about how your results might change if you changed any of the following:\n\nUsing one of the other data sets that we created, or even the entire data set!\nApplying the CountVectorizer or TfidfVectorizer before selecting out any of our ‘sub’ data sets.\nUsing the visualisation of information to improve our regex selection process.\nReducing, increasing, or constraining (i.e. ngrams=(2,2)) the size of the ngrams while bearing in mind the impact on processing time and interpretability.\nFiltering by type of listing or host instead of keywords found in the description (for instance, what if you applied TF/IDF to the entire data set and then selected out ‘Whole Properties’ before splitting into those advertised by hosts with only one listing vs. those with multiple listings?).\nLinking this back to the geography.\n\nOver the next few weeks we’ll also consider alternative means of visualising the data!"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data-Pt2.html#resources",
    "href": "practicals/Practical-08-Textual_Data-Pt2.html#resources",
    "title": "Practical 8bis: Working with Text (Part 2)",
    "section": "Resources",
    "text": "Resources\nThere is a lot more information out there, including a whole book and your standard O’Reilly text.\nAnd some more useful links:\n\nPandas String Contains Method\nUsing Regular Expressions with Pandas\nSummarising Chapters from Frankenstein using TF/IDF"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html",
    "href": "practicals/Practical-05-Objects.html",
    "title": "Practical 5: Objects",
    "section": "",
    "text": "This is a very challenging notebook because it takes you both through the process of building a function incrementally and through a ‘simple’ example of how Python classes actually work. You will need to understand these two very different elements in order to make the most of the remaining 6 weeks of term, because we both improve our code incrementally and make use of objects and their inheritances extensively. You also get an extra chance to revisit the differences between LoLs and DoLs because you will undoubtedly encounter and make use of these data structures even after you become a skillfull Python programmer."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#the-way-that-doesnt-work",
    "href": "practicals/Practical-05-Objects.html#the-way-that-doesnt-work",
    "title": "Practical 5: Objects",
    "section": "The Way That Doesn’t Work",
    "text": "The Way That Doesn’t Work\nRecall that this is how four rows of ‘data’ for city sizes organised by row as a list-of-lists look:\n\nmyData = [\n    ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n    ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n    ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n    ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\n\nTo print out a list of every city in the data set when we don’t know where the Name column is in the file we have to jump through some hoops:\n\ncities = []\n\ncol    = myData[0].index('Name')\nfor i in range(1, len(myData)):\n    cities.append(myData[i][col])\n\nprint(\"The cities in the data set are: \" + \", \".join(cities))\n\nThe cities in the data set are: Greater London, Greater Manchester, West Midlands\n\n\nAnd it’s the same kind of faff if we want to find out if Edinburgh is included in the data set:\n\ncol   = myData[0].index('Name')\nfound = False\nfor i in range(1, len(myData)):\n    if myData[i][col] == 'Edinburgh':\n        print(\"Found Edinburgh in the data set!\")\n        found = True\n        break\n\nif found == False:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nDidn't find Edinburgh in the data set."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#the-way-that-does-work",
    "href": "practicals/Practical-05-Objects.html#the-way-that-does-work",
    "title": "Practical 5: Objects",
    "section": "The Way That Does Work",
    "text": "The Way That Does Work\nCompare that code to how it works for a dictionary-of-lists organised by column. Now try printing out the cities in the data:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['Greater London', 'Greater Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nTo print out a list of every city in the data set:\n\nprint(\", \".join(myData['Name']))\n\nGreater London, Greater Manchester, Birmingham, Edinburgh, Inverness, Lerwick\n\n\nTo find out if Edinburgh is included in the list of data:\n\nif 'Edinburgh' in myData['Name']:\n    print(\"Found Edinburgh in the data set!\")\nelse:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nFound Edinburgh in the data set!\n\n\nSee how even basic questions like “Is Edinburgh in our list of data?” are suddenly easy to answer? We no longer need to loop over the entire data set in order to find one data point. In addition, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already. But let’s test this out and see how it works."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#calculate-mean",
    "href": "practicals/Practical-05-Objects.html#calculate-mean",
    "title": "Practical 5: Objects",
    "section": "Calculate Mean",
    "text": "Calculate Mean\nLet’s start by calculating the sample mean (use Google: Python numpy mean...):\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nmean = np.mean(myData['Population'])\nprint(f\"City distribution has a mean of {mean:,.0f}.\")\n\nCity distribution has a mean of 2,435,442."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#calculate-standard-deviation",
    "href": "practicals/Practical-05-Objects.html#calculate-standard-deviation",
    "title": "Practical 5: Objects",
    "section": "Calculate Standard Deviation",
    "text": "Calculate Standard Deviation\n\n\n\n\n\n\nDifficulty level: Low-ish.\n\n\n\n\n\n\n\n\n\nNow let’s do the standard deviation:\n\nQuestion\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nstd  = np.??(??)\nprint(f\"City distribution has a standard deviation of {std:,.2f}.\")\n\nSo the numpy package gives us a way to calculate the mean and standard deviation quickly and without having to reinvent the wheel. The other potentially new thing here is {std:,.2f}. This is about string formatting and the main thing to recognise is that this means ‘format this float with commas separating the thousands/millions and 2 digits to the right’. The link I’ve provided uses the slightly older approach of &lt;str&gt;.format() but the formatting approach is the same."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#for-loops-without-for-loops",
    "href": "practicals/Practical-05-Objects.html#for-loops-without-for-loops",
    "title": "Practical 5: Objects",
    "section": "For Loops Without For Loops",
    "text": "For Loops Without For Loops\n\n\n\n\n\n\nDifficulty level: Medium.\n\n\n\n\n\n\n\n\n\nNow we’re going to see something called a List Comprehension.\nIn Python you will see code like this a lot: [x for x in list]. This syntax is known as a ‘list comprehension’ and is basically a for loop on one line with the output being assigned to a list. So we can apply an operation (converting to a string, subtracting a value, etc.) to every item in a list without writing out a full for loop.\nHere’s a quick example just to show you what’s going on:\n\ndemo = range(0,10) # &lt;- a *range* of numbers between 0 and 9 (stop at 10)\nprint([x**2 for x in demo]) # square every element of demo\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNow let’s apply this to our problem. We calculated the the mean and standard deviation above, so now we want to apply the z-score formula to every element of the Population list… Remember that the format for the z-score (when dealing with a sample) is:\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\]\nAnd the population standard deviation (by which I mean, if you are dealing with all the data, and not a subsample as we are here) is:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\nQuestion\n\nrs = [(x - ??)/?? for x in myData['Population']] # rs == result set\nprint([f\"{x:.3f}\" for x in rs])"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#appending",
    "href": "practicals/Practical-05-Objects.html#appending",
    "title": "Practical 5: Objects",
    "section": "Appending",
    "text": "Appending\n\n\n\n\n\n\nDifficulty level: trivial\n\n\n\n\n\n\n\n\n\nAnd now let’s add it to the data set:\n\nmyData['Std. Population'] = rs\nprint(myData['Std. Population'])\n\n[2.1579383252868527, 0.0791199354729932, -0.3797024575689938, -0.45025269939207097, -0.6942995760276591, -0.7128035277711219]\n\n\nAnd just to show how everything is in a single data structure:\n\nfor c in myData['Name']:\n    idx = myData['Name'].index(c)\n    print(f\"{c} has a population of {myData['Population'][idx]:,} and standardised score of {myData['Std. Population'][idx]:.3f}\")\n\nGreater London has a population of 9,787,426 and standardised score of 2.158\nGreater Manchester has a population of 2,705,000 and standardised score of 0.079\nBirmingham has a population of 1,141,816 and standardised score of -0.380\nEdinburgh has a population of 901,455 and standardised score of -0.450\nInverness has a population of 70,000 and standardised score of -0.694\nLerwick has a population of 6,958 and standardised score of -0.713"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#downloading-from-a-url",
    "href": "practicals/Practical-05-Objects.html#downloading-from-a-url",
    "title": "Practical 5: Objects",
    "section": "Downloading from a URL",
    "text": "Downloading from a URL\nLet’s focus on the first part first because that’s the precondition for everything else. If we can get the ‘download a file from a URL’ working then the rest will gradually fall into place through iterative improvments!\n\nFinding an Existing Answer\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nFirst, let’s be sensibly lazy–we’ve already written code to read a file from the Internet and turn it into a list of lists. So I’ve copy+pasted that into the code block below since we’re going to start from this point; however, just to help you check your own understanding, I’ve removed a few bits and replaced them with ??. Sorry, it’s good practice. 😈\n\nQuestion\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n\nurlData = [] # Somewhere to store the data\n\nresponse = urlopen(url) # Get the data using the urlopen function\ncsvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n\nfor row in csvfile:\n    urlData.append(??)\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\n\nYou should get:\nurlData has 11 rows and 4 columns.  [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]\n\n\n\nGetting Organised\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nLet’s take the code above and modify it so that it is:\n\nA function that takes two arguments: a URL; and a destination filename.\nImplemented as a function that checks if a file exists already before downloading it again.\n\nYou will find that the os module helps here because of the path function. And you will need to Google how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an explanation included of why a particular answer is a ‘good one’. I also look at which answers got the most votes (not always the same as the one that was the ‘accepted answer’). In this particular case, I also found this answer useful.\nI would start by setting my inputs:\n\nimport os\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\nout = os.path.join('data','Wikipedia-Cities.csv') # Print `out` if you aren't sure what this has done!\n\n\n\nSketching the Function\n\n\n\n\n\n\nDifficulty level: Low, if you’ve watched the videos…\n\n\n\n\n\n\n\n\n\nThen I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:\n\nQuestion\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n    \n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(??):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n        \nget_url(url, out)\n\n\n\n\nFleshing Out the Function\n\n\n\n\n\n\nDifficulty level: Medium\n\n\n\n\n\nIf you really explore what’s going on in the function rather than just running it and moving on.\n\n\n\nI would then flesh out the code so that it downloads the file if it isn’t found and then, either way, returns the local file path for our CSV reader to extract:\n\ndef get_url(src, dest):\n    \n    # Check if dest does *not* exist -- that\n    # would mean we had to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found locally!\")\n    else:\n        print(f\"{dest} not found, downloading!\")\n        \n        # Get the data using the urlopen function\n        response = urlopen(src) \n        filedata = response.read().decode('utf-8')\n        \n        # Extract the part of the dest(ination) that is *not*\n        # the actual filename--have a look at how \n        # os.path.split works using `help(os.path.split)`\n        path = list(os.path.split(dest)[:-1])\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how \n        # to use the 'splat' operator! os.makedirs creates missing \n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n        \n        with open(dest, 'w') as f:\n            f.write(filedata)\n            \n        print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#decorating",
    "href": "practicals/Practical-05-Objects.html#decorating",
    "title": "Practical 5: Objects",
    "section": "Decorating!",
    "text": "Decorating!\nLet’s now look into simplifying this code using a dectorator! Our function has become a bit unwieldy and we want to look at how we can simplify that.\nThe ‘obvious’ (i.e. not obvious) way to do this is to implement the check for a local copy as a decorator on the downloading function. So we have a function that downloads, and a decorator function that checks if the download should even be triggered.\n\nfrom functools import wraps\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        src  = args[0]\n        dest = args[1]\n        if os.path.isfile(dest):\n            print(f\"{dest} found locally!\")\n            return(dest)\n        else:\n            print(f\"{dest} not found, downloading!\")\n            return(f(src, dest))\n    return wrapper\n\n@check_cache\ndef get_url(src, dest):    \n    # Get the data using the urlopen function\n    response = urlopen(src) \n    filedata = response.read().decode('utf-8')\n     \n    # Extract the part of the dest(ination) that is *not*\n    # the actual filename--have a look at how \n    # os.path.split works using `help(os.path.split)`\n    path = list(os.path.split(dest)[:-1])\n     \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how \n    # to use the 'splat' operator! os.makedirs creates missing \n    # directories in a path automatically.\n    if len(path) &gt;= 1 and path[0] != '':\n        os.makedirs(os.path.join(*path), exist_ok=True)\n     \n    with open(dest, 'w') as f:\n        f.write(filedata)\n         \n    print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!\n\n\nI’m not going to pretend that’s the best use of a decorator, but it does neatly separate the downloading function from the caching function. In fact, there is already a cache decorator and some of these have unlimited capacity; however, they are intended to run in a ‘live’ context, so you’d still need to download the file again any time you start a new notebook or restart Docker. This caching function saves the actual data locally to dest.\n\n\n\n\n\n\nStop!\n\n\n\nIt really would be a good idea to put in the effort to make sense of how this function works. There is a lot going on here and understanding how this function works will help you to understand how to code. You should notice that we don’t try to check if the data file contains any useful data! So if you download or create an empty file while testing, you won’t necessarily get an error until you try to turn it into data afterwards!"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#parse-the-csv-file",
    "href": "practicals/Practical-05-Objects.html#parse-the-csv-file",
    "title": "Practical 5: Objects",
    "section": "Parse the CSV File",
    "text": "Parse the CSV File\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nNow we turn to the next task: parsing the file if it’s a CSV. This implies that it might not be so that’s something we should also consider!\n\nQuestion\n\nimport csv\n\ndef read_csv(src):\n    \n    csvdata = []\n    with open(src, 'r') as f:\n        csvr = csv.??(f)\n        \n        for r in csvr:\n            csvdata.append(??)\n    \n    # Return list of lists\n    return ??\n\nread_csv(src)\n#read_csv('foo.bar') # &lt;- Notice what happens if you try to run this code\n#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!\n\nYou should get:\n [[‘City’, ‘Population’, ‘Latitude’, ‘Longitude’], [‘Perth’, ‘45770’, ‘56.39583’, ‘-3.43333’], [‘Armagh’, ‘14777’, ‘54.3499’, ‘-6.6546’], [‘Dundee’, ‘147268’, ‘56.462’, ‘-2.9707’], [‘Colchester’, ‘194706’, ‘51.88861’, ‘0.90361’], [‘Salisbury’, ‘40302’, ‘51.07’, ‘-1.79’], [‘Portsmouth’, ‘205056’, ‘50.80583’, ‘-1.08722’], [‘Wakefield’, ‘325837’, ‘53.683’, ‘-1.499’], [‘Bradford’, ‘522452’, ‘53.792’, ‘-1.754’], [‘Lancaster’, ‘138375’, ‘54.047’, ‘-2.801’], [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]]"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#convert-the-csv-into-a-dol",
    "href": "practicals/Practical-05-Objects.html#convert-the-csv-into-a-dol",
    "title": "Practical 5: Objects",
    "section": "Convert the CSV into a DoL",
    "text": "Convert the CSV into a DoL\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow we can focus on converting the CSV data to a dictionary-of-lists! We’re going to start with the same function name but expand what the function does. This kind of iteration is common in software development.\n\nQuestion\n\ndef read_csv(src):\n    \n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(??, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in ??: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[??].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nread_csv(src)\n\nYou should get something that starts:\n\n\n{'City': ['Perth', 'Armagh', 'Dundee', 'Colchester', 'Salisbury', 'Portsmou..."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#adding-docstring",
    "href": "practicals/Practical-05-Objects.html#adding-docstring",
    "title": "Practical 5: Objects",
    "section": "Adding Docstring",
    "text": "Adding Docstring\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nWe’ve assumed that the first row of our data set is always a header (i.e. list of column names). If it’s not then this code is going to have problems. A robust function would allow us to specify column names, skip rows, etc. when we create the data structure, but let’s not get caught up in that level of detail. Notice that I’ve also, for the first time:\n\nUsed the docstring support offered by Python. You’ll be able to use help(...) and get back the docstring help!\nProvided hints to Python about the expected input and output data types. This can help to ensure consistency and is also critical in testing / continuous integration when working with others on a codebase.\n\n\ndef read_csv(src:str) -&gt; dict:\n    \"\"\"\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n    \n    param src: a local CSV file\n    returns: a dictionary-of-lists\n    \"\"\"\n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in csvr: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[c].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nds = read_csv(src)\n\n\nhelp(read_csv)\n\nHelp on function read_csv in module __main__:\n\nread_csv(src: str) -&gt; dict\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n\n    param src: a local CSV file\n    returns: a dictionary-of-lists\n\n\n\n\nprint(\"Columns are: \" + \", \".join(ds.keys()))\nprint(f\"First two cities are: {ds['City'][:2]}\")\nprint(f\"First two populations are: {ds['Population'][:2]}\")\nprint(f\"First two latitudes are: {ds['Latitude'][:2]}\")\n\nColumns are: City, Population, Latitude, Longitude\nFirst two cities are: ['Perth', 'Armagh']\nFirst two populations are: ['45770', '14777']\nFirst two latitudes are: ['56.39583', '54.3499']"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#creating-a-package",
    "href": "practicals/Practical-05-Objects.html#creating-a-package",
    "title": "Practical 5: Objects",
    "section": "Creating a Package",
    "text": "Creating a Package\nWe’re not going to tackle this now, but it’s important that you understand how what we’ve done connects to what we’re about to do, and the concept of a package is the bridge. We’ve already covered this in the pre-recorded lectures, but if you want to actually try to create your own package, the simplest way to do this is to:\n\nCopy the read_csv into a new file called, for instance, utils.py.\nMake sure you delete this function from the current ‘namespace’ (del(read_csv)) by which I mean that the read_csv function no longer exists (running help(read_csv) should give you an error!).\nTry importing the function from the file: from utils import read_csv and run the help(read_csv) code again.\n\nAssuming that you’ve done everything correctly, we’ve now brought in code from another file without having to write it into our main Python script file. In Python, many of the most complex libraries are spread across the equivalent of many utils.py files, but on top of that when we import and run them they are also creating objects from classes defined in those files.\nWhat we now want to do is use a fairly simple example using different ‘shapes’ (pyramids, cubes, etc.) that allow us to explore how classes work through inheritance from parents and can extend of overwrite the functionality provided by the parent class. We’ll need this understanding in order to grasp how Pandas and GeoPandas work specifically, but also how Python works more generally."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#abstract-base-class",
    "href": "practicals/Practical-05-Objects.html#abstract-base-class",
    "title": "Practical 5: Objects",
    "section": "Abstract Base Class",
    "text": "Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\nWe can now create a new shape object (an instance of the shape class) but we can’t do much that is useful with it:\n\ns = shape(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nI am a Shape\nError: Unimplmented method error."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#cube",
    "href": "practicals/Practical-05-Objects.html#cube",
    "title": "Practical 5: Objects",
    "section": "Cube",
    "text": "Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nQuestion\nCan you work out the missing elements that will allow you to create a cube class?\n\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return (3 * self.??**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return ??*(self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = cube(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#sphere",
    "href": "practicals/Practical-05-Objects.html#sphere",
    "title": "Practical 5: Objects",
    "section": "Sphere",
    "text": "Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2r\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nQuestion\nCan you work out the missing elements that will allow you to create a cube class?\n\n# Sphere class\nfrom math import pi\nclass sphere(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        # Something...\n\n    def diameter(self):\n        # Something...\n\n    def volume(self):\n        # Something\n\n    def surface(self):\n        # Something\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = sphere(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#regular-pyramid",
    "href": "practicals/Practical-05-Objects.html#regular-pyramid",
    "title": "Practical 5: Objects",
    "section": "Regular Pyramid",
    "text": "Regular Pyramid\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\n\n\n\n\n\n\nClass Variables\n\n\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them! This class variable is set automatically for all instances of the pyramid class. Changing this variable can have weird effects so they’re not often changed.\n\n\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n\n    has_mummies = True # This is for *all* regular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n\n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n\n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n\n    def volume(self):\n        return self.dim**2 * self.height() / 3\n\n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#triangular-pyramid",
    "href": "practicals/Practical-05-Objects.html#triangular-pyramid",
    "title": "Practical 5: Objects",
    "section": "Triangular Pyramid",
    "text": "Triangular Pyramid\nWe have chosen for triangular pyramid to inherit from regular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nJust to note, as well, that since all sides are equal this is an equilateral triangular pyramid. Anyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\nTriangular pyramids do not have a problem with mummies.\nWhy don’t you add some documentation to this class and the regular pyramid class so that we know how to use them correctly?\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n\n    has_mummies = False # This is for all triangular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n\n    def diameter(self):\n        return self.dim\n\n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n\n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n\n    def volume(self):\n        return (1/3) * self.base() * self.height()\n\n    def surface(self):\n        return 4 * self.base()"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#testing-your-classes",
    "href": "practicals/Practical-05-Objects.html#testing-your-classes",
    "title": "Practical 5: Objects",
    "section": "Testing Your Classes",
    "text": "Testing Your Classes\nIf you’ve implemented everything correctly then the following code should run.\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\nI get the following output:\n\n\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaargh!\n\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#packaging-it-up",
    "href": "practicals/Practical-05-Objects.html#packaging-it-up",
    "title": "Practical 5: Objects",
    "section": "Packaging It Up",
    "text": "Packaging It Up\nWait, you’re still working on this practical and haven’t thrown up your hands in disgust yet? OK, in that case you can have one more thing to do: turn all the shapes into a package that can be loaded via an import statement.\n\nCell Magic\nThis code allows Jupyter to reload external libraries if they are edited after you import them. When you are working on your own packages this is rather useful since you tend to make a lot of mistakes when packaging code up this way and it’s handy not to have to restart the entire notebook every time you fix a typo or change a function.\n%load_ext autoreload\n%autoreload 2\n\n\nImport Shapes\nMy suggestion is that you create a directory called shapes and copy all of the shape code (that’s the code for shape, cube, sphere, pyramid, tpyramid) into a file called __init__.py inside the shapes directory. You should then able to run the following:\nfor s in ['shape','sphere','cube','pyramid','t_pyramid']:\n    if s in locals():\n        del(s)\nfrom shapes import *\nWe need those first three lines of code to delete the existing classes from Python’s ‘memory’ so that we can be sure we’re importing the versions we saved to shapes/__init__.py.\n\n\nAdding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\nThis would then allow you to run:\nfrom shapes import * # &lt;-- Change this if you didn't call your package `shapes`!\nhelp(shape)\nhelp(shape.type)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html",
    "href": "practicals/Practical-11-Dimensions_in_Data.html",
    "title": "Practical 11: Dimensions in Data",
    "section": "",
    "text": "In this session the focus is on MSOA-level Census data from 2011. We’re going to explore this as a possible complement to the InsideAirbnb data. Although it’s not ideal to use 2011 data with scraped from Airbnb this year, we:\nUltimately, however, you don’t need to use this for your analysis, this practical is intended as a demonstration of how transformation and dimensionality reduction work in practice and the kinds of issues that come up."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#load-msoa-excel-file",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#load-msoa-excel-file",
    "title": "Practical 11: Dimensions in Data",
    "section": "Load MSOA Excel File",
    "text": "Load MSOA Excel File\n\n\n\n\n\n\nDifficulty level: Low.\n\n\n\n\n\n\n\n\n\nYou might like to load the cached copy of the file into Excel so that you can see how the next bit works. You can find the rest of the MSOA Atlas here.\n\nsrc_url   = 'https://data.london.gov.uk/download/msoa-atlas/39fdd8eb-e977-4d32-85a4-f65b92f29dcb/msoa-data.xls'\ndest_path = os.path.join('data','msoa')\n\n\nQuestionAnswer\n\n\nexcel_atlas = pd.read_excel(\n    cache_data(src_url, dest_path), \n    ???, # Which sheet is the data in?\n    header=[0,1,2])            # Where are the column names... there's three of them!\n\n\n\nexcel_atlas = pd.read_excel(\n    cache_data(src_url, dest_path),\n    sheet_name='iadatasheet1', # Which sheet is the data in?\n    header=[0,1,2])            # Where are the column names... there's three of them!\n\nFound data/msoa/msoa-data.xls locally!\n    Size is 2 MB (1,979,904 bytes)\n\n\n\n\n\nNotice the format of the output and notice that all of the empty cells in the Excel sheet have come through as Unnamed: &lt;col_no&gt;_level_&lt;level_no&gt;:\n\nexcel_atlas.head(1)\n\n\n\n\n\n\n\n\nUnnamed: 0_level_0\nUnnamed: 1_level_0\nAge Structure (2011 Census)\nMid-year Estimate totals\n...\nRoad Casualties\n\n\n\nUnnamed: 0_level_1\nUnnamed: 1_level_1\nAll Ages\n0-15\n16-29\n30-44\n45-64\n65+\nWorking-age\nAll Ages\n...\n2010\n2011\n2012\n\n\n\nMSOA Code\nMSOA Name\nUnnamed: 2_level_2\nUnnamed: 3_level_2\nUnnamed: 4_level_2\nUnnamed: 5_level_2\nUnnamed: 6_level_2\nUnnamed: 7_level_2\nUnnamed: 8_level_2\n2002\n...\nSlight\n2010 Total\nFatal\nSerious\nSlight\n2011 Total\nFatal\nSerious\nSlight\n2012 Total\n\n\n\n\n0\nE02000001\nCity of London 001\n7375.0\n620.0\n1665.0\n2045.0\n2010.0\n1035.0\n5720.0\n7280.0\n...\n334.0\n374.0\n0.0\n46.0\n359.0\n405.0\n2.0\n51.0\n361.0\n414.0\n\n\n\n\n1 rows × 207 columns\n\n\n\nprint(f\"Shape of the MSOA Atlas data frame is: {excel_atlas.shape[0]:,} x {excel_atlas.shape[1]:,}\")\nYou should get: Shape of the MSOA Atlas data frame is: 984 x 207, but how on earth are you going to access the data?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#accessing-multiindexes",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#accessing-multiindexes",
    "title": "Practical 11: Dimensions in Data",
    "section": "Accessing MultiIndexes",
    "text": "Accessing MultiIndexes\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\nThe difficulty is conceptual, not technical.\n\n\n\nUntil now we have understood the pandas index as a single column-like ‘thing’ in a data frame, but pandas also supports hierarchical and grouped indexes that allow us to interact with data in more complex ways… should we need it. Generally:\n\nMultiIndex == hierarchical index on columns\nDataFrameGroupBy == iterable pseudo-hierarchical index on rows\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nWe’ll be looking at Grouping Data in much more detail in next week, so the main thing to remember is that grouping is for rows, multi-indexing is about columns.\n\n\n\nDirect Access\nOf course, one way to get at the data is to use .iloc[...] since that refers to columns by position and ignores the complexity of the index. Try printing out the the first five rows of the first column using iloc:\nexcel_atlas.iloc[???]\nYou should get:\n\n\n0    E02000001\n1    E02000002\n2    E02000003\n3    E02000004\n4    E02000005\nName: (Unnamed: 0_level_0, Unnamed: 0_level_1, MSOA Code), dtype: object\n\n\n\n\nNamed Access\nBut to do it by name is a little trickier:\n\nexcel_atlas.columns.tolist()[:5]\n\n[('Unnamed: 0_level_0', 'Unnamed: 0_level_1', 'MSOA Code'),\n ('Unnamed: 1_level_0', 'Unnamed: 1_level_1', 'MSOA Name'),\n ('Age Structure (2011 Census)', 'All Ages', 'Unnamed: 2_level_2'),\n ('Age Structure (2011 Census)', '0-15', 'Unnamed: 3_level_2'),\n ('Age Structure (2011 Census)', '16-29', 'Unnamed: 4_level_2')]\n\n\nNotice how asking for the first five columns has given us a list of… what exactly?\n\nQuestionAnswer\n\n\nSo to get the same output by column name what do you need to copy from above:\nexcel_atlas.loc[0:5, ???]\n\n\n\nexcel_atlas.loc[0:5, ('Unnamed: 0_level_0','Unnamed: 0_level_1','MSOA Code')]\n\n0    E02000001\n1    E02000002\n2    E02000003\n3    E02000004\n4    E02000005\n5    E02000007\nName: (Unnamed: 0_level_0, Unnamed: 0_level_1, MSOA Code), dtype: object\n\n\n\n\n\nThe answer is really awkward, so we’re going to look for a better way…\n\n\nGrouped Access\nDespite this, one way that MultiIndexes can be useful is for accessing column-slices from a ‘wide’ dataframe. We can, for instance, select all of the Age Structure columns in one go and it will be simpler than what we did above.\n\nexcel_atlas.loc[0:5, ('Age Structure (2011 Census)')]\n\n\n\n\n\n\n\n\nAll Ages\n0-15\n16-29\n30-44\n45-64\n65+\nWorking-age\n\n\n\nUnnamed: 2_level_2\nUnnamed: 3_level_2\nUnnamed: 4_level_2\nUnnamed: 5_level_2\nUnnamed: 6_level_2\nUnnamed: 7_level_2\nUnnamed: 8_level_2\n\n\n\n\n0\n7375.0\n620.0\n1665.0\n2045.0\n2010.0\n1035.0\n5720.0\n\n\n1\n6775.0\n1751.0\n1277.0\n1388.0\n1258.0\n1101.0\n3923.0\n\n\n2\n10045.0\n2247.0\n1959.0\n2300.0\n2259.0\n1280.0\n6518.0\n\n\n3\n6182.0\n1196.0\n1277.0\n1154.0\n1543.0\n1012.0\n3974.0\n\n\n4\n8562.0\n2200.0\n1592.0\n1995.0\n1829.0\n946.0\n5416.0\n\n\n5\n8791.0\n2388.0\n1765.0\n1867.0\n1736.0\n1035.0\n5368.0\n\n\n\n\n\n\n\n\n\nUnderstanding Levels\nThis works because the MultiIndex tracks the columns using levels, with level 0 at the ‘top’ and level 2 (in our case) at the bottom. These are the unique values for the top level (‘row 0’):\n\nexcel_atlas.columns.levels[0]\n\nIndex(['Adults in Employment (2011 Census)', 'Age Structure (2011 Census)',\n       'Car or van availability (2011 Census)',\n       'Central Heating (2011 Census)', 'Country of Birth (2011)',\n       'Dwelling type (2011)', 'Economic Activity (2011 Census)',\n       'Ethnic Group (2011 Census)', 'Health (2011 Census)', 'House Prices',\n       'Household Composition (2011)', 'Household Income Estimates (2011/12)',\n       'Household Language (2011)', 'Households (2011)', 'Incidence of Cancer',\n       'Income Deprivation (2010)', 'Land Area', 'Life Expectancy',\n       'Lone Parents (2011 Census)', 'Low Birth Weight Births (2007-2011)',\n       'Mid-year Estimate totals', 'Mid-year Estimates 2012, by age',\n       'Obesity', 'Population Density', 'Qualifications (2011 Census)',\n       'Religion (2011)', 'Road Casualties', 'Tenure (2011)',\n       'Unnamed: 0_level_0', 'Unnamed: 1_level_0'],\n      dtype='object')\n\n\nThese are the values for those levels across the actual columns in the data frame, notice the repeated ‘Age Structure (2011 Census)’:\n\nexcel_atlas.columns.get_level_values(0)[:10]\n\nIndex(['Unnamed: 0_level_0', 'Unnamed: 1_level_0',\n       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n       'Age Structure (2011 Census)', 'Mid-year Estimate totals'],\n      dtype='object')\n\n\nAnd here are the values for the second level of the index (‘row 1’ in the Excel file):\n\nexcel_atlas.columns.get_level_values(1)[:10]\n\nIndex(['Unnamed: 0_level_1', 'Unnamed: 1_level_1', 'All Ages', '0-15', '16-29',\n       '30-44', '45-64', '65+', 'Working-age', 'All Ages'],\n      dtype='object')\n\n\nBy extension, if we drop a level 0 index then all of the columns that it supports at levels 1 and 2 are also dropped: so when we drop Mid-year Estimate totals from level 0 then all 11 of the ‘Mid-year Estimate totals (2002…2012)’ columns are dropped in one go.\n\nexcel_atlas[['Mid-year Estimate totals']].head(3)\n\n\n\n\n\n\n\n\nMid-year Estimate totals\n\n\n\nAll Ages\n\n\n\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n\n\n\n\n0\n7280.0\n7115.0\n7118.0\n7131.0\n7254.0\n7607.0\n7429.0\n7472.0\n7338.0\n7412.0\n7604.0\n\n\n1\n6333.0\n6312.0\n6329.0\n6341.0\n6330.0\n6323.0\n6369.0\n6570.0\n6636.0\n6783.0\n6853.0\n\n\n2\n9236.0\n9252.0\n9155.0\n9072.0\n9144.0\n9227.0\n9564.0\n9914.0\n10042.0\n10088.0\n10218.0\n\n\n\n\n\n\n\n\ntest = excel_atlas.drop(columns=['Mid-year Estimate totals'], axis=1, level=0)\n\nprint(f\"Excel source had {excel_atlas.shape[1]} columns.\")\nprint(f\"Test now has {test.shape[1]} columns.\")\n\nExcel source had 207 columns.\nTest now has 196 columns.\n\n\n\n# Tidy up if the variable exists\nif 'test' in locals():\n    del(test)\n\n\n\nQuestions\n\nWhat data type is used for storing/accessing MultiIndexes?\nWhy is this is the appropriate data type?\nHow (conceptually) are the header rows in Excel are mapped on to levels in pandas?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#tidying-up",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#tidying-up",
    "title": "Practical 11: Dimensions in Data",
    "section": "Tidying Up",
    "text": "Tidying Up\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\nAlthough there’s a lot of dealing with column names.\n\n\n\n\nDropping Named Levels\nThere’s a lot of data in the data frame that we don’t need for our Airbnb work, so let’s go a bit further with the dropping of column-groups using the MultiIndex.\n\nto_drop = ['Mid-year Estimate totals','Mid-year Estimates 2012, by age','Religion (2011)',\n           'Land Area','Lone Parents (2011 Census)','Central Heating (2011 Census)','Health (2011 Census)',\n           'Low Birth Weight Births (2007-2011)','Obesity','Incidence of Cancer','Life Expectancy',\n           'Road Casualties']\ntidy = excel_atlas.drop(to_drop, axis=1, level=0)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n\nShape of the MSOA Atlas data frame is now: 984 x 111\n\n\nThis should drop you down to 984 x 111. Notice below that the multi-level index has not changed but the multi-level values remaining have!\n\nprint(f\"There are {len(tidy.columns.levels[0].unique())} categories.\") # The categories\nprint(f\"But only {len(tidy.columns.get_level_values(0).unique())} values.\") # The actual values\n\nThere are 30 categories.\nBut only 18 values.\n\n\n\n\nSelecting Columns using a List Comprehension\nNow we need to drop all of the percentages from the data set. These can be found at level 1, though they are specified in a number of different ways so you’ll need to come up with a way to find them in the level 1 values using a list comprehension…\nI’d suggest looking for: “(%)”, “%”, and “Percentages”. You may need to check both start and end of the string. You could also use a regular expression here instead of multiple tests. Either way works, but have a think about the tradeoffs between intelligibility, speed, and what you understand…\n\nQuestionAnswer\n\n\nSelection using multiple logical tests:\nto_drop = [x for x in tidy.columns.get_level_values(1) if (???)]\nprint(to_drop)\nSelection using a regular expression:\nprint([x for x in tidy.columns.get_level_values(1) if re.search(???, x)])\n\n\n\nto_drop = [x for x in tidy.columns.get_level_values(1) if (\n    x.endswith(\"(%)\") or x.startswith(\"%\") or x.endswith(\"Percentages\") or x.endswith(\"%\"))]\nprint(to_drop)\n\n['Percentages', 'Percentages', 'Percentages', 'Percentages', 'Percentages', 'White (%)', 'Mixed/multiple ethnic groups (%)', 'Asian/Asian British (%)', 'Black/African/Caribbean/Black British (%)', 'Other ethnic group (%)', 'BAME (%)', 'United Kingdom (%)', 'Not United Kingdom (%)', '% of people aged 16 and over in household have English as a main language', '% of households where no people in household have English as a main language', 'Owned: Owned outright (%)', 'Owned: Owned with a mortgage or loan (%)', 'Social rented (%)', 'Private rented (%)', 'Household spaces with at least one usual resident (%)', 'Household spaces with no usual residents (%)', 'Whole house or bungalow: Detached (%)', 'Whole house or bungalow: Semi-detached (%)', 'Whole house or bungalow: Terraced (including end-terrace) (%)', 'Flat, maisonette or apartment (%)', 'Economically active %', 'Economically inactive %', '% of households with no adults in employment: With dependent children', '% living in income deprived households reliant on means tested benefit', '% of people aged over 60 who live in pension credit households', 'No cars or vans in household (%)', '1 car or van in household (%)', '2 cars or vans in household (%)', '3 cars or vans in household (%)', '4 or more cars or vans in household (%)']\n\n\n\nprint([x for x in tidy.columns.get_level_values(1) if re.search(\"(?:%|Percentages)\", x)])\n\n['Percentages', 'Percentages', 'Percentages', 'Percentages', 'Percentages', 'White (%)', 'Mixed/multiple ethnic groups (%)', 'Asian/Asian British (%)', 'Black/African/Caribbean/Black British (%)', 'Other ethnic group (%)', 'BAME (%)', 'United Kingdom (%)', 'Not United Kingdom (%)', '% of people aged 16 and over in household have English as a main language', '% of households where no people in household have English as a main language', 'Owned: Owned outright (%)', 'Owned: Owned with a mortgage or loan (%)', 'Social rented (%)', 'Private rented (%)', 'Household spaces with at least one usual resident (%)', 'Household spaces with no usual residents (%)', 'Whole house or bungalow: Detached (%)', 'Whole house or bungalow: Semi-detached (%)', 'Whole house or bungalow: Terraced (including end-terrace) (%)', 'Flat, maisonette or apartment (%)', 'Economically active %', 'Economically inactive %', '% of households with no adults in employment: With dependent children', '% living in income deprived households reliant on means tested benefit', '% of people aged over 60 who live in pension credit households', 'No cars or vans in household (%)', '1 car or van in household (%)', '2 cars or vans in household (%)', '3 cars or vans in household (%)', '4 or more cars or vans in household (%)']\n\n\n\n\n\nWith both you should get:\n\n\n\n['Percentages',\n 'Percentages',\n 'Percentages',\n 'Percentages',\n 'Percentages',\n 'White (%)',\n 'Mixed/multiple ethnic groups (%)',\n 'Asian/Asian British (%)',\n 'Black/African/Caribbean/Black British (%)',\n 'Other ethnic group (%)',\n 'BAME (%)',\n 'United Kingdom (%)',\n 'Not United Kingdom (%)',\n '% of people aged 16 and over in household have English as a main language',\n '% of households where no people in household have English as a main language',\n 'Owned: Owned outright (%)',\n 'Owned: Owned with a mortgage or loan (%)',\n 'Social rented (%)',\n 'Private rented (%)',\n 'Household spaces with at least one usual resident (%)',\n 'Household spaces with no usual residents (%)',\n 'Whole house or bungalow: Detached (%)',\n 'Whole house or bungalow: Semi-detached (%)',\n 'Whole house or bungalow: Terraced (including end-terrace) (%)',\n 'Flat, maisonette or apartment (%)',\n 'Economically active %',\n 'Economically inactive %',\n '% of households with no adults in employment: With dependent children',\n '% living in income deprived households reliant on means tested benefit',\n '% of people aged over 60 who live in pension credit households',\n 'No cars or vans in household (%)',\n '1 car or van in household (%)',\n '2 cars or vans in household (%)',\n '3 cars or vans in household (%)',\n '4 or more cars or vans in household (%)']\n\n\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nSee how regular expressions keep coming baaaaaaaaack? That said, you can also often make use of simple string functions like startswith and endswith for this problem.\n\n\n\n\nDrop by Level\nYou now need to drop these columns using the level keyword as part of your drop command. You have plenty of examples of how to drop values in place, but I’d suggest first getting the command correct (maybe duplicate the cell below and change the code so that the result is saved to a dataframe called test before overwriting tidy?) and then saving the change.\n\nQuestionAnswer\n\n\ntidy = tidy.drop(to_drop, axis=1, level=???)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n\n\n\ntidy.drop(to_drop, axis=1, level=1, inplace=True)\nprint(f\"Shape of the MSOA Atlas data frame is now: {tidy.shape[0]} x {tidy.shape[1]}\")\n\nShape of the MSOA Atlas data frame is now: 984 x 76\n\n\n\n\n\nThe data frame should now be 984 x 76. This is a bit more manageable though still a lot of data columns. Depending on what you decide to do for your final project you might want to revisit some of the columns that we dropped above…\n\n\nFlattening the Index\nAlthough this ia big improvement, you’ll have trouble saving or linking this data to other inputs. The problem is that Level 2 of the multi-index is mainly composed of ‘Unnamed’ values and so we need to merge it with Level 1 to simplify our data frame, and then merge that with level 0…\n\ntidy.columns.values[:3]\n\narray([('Unnamed: 0_level_0', 'Unnamed: 0_level_1', 'MSOA Code'),\n       ('Unnamed: 1_level_0', 'Unnamed: 1_level_1', 'MSOA Name'),\n       ('Age Structure (2011 Census)', 'All Ages', 'Unnamed: 2_level_2')],\n      dtype=object)\n\n\nLet’s use code to sort this out!\n\nnew_cols = []\nfor c in tidy.columns.values:\n    \n    #print(f\"Column label: {c}\")\n    l1 = f\"{c[0]}\"\n    l2 = f\"{c[1]}\"\n    l3 = f\"{c[2]}\"\n    \n    # The new column label\n    clabel = ''\n    \n    # Assemble new label from the levels\n    if not l1.startswith(\"Unnamed\"):\n        l1 = l1.replace(\" (2011 Census)\",'').replace(\" (2011)\",'').replace(\"Household \",'').replace(\"House Prices\",'').replace(\"Car or van availability\",'Vehicles').replace(' (2011/12)','')\n        l1 = l1.replace('Age Structure','Age').replace(\"Ethnic Group\",'').replace('Dwelling type','').replace('Income Estimates','')\n        clabel += l1\n    if not l2.startswith(\"Unnamed\"):\n        l2 = l2.replace(\"Numbers\",'').replace(\" House Price (£)\",'').replace(\"Highest level of qualification: \",'').replace(\"Annual Household Income (£)\",'hh Income').replace('Whole house or bungalow: ','').replace(' qualifications','')\n        l2 = l2.replace('At least one person aged 16 and over in household has English as a main language',\"1+ English as a main language\").replace(\"No people in household have English as a main language\",\"None have English as main language\")\n        clabel += ('-' if clabel != '' else '') + l2\n    if not l3.startswith(\"Unnamed\"):\n        clabel += ('-' if clabel != '' else '') + l3\n    \n    # Replace other commonly-occuring verbiage that inflates column name width\n    clabel = clabel.replace('--','-').replace(\" household\",' hh').replace('Owned: ','')\n    \n    #clabel = clabel.replace(' (2011 Census)','').replace(' (2011)','').replace('Sales - 2011.1','Sales - 2012')\n    #clabel = clabel.replace('Numbers - ','').replace(' (£)','').replace('Car or van availability','Vehicles')\n    #clabel = clabel.replace('Household Income Estimates (2011/12) - ','').replace('Age Structure','Age')\n    \n    new_cols.append(clabel)\n\n\n\nnew_cols\n\n['MSOA Code',\n 'MSOA Name',\n 'Age-All Ages',\n 'Age-0-15',\n 'Age-16-29',\n 'Age-30-44',\n 'Age-45-64',\n 'Age-65+',\n 'Age-Working-age',\n 'Households-All Households',\n 'Composition-Couple hh with dependent children',\n 'Composition-Couple hh without dependent children',\n 'Composition-Lone parent hh',\n 'Composition-One person hh',\n 'Composition-Other hh Types',\n 'White',\n 'Mixed/multiple ethnic groups',\n 'Asian/Asian British',\n 'Black/African/Caribbean/Black British',\n 'Other ethnic group',\n 'BAME',\n 'Country of Birth-United Kingdom',\n 'Country of Birth-Not United Kingdom',\n 'Language-1+ English as a main language',\n 'Language-None have English as main language',\n 'Tenure-Owned outright',\n 'Tenure-Owned with a mortgage or loan',\n 'Tenure-Social rented',\n 'Tenure-Private rented',\n 'Household spaces with at least one usual resident',\n 'Household spaces with no usual residents',\n 'Detached',\n 'Semi-detached',\n 'Terraced (including end-terrace)',\n 'Flat, maisonette or apartment',\n 'Population Density-Persons per hectare (2012)',\n 'Median-2005',\n 'Median-2006',\n 'Median-2007',\n 'Median-2008',\n 'Median-2009',\n 'Median-2010',\n 'Median-2011',\n 'Median-2012',\n 'Median-2013 (p)',\n 'Sales-2005',\n 'Sales-2006',\n 'Sales-2007',\n 'Sales-2008',\n 'Sales-2009',\n 'Sales-2010',\n 'Sales-2011',\n 'Sales-2011.1',\n 'Sales-2013(p)',\n 'Qualifications-No',\n 'Qualifications-Level 1',\n 'Qualifications-Level 2',\n 'Qualifications-Apprenticeship',\n 'Qualifications-Level 3',\n 'Qualifications-Level 4 and above',\n 'Qualifications-Other',\n 'Qualifications-Schoolchildren and full-time students: Age 18 and over',\n 'Economic Activity-Economically active: Total',\n 'Economic Activity-Economically active: Unemployed',\n 'Economic Activity-Economically inactive: Total',\n 'Economic Activity-Unemployment Rate',\n 'Adults in Employment-No adults in employment in hh: With dependent children',\n 'Total Mean hh Income',\n 'Total Median hh Income',\n 'Vehicles-No cars or vans in hh',\n 'Vehicles-1 car or van in hh',\n 'Vehicles-2 cars or vans in hh',\n 'Vehicles-3 cars or vans in hh',\n 'Vehicles-4 or more cars or vans in hh',\n 'Vehicles-Sum of all cars or vans in the area',\n 'Vehicles-Cars per hh']\n\n\n\n\n\n\n\n\n\nStop\n\n\n\nMake sure you understand what is happening here before just moving on to the next thing. Try adding print() statements if it will help it to make sense. This sort of code comes up a lot in the real world.\n\n\n\ntidy.columns = new_cols  # &lt;- Blow away complex index, replace with simple\ntidy.head()\n\n\n\n\n\n\n\n\nMSOA Code\nMSOA Name\nAge-All Ages\nAge-0-15\nAge-16-29\nAge-30-44\nAge-45-64\nAge-65+\nAge-Working-age\nHouseholds-All Households\n...\nAdults in Employment-No adults in employment in hh: With dependent children\nTotal Mean hh Income\nTotal Median hh Income\nVehicles-No cars or vans in hh\nVehicles-1 car or van in hh\nVehicles-2 cars or vans in hh\nVehicles-3 cars or vans in hh\nVehicles-4 or more cars or vans in hh\nVehicles-Sum of all cars or vans in the area\nVehicles-Cars per hh\n\n\n\n\n0\nE02000001\nCity of London 001\n7375.0\n620.0\n1665.0\n2045.0\n2010.0\n1035.0\n5720.0\n4385.0\n...\n38.0\n59728.481886\n46788.295472\n3043.0\n1100.0\n173.0\n51.0\n18.0\n1692.0\n0.385861\n\n\n1\nE02000002\nBarking and Dagenham 001\n6775.0\n1751.0\n1277.0\n1388.0\n1258.0\n1101.0\n3923.0\n2713.0\n...\n319.0\n31788.185996\n27058.703760\n1020.0\n1186.0\n424.0\n66.0\n17.0\n2305.0\n0.849613\n\n\n2\nE02000003\nBarking and Dagenham 002\n10045.0\n2247.0\n1959.0\n2300.0\n2259.0\n1280.0\n6518.0\n3834.0\n...\n268.0\n43356.931547\n36834.528738\n1196.0\n1753.0\n691.0\n155.0\n39.0\n3766.0\n0.982264\n\n\n3\nE02000004\nBarking and Dagenham 003\n6182.0\n1196.0\n1277.0\n1154.0\n1543.0\n1012.0\n3974.0\n2318.0\n...\n122.0\n46701.436554\n39668.206433\n556.0\n1085.0\n515.0\n128.0\n34.0\n2650.0\n1.143227\n\n\n4\nE02000005\nBarking and Dagenham 004\n8562.0\n2200.0\n1592.0\n1995.0\n1829.0\n946.0\n5416.0\n3183.0\n...\n307.0\n34293.820288\n29155.683536\n1080.0\n1423.0\n551.0\n109.0\n20.0\n2937.0\n0.922714\n\n\n\n\n5 rows × 76 columns\n\n\n\nYou might want to have a look at what the code below drops first before just running it… remember that you can pull apart any complex code into pieces:\n\ntidy['MSOA Code'].isna()\ntidy[tidy['MSOA Code'].isna()].index\n\nIndex([983], dtype='int64')\n\n\n\ntidy.drop(index=tidy[tidy['MSOA Code'].isna()].index, inplace=True)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#add-innerouter-london-mapping",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#add-innerouter-london-mapping",
    "title": "Practical 11: Dimensions in Data",
    "section": "Add Inner/Outer London Mapping",
    "text": "Add Inner/Outer London Mapping\n\n\n\n\n\n\nDifficulty: Moderate, since I’m not giving you many clues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nWe touched on lambda functions last week; it’s a ‘trivial’ function that we don’t even want to bother defining with def. We also used the lambda function in the context of apply so this is just another chance to remind yourself how this works. This is quite advanced Python, so don’t panic if you don’t get it right away and have to do some Googling…\n\n\nWe want to add the borough name and a ‘subregion’ name. We already have the borough name buried in a separate column, so step 1 is to extract that from the MSOA Name. Step 2 is to use the borough name as a lookup to the subregion name using a lambda function. The format for a lambda function is usually lambda x: &lt;code that does something with x and returns a value&gt;. Hint: you’ve got a dictionary and you know how to use it!\n\nAdd Boroughs\nWe first need to extract the borough names from one of the existing fields in the data frame… a regex that does replacement would be fastest and easiest: focus on what you don’t need from the MSOA Name string and replacing that using a regex…\n\nQuestionAnswer\n\n\ntidy['Borough'] = tidy['MSOA Name'].???\ntidy.Borough.unique()\n\n\n\ntidy['Borough'] = tidy['MSOA Name'].str.replace(r' \\d+$','',regex=True)\ntidy.Borough.unique()\n\narray(['City of London', 'Barking and Dagenham', 'Barnet', 'Bexley',\n       'Brent', 'Bromley', 'Camden', 'Croydon', 'Ealing', 'Enfield',\n       'Greenwich', 'Hackney', 'Hammersmith and Fulham', 'Haringey',\n       'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Islington',\n       'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth',\n       'Lewisham', 'Merton', 'Newham', 'Redbridge',\n       'Richmond upon Thames', 'Southwark', 'Sutton', 'Tower Hamlets',\n       'Waltham Forest', 'Wandsworth', 'Westminster'], dtype=object)\n\n\n\n\n\nYou should get:\n\n\narray(['City of London', 'Barking and Dagenham', 'Barnet', 'Bexley',\n       'Brent', 'Bromley', 'Camden', 'Croydon', 'Ealing', 'Enfield',\n       'Greenwich', 'Hackney', 'Hammersmith and Fulham', 'Haringey',\n       'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Islington',\n       'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth',\n       'Lewisham', 'Merton', 'Newham', 'Redbridge',\n       'Richmond upon Thames', 'Southwark', 'Sutton', 'Tower Hamlets',\n       'Waltham Forest', 'Wandsworth', 'Westminster'], dtype=object)\n\n\n\n\nMap Boroughs to Subregions\nAnd now you need to understand how to apply the mapping ot the Borough field using a lambda function. It’s fairly straightforward once you know the syntax: just a dictionary lookup. But as usual, you might want to first create a new cell and experiment with the output from the apply function before using it to write the Subregion field of the data frame…\n\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden','City of London']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet']:\n    mapping[b]='Outer West and North West'\n\n\nQuestionAnswer\n\n\ntidy['Subregion'] = tidy.Borough.apply(???)\n\n\n\ntidy['Subregion'] = tidy.Borough.apply(lambda x: mapping[x])\n\n\n\n\n\n\nAnd Save\nThere’s a little snipped of useful code to work out here: we need to check if the clean directory exists in the data directory; if we don’t then the tidy.to_parquet() call will fail.\n\nif not os.path.exists(os.path.join('data','clean')):\n    os.makedirs(os.path.join('data','clean'))\ntidy.to_parquet(os.path.join('data','clean','MSOA_Atlas.parquet'))\nprint(\"Done.\")\n\nDone.\n\n\n\n\nQuestions\n\nWhat are the advantages to apply and lambda functions over looping and named functions?\nWhen might you choose a named function over a lambda function?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#merge-data-geography",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#merge-data-geography",
    "title": "Practical 11: Dimensions in Data",
    "section": "Merge Data & Geography",
    "text": "Merge Data & Geography\n\n\n\n\n\n\nDifficulty: Low, except for plotting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nWe’ll cover joins (of which a merge is just one type) in the final week’s lectures, but between what you’d done in GIS and what we have here there should be enough here for you to start being able to make sense of how they work so that you don’t have to wait until Week 10 to think about how this could help you with your Group Assessment.\n\n\nFirst, we need to download the MSOA source file, which is a zipped archive of a Shapefile:\n\n# Oh look, we can read a Shapefile without needing to unzip it!\nmsoas = gpd.read_file(\n    cache_data('https://github.com/jreades/fsds/blob/master/data/src/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip?raw=true', \n               os.path.join('data','geo')), driver='ESRI Shapefile')\n\nFound data/geo/Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip locally!\n    Size is 7 MB (7,381,177 bytes)\n\n\n\nIdentifying Matching Columns\nLooking at the first few columns of each data frame, which one might allow us to link the two files together? You’ve done this in GIS. Remember: the column names don’t need to match for us to use them in a join, it’s the values that matter.\n\nprint(f\"Column names: {', '.join(tidy.columns.tolist()[:5])}\")\ntidy.iloc[:3,:5]\n\nColumn names: MSOA Code, MSOA Name, Age-All Ages, Age-0-15, Age-16-29\n\n\n\n\n\n\n\n\n\nMSOA Code\nMSOA Name\nAge-All Ages\nAge-0-15\nAge-16-29\n\n\n\n\n0\nE02000001\nCity of London 001\n7375.0\n620.0\n1665.0\n\n\n1\nE02000002\nBarking and Dagenham 001\n6775.0\n1751.0\n1277.0\n\n\n2\nE02000003\nBarking and Dagenham 002\n10045.0\n2247.0\n1959.0\n\n\n\n\n\n\n\n\n\nMerge\nOne more thing: if you’ve got more than one choice I’d always go with a code over a name because one is intended for matching and other is not…\n\nQuestionAnswer\n\n\ngdf = pd.merge(msoas, tidy, left_on=???, right_on=???, how='inner')\ngdf = gdf.drop(columns=['MSOA11CD','MSOA11NM','OBJECTID'])\n\nprint(f\"Final MSOA Atlas data frame has shape {gdf.shape[0]:,} x {gdf.shape[1]}\")\n\n\n\ngdf = pd.merge(msoas, tidy, left_on='MSOA11CD', right_on='MSOA Code', how='inner')\ngdf = gdf.drop(columns=['MSOA11CD','MSOA11NM','OBJECTID'])\n\nprint(f\"Final MSOA Atlas data frame has shape {gdf.shape[0]:,} x {gdf.shape[1]}\")\n\nFinal MSOA Atlas data frame has shape 983 x 86\n\n\n\n\n\nYou should get Final data frame has shape 983 x 86.\n\n\nPlot Choropleth\nLet’s plot the median income in 2011 column using the plasma colour ramp… The rest is to show you how to customise a legend.\ncol = 'Median-2011'\nfig = gdf.plot(column=???, cmap='???', \n         scheme='FisherJenks', k=7, edgecolor='None', \n         legend=True, legend_kwds={'frameon':False, 'fontsize':8},\n         figsize=(8,7));\nplt.title(col.replace('-',' '));\n\n# Now to modify the legend: googling \"geopandas format legend\"\n# brings me to: https://stackoverflow.com/a/56591102/4041902\nleg = fig.get_legend()\nleg._loc = 3\n\nfor lbl in leg.get_texts():\n    label_text = lbl.get_text()\n    [low, hi] = label_text.split(', ')\n    new_text = f'£{float(low):,.0f} - £{float(hi):,.0f}'\n    lbl.set_text(new_text)\n\nplt.show();\n\n\nSave\ngdf.to_geoparquet(os.path.join('data','geo','MSOA_Atlas.geoparquet'))\n\n\nQuestions\n\nTry changing the colour scheme, classification scheme, and number of classes to see if you feel there’s a better opeion than the one shown above… Copy the cell (click on anywhere outside the code and then hit C to copy. Then click on this cell once, and hit V to paste."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#reload",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#reload",
    "title": "Practical 11: Dimensions in Data",
    "section": "Reload",
    "text": "Reload\n\n\n\n\n\n\nTip\n\n\n\nIn future 'runs' of this notebook you can now just pick up here and skip all of Task 1.\n\n\nOn subsequent runs of this notebook you might just want to start here!\n# Notice this handy code: we check if the data is already\n# in memory. And notice this is just a list comprehension\n# to see what is locally loaded.\nif 'gdf' not in locals():\n    gdf = gpd.read_parquet(os.path.join('data','geo','MSOA_Atlas.geoparquet'))\nprint(gdf.shape)\ncategoricals = ['Borough','Subregion']\nfor c in categoricals:\n    gdf[c] = gdf[c].astype('category')"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#split",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#split",
    "title": "Practical 11: Dimensions in Data",
    "section": "Split",
    "text": "Split\n\n\n\n\n\n\nDifficulty: Low!\n\n\n\n\n\n\n\n\n\nFor our purposes this is a little bit overkill as you could also use pandas’ sample(frac=0.2) and the indexes, but it’s useful to see how this works. You use test/train split to get four data sets out: the training data gives you two (predictors + target as separate data sets) and the testing data gives you two as well (predictors + target as separate data sets). These are sized accoridng to the test_size specfied in the test_train_split parameters.\nfrom sklearn.model_selection import train_test_split \n\npdf = gdf['Median-2011'].copy() # pdf for Median *P*rice b/c we need *something*\n\n# df == *data* frame (a.k.a. predictors or independent variables)\n# pr == *predicted* value (a.k.a. dependent variable)\n# Notice we don't want the median price included in our training data\ndf_train, df_test, pr_train, pr_test = train_test_split(\n                gdf.drop(columns=['Median-2011']), pdf, test_size=0.2, random_state=44)\nBelow you should see that the data has been split roughly on the basis of the test_size parameter.\nprint(f\"Original data size: {gdf.shape[0]:,} x {gdf.shape[1]}\")\nprint(f\"  Training data size: {df_train.shape[0]:,} x {df_train.shape[1]} ({(df_train.shape[0]/gdf.shape[0])*100:.0f}%)\")\nprint(f\"  Testing data size:  {df_test.shape[0]:,} x {df_test.shape[1]} ({(df_test.shape[0]/gdf.shape[0])*100:.0f}%)\")\nAlso notice the indexes of each pair of data sets match:\nprint(\", \".join([str(x) for x in df_train.index[:10]]))\nprint(\", \".join([str(x) for x in pr_train.index[:10]]))"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#plot-testtrain-data",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#plot-testtrain-data",
    "title": "Practical 11: Dimensions in Data",
    "section": "Plot Test/Train Data",
    "text": "Plot Test/Train Data\n\n\n\n\n\n\nDifficulty: Low, but important!\n\n\n\n\n\n\n\n\n\nboros = gpd.read_file(os.path.join('data','geo','Boroughs.gpkg'))\nf,axes = plt.subplots(1,2, figsize=(12,5))\ndf_train.plot(ax=???)\ndf_test.plot(ax=???)\nboros.plot(ax=???, facecolor='none', edgecolor='r', linewidth=.5, alpha=0.4)\nboros.plot(ax=???, facecolor='none', edgecolor='r', linewidth=.5, alpha=0.4)\naxes[0].set_title('Training Data')\naxes[1].set_title('Testing Data');\naxes[0].set_ylim(150000,210000)\naxes[1].set_ylim(150000,210000)\naxes[0].set_xlim(500000,565000)\naxes[1].set_xlim(500000,565000)\naxes[1].set_yticks([]);\n\nQuestions\n\nWhy might it be useful to produce a map of a test/train split?\nWhy might it matter more if you were dealing with user locations or behaviours?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#select-columns",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#select-columns",
    "title": "Practical 11: Dimensions in Data",
    "section": "Select Columns",
    "text": "Select Columns\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nOne thing you’ll need to explain is why I keep writing df[cols+['Subregion'] and why I don’t just add it to the cols variable at the start? Don’t try to answer this now, get through the rest of Tasks 3 and 4 and see what you think.\ncols = ['Tenure-Owned outright', 'Tenure-Owned with a mortgage or loan',\n        'Tenure-Social rented', 'Tenure-Private rented']\nAnswer: one part of the answer is that it makes it easy to change the columns we select without having to remember to keep Subregion, but the more important reason is that it allows us to re-use this ‘definition’ of cols elsewhere throughout the rest of this practical without needing to remember to remove Subregion.\ntr_raw  = df_train[cols+['Subregion']].copy() # train raw\ntst_raw = df_test[cols+['Subregion']].copy()  # test raw"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#fit-to-data",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#fit-to-data",
    "title": "Practical 11: Dimensions in Data",
    "section": "Fit to Data",
    "text": "Fit to Data\n\n\n\n\n\n\nDifficulty: Moderate if you want to understand what reshape is doing.\n\n\n\n\n\n\n\n\n\nFit the training data:\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Notice what this is doing! See if you can explain it clearly.\nscalers = [???.fit(???[x].values.reshape(-1,1)) for x in cols]"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#apply-transformations",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#apply-transformations",
    "title": "Practical 11: Dimensions in Data",
    "section": "Apply Transformations",
    "text": "Apply Transformations\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nTrain:\ntr_normed = tr_raw.copy()\n\nfor i, sc in enumerate(scalers):\n    # Ditto this -- can you explain what this code is doing\n    tr_normed[cols[i]] = sc.transform(df_???[cols[i]].values.reshape(-1,1))\nTest:\ntst_normed = tst_raw.copy()\n\nfor i, sc in enumerate(scalers):\n    tst_normed[cols[i]] = sc.transform(df_???[cols[i]].values.reshape(-1,1))\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: You don't _have_ to fully understand the next section, but if you are able to get your head around it then this will seriously help you to prepare for the use of more advanced techniques in modelling and programming.\n\n\nCheck out the properties of tst_normed below. If you’ve understood what the MinMaxScaler is doing then you should be able to spot something unexpected in the transformed test outputs. If you’ve really understood this, you’ll see why this result is problematic for models. Hint: one way to think of it is an issue of extrapolation.\nfor c in cols:\n    print(f\"  Minimum: {tst_normed[c].min():.4f}\")\n    ???"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions",
    "title": "Practical 11: Dimensions in Data",
    "section": "Plot Distributions",
    "text": "Plot Distributions\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\ntr_raw.columns     = [re.sub('(-|/)',\"\\n\",x) for x in tr_raw.columns.values]\ntst_raw.columns    = [re.sub('(-|/)',\"\\n\",x) for x in tst_raw.columns.values]\ntr_normed.columns  = [re.sub('(-|/)',\"\\n\",x) for x in tr_normed.columns.values]\ntst_normed.columns = [re.sub('(-|/)',\"\\n\",x) for x in tst_normed.columns.values]\nsns.pairplot(data=tr_raw, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);\nsns.pairplot(data=tr_normed, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);\n\nQuestions\n\nWhy do I keep writing df[cols+['Subregion']? Why I don’t just add Subregions to the cols variable at the start?\nWhat has changed between the two plots (of tr_raw and tr_normed)?\nWhat is the potential problem that the use of the transformer fitted on tr_normed to data from tst_normed might cause? Hint: this is why I asked you to investigate the data in the empty code cell above.\nCan you explain what this is doing: [MinMaxScaler().fit(df_train[x].values.reshape(-1,1)) for x in cols]?\nCan you explain what this is doing: sc.transform(df_test[cols[i]].values.reshape(-1,1))?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#z-score-standardisation",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#z-score-standardisation",
    "title": "Practical 11: Dimensions in Data",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nstsc = StandardScaler().fit(tr[col].values.reshape(-1,1))\n\ntr[f\"Z. {col}\"]  = stsc.transform(???)\ntst[f\"Z. {col}\"] = stsc.transform(???)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#inter-quartile-standardisation",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#inter-quartile-standardisation",
    "title": "Practical 11: Dimensions in Data",
    "section": "Inter-Quartile Standardisation",
    "text": "Inter-Quartile Standardisation\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nrs = ???(quantile_range=(25.0, 75.0)).fit(???)\n\ntr[f\"IQR. {col}\"] = rs.transform(???)\ntst[f\"IQR. {col}\"] = rs.transform(???)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions-1",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#plot-distributions-1",
    "title": "Practical 11: Dimensions in Data",
    "section": "Plot Distributions",
    "text": "Plot Distributions\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: The point of these next plots is simply to show that *linear* transformations (which are 'reversible') is about changing things like the magnitude/scale of our data but doesn't fundamentally change relationships *between* observations.\n\n\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nsns.jointplot(data=tr, x=f\"{col}\", y=f\"Z. {col}\", kind='kde'); # hex probably not the best choice\nsns.jointplot(data=tr, x=f\"{col}\", y=f\"IQR. {col}\", kind='kde'); # hex probably not the best choice\nsns.jointplot(data=tr, x=f\"Z. {col}\", y=f\"IQR. {col}\", kind='hex'); # hex probably not the best choice\nPerhaps a little more useful…\nax = sns.kdeplot(tr[f\"Z. {col}\"])\nsns.kdeplot(tr[f\"IQR. {col}\"], color='r', ax=ax)\nplt.legend(loc='upper right', labels=['Standard', 'Robust']) # title='Foo'\nax.ticklabel_format(useOffset=False, style='plain')\nax.set_xlabel(\"Standardised Value for No cars or vans in hh\")\n\nQuestions?\n\nCan you see the differences between these two rescalers?\nCan you explain why you might want to choose one over the other?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#the-normal-distribution",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#the-normal-distribution",
    "title": "Practical 11: Dimensions in Data",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nZ-scores are often associated with the normal distribution because their interpretation implicitly assumes a normal distribution. Or to put it another way… You can always calculate z-scores for your data (it’s just a formula applied to data points), but their intuitive meaning is lost if your data don’t have something like a normal distribution (or follow the 68-95-99.7 rule).\nBut… what if our data are non-normal? Well, Just because data are non-normal doesn’t mean z-scores can’t be calculated (we already did that above); we just have to be careful what we do with them… and sometimes we should just avoid them entirely.\n\nCreating a Normal Distribution\nBelow is a function to create that theoretical normal distribution. See if you can understand what’s going and add comments to the code to explain what each line does.\ndef normal_from_dist(series): \n    mu = ???          # mean of our data\n    sd = ???          # standard deviation of our data\n    n  = ???          # count how many observations are in our data\n    s = np.random.normal(???, ???, ???)   #use the parameters of the data just calculated to generate n random numbers, drawn from a normal distributions \n    return s                   #return this set of random numbers\nTo make it easier to understand what the function above is doing, let’s use it! We’ll use the function to plot both a distribution plot with both histogram and KDE for our data, and then add a second overplot distplot to the same fig showing the theoretical normal distribution (in red). We’ll do this in a loop for each of the three variables we want to examine.\n\n\nVisual Comparisons\nLooking at the output, which of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation most appropriate as measures of centrality and spread?\nAlso, how would you determine the meaning of some of the departures from the normal distribution?\nselection = [x for x in df_train.columns.values if x.startswith('Composition')]\n\nfor c in selection:\n    ax = sns.kdeplot(df_train[c])\n    sns.kdeplot(normal_from_dist(df_train[c]), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title='Foo'\n    ax.ticklabel_format(useOffset=False, style='plain')\n    if ax.get_xlim()[1] &gt; 999999:\n        plt.xticks(rotation=45)\n    plt.show()\n\n\nQuestions\n\nWhich, if any, of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation most appropriate as measures of centrality and spread?\nHow might you determine the significance of some of the departures from the normal distribution?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#logarithmic-transformations",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#logarithmic-transformations",
    "title": "Practical 11: Dimensions in Data",
    "section": "Logarithmic Transformations",
    "text": "Logarithmic Transformations\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nTo create a new series in the data frame containing the natural log of the original value it’s a similar process to what we’ve done before, but since pandas doesn’t provide a log-transform operator (i.e. you can’t call df['MedianIncome'].log() ) we need to use the numpy package since pandas data series are just numpy arrays with some fancy window dressing that makes them even more useful.\nLet’s perform the transform then compare to the un-transformed data. Comment the code below to ensure that you understand what it is doing.\n\nApply and Plot\ncols = ['Median-2012','Total Mean hh Income']\n\nfor m in cols:\n    s  = df_train[m] # s == series\n    ts = ???.???(s)   # ts == transformed series\n    \n    ax = sns.kdeplot(s)\n    sns.kdeplot(normal_from_dist(s), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title also an option\n    plt.title(\"Original Data\")\n    \n    ### USEFUL FORMATTING TRICKS ###\n    # This turns off scientific notation in the ticklabels\n    ax.ticklabel_format(useOffset=False, style='plain')\n    # Notice this snippet of code\n    ax.set_xlabel(ax.get_xlabel() + \" (Raw Distribution)\")\n    # Notice this little code snippet too\n    if ax.get_xlim()[1] &gt; 999999:\n        plt.xticks(rotation=45)\n    \n    plt.show()\n    \n    ax = sns.kdeplot(ts)\n    sns.kdeplot(normal_from_dist(ts), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n    ax.ticklabel_format(useOffset=False, style='plain')\n    ax.set_xlabel(ax.get_xlabel() + \" (Logged Distribution)\")\n    if ax.get_xlim()[1] &gt; 999999:\n        plt.xticks(rotation=45)\n    plt.title(\"Log-Transformed Data\")\n    plt.show()\nHopefully, you can see that the transformed data do indeed look ‘more normal’; the peak of the red and blue lines are closer together and the blue line at the lower extreme is also closer to the red line, but we can check this by seeing what has happened to the z-scores."
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#power-transformations",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#power-transformations",
    "title": "Practical 11: Dimensions in Data",
    "section": "Power Transformations",
    "text": "Power Transformations\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\ncols = ['Median-2012','Total Mean hh Income']\npt = ???(method='yeo-johnson')\n\nfor m in cols:\n    s  = df_train[m] # s == series\n    ts = pt.fit_transform(s.values.reshape(-1,1))\n    print(f\"Using lambda (transform 'exponent') of {pt.lambdas_[0]:0.5f}\")\n    \n    ax = sns.kdeplot(ts.reshape(-1,))\n    \n    sns.kdeplot(normal_from_dist(???), color='r', fill=True, ax=ax)\n    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n    ax.ticklabel_format(useOffset=False, style='plain')\n    ax.set_xlabel(m + \" (Transformed Distribution)\")\n    if ax.get_xlim()[1] &gt; 999999: # &lt;-- What does this do?\n        plt.xticks(rotation=45)\n    plt.title(\"Power-Transformed Data\")\n    plt.show();"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#calculating-shares",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#calculating-shares",
    "title": "Practical 11: Dimensions in Data",
    "section": "Calculating Shares",
    "text": "Calculating Shares\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nSadly, there’s no transformer to work this out for you automatically, but let’s start by converting the raw population and household figures to shares so that our later dimensionality reduction steps aren’t impacted by the size of the MSOA.\ngdf[['Age-All Ages','Households-All Households']].head(5)\n\nSpecify Totals Columns\ntotal_pop = gdf['Age-All Ages']\ntotal_hh  = gdf['Households-All Households']\ntotal_vec = gdf['Vehicles-Sum of all cars or vans in the area']\n\n\nSpecify Columns for Pop or HH Normalisation\npop_cols  = ['Age-', 'Composition-', 'Qualifications-', 'Economic Activity-', 'White', 'Mixed/multiple',\n             'Asian/Asian British', 'Black/African', 'BAME', 'Other ethnic',\n             'Country of Birth-']\nhh_cols   = [???, ???, ???, 'Detached', 'Semi-detached', 'Terraced', 'Flat, ']\npopre = re.compile(r'^(?:' + \"|\".join(pop_cols) + r')')\nhhre  = re.compile(r'^(?:' + \"|\".join(???) + r')')\n\n\nApply to Columns\ntr_gdf = gdf.copy()\ntr_gdf['Mean hh size'] = tr_gdf['Age-All Ages']/tr_gdf['Households-All Households']\n\nfor c in gdf.columns:\n    print(c)\n    if popre.match(c):\n        print(\"  Normalising by total population.\")\n        tr_gdf[c] = gdf[c]/???\n    elif ???.match(???):\n        print(\"  Normalising by total households.\")\n        tr_gdf[c] = gdf[c]/???\n    elif c.startswith('Vehicles-') and not c.startswith('Vehicles-Cars per hh'):\n        print(\"  Normalising by total vehicles.\")\n        tr_gdf[c] = gdf[c]/???\n    else:\n        print(\"  Passing through.\")"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#removing-columns",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#removing-columns",
    "title": "Practical 11: Dimensions in Data",
    "section": "Removing Columns",
    "text": "Removing Columns\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nTo perform dimensionality we can only have numeric data. In theory, categorical data can be converted to numeric and retained, but there are two issues:\n\nNominal data has no innate order so we can’t convert &gt; 2 categories to numbers and have to convert them to One-Hot Encoded values.\nA binary (i.e. One-Hot Encoded) variable will account for a lot of variance in the data because it’s only two values they are 0 and 1!\n\nSo in practice, it’s probably a good idea to drop categorical data if you’re planning to use PCA.\n\nDrop Totals Columns\npcadf = tr_gdf.drop(columns=['Age-All Ages', 'Households-All Households',\n                             'Vehicles-Sum of all cars or vans in the area'])\npcadf = pcadf.set_index('MSOA Code')\n\n\nDrop Non-Numeric Columns\npcadf.select_dtypes(['category','object']).columns\npcadf.drop(columns=pcadf.select_dtypes(['category','object']).columns.to_list(), inplace=True)\npcadf.drop(columns=['BNG_E','BNG_N','geometry', 'LONG', 'LAT','Shape__Are', 'Shape__Len'], inplace=True)\npcadf.columns"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#rescale-reduce",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#rescale-reduce",
    "title": "Practical 11: Dimensions in Data",
    "section": "Rescale & Reduce",
    "text": "Rescale & Reduce\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nIn order to ensure that our results aren’t dominated by a single scale (e.g. House Prices!) we need to rescale all of our data. You could easily try different scalers as well as a different parameters to see what effect this has on your results.\n\nRobustly Rescale\nSet up the Robust Rescaler for inter-decile standardisation: 10th and 90th quantiles.\nrs = ???\n\nfor c in pcadf.columns.values:\n    pcadf[c] = rs.fit_transform(pcadf[c].values.reshape(-1, 1))\n\n\nPCA Reduce\nfrom sklearn.decomposition import PCA \n\npca = PCA(n_components=50, whiten=True) \n\npca.fit(pcadf)\n\nexplained_variance = pca.explained_variance_ratio_\nsingular_values = pca.singular_values_\n\n\nExamine Explained Variance\nx = np.arange(1,???)\nplt.plot(x, explained_variance)\nplt.ylabel('Share of Variance Explained')\nplt.show()\nfor i in range(0, 20):\n    print(f\"Component {i:&gt;2} accounts for {explained_variance[i]*100:&gt;2.2f}% of variance\")\nYou should get that Component 0 accounts for 31.35% of the variance and Component 19 accounts for 0.37%.\n###: How Many Components?\nThere are a number of ways that we could set a threshold for dimensionality reduction: - The most common is to look for the ‘knee’ in the Explained Variance plot above. That would put us at about 5 retained components. - Another is to just keep all components contributing more than 1% of the variance. That would put us at about 10 components. - You can also (I discovered) look to shuffle the data and repeatedly perform PCA to build confidence intervals. I have not implemented this (yet).\nIn order to do anything with these components we need to somehow reattach them to the MSOAs. So that entails taking the transformed results (X_train and X_test)\nkn = knee_locator.KneeLocator(x, explained_variance, \n                              curve='convex', direction='decreasing', \n                              interp_method='interp1d')\nprint(f\"Knee detected at: {kn.knee}\")\nkn.plot_knee()\nkeep_n_components = 7\n\n# If we weren't changing the number of components we\n# could re-use the pca object created above. \npca = PCA(n_components=keep_n_components, whiten=True)\n\nX_train = pca.fit_transform(???)\n\n# Notice that we get the _same_ values out,\n# so this is a *deterministic* process that\n# is fully replicable (allowing for algorithmic\n# and programming language differences).\nprint(f\"Total explained variance: {pca.explained_variance_ratio_.sum()*100:2.2f}%\")\nfor i in range(0, keep_n_components):\n    print(f\"  Component {i:&gt;2} accounts for {pca.explained_variance_ratio_[i]*100:&gt;5.2f}% of variance\")\n\n# Notice...\nprint(f\"X-train shape: {len(X_train)}\")\nprint(f\"PCA df shape: {pcadf.shape[0]}\")\n# So each observation has a row in X_train and there is \n# 1 column for each component. This defines the mapping\n# of the original data space into the reduced one\nprint(f\"Row 0 of X-train contains {len(X_train[0])} elements.\")"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#components-to-columns",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#components-to-columns",
    "title": "Practical 11: Dimensions in Data",
    "section": "Components to Columns",
    "text": "Components to Columns\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nYou could actually do this more quickly (but less clearly) using X_train.T to transpose the matrix!\nfor i in range(0,keep_n_components):\n    s = pd.Series(X_train[:,???], index=pcadf.???)\n    pcadf[f\"Component {i+1}\"] = s\npcadf.sample(3).iloc[:,-10:-4]"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#reattaching-geodata",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#reattaching-geodata",
    "title": "Practical 11: Dimensions in Data",
    "section": "(Re)Attaching GeoData",
    "text": "(Re)Attaching GeoData\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nmsoas = gpd.read_file(os.path.join('data','geo','Middle_Layer_Super_Output_Areas__December_2011__EW_BGC_V2-shp.zip'), driver='ESRI Shapefile')\nmsoas = msoas.set_index('MSOA11CD')\nprint(msoas.columns)\nmsoas.head(1)\npcadf.head(1)\ngpcadf = pd.merge(msoas.set_index(['MSOA11CD'], drop=True), pcadf, left_index=True, right_index=True, how='inner')\nprint(f\"Geo-PCA df has shape {gpcadf.shape[0]} x {gpcadf.shape[1]}\")\nYou should get PCA df has shape 983 x 89.\ngpcadf['Borough'] = gpcadf.MSOA11NM.apply(???)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#map-the-first-n-components",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#map-the-first-n-components",
    "title": "Practical 11: Dimensions in Data",
    "section": "Map the First n Components",
    "text": "Map the First n Components\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nHow would you automate this so that the loop creates one plot for each of the first 3 components? How do you interpret these?\nfor comp in [f\"Component {x}\" for x in range(1,3)]:\n    ax = gpcadf.plot(column=???, cmap='plasma', \n         scheme='FisherJenks', k=7, edgecolor='None', legend=True, figsize=(9,7));\n    boros.plot(ax=ax, edgecolor='w', facecolor='none', linewidth=1, alpha=0.7)\n    ax.set_title(f'PCA {comp}')\nYour first component map should look something like this:\n\n\n\nPCA Component 1"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#umap-on-raw-data",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#umap-on-raw-data",
    "title": "Practical 11: Dimensions in Data",
    "section": "UMAP on Raw Data",
    "text": "UMAP on Raw Data\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nfrom umap import UMAP\n\n# You might want to experiment with all\n# 3 of these values -- it may make sense \n# to package a lot of this up into a function!\nkeep_dims=2\nrs=42\n\nu = UMAP(\n    n_neighbors=25,\n    min_dist=0.01,\n    n_components=keep_dims,\n    random_state=rs)\n\nX_embedded = u.fit_transform(???)\nprint(X_embedded.shape)"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#write-to-data-frame",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#write-to-data-frame",
    "title": "Practical 11: Dimensions in Data",
    "section": "Write to Data Frame",
    "text": "Write to Data Frame\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nCan probably also be solved using X_embedded.T.\nfor ix in range(0,X_embedded.shape[1]):\n    print(ix)\n    s = pd.Series(X_embedded[:,???], index=pcadf.???)\n    gpcadf[f\"Dimension {ix+1}\"] = s"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#visualise",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#visualise",
    "title": "Practical 11: Dimensions in Data",
    "section": "Visualise!",
    "text": "Visualise!\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nrddf = gpcadf.copy() # Reduced Dimension Data Frame\n\nSimple Scatter\nf,ax = plt.subplots(1,1,figsize=(8,6))\nsns.scatterplot(x=rddf[???], y=rddf[???], hue=rddf['Borough'], legend=False, ax=ax)\n\n\nSeaborn Jointplot\nThat is suggestive of there being struccture in the data, but with 983 data points and 33 colours it’s hard to make sense of what the structure might imply. Let’s try this again using the Subregion instead and taking advantage of the Seaborn visualisation library’s jointplot (joint distribution plot):\nrddf['Subregion'] = rddf.Borough.apply(lambda x: mapping[x])\n# Sets some handy 'keywords' to tweak the Seaborn plot\nkwds = dict(s=7,alpha=0.95,edgecolor=\"none\")\n# Set the *hue order* so that all plots have some colouring by Subregion\nho = ['Inner East','Inner West','Outer West and North West','Outer South','Outer East and North East']\ng = sns.jointplot(data=rddf, x=???, y=???, height=8, \n                  hue=???, hue_order=ho, joint_kws=kwds)\ng.ax_joint.legend(loc='upper right', prop={'size': 8});\nYour jointplot should look like this:\n\n\n\nUMAP Jointplot\n\n\nWhat do you make of this?\nMaybe let’s give this one last go splitting the plot out by subregion so that we can see how these vary:\nfor r in rddf.Subregion.unique():\n    g = sns.jointplot(data=rddf[rddf.Subregion==r], x='Dimension 1', y='Dimension 2', \n                  hue='Borough', joint_kws=kwds)\n    g.ax_joint.legend(loc='upper right', prop={'size': 8});\n    g.ax_joint.set_ylim(0,15)\n    g.ax_joint.set_xlim(0,15)\n    plt.suptitle(r)\nWe can’t unfortunately do any clustering at this point to create groups from the data (that’s next week!) so for now note that there are several large-ish groups (in terms of membership) and few small ones picked up by t-SNE. Alos note that there is strong evidence of some incipient structure: Inner East and West largely clump together, while Outher East and Outer South also seem to group together, with Outer West being more distinctive. If you look back at the PCA Components (especially #1) you might be able to speculate about some reasons for this! Please note: this is only speculation at this time!\nNext week we’ll also add the listings data back in as part of the picture!"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#map-the-n-dimensions",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#map-the-n-dimensions",
    "title": "Practical 11: Dimensions in Data",
    "section": "Map the n Dimensions",
    "text": "Map the n Dimensions\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nfor comp in [f\"Dimension {x}\" for x in range(1,3)]:\n    f, ax = plt.subplots(1,1,figsize=(12,8))\n    rddf.plot(???);\n    boros.plot(edgecolor='w', facecolor='none', linewidth=1, alpha=0.7, ax=ax)\n    ax.set_title(f'UMAP {comp}')\nYour first dimension map should look something like this:\n\n\n\nUMAP Dimension 1"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#and-save-1",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#and-save-1",
    "title": "Practical 11: Dimensions in Data",
    "section": "And Save",
    "text": "And Save\nrddf.to_parquet(os.path.join('data','clean','Reduced_Dimension_Data.geoparquet'))\n\nQuestions\n\nHow would you compare/contrast PCA components with UMAP dimensions? Why do they not seem to show the same thing even though both seem to show something?\nWhat might you do with the output of either the PCA or UMAP processes?"
  },
  {
    "objectID": "practicals/Practical-11-Dimensions_in_Data.html#credits",
    "href": "practicals/Practical-11-Dimensions_in_Data.html#credits",
    "title": "Practical 11: Dimensions in Data",
    "section": "Credits!",
    "text": "Credits!\n\nContributors:\nThe following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk).\n\n\nLicense\nThese teaching materials are licensed under a mix of The MIT License and the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license.\n\n\nPotential Dependencies:\nThis notebook may depend on the following libraries: pandas, geopandas, sklearn, matplotlib, seaborn"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html",
    "href": "practicals/Practical-08-Textual_Data.html",
    "title": "Practical 8: Working with Text",
    "section": "",
    "text": "A lot of the content here is provided to help you understand what text-cleaning does and how it generates tokens that can be processed by the various analytical approaches commonly-used in NLP. The best way to think about this is as a practical in two parts, with a bonus ‘Part 2’ that you should not expect to complete unless you probably shouldn’t be taking CASA0013 in the first place:\nThe later parts in Part 2 are largely complete and ready to run; however, that doesn’t mean you should just skip over them and think you’ve grasped what’s happening and it will be easy to apply in your own analyses. I would not pay as much attention to LDA topic mining since I don’t think it’s results are that good, but I’ve included it here as it’s still commonly-used in the Digital Humanities and by Marketing folks. Word2Vec is much more powerful and forms the basis of the kinds of advances seen in ChatGPT and other LLMs."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#required-modules",
    "href": "practicals/Practical-08-Textual_Data.html#required-modules",
    "title": "Practical 8: Working with Text",
    "section": "Required Modules",
    "text": "Required Modules\n\n\n\n\n\n\nNote\n\n\n\nNotice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: sklearn, nltk, and spacy.\n\n\nStandard libraries we’ve seen before.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport re\nimport math\nimport matplotlib.pyplot as plt\n\nThis next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:\n\nfrom IPython.display import display_markdown\n\ndef as_markdown(head='', body='Some body text'):\n    if head != '':\n        display_markdown(f\"##### {head}\\n\\n&gt;{body}\\n\", raw=True)\n    else:\n        display_markdown(f\"&gt;{body}\\n\", raw=True)\n\nas_markdown('Result!', \"Here's my output...\")\n\nResult!\n\nHere’s my output…"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#loading-data",
    "href": "practicals/Practical-08-Textual_Data.html#loading-data",
    "title": "Practical 8: Working with Text",
    "section": "Loading Data",
    "text": "Loading Data\n\n\n\n\n\n\n🔗 Connections\n\n\n\nBecause I generally want each practical to stand on its own (unless I’m trying to make a point), I’ve not moved this to a separate Python file (e.g. utils.py, but in line with what we covered back in the lectures on Functions and Packages, this sort of thing is a good candidate for being split out to a separate file to simplify re-use.\n\n\nRemember this function from last week? We use it to save downloading files that we already have stored locally. But notice I’ve made some small changes… what do these do to help the user?\n\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -&gt; str:\n    \"\"\"Downloads and caches a remote file locally.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dest : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[-1] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    # Check if dest+filename does *not* exist -- \n    # that would mean we have to download it! We\n    # also check for *very* small files that are \n    # likely to represent an incomplete download.\n    if not os.path.isfile(dfn) or os.stat(dfn).st_size &lt; 250:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        # Convert the path back into a list (without)\n        # the filename -- we need to check that directories\n        # exist first.\n        path = os.path.split(dest)\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how\n        # to use the 'splat' operator! os.makedirs creates missing\n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        # Download and write the file\n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(response.content)\n            \n        print(\"\\tDone downloading...\")\n\n        # What's this doing???\n        f_size = os.stat(dfn).st_size\n        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n        # And why is it here as well???\n        f_size = os.stat(dfn).st_size\n        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n        \n    return dfn\n\n\n\n\n\n\n\nTip\n\n\n\nFor very large non-geographic data sets, remember that you can use_cols (or columns depending on the file type) to specify a subset of columns to load.\n\n\nLoad the main data set:\n\nymd  = '20240614'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.geoparquet'\n\n\ngdf = gpd.read_parquet( cache_data(url, os.path.join('data','geo')), \n                      columns=['geometry', 'listing_url', 'name', \n                               'description', 'amenities', 'price'])\n\ngdf = gdf.to_crs('epsg:27700')\n\nprint(f\"gdf has {gdf.shape[0]:,} rows and CRS is {gdf.crs.name}.\")\n\nFound data/geo/20240614-London-listings.geoparquet locally!\n    Size is 42 MB (44,000,824 bytes)\ngdf has 85,127 rows and CRS is OSGB36 / British National Grid.\n\n\nLoad supporting Geopackages:\n\nddir  = os.path.join('data','geo') # destination directory\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\n\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n\nFound data/geo/Boroughs.gpkg locally!\n    Size is 1 MB (905,216 bytes)\nFound data/geo/Water.gpkg locally!\n    Size is 0 MB (208,896 bytes)\nFound data/geo/Greenspace.gpkg locally!\n    Size is 1 MB (1,146,880 bytes)\nDone."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#the-description-field",
    "href": "practicals/Practical-08-Textual_Data.html#the-description-field",
    "title": "Practical 8: Working with Text",
    "section": "The Description Field",
    "text": "The Description Field\n\n\n\n\n\n\nDifficulty level: Moderate, because of the questions.\n\n\n\n\n\n\n\n\n\nTo explore the description field properly you’ll need to filter out any NA/NaN descriptions before sampling the result. Hint: you’ll need to think about negation (~) of a method output that tells you if a field is NA.\n\nQuestion\n\ngdf[???].sample(5, random_state=42)[['description']]\n\n\n\n\n\n\n\nStop\n\n\n\nWhat do you notice about the above? Are they simple text? Are there patterns of problems? Are there characters that represent things other than words and simple punctuation?\n\n\n\n\nQuestions\n\nWhat patterns can you see that might need ‘dealing with’ for text-mining to work?\nWhat non-text characters can you see? (Things other than A-Z, a-z, and simple punctuation!)"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#the-amenities-field",
    "href": "practicals/Practical-08-Textual_Data.html#the-amenities-field",
    "title": "Practical 8: Working with Text",
    "section": "The Amenities Field",
    "text": "The Amenities Field\n\n\n\n\n\n\nDifficulty level: Moderate, because of the questions.\n\n\n\n\n\n\n\n\n\nThis field presents a subtle issue that might not be obvious here:\n\ngdf.amenities.sample(5, random_state=42)\n\n17638    [\"Free dryer \\u2013 In unit\", \"Hot water kettle\", \"Shampoo\", \"Luggage dropoff allowed\", \"Microwave\", \"Dining table\", \"Bed linens\", \"Hot water\", \"Iron\", \"Host greets you\", \"First aid kit\", \"Garden view\", \"Wine glasses\", \"Outdoor furniture\", \"Porta...\n84107    [\"Bathtub\", \"Clothing storage: closet\", \"Portable heater\", \"Microwave\", \"Hot water\", \"Host greets you\", \"First aid kit\", \"Security cameras on property\", \"Washer\", \"Fire extinguisher\", \"Freezer\", \"Lock on bedroom door\", \"Refrigerator\", \"Room-darke...\n53166                                                        [\"Hot water\", \"Iron\", \"Outdoor dining area\", \"Lock on bedroom door\", \"Wifi\", \"Shampoo\", \"Luggage dropoff allowed\", \"Smoke alarm\", \"Hangers\", \"Pets allowed\", \"Essentials\", \"Long term stays allowed\"]\n72159    [\"Free dryer \\u2013 In unit\", \"Clothing storage: closet\", \"Hot water kettle\", \"Shampoo\", \"Microwave\", \"Self check-in\", \"Dining table\", \"Bed linens\", \"Hot water\", \"Iron\", \"Conditioner\", \"Wine glasses\", \"Washer\", \"Coffee maker: Nespresso\", \"Inducti...\n84441    [\"Bathtub\", \"Clothing storage: closet\", \"Elevator\", \"Hot water\", \"Carbon monoxide alarm\", \"Central heating\", \"Paid street parking off premises\", \"Single oven\", \"Wifi\", \"Free washer \\u2013 In unit\", \"Refrigerator\", \"Smoke alarm\", \"Kitchen\", \"Dryer...\nName: amenities, dtype: object\n\n\nBut look what happens now, can you see the issue a little more easily?\n\ngdf.amenities.iloc[0]\n\n'[\"Heating\", \"TV with standard cable\", \"Wifi\", \"Smoke alarm\", \"Dryer\", \"Kitchen\", \"Washer\", \"Essentials\"]'\n\n\n\nQuestions\n\nWhat’s the implicit format of the Amenities columns?\nHow could you represent the data contained in the column?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#remove-nan-values",
    "href": "practicals/Practical-08-Textual_Data.html#remove-nan-values",
    "title": "Practical 8: Working with Text",
    "section": "Remove NaN Values",
    "text": "Remove NaN Values\n\n\n\n\n\n\nNote\n\n\n\nI would be wary of doing the below in a ‘proper’ application without doing some careful research first, but to make our lives easier, we’re going to drop rows where one of these values is NaN now so it will simplify the steps below. In reality, I would spend quite a bit more time investigating which values are NaN and why before simply dropping them.\n\n\nAnyway, drop all rows where either the description or amenities (or both) are NA:\n\nQuestion\n\ngdf = gdf.dropna(???)\nprint(f\"Now gdf has {gdf.shape[0]:,} rows.\")\n\nYou should get that there are 84,266 rows."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#luxury-listings",
    "href": "practicals/Practical-08-Textual_Data.html#luxury-listings",
    "title": "Practical 8: Working with Text",
    "section": "Luxury Listings",
    "text": "Luxury Listings\n\n\n\n\n\n\nDifficulty level: Hard, because of the regular expression and questions.\n\n\n\n\n\n\n\n\n\nI would like you to find listings that might (on the basis of word choice) indicate ‘luxury’ accommodation.\n\nCreate the Regular Expression\nYou should start with variations on ‘luxury’ (i.e. luxurious, luxuriate, …) and work out a single regular expression that works for variations on this one word. Later, I would encourage you to come back to this and consider what other words might help to signal ‘luxury’… perhaps words like ‘stunning’ or ‘prestigious’? Could you add those to the regex as well?\nHints: this is a toughy, but…\n\nAll regular expressions work best using the r'...' (which means raw string) syntax.\nYou need to be able to group terms. Recall, however, that in Python a ‘group’ of the form r'(some text)' refers to matching (some text will be ‘memoized’/remembered), whereas what you need here is a “non-capturing group” of the positive lookahead type. That’s a Google clue right there, but you’ve also seen this in the lecture.\n\nIn fact, in my real-world applications you might even need more than one group/non-capturing group in a nested structure.\n\nQuestion\n\ngdf[\n    gdf.description.str.contains(r'???', regex=True, flags=re.IGNORECASE) # &lt;-- The regex\n].sample(3, random_state=42)[['description']]\n\n\n\n\nApply it to Select Data\nAssign it to a new data frame called lux:\n\nQuestion\n\nlux = gdf[gdf.description.str.contains(r'???', regex=True, flags=re.IGNORECASE)].copy()\nlux.to_parquet(os.path.join('data','clean','luxury.geopackage'))\nprint(f\"Found {lux.shape[0]:,} records for 'luxury' flats\")\n\nYou should get that there are 10,367 rows.\n\n\n\nPlot the Data\nNow we are going to create a more complex plot that will give space to both the spatial and price distributions using subplot2grid.\nhelp(plt.subplot2grid)\nNotice that there are two ways to create the plot specified above. I chose route 1, but in some ways route 2 (where you specify a gridspec object and then add the axes might be a bit simpler to work out if you’re starting from scratch.\nThe critical thing here is to understand how we’er initialising a plot that has 4 rows and 1 column even though it is only showing 2 plots. What we’re going to do is set the first plot to span 3 rows so that it takes up 75% of the plot area (3/4), while the second plot only takes up 25% (1/4). They will appear one above the other, so there’s only 1 column. Here’s how to read the key parts of subplot2grid:\n\nnrows – how many rows of plots in the figure.\nncols – how many columns of plots in the figure.\nrow – what row of the figure does this plot start on (0-indexed like a list in Python).\ncol – what column of the figure does this plot start on (0-indexed like a list in Python).\nrowspan – how many rows of the figure does this plot span (not 0-indexed because it’s not list-like).\ncolspan – how many columns of the figure does this plot span (not 0-indexed because it’s not list-like).\n\nEvery time you call subplot2grid you are initialising a new axis-object into which you can then draw with your geopackage or pandas plotting methods.\n\nQuestion\n\n\nf,ax = plt.subplots(1,1,figsize=(9,6))\nax.remove()\n\n# The first plot \nax1 = plt.subplot2grid((4, 1), (???), rowspan=???)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nlux.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((???), (???), rowspan=1)\nlux.price.plot.hist(bins=250, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Luxury\") # &lt;-- How does this differ from title? Change it and see!\nplt.tight_layout() # &lt;-- Try creating the plot *without* this to see what it changes\nplt.show()\n\nYour result should look similar to:\n\n\n\n\n\n‘Luxury’ listings in London\n\n\n\n\n\n\nQuestion\n\nWhat does suptitle do and how is it different from title? Could you use this as part of your plot-making process?\nWhat does tight_layout do?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#budget-listings",
    "href": "practicals/Practical-08-Textual_Data.html#budget-listings",
    "title": "Practical 8: Working with Text",
    "section": "Budget Listings",
    "text": "Budget Listings\n\n\n\n\n\n\nDifficulty level: Easy, because you’ve worked out the hard bits already.\n\n\n\n\n\n\n\n\n\n\nCreate the Regular Expression\nWhat words can you think of that might help you to spot affordable and budget accommodation? Start with just a couple of words and then I would encourage you to consider what other words might help to signal ‘affordability’… perhaps words like ‘cosy’ or ‘charming’ and then think about how you could you add those to the regex?\nHints: this just builds on what you did above with one exception:\n\nI’d try adding word boundary markers to the regex (\\b) where appropriate…\n\n\nQuestion\n\ngdf[\n    gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)\n].sample(5, random_state=42)[['description']]\n\n\n\n\nApply it to Select Data\n\nQuestion\n\naff = gdf[gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)].copy()\naff.to_parquet(os.path.join('data','clean','affordable.geopackage'))\nprint(f\"There are {aff.shape[0]:,} rows flagged as 'affordable'.\")\n\nYou should get that there are 8,937 rows.\n\n\n\nPlot the Data\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\naff.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\naff.price.plot.hist(bins=100, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Affordability\")\nplt.tight_layout()\n#plt.savefig(\"Affordable_Listings.png\", dpi=150)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nDo you think that this is a good way to select affordable options?\nDo you understand what dpi means and how savefig works?\nCopy the code from above but modify it to constrain the histogram on a more limited distribution by filtering out the outliers before drawing the plot. I would copy the cell above to one just below here so that you keep a working copy available and can undo any changes that break things."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#near-bluespace",
    "href": "practicals/Practical-08-Textual_Data.html#near-bluespace",
    "title": "Practical 8: Working with Text",
    "section": "Near Bluespace",
    "text": "Near Bluespace\n\n\n\n\n\n\nDifficulty level: Medium, because you’re still learning about regexes.\n\n\n\n\n\n\n\n\n\nNow see if you can work out a regular expression to find accommodation that emphasises accessibility to the Thames and other ‘blue spaces’ as part of the description? One thing you’ll need to tackle is that some listings seem to say something about Thameslink and you wouldn’t want those be returned as part of a regex looking for rivers. So by way of a hint:\n\nYou probably need to think about the Thames, rivers, and water.\nThese will probably be followed by a qualifier like a ‘view’ (e.g. Thames-view) or a front (e.g. water-front).\nBut you need to rule out things like “close the Thameslink station…”\n\n\nCreate the regular Expression\n\nQuestion\n\ngdf[\n    gdf.description.str.contains(???, regex=???, flags=???)\n].sample(5, random_state=42)[['description']]\n\n\n\n\nApply it to the Select Data\n\nQuestion\n\nbluesp = gdf[\n    (gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)) |\n    (gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE))\n].copy()\nbluesp.to_parquet(os.path.join('data','clean','bluespace.geopackage'))\nprint(f\"Found {bluesp.shape[0]:,} rows.\")\n\nYou should get that there are 408 rows.\n\n\n\nPlot the Data\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nwater.plot(edgecolor='none', facecolor=(.25, .25, .7, .25), ax=ax1)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nbluesp.plot(markersize=2, column='price', cmap='viridis', alpha=0.5, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\nbluesp.price.plot.hist(bins=100, ax=ax2)\n\nplt.suptitle(\"Bluespace Listings\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nHow else might you select listings with a view of the Thames or other bluespaces?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#resources",
    "href": "practicals/Practical-08-Textual_Data.html#resources",
    "title": "Practical 8: Working with Text",
    "section": "Resources",
    "text": "Resources\nThere is a lot more information out there, including a whole book and your standard O’Reilly text.\nAnd some more useful links:\n\nPandas String Contains Method\nUsing Regular Expressions with Pandas\nSummarising Chapters from Frankenstein using TF/IDF"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html",
    "href": "extra/Live-03-Foundations_2.html",
    "title": "Task 0: LoLs and DoLs",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 3: Foundations (Part 2)&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with Dictionaries, LOLs and DOLs, Packages and Functions&lt;/h2&gt;\nCome to class prepared to present/discuss:"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-1-reading-a-remote-file",
    "href": "extra/Live-03-Foundations_2.html#task-1-reading-a-remote-file",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 1: Reading a Remote File",
    "text": "Task 1: Reading a Remote File\n\nTask 1.1: Research Solution\n\nGoogle: read remote CSV file Python\nOooh look, Stack Overflow link\nLet’s review this–note: Python2 vs Python3\n\nLook at dates on answers to see if reasonably useful\nRead responses to what looks like useful answer\nWhich answer was accepted?\n\nOK, so it looks like urllib and csv will be useful.\n\nHow do we work out what’s possible?\nHow do we read help for a function?\n\n\n\nfrom urllib.request import urlopen\n?urlopen\n\n\n\nTask 1.2: Implementing Solution\nOK, so now we know what to do, how do we do it?\n\nSet a url variable\nCapture the response\nRead it, what’s the decoding thing?\nLook at the ‘.’ cropping up: we’ll deal with that later.\n\n\nfrom urllib.request import urlopen\n\n# Given the info you were given above, what do you \n# think the value of 'url' should be? What\n# type of variable is it? int or string? \nurl = 'https://github.com/jreades/fsds/raw/master/data/src/2022-sample-Crime.csv'\n\n# Read the URL stream into variable called 'response'\n# using the function that we imported above\nresponse = urlopen(url)\n\n# Now read from the stream, decoding so that we get actual text\ndatafile = response.read().decode('utf-8')\n\n# You might want to explore what `__class__` and `__name__`\n# offer, but basically the give us a way of finding out what\n# is 'behind' more complex variables\nprint(\"datafile variable is of type: '\" + datafile.__class__.__name__ + \"'.\\n\")\n\ndatafile variable is of type: 'str'.\n\n\n\n\n\nTask 1.3: Checking Solution\nIt’s tempting to just print out the contents of datafile, but what should we do? - Print out the size of the variable (how do we do this for a string?) - Print out some of the top of the file (how do we do this for the first n chars in a string?)\n\nprint(len(datafile))\nprint(datafile[:600])\n\n3809403\nID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location\n11570600,JB377499,08/03/2018 01:16:00 PM,046XX S ST LAWRENCE AVE,2024,NARCOTICS,POSSESS - HEROIN (WHITE),VEHICLE NON-COMMERCIAL,true,false,0221,002,4,38,18,,,2018,09/10/2022 04:50:59 PM,,,\n12457866,JE332953,08/10/2021 04:20:00 PM,016XX W VAN BUREN ST,2018,NARCOTICS,MANUFACTURE / DELIVER - SYNTHETIC DRUGS,VEHICLE NON-COMMERCIAL,true,false,1231,012,27,28,18,,,2021,09/10/2022 04:50:59 PM,,,\n128"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-2-parsing-a-csv-file-using-a-package",
    "href": "extra/Live-03-Foundations_2.html#task-2-parsing-a-csv-file-using-a-package",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 2: Parsing a CSV file using a Package",
    "text": "Task 2: Parsing a CSV file using a Package\nRight, so we’ve got our data in datafile, what are we going to do with it now?\n\nTask 2.1: Research Solution\n\nWe need to turn it into data by reading the CSV\n\nGoogle what to do\nHmmm, this looks useful,\nMaybe also try read csv file python example\n\nOK, so it looks like we need to splitlines first.\nThen let’s read it into a list (What data structure is this?)\nNow, how would we print out the number of rows and columns?\n\n\nimport csv \n\nurlData = [] # Somewhere to store the data\n\ncsvfile = csv.reader(datafile.splitlines())\n\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(f\"urlData has {len(urlData)} rows and {len(urlData[0])} columns.\")\n\nurlData has 16956 rows and 22 columns.\n\n\n\n\nTask 2.2: Selecting a Sample Row\nHow could we print out a random row? Let’s Google it. - Library random seems promising - Which function do we want?\n\nimport random\ndir(random)\nhelp(random.randint)\n\nHelp on method randint in module random:\n\nrandint(a, b) method of random.Random instance\n    Return random integer in range [a, b], including both end points.\n\n\n\n\nurlData[random.randint(1,len(urlData))]\n\n['12812099',\n 'JF377432',\n '05/27/2022 12:00:00 AM',\n '063XX S EBERHART AVE',\n '1153',\n 'DECEPTIVE PRACTICE',\n 'FINANCIAL IDENTITY THEFT OVER $ 300',\n '',\n 'false',\n 'false',\n '0312',\n '003',\n '20',\n '42',\n '11',\n '',\n '',\n '2022',\n '10/07/2022 04:46:51 PM',\n '',\n '',\n '']"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-3-reading-file-as-dictionary-of-lists",
    "href": "extra/Live-03-Foundations_2.html#task-3-reading-file-as-dictionary-of-lists",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 3: Reading File as Dictionary of Lists",
    "text": "Task 3: Reading File as Dictionary of Lists\n\nTask 3.1: Finding the Header Row\nSometimes this is easy (it’s the very first row in a CSV file), but often (especially with Excel data from, e.g., the Office for National Statistics) it’s not. So here is where doing some quick, initial checks using head can be helpful.\n\nurlData[0]\n\n['ID',\n 'Case Number',\n 'Date',\n 'Block',\n 'IUCR',\n 'Primary Type',\n 'Description',\n 'Location Description',\n 'Arrest',\n 'Domestic',\n 'Beat',\n 'District',\n 'Ward',\n 'Community Area',\n 'FBI Code',\n 'X Coordinate',\n 'Y Coordinate',\n 'Year',\n 'Updated On',\n 'Latitude',\n 'Longitude',\n 'Location']\n\n\nOK, some questions: - What does row 0 give us? Are these data, or something else? - If we were making a dictionary-of-lists, how would we use Row 0? - What do we need to do to set this up?\n\n\nTask 3.2: Creating a DOL from Data\nHow would we use the header row to initialise our Dictionary-of-Lists.\n\nds = {}\ncol_names = urlData[0]\nfor c in col_names:\n    ds[c] = []\n\nNext… - How would we print out all of the column names? - How would we go about adding all of the data? - What kind of loop would this use?\n\nprint(ds.keys())\n\ndict_keys(['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude', 'Location'])\n\n\n\nfor r in urlData[1:len(urlData)]:\n    for c in range(0,len(col_names)):\n        ds[ col_names[c] ].append( r[c] )\n\n\n\nTask 3.3: Validating/Checking\nLet’s check a few columns to see if the data makes sense!\n\nprint(ds['Case Number'][:20])\nprint()\nprint(ds['Primary Type'][:20])\nprint()\n\n['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989', 'JF413871', 'JF380003', 'JF378902', 'JF415101', 'JF379014', 'JF378985', 'JF415451', 'JF415893', 'JF378620', 'JF380820', 'JF380344', 'JF380066', 'JF378882', 'JF379811', 'JF381657']\n\n['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n\n\n\nActually, is there a better way to do this? Rather than printing each out in turn, could we do this as a loop?\n\nfor c in ds.keys():\n    print(ds[c][:5])\n\n['11570600', '12457866', '12842196', '12842170', '12842271']\n['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989']\n['08/03/2018 01:16:00 PM', '08/10/2021 04:20:00 PM', '03/01/2020 12:00:00 AM', '02/28/2018 09:00:00 AM', '01/01/2014 08:00:00 PM']\n['046XX S ST LAWRENCE AVE', '016XX W VAN BUREN ST', '069XX S CALUMET AVE', '128XX S LOWE AVE', '035XX S STATE ST']\n['2024', '2018', '1153', '1153', '1153']\n['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n['POSSESS - HEROIN (WHITE)', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300']\n['VEHICLE NON-COMMERCIAL', 'VEHICLE NON-COMMERCIAL', 'RESIDENCE', 'RESIDENCE', 'CTA TRAIN']\n['true', 'true', 'false', 'false', 'false']\n['false', 'false', 'false', 'false', 'false']\n['0221', '1231', '0322', '0523', '0213']\n['002', '012', '003', '005', '002']\n['4', '27', '6', '9', '3']\n['38', '28', '69', '53', '35']\n['18', '18', '11', '11', '11']\n['', '', '', '', '']\n['', '', '', '', '']\n['2018', '2021', '2020', '2018', '2014']\n['09/10/2022 04:50:59 PM', '09/10/2022 04:50:59 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM']\n['', '', '', '', '']\n['', '', '', '', '']\n['', '', '', '', '']\n\n\nHow would we improve this?\n\nfor c in ds.keys():\n    print(f\"{c}:\\t{ds[c][:5]}\")\n    print()\n\nID: ['11570600', '12457866', '12842196', '12842170', '12842271']\n\nCase Number:    ['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989']\n\nDate:   ['08/03/2018 01:16:00 PM', '08/10/2021 04:20:00 PM', '03/01/2020 12:00:00 AM', '02/28/2018 09:00:00 AM', '01/01/2014 08:00:00 PM']\n\nBlock:  ['046XX S ST LAWRENCE AVE', '016XX W VAN BUREN ST', '069XX S CALUMET AVE', '128XX S LOWE AVE', '035XX S STATE ST']\n\nIUCR:   ['2024', '2018', '1153', '1153', '1153']\n\nPrimary Type:   ['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n\nDescription:    ['POSSESS - HEROIN (WHITE)', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300']\n\nLocation Description:   ['VEHICLE NON-COMMERCIAL', 'VEHICLE NON-COMMERCIAL', 'RESIDENCE', 'RESIDENCE', 'CTA TRAIN']\n\nArrest: ['true', 'true', 'false', 'false', 'false']\n\nDomestic:   ['false', 'false', 'false', 'false', 'false']\n\nBeat:   ['0221', '1231', '0322', '0523', '0213']\n\nDistrict:   ['002', '012', '003', '005', '002']\n\nWard:   ['4', '27', '6', '9', '3']\n\nCommunity Area: ['38', '28', '69', '53', '35']\n\nFBI Code:   ['18', '18', '11', '11', '11']\n\nX Coordinate:   ['', '', '', '', '']\n\nY Coordinate:   ['', '', '', '', '']\n\nYear:   ['2018', '2021', '2020', '2018', '2014']\n\nUpdated On: ['09/10/2022 04:50:59 PM', '09/10/2022 04:50:59 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM']\n\nLatitude:   ['', '', '', '', '']\n\nLongitude:  ['', '', '', '', '']\n\nLocation:   ['', '', '', '', '']"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-4-fixing-column-types",
    "href": "extra/Live-03-Foundations_2.html#task-4-fixing-column-types",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 4: Fixing Column Types",
    "text": "Task 4: Fixing Column Types\nOK, so we have a few columns that aren’t really of the right type. We have date-time types in Python that we’re not going to get stuck into now, but we also very obviously have numbers and booleans as well that we need to deal with!\nSo how we would do this? The process for each float would be the same. The process for each int would be the same. The process for each boolean would be the same. Sounds like a good opportunity for a function!\n\nTask 4.1: What Type Am I?\nGiven these data… What Python data type should each one be?\n\nID:\nCase Number:\nDate:\nPrimary Type:\nDescription:\nLocation Description:\nArrest:\nDomestic:\nYear:\nLatitude:\nLongitude:\n\n\n\nTask 4.2 Converting One Column\n\ndef to_bool(col_data):\n    fdata = []\n    for c in col_data:\n        fdata.append( c=='true' )\n    return fdata\n\nbool_cols = ['Arrest','Domestic']\nfor b in bool_cols:\n    ds[ b ] = to_bool( ds[b] )\n\n\nds['Arrest'][:6]\n\n[True, True, False, False, False, False]\n\n\n\nimport numpy as np\n\n\narrest_made = list(np.where(ds['Arrest']))[0]\n\narrest_reasons = set()\n\nfor a in arrest_made:\n    arrest_reasons.add(ds['Description'][a])\n\nprint(arrest_reasons)\nprint()\nprint(\", \".join([i.title() for i in sorted(arrest_reasons)]))\n\n{'POSSESS - CANNABIS MORE THAN 30 GRAMS', 'THEFT OF LABOR / SERVICES', 'MANUFACTURE / DELIVER - CANNABIS OVER 10 GRAMS', 'POSSESS - HALLUCINOGENS', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'MANUFACTURE / DELIVER - BARBITURATES', 'THEFT OF LOST / MISLAID PROPERTY', 'POSSESS - HEROIN (TAN / BROWN TAR)', 'POSSESS - COCAINE', 'MANUFACTURE / DELIVER - HEROIN (TAN / BROWN TAR)', 'POSSESSION OF DRUG EQUIPMENT', 'BOGUS CHECK', 'CRIMINAL DRUG CONSPIRACY', 'POSSESS - PCP', 'ILLEGAL USE CASH CARD', 'MANUFACTURE / DELIVER -  HEROIN (WHITE)', 'MANUFACTURE / DELIVER - HEROIN (BLACK TAR)', 'SOLICIT NARCOTICS ON PUBLIC WAY', 'POSSESS - SYNTHETIC DRUGS', 'MANUFACTURE / DELIVER - COCAINE', 'FRAUD OR CONFIDENCE GAME', 'POSSESS - BARBITURATES', 'THEFT BY LESSEE, MOTOR VEHICLE', 'MANUFACTURE / DELIVER - CRACK', 'ILLEGAL POSSESSION CASH CARD', 'MANUFACTURE / DELIVER - AMPHETAMINES', 'FORGERY', 'FOUND SUSPECT NARCOTICS', 'POSSESS - AMPHETAMINES', 'STOLEN PROPERTY BUY / RECEIVE / POSSESS', 'COUNTERFEIT CHECK', 'CREDIT CARD FRAUD', 'EMBEZZLEMENT', 'POSSESS - HEROIN (WHITE)', 'FORFEIT PROPERTY', 'COUNTERFEITING DOCUMENT', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'POSSESS - CANNABIS 30 GRAMS OR LESS', 'POSSESS - HEROIN (BLACK TAR)', 'DECEPTIVE COLLECTION PRACTICES', 'ATTEMPT - FINANCIAL IDENTITY THEFT', 'MANUFACTURE / DELIVER - HALLUCINOGEN', 'MANUFACTURE / DELIVER - PCP', 'POSSESS - HYPODERMIC NEEDLE', 'IMPERSONATION', 'POSSESS - CRACK', 'MANUFACTURE / DELIVER - CANNABIS 10 GRAMS OR LESS', 'MANUFACTURE / DELIVER - METHAMPHETAMINE', 'FINANCIAL EXPLOITATION OF AN ELDERLY OR DISABLED PERSON', 'UNAUTHORIZED VIDEOTAPING', 'SALE / DELIVER - DRUG PARAPHERNALIA', 'ATTEMPT POSSESSION NARCOTICS', 'POSSESS - METHAMPHETAMINE', 'ALTER / FORGE PRESCRIPTION'}\n\nAlter / Forge Prescription, Attempt - Financial Identity Theft, Attempt Possession Narcotics, Bogus Check, Counterfeit Check, Counterfeiting Document, Credit Card Fraud, Criminal Drug Conspiracy, Deceptive Collection Practices, Embezzlement, Financial Exploitation Of An Elderly Or Disabled Person, Financial Identity Theft Over $ 300, Forfeit Property, Forgery, Found Suspect Narcotics, Fraud Or Confidence Game, Illegal Possession Cash Card, Illegal Use Cash Card, Impersonation, Manufacture / Deliver -  Heroin (White), Manufacture / Deliver - Amphetamines, Manufacture / Deliver - Barbiturates, Manufacture / Deliver - Cannabis 10 Grams Or Less, Manufacture / Deliver - Cannabis Over 10 Grams, Manufacture / Deliver - Cocaine, Manufacture / Deliver - Crack, Manufacture / Deliver - Hallucinogen, Manufacture / Deliver - Heroin (Black Tar), Manufacture / Deliver - Heroin (Tan / Brown Tar), Manufacture / Deliver - Methamphetamine, Manufacture / Deliver - Pcp, Manufacture / Deliver - Synthetic Drugs, Possess - Amphetamines, Possess - Barbiturates, Possess - Cannabis 30 Grams Or Less, Possess - Cannabis More Than 30 Grams, Possess - Cocaine, Possess - Crack, Possess - Hallucinogens, Possess - Heroin (Black Tar), Possess - Heroin (Tan / Brown Tar), Possess - Heroin (White), Possess - Hypodermic Needle, Possess - Methamphetamine, Possess - Pcp, Possess - Synthetic Drugs, Possession Of Drug Equipment, Sale / Deliver - Drug Paraphernalia, Solicit Narcotics On Public Way, Stolen Property Buy / Receive / Possess, Theft By Lessee, Motor Vehicle, Theft Of Labor / Services, Theft Of Lost / Mislaid Property, Unauthorized Videotaping\n\n\n\n\nTask 4.3: Converting Another Column\nFloats first!\n\nimport numpy as np\n\ndef to_float(col_data):\n    fdata = []\n    for c in col_data:\n        try:\n            fdata.append( float(c) )\n        except (ValueError, TypeError):\n            fdata.append( np.nan )\n    return fdata\n\nfloat_cols = ['Latitude','Longitude']\nfor f in float_cols:\n    ds[ f ] = to_float( ds[f] )\n\n\nds['Latitude'][:6]\n\n[nan, nan, nan, nan, nan, nan]\n\n\n\nprint(f\"Have found {len(list(np.where(~np.isnan(ds['Latitude'])))[0]):,} records with lat/long coordinates\")\n\nHave found 9,801 records with lat/long coordinates\n\n\n\nfor i in list(np.where(~np.isnan(ds['Latitude'])))[0][:15]:\n    print(ds['Latitude'][i])\n\n41.885739108\n41.893676531\n41.885931086\n41.786188054\n41.788414065\n41.701933304\n41.976290414\n41.772998982\n41.881857098\n41.722597651\n41.80184271\n41.763268313\n41.986788545\n41.793924823\n41.894327846\n\n\nThen booleans!\nThen ints! (Drop if time is short)\n\ndef to_int(col_data):\n    fdata = []\n    for c in col_data:\n        fdata.append( int(c) )\n    return fdata\n\nint_cols = ['ID','Year']\nfor i in int_cols:\n    ds[ i ] = to_int( ds[i] )\n\n\nds['Year'][:6]\n\n[2018, 2021, 2020, 2018, 2014, 2020]\n\n\n\nprint(f\"There are {len(np.where(np.asarray(ds['Arrest']))[0]):,} arrests\")\nprint(f\"There are {len(set(np.where(np.asarray(ds['Year']) &gt; 2021)[0])):,} records from 2022 onwards\")\n\nThere are 4,108 arrests\nThere are 13,845 records from 2022 onwards\n\n\n\nintersect = set(np.where(np.asarray(ds['Arrest']))[0]).intersection(set(np.where(np.asarray(ds['Year']) &gt; 2021)[0]))\nprint(f\"There are {len(intersect):,} records that are both.\")\n\nThere are 3,058 records that are both."
  },
  {
    "objectID": "lectures/4.2-Decorators.html#a-basic-function",
    "href": "lectures/4.2-Decorators.html#a-basic-function",
    "title": "Decorators",
    "section": "A Basic Function",
    "text": "A Basic Function\n\ndef hello():\n  return('hello world')\n\nprint(hello())\n\nhello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#modifying-the-function",
    "href": "lectures/4.2-Decorators.html#modifying-the-function",
    "title": "Decorators",
    "section": "Modifying the Function",
    "text": "Modifying the Function\nLet’s get meta:\n\ndef better():\n  def hello():\n    print(\"  + Defining hello!\")\n    return('hello world')\n  print(\"+ Calling hello...\")\n  rv = hello().title() + \"!\"\n  print(f\"+ hello returned {rv}\")\n  return(rv)\n\nbetter()\n\n+ Calling hello...\n  + Defining hello!\n+ hello returned Hello World!\n\n\n'Hello World!'"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#decorating-the-function",
    "href": "lectures/4.2-Decorators.html#decorating-the-function",
    "title": "Decorators",
    "section": "Decorating the Function",
    "text": "Decorating the Function\n\ndef better(func):\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())\n\nHello World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#wtf",
    "href": "lectures/4.2-Decorators.html#wtf",
    "title": "Decorators",
    "section": "WTF?",
    "text": "WTF?\nHow did that happen?\n\ndef better(func):\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#the-decorator-function",
    "href": "lectures/4.2-Decorators.html#the-decorator-function",
    "title": "Decorators",
    "section": "The Decorator Function",
    "text": "The Decorator Function\nLet’s unpack the wrapper:\n\ndef better(func):\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#reusing-a-decorator",
    "href": "lectures/4.2-Decorators.html#reusing-a-decorator",
    "title": "Decorators",
    "section": "Reusing a Decorator",
    "text": "Reusing a Decorator\nEverything’s ‘better’ now:\n\n@better\ndef goodbye():\n  return('GooDBye world')\n\nprint(goodbye())\n\nGoodbye World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#but",
    "href": "lectures/4.2-Decorators.html#but",
    "title": "Decorators",
    "section": "But…",
    "text": "But…\nBut this:\n\n@better\ndef bad_func():\n  return(2)\n\nprint(bad_func())\n\nWill trigger this:\n      2 def wrapper():\n----&gt; 3   return func().title() + \"!\"\n\nAttributeError: 'int' object has no attribute 'title'"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#chaining-decorators",
    "href": "lectures/4.2-Decorators.html#chaining-decorators",
    "title": "Decorators",
    "section": "Chaining Decorators1",
    "text": "Chaining Decorators1\n\ndef splitter(func):\n  def wrapper():\n    return func().split()\n  return wrapper\n\n@splitter\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())\n\n['Hello', 'World!']\n\n\nDatacamp"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#using-functiontools",
    "href": "lectures/4.2-Decorators.html#using-functiontools",
    "title": "Decorators",
    "section": "Using Func(tion)Tools",
    "text": "Using Func(tion)Tools\nAnd there are decorators for decorators…\n\nfrom functools import wraps\ndef better(func):\n  @wraps(func)\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  return('hello world')\n\nprint(hello())\n\nHello World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#unpicking-functools",
    "href": "lectures/4.2-Decorators.html#unpicking-functools",
    "title": "Decorators",
    "section": "Unpicking Functools",
    "text": "Unpicking Functools\n\nfrom functools import wraps\ndef better(func):\n  @wraps(func)\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n..."
  },
  {
    "objectID": "lectures/4.2-Decorators.html#making-use-of-metadata",
    "href": "lectures/4.2-Decorators.html#making-use-of-metadata",
    "title": "Decorators",
    "section": "Making Use of Metadata",
    "text": "Making Use of Metadata\nCompare:\n\n\n\ndef better(func):\n  '''Better formatting of a string'''\n  def wrapper():\n    '''Wraps a function to format it.'''\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  '''Prints hello world'''\n  return('hello world')\n\nprint(hello.__name__)\nprint(hello.__doc__)\n\nwrapper\nWraps a function to format it.\n\n\n\n\nfrom functools import wraps\ndef better(func):\n  @wraps(func)\n  def wrapper():\n    return func().title() + \"!\"\n  return wrapper\n\n@better\ndef hello():\n  '''Prints hello world'''\n  return('hello world')\n\nprint(hello.__name__)\nprint(hello.__doc__)\n\nhello\nPrints hello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#some-applications-1",
    "href": "lectures/4.2-Decorators.html#some-applications-1",
    "title": "Decorators",
    "section": "Some Applications 11",
    "text": "Some Applications 11\n\ndef simple_logger(func):\n  def wrapper(*args, **kwargs):\n    print(f\"+ Executing '{func.__name__}' with args: {args}\")\n    result = func(*args, **kwargs)\n    print(f\"  + Result is: {result}\")\n    return result\n  return wrapper\n\n@simple_logger\ndef add(a,b):\n  return a+b\n\nadd(2,5)\n\n+ Executing 'add' with args: (2, 5)\n  + Result is: 7\n\n\n7\n\n\nKDNuggets: 8 Built-in Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#some-applications-2",
    "href": "lectures/4.2-Decorators.html#some-applications-2",
    "title": "Decorators",
    "section": "Some Applications 21",
    "text": "Some Applications 21\n\nimport atexit\n\n# Register the exit_handler function\n@atexit.register\ndef exit_handler():\n    print(\"Exiting the program. Cleanup tasks can be performed here.\")\n\n# Rest of the program\ndef main():\n    print(\"Inside the main function.\")\n    # Your program logic goes here.\n\nif __name__ == \"__main__\":\n    main()\n\nThis would output:\nInside the main function.\nExiting the program. Cleanup tasks can be performed here.\nKDNuggets: 8 Built-in Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#benefits-of-decorators",
    "href": "lectures/4.2-Decorators.html#benefits-of-decorators",
    "title": "Decorators",
    "section": "Benefits of Decorators1",
    "text": "Benefits of Decorators1\n\nCode readability: each function is more narrowly focussed on the ‘thing’ it’s supposed to do, without extraneous validation, logging, or authentication ‘cruft’.\nCode reuse: keep your code ‘DRY’ (Don’t Repeat Yourself) by applying the same code across multiple functions (e.g. log the arguments this function received)\nModification without alteration: extending the behaviour of something else (e.g. you can’t/don’t want to modify someone else’s code, but need some additional step to be performend)\nLogging made simple: add/remove debugging and logging functionality quickly, easily, and consistently.\nDevelopment: Python web frameworks (Django, Flask) use decorators to handle requests.\nError handling: manage error-handling ‘centrally’ by placing try and except around every function you want to manage.\n\n5 Key Benefits of Using Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#theres-more",
    "href": "lectures/4.2-Decorators.html#theres-more",
    "title": "Decorators",
    "section": "There’s More…",
    "text": "There’s More…\nWe’ve barely scratched the surface, decorators can:\n\nTake arguments (which might alter the behavour of the wrapped function).\nHelp to make classes and methods more useful (see the Methods and Classes lectures).\nManage common tasks like authorisation and permissions.\n\nThere’s lots, lost more, but using decorators effectively will seriously impress anyone interviewing you for a job while also helping you to understand a lot more about good programming!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#resources",
    "href": "lectures/4.2-Decorators.html#resources",
    "title": "Decorators",
    "section": "Resources",
    "text": "Resources\n\n\n\nSpecial /ht to @wraps Explained in 30 seconds\nPython Decorators Explained\nPython Tips: Decorators\nPython Decorators: How to Use it and Why?\n5 Key Benefits of Using Python Decorators for Optimized COding Practices\n\n\n\nPython’s Most Powerful Decorator\nChaining Decorators\nDecorators and Generators in Python\nAvailable Standard Decorators in Python\nAwesome Python Decorators\n\n\n\n\n\n\nDecorators • Jon Reades"
  },
  {
    "objectID": "sessions/week8.html",
    "href": "sessions/week8.html",
    "title": "Textual Data",
    "section": "",
    "text": "Although the direct use of textual (both structured and unstructured) data is still relatively rare in spatial analyses, the growth of crowd-sourced and user-generated content points to the growing importance of this area. he tools and approaches in this area are also evolving quickly and changing rapidly, so this week is intended primarily to familiarise you with the basic landscape in preparation for you developing your skills further in your own time!\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nAn awareness of the benefits of separating content from presentation.\nA basic understanding of pattern-matching in Python (you will have been exposed to this Week 2 of CASA0005)\nA basic understanding of how text can be ‘cleaned’ to make it more amenable for analysis\nAn appreciation of parallelisation in the context of text processing.\nAn appreciation of how text can be analysed.\n\n\n\n\nThe manipulation of text requires a high level of abstraction – of thinking about words as data in ways that are deeply counter-intuitive – but the ability to do forms a critical bridge between this block and the subsequent one, while also reinforcing the idea that numerical, spatial, and textual data analyses provide alternative (and often complementary) views into the data.",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#overview",
    "href": "sessions/week8.html#overview",
    "title": "Textual Data",
    "section": "",
    "text": "Although the direct use of textual (both structured and unstructured) data is still relatively rare in spatial analyses, the growth of crowd-sourced and user-generated content points to the growing importance of this area. he tools and approaches in this area are also evolving quickly and changing rapidly, so this week is intended primarily to familiarise you with the basic landscape in preparation for you developing your skills further in your own time!\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nAn awareness of the benefits of separating content from presentation.\nA basic understanding of pattern-matching in Python (you will have been exposed to this Week 2 of CASA0005)\nA basic understanding of how text can be ‘cleaned’ to make it more amenable for analysis\nAn appreciation of parallelisation in the context of text processing.\nAn appreciation of how text can be analysed.\n\n\n\n\nThe manipulation of text requires a high level of abstraction – of thinking about words as data in ways that are deeply counter-intuitive – but the ability to do forms a critical bridge between this block and the subsequent one, while also reinforcing the idea that numerical, spatial, and textual data analyses provide alternative (and often complementary) views into the data.",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#preparatory-lectures",
    "href": "sessions/week8.html#preparatory-lectures",
    "title": "Textual Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nNotebooks as Documents\nVideo\nSlides\n\n\nPatterns in Text\nVideo\nSlides\n\n\nCleaning Text\nVideo\nSlides\n\n\nAnalysing Text\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#other-preparation",
    "href": "sessions/week8.html#other-preparation",
    "title": "Textual Data",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nMiller and Goodchild (2015)\nURL\nN/A\n\n\nDelmelle and Nilsson (2021)\nURL\nN/A\n\n\nReades et al. (in review)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nConceptually, this is by far the hardest week of the entire term: there is very little upon which to draw from other modules, and the processing of text with computers rarely makes it beyond simple regular expressions; however, the growth in data that is ‘accidental, open, and everywhere’ (Arribas-Bel 2014) means that a lot more of it is unstructured and contains free-text written by humans as well as numerical and coordinate data generated by sensors or transactions.\n\n\nIf you’re feeling ambitious then you can use the tutorial from the Programming Historian to look at the foundations of text processing and how we can extract important terms from a document as well as, ultimately, the foundations upon which modern Large Language Models are built.",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#practical",
    "href": "sessions/week8.html#practical",
    "title": "Textual Data",
    "section": "Practical",
    "text": "Practical\nIn the practical we will continue to work with the InsideAirbnb data, here focussing on the third ‘class’ of data in the data set: text. We will see how working with text is more complex than working with numeric or spatial data and, consequently, why the computational costs rise accordingly. This practical should suggest some new lines of inquiry for Group Project.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nApplying simple regular expressions to find patterns in text.\nHow to clean text in preparation for further analysis.\nSimple transformations that allow you to analyse text (e.g. TF/IDF)\nWays of exploring groups/similarity in textual data.\n\n\n\nTo access the practical:\n\nPreview\nDownload\n\nBonus material (not necessary for the assessment, just ‘nice to know’ if you’re interested in the topic) containing material related to Natural Language Processing (NLP):\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "8. Textual Data"
    ]
  },
  {
    "objectID": "lectures/5.5-Assessments.html#organise-your-group",
    "href": "lectures/5.5-Assessments.html#organise-your-group",
    "title": "Assessments",
    "section": "Organise (Your Group)",
    "text": "Organise (Your Group)\nI am working to allocate everyone to a group of at least four so:\n\nIf you are in a group of less than four you may be partnered with students in another practical session to make a ‘full strength’ group.\nIn this case, you will be reallocated to a mutually convenient practical session so that you are not penalised by being unable to work together.\nI would encourage you to start sitting and working together, and for one of you to set up and share a private GitHub repo for Assessment #2 (more on this in a second).\n\nAnd remember: talk about how you want your group to work! See: Group Working talk.1\nNote: In my experience overly narrow specialisation does not work."
  },
  {
    "objectID": "lectures/5.5-Assessments.html#assessment-1",
    "href": "lectures/5.5-Assessments.html#assessment-1",
    "title": "Assessments",
    "section": "Assessment #1",
    "text": "Assessment #1\n1. What is it?\nTimed, Open Book Exam\n2. What does ‘timed’ mean?\nOnce you start the assessment you will have a fixed amount of time in which to complete it; 2 hours for most students, but students with relevant SORAs have 3 hours (please email me).\nOk, so when is it?\nFriday, 22 November 2024\nWhat is the window?\n9-11am or 11-1pm1\nThis is the ‘window’ for doing the assessment. You can start any time after the window opens but must finish before the window closes."
  },
  {
    "objectID": "lectures/5.5-Assessments.html#assessment-2",
    "href": "lectures/5.5-Assessments.html#assessment-2",
    "title": "Assessments",
    "section": "Assessment #2",
    "text": "Assessment #2\nThe reproducible analysis must be a runnable QMD (Quarto Markdown Document) file that addresses the set questions provided in class. The QMD file will be assessed on two components:\n\nIts content (60% of this assessment): do the answers written by the group engage through a mix of literature, critical thinking, and data analysis with the set questions?\nIts reproducibility (40% of this assessment): do the analyses employed, and outputs created by the group run fully and without errors on a different computer, and do they show evidence of thought in relation to the quality of coding and outputs?\n\nA template has been provided. You can see both PDF and HTML output, but please only submit the PDF!"
  },
  {
    "objectID": "lectures/5.5-Assessments.html#assessment-3",
    "href": "lectures/5.5-Assessments.html#assessment-3",
    "title": "Assessments",
    "section": "Assessment #3",
    "text": "Assessment #3\nWe know that people contribute differently to groups or, sometimes, not at all. This self- and peer-assessment seeks to quantify that contribution while also prompting you to reflect on what you contributed to your group. This assessment also has two components:\n\nA free-text self-assessment: this component is formative but may be used to moderate the summative component in the event of a complaint or disagreement.\nA Likert-scale (1–6) peer-assessment: this component is summative and you will be asked to rank other members of the group on their contribution to the group project together with a short free-text justification of that score. The free-text feedback will not be shared with the other group members, but may be used to moderate the summative component in the event of a compalint or disagreement."
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Selecting Data",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nYou have formalised your understanding of how to link data in Python.\nYou are working on the group project.",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#overview",
    "href": "sessions/week9.html#overview",
    "title": "Selecting Data",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nYou have formalised your understanding of how to link data in Python.\nYou are working on the group project.",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#preparatory-lectures",
    "href": "sessions/week9.html#preparatory-lectures",
    "title": "Selecting Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nYou are strongly advised to watch these videos on linking data and spatial data, as well how to group data within pandas; however, you will not be asked to present any of these because our attention is now on the final assessments. As well, you should by now be familiar with the concept of how to join data from the GIS module (CASA0005), so this focusses on two things: 1) how to do this in Python (with a bit of SQL thrown in); and 2) how to approach this process with large or mismatched data sets.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLinking Data\nVideo\nSlides\n\n\nLinking Spatial Data\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#other-preparation",
    "href": "sessions/week9.html#other-preparation",
    "title": "Selecting Data",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nElwood and Wilson (2017)\nURL\nN/A\n\n\nO’Sullivan and Manson (2015)\nURL\nN/A\n\n\nXie (2024)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nHere we focus on what you can now bring to the table that might help you to dinstinguish yourself from someone who did a ‘data science degree’; through what we study here (and in your other modules) you have been exposed to ways of thinking about data critically and ethically that are rarely part of an Informatics or Machine Learning degree. But as we hope you’re now conviced: these things matter. It’s not just that being critical and ethical is a good way to do your job (whatever that might end up being), it’s that being critical and ethical is a good way to do your job better. You will writing better code. You will write better assessments. You will draw better conclusions.",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week9.html#practical",
    "href": "sessions/week9.html#practical",
    "title": "Selecting Data",
    "section": "Practical",
    "text": "Practical\nThe practical will lead you through the selection of data in pandas and the equivalent using SQL via DuckDB.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nComparing different approaches to data linkage\nLinking data as part of a visualisation process.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "9. Selecting Data"
    ]
  },
  {
    "objectID": "sessions/week6.html",
    "href": "sessions/week6.html",
    "title": "Numeric Data",
    "section": "",
    "text": "This week we will be introducing the use of the pandas library for data analysis and management through a focus on numeric data and its distribution(s). This marks a major shift from working with concepts (lists, dictionaries, functions, etc.) largely in isolation to encountering all of them together ‘in the wild’ as part of a full data science workflow. So we are moving from the acquisition of concepts to their integration in the same way that we will — over the course of these three sessions — be coming from data acquisition to data integration.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn appreciation of how and why this module differs from (QM) CASA0007.\nThe beginnings of a more integrative understanding of foundational computer science concepts and the practice(s) of data science.\nA basic understanding of data acquisition and manipulation in Python.",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#overview",
    "href": "sessions/week6.html#overview",
    "title": "Numeric Data",
    "section": "",
    "text": "This week we will be introducing the use of the pandas library for data analysis and management through a focus on numeric data and its distribution(s). This marks a major shift from working with concepts (lists, dictionaries, functions, etc.) largely in isolation to encountering all of them together ‘in the wild’ as part of a full data science workflow. So we are moving from the acquisition of concepts to their integration in the same way that we will — over the course of these three sessions — be coming from data acquisition to data integration.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn appreciation of how and why this module differs from (QM) CASA0007.\nThe beginnings of a more integrative understanding of foundational computer science concepts and the practice(s) of data science.\nA basic understanding of data acquisition and manipulation in Python.",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#preparatory-lectures",
    "href": "sessions/week6.html#preparatory-lectures",
    "title": "Numeric Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nLogic\nVideo\nSlides\n\n\nRandomness\nVideo\nSlides\n\n\nData\nVideo\nSlides\n\n\nPandas\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#other-preparation",
    "href": "sessions/week6.html#other-preparation",
    "title": "Numeric Data",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nAnderson (2008)\nURL\nN/A\n\n\nD’Ignazio and Klein (2020a) Ch.4\nURL\nN/A\n\n\nCox and Slee (2016)\nURL\nN/A\n\n\nAmoore (2019)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nRead D’Ignazio and Klein (2020b) to highlight the importance of thinking about what a data set captures… and what it excludes. Cox and Slee (2016) is also a first introduction to the underlying data that we’ll be working with over the rest of term. D’Ignazio and Klein (2020b) should be getting you thinking about how ‘cleaning’ is not just about hygiene but has real implications for what you can and will find in your data: data is often a lot less ‘tidy’ than tidyr might lead you think! You should almost never be claiming that your (social) data represents the ‘universe’ of behaviours or is somehow ‘complete’.",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week6.html#practical",
    "href": "sessions/week6.html#practical",
    "title": "Numeric Data",
    "section": "Practical",
    "text": "Practical\nIn this practical we will begin working with the InsideAirbnb data, which you will have briefly examined in CASA0005. This week we focus on the first ‘class’ of data in the data set: simple numeric columns. We will see how to use Pandas for (simple) visualisation and (the beginnings of) analysis. It is hoped that you will see how Pandas combines and builds on techniques that we’ve already seen: while Pandas is incredibly sophisticated, the underlying concepts have been covered in the preceding three weeks! At this point we will also begin to make use of Pandas functionality to subset and explore the data.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nSeeing how Pandas is ‘just’ a sophisticated extension of what we’ve already done.\nFamiliarising yourself with Pandas functionality.\nPerforming basic data cleaning and exploration tasks (including visualisation).\nSelecting and aggregating data in pandas.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "6. Numeric Data"
    ]
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Reduce, Reuse, Recycle",
    "section": "",
    "text": "This week we will look at how frequently-used code can be packaged up in functions and libraries. This is the point at which we begin to engage with code in a more abstract way because we are increasingly interested in reusability and flexibility.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTo develop an understanding of code re-use through functions.\nTo develop an appreciation of the utility of packages and namespaces.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#overview",
    "href": "sessions/week4.html#overview",
    "title": "Reduce, Reuse, Recycle",
    "section": "",
    "text": "This week we will look at how frequently-used code can be packaged up in functions and libraries. This is the point at which we begin to engage with code in a more abstract way because we are increasingly interested in reusability and flexibility.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTo develop an understanding of code re-use through functions.\nTo develop an appreciation of the utility of packages and namespaces.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#preparatory-lectures",
    "href": "sessions/week4.html#preparatory-lectures",
    "title": "Reduce, Reuse, Recycle",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nFunctions\nVideo\nSlides\n\n\nDecorators\nVideo\nSlides\n\n\nPackages\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#other-preparation",
    "href": "sessions/week4.html#other-preparation",
    "title": "Reduce, Reuse, Recycle",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nMassey (1996)\nURL\nSummary\n\n\nD’Ignazio and Klein (2020) Ch.3\nURL\nSummary\n\n\nSmith (2010)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nThese are two of the more challenging readings this term, but they are critical to understanding what we are trying to teach you: it’s not just about learning to code, it’s about learning how to deploy code/quantitative methods to support your argument, while maintaining a keen eye on how bias – in both the technical and the ethical senses – can creep into your thinking and, consequently, your results!",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "sessions/week4.html#practical",
    "href": "sessions/week4.html#practical",
    "title": "Reduce, Reuse, Recycle",
    "section": "Practical",
    "text": "Practical\nThis week’s practical will be looking at how functions (and variables) can be collected into resuable packages that we can either make ourselves or draw on a worldwide bank of experts – I know who I’d rather depend on when the opportunity arises! However, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling the following sessions:\n\nFunctions\nPackages\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nSeeing how functions and decorators can help us to reuse code efficiently.\nBeginning to make use of packages to access/interact with data.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "4. Efficient Code"
    ]
  },
  {
    "objectID": "assessments/peer.html",
    "href": "assessments/peer.html",
    "title": "Group Self-Evaluation",
    "section": "",
    "text": "This individual reflection and peer mark assessment (10%) due Friday, 20 December 2024 (@18:00) asks you to reflect on the process of ‘doing data science’ as part of a team, since this format is typical of real-world projects. Many companies (e.g. Apple, Google, etc.) employ an Agile project format in which teams undertake a ‘Retrospective’ at the end of a project in order to identify ways to improve how they work in the future. We are not asking you to do this as a group (indeed, it’s an individual reflection), but we hope that this will help you to develop as a budding analyst or data scientist.",
    "crumbs": [
      "Elements",
      "Group Self-Evaluation"
    ]
  },
  {
    "objectID": "assessments/peer.html#format",
    "href": "assessments/peer.html#format",
    "title": "Group Self-Evaluation",
    "section": "Format",
    "text": "Format\nYou must answer these 2 questions for formative purposes as part of the submission process and prior to submitting the peer marks:\n\nWhat factors do you think help to explain what went well/not well in the group work? (100 words maximum)\nHow do you think the other students in your group would evaluate your contribution to each of these outcomes? (250 words maximum)\n\nYou will then be asked to score the rest of the group on their contribution to the project. You should be doing this with an eye both to contributions that may be more or less visible (e.g. someone who wrote the code may be more visible than the person who ensured that the code was answering the set question) as well as to the fact that you’re much more likely to be aware of your own contributions than you are of the contributions of others.\nThe guidance for this is:\n\nThinking about the many ways — visible and less visible — that a student can contribute to the success of a group project, how would you rate Student B’s contribution on a Scale of 1–6, where 1 means that they did not contribute to the group’s submission, and 6 means that their contribution was critical to the group’s submission.\n\nJustify this score with reference to the guidance provided in class (and linked to here, here, and here).",
    "crumbs": [
      "Elements",
      "Group Self-Evaluation"
    ]
  },
  {
    "objectID": "assessments/peer.html#restrictions",
    "href": "assessments/peer.html#restrictions",
    "title": "Group Self-Evaluation",
    "section": "Restrictions",
    "text": "Restrictions\nThere is no word limit for the formative parts of the assessment, but as an indication ca. 125 words should be enough for a brief, thoughtful engagement. Together with the GitHub repo history, the formative component is the principal source of evidence we will take into account in the event of discrepancies in the group self-evaluation (aka. peer mark).",
    "crumbs": [
      "Elements",
      "Group Self-Evaluation"
    ]
  },
  {
    "objectID": "sessions/week12.html",
    "href": "sessions/week12.html",
    "title": "Grouping Data",
    "section": "",
    "text": "This session is ‘supplemental’, meaning that it is here to help you integrate ideas seen across Term 1 (and which will be encountered again in Term 2) in a way that supports long-term learning. It is not essential to passing the course and there are no ‘bonus points’ for using methods found in this session.\n\nThis week we will be looking at various ways of grouping data, whether it is by variable or by algorithm. So we begin by covering how data can be aggregated in Python using Pandas before turning to the practical challenges of classification (labeled data) and clustering (unlabeled data).\nWe hare now ‘completing’ the pipeline begun in Week 5 using the concepts introduced in Weeks 1–4, but if you remember your ‘epicycles of analysis’ then you’ll realise that this is, at best, a first pass through the data science process and there are multiple places where insights derived from the practicals (on outliers/problematic records, on data quality issues, on data selection, etc.) could be fed back through the pipeline to adjust and improve the analytical outputs.\nWe will also be shifting our focus in the live session to the final parts of the group submission, but you should also be looking at how this module connects and integrates ideas covered in CASA0001 (UST), CASA0005 (GIS), and CASA0007 (QM). So there will be only a minimal live-coding session in order to leave as much time as possible for the groups to meet and start working on their final projects.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn understanding of the differences between aggregation, classification, and clustering.\nAn appreciation of the utility of deriving grouped variables and proxies from raw data.\nAn appreciation of how clustering as part of an analytical pipeline differs from the material covered in CASA0007 and so enhances our understanding of ‘paradigms’ in CASA0001.\nA general appreciation of how different clustering algorithms work and how this differs from classifcation.",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#overview",
    "href": "sessions/week12.html#overview",
    "title": "Grouping Data",
    "section": "",
    "text": "This session is ‘supplemental’, meaning that it is here to help you integrate ideas seen across Term 1 (and which will be encountered again in Term 2) in a way that supports long-term learning. It is not essential to passing the course and there are no ‘bonus points’ for using methods found in this session.\n\nThis week we will be looking at various ways of grouping data, whether it is by variable or by algorithm. So we begin by covering how data can be aggregated in Python using Pandas before turning to the practical challenges of classification (labeled data) and clustering (unlabeled data).\nWe hare now ‘completing’ the pipeline begun in Week 5 using the concepts introduced in Weeks 1–4, but if you remember your ‘epicycles of analysis’ then you’ll realise that this is, at best, a first pass through the data science process and there are multiple places where insights derived from the practicals (on outliers/problematic records, on data quality issues, on data selection, etc.) could be fed back through the pipeline to adjust and improve the analytical outputs.\nWe will also be shifting our focus in the live session to the final parts of the group submission, but you should also be looking at how this module connects and integrates ideas covered in CASA0001 (UST), CASA0005 (GIS), and CASA0007 (QM). So there will be only a minimal live-coding session in order to leave as much time as possible for the groups to meet and start working on their final projects.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nAn understanding of the differences between aggregation, classification, and clustering.\nAn appreciation of the utility of deriving grouped variables and proxies from raw data.\nAn appreciation of how clustering as part of an analytical pipeline differs from the material covered in CASA0007 and so enhances our understanding of ‘paradigms’ in CASA0001.\nA general appreciation of how different clustering algorithms work and how this differs from classifcation.",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#preparatory-lectures",
    "href": "sessions/week12.html#preparatory-lectures",
    "title": "Grouping Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nYou should, by now, be familiar with the concept of how to cluster data from the QM module (CASA0007), so this week is actually focussed on how to move beyond k-means. The point is to contextualise these approaches as part of a data science ‘pipeline’ and to contrast to them with the more theoretical aspects covered elsewhere. We are less interested in the mathematical and technical aspects, and more interested in how one might go about selecting the appropriate algorithm for a particular problem.\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nGrouping Data\nVideo\nSlides\n\n\nClassification\nVideo\nSlides\n\n\nClustering\nVideo\nSlides\n\n\nClustering and Geography\nVideo\nSlides",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#other-preparation",
    "href": "sessions/week12.html#other-preparation",
    "title": "Grouping Data",
    "section": "Other Preparation",
    "text": "Other Preparation\n\n\n\n\n\n\nConnections\n\n\n\nWe’re trying to move between technical and critical representations of data and methods – showing (again) how all data analysis represents a series of choices about what matters. Ultimately, it’s up to us whether we make these consciously or unconsciously: being a ‘critical’ (spatial) data scientist positions us to question the data constructively to ensure that it is ‘fit for purpose’ – that it is appropriate and adequate to the the processes or behaviours that we wish to study – be it for profit, policy, and public engagement.\n\n\n\nThe follow readings may be useful for reflecting on the topics covered in this session:\n\nBadger, Bui, and Gebeloff (2019) &lt;URL&gt;\nShapiro and Yavuz (2017) &lt;URL&gt;",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week12.html#practical",
    "href": "sessions/week12.html#practical",
    "title": "Grouping Data",
    "section": "Practical",
    "text": "Practical\nThe previous week has set up nicely for approaching aggregation, classification, and clustering as functions of the (transformed and reduced) data space. With this, you have essentially covered a full data science analytical pipeline from start (setting up) to finish (cluster/classification analysis) and can hopefully see how these pieces fit together to support one another, and how there is no such thing as a ‘right’ way to approach an analysis… but that there are better and worse ways.\nNote that, while you should be trying to advance your understanding of clustering and classification in Python, these final practicals are also a very good time to be working on your group project. So look at whether the techniques covered this week can help (or distract) you on this work and adjust the time given accordingly.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nHow to group and aggregate data.\nThe connections between classification and clustering.\nThe use of classification as a predictive process with labeled data.\nThe choice of k in k-means and extraction of representative centroids.\nThe use of alternative clustering algorithms (DBSCAN, OPTICS, Self-Organising Maps, and ADBSCAN).\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Bonus",
      "12. Grouping Data"
    ]
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Presenting Data",
    "section": "",
    "text": "This week is focussed on formalising our understanding of how to tune matplotlib plots to ‘publication quality’ by tweaking fine-grained elements. So this week we face off with the monster that most Python visualisation libraries ultimately depend on: matplotlib.\nAs a tool it is both incredibly powerful and intensely counter-intuitive if you are used to R’s ggplot. We will also be looking more widely to the future of quantitative methods, the potential of a geographic data science, and the ways in which we can move between spatial and non-spatial paradigms of analysis within the same piece of work.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou have broadened your thinking about the purpose of data visualisation.\nYou are working intensively on the group project.",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#overview",
    "href": "sessions/week10.html#overview",
    "title": "Presenting Data",
    "section": "",
    "text": "This week is focussed on formalising our understanding of how to tune matplotlib plots to ‘publication quality’ by tweaking fine-grained elements. So this week we face off with the monster that most Python visualisation libraries ultimately depend on: matplotlib.\nAs a tool it is both incredibly powerful and intensely counter-intuitive if you are used to R’s ggplot. We will also be looking more widely to the future of quantitative methods, the potential of a geographic data science, and the ways in which we can move between spatial and non-spatial paradigms of analysis within the same piece of work.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou have broadened your thinking about the purpose of data visualisation.\nYou are working intensively on the group project.",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#preparatory-lectures",
    "href": "sessions/week10.html#preparatory-lectures",
    "title": "Presenting Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nData Visualisation\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#other-preparation",
    "href": "sessions/week10.html#other-preparation",
    "title": "Presenting Data",
    "section": "Other Preparation",
    "text": "Other Preparation\nYou might want to look at the following reports / profiles with a view to thinking about employability and how the skills acquired in this module can be applied beyond the end of your MSc:\n\nGeospatial Skills Report &lt;URL&gt;\nAAG Profile of Nicolas Saravia &lt;URL&gt;\nWolf et al. (2021) &lt;URL&gt;\n\n\n\n\n\n\n\nConnections\n\n\n\nWhile I expect most of you will be focussed on assessments, you should seriously consider returning to the three readings assigned over Reading Week as they will help you to reflect on what you’ve learned this term in this module and across the programme as a whole. The other three might be useful in terms of looking at the direction of the field, the opportunities in industry, and the kinds of work that people with (sptial) data science skills can do.",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/week10.html#practical",
    "href": "sessions/week10.html#practical",
    "title": "Presenting Data",
    "section": "Practical",
    "text": "Practical\nThe practical will lead you through the fine-tuning of data visualisations in Matplotlib/Seaborn. In many ways, this should be seen as largely a recap of material encountered in previous sessions. However, you should see this as an important step in the production of outputs and analyses needed for the final project. That said, you would be better off spending time on the substance of the report first and only turning to the fine-tuning of the visualisations if time permits.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nAutomating the production of map outputs in Python to create an ‘atlas’.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "10. Presenting Data"
    ]
  },
  {
    "objectID": "sessions/reading_week.html",
    "href": "sessions/reading_week.html",
    "title": "Reading Week",
    "section": "",
    "text": "Important\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any missed reading(s).\nStart to work on the first few questions of the group assessment.\nStart to test your ability to work with the Inside Airbnb data (Week 5 Practical).\n\n\n\nPast student performance strongly suggests that this is a good week to:\n\nCatch up on readings, particularly the more critical ones and the ones focussing on the impact of Airbnb.\nGo back over the first five notebooks in order to self-test and check your understanding. This doesn’t mean re-doing the full notebook, but (for example) seeing if you can use pandas to analyse the simpler data we looked at in weeks 3 and 4.\n\nLooking ahead to the Group Work, I’d also strongly suggest that you browse the Full Reference List for ideas. The bibliography is a working document and I will add more items as and when I come across them or new works are published, but this is a good time to start reading about the ethical and practical issues arising from Airbnb’s operations and the data to which we have access.",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "sessions/reading_week.html#overview",
    "href": "sessions/reading_week.html#overview",
    "title": "Reading Week",
    "section": "",
    "text": "Important\n\n\n\nThis week’s learning objectives are:\n\nGet caught up on any missed reading(s).\nStart to work on the first few questions of the group assessment.\nStart to test your ability to work with the Inside Airbnb data (Week 5 Practical).\n\n\n\nPast student performance strongly suggests that this is a good week to:\n\nCatch up on readings, particularly the more critical ones and the ones focussing on the impact of Airbnb.\nGo back over the first five notebooks in order to self-test and check your understanding. This doesn’t mean re-doing the full notebook, but (for example) seeing if you can use pandas to analyse the simpler data we looked at in weeks 3 and 4.\n\nLooking ahead to the Group Work, I’d also strongly suggest that you browse the Full Reference List for ideas. The bibliography is a working document and I will add more items as and when I come across them or new works are published, but this is a good time to start reading about the ethical and practical issues arising from Airbnb’s operations and the data to which we have access.",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "sessions/reading_week.html#readings",
    "href": "sessions/reading_week.html#readings",
    "title": "Reading Week",
    "section": "Readings",
    "text": "Readings\nThe following articles will be discussed in-class over the coming weeks:\n\nAmoore (2019) &lt;URL&gt;\nMattern (2017) &lt;URL&gt;\nMattern (2015) &lt;URL&gt;\n\nFor the group work you might also want to at least consider the following articles and outputs:\n\nElwood and Leszczynski (2018) &lt;URL&gt;\nBemt et al. (2018) &lt;URL&gt;\nCrawford and Finn (2015) &lt;URL&gt;\n\n\n\n\n\n\n\nConnections\n\n\n\nI am not suggesting that you read every one of these readings (though split across a group this is light work!); rather, I am pointing you towards content that can inform your responses to the set questions in the Group Work. These readings focus on the ethical issues raised by spatial data science and algorithmic approaches to decision-making. You should be drawing connections to D’Ignazio and Klein (2020) and their data-informed critiques of ‘just let the data scientists crunch the data’ thinking. Some of these readings may feel very challenging in terms of their language or approach to Machine Learning/AI, but they will reward attention and reflection, and you can expect to see this reflected in a better mark on the group project as your engagement with the substance of the assesmsnet will also be more reflective and relevant.",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "sessions/reading_week.html#additional-context",
    "href": "sessions/reading_week.html#additional-context",
    "title": "Reading Week",
    "section": "Additional Context",
    "text": "Additional Context\nYou might also find the following content interesting in terms of the practical limitations of ‘AI’ tools and the ways in which they reproduce errors in our own thinking rather that offering a neutral insight into processes:\n\nAI translation jeopardises Afghan asylum claims\nAn Iowa school district is using ChatGPT to decide which books to ban\nSingapore’s tech-utopia dream is turning into a surveillance state nightmare\n\nAnd here’s a nice example of why it’s not about the algorithm:\n\n\n\nAlgorithmic Perfection\n\n\nSource: Zemanek (1983)\n\nHow I’m fighting bias in algorithms\n\n\n\nEthics, Politics & Data-driven Research and Technology",
    "crumbs": [
      "Reading Week"
    ]
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html",
    "href": "practicals/Practical-01-Getting_Started.html",
    "title": "Practical 1: Getting Started",
    "section": "",
    "text": "This week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2, and familiarising you with ‘how people do data science’. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: get out of the way of doing ‘stuff’; support teams in negotating conflicts in code; make it easy to share results; and make it easy to ensure that everyone is ‘on the same page’.\nYou will find things confusing this week, but they will start to make more sense as we move further into the module. The key is to keep trying things out and to ask for help when you get stuck.\nIf you haven’t done this already, you need to follow the instructions for installing all of the tools listed on on the CASA Computing Environment web page. This process will take time, so please keep reading or work on other things to do while you wait!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#starting-up-right",
    "href": "practicals/Practical-01-Getting_Started.html#starting-up-right",
    "title": "Practical 1: Getting Started",
    "section": "Starting Up ‘Right’",
    "text": "Starting Up ‘Right’\nOne of the most confusing things about starting a Docker container with a “local volume mount” (i.e. a location on your computer that Docker connects to the container’s file system) is that it seems like magic and it’s often hard to understand why you’re seeing what you are under the work directory.\nSo before you do anything else please spend a minute in the Terminal (macOS) or Power Shell (Windows) learning how to get to your home directory and, within that, to a CASA directory where you can store your work and keep Docker from accessing data that it shouldn’t.\nWhat we are doing is creating a directory on your computer that you can access from Docker. This is where you will store your notebooks, data, and any other files that you need to work with. The ultimate structure we’ll produce this:\n\n\n\n‘Target’ Directory Structure\n\n\nOn both a Mac and a PC you should be able to run the following:\n\ncd $HOME – this will change directory to your home directory (on a Mac it will be /Users/&lt;your username&gt;, on a PC it will be something like C:\\Users\\&lt;your username&gt;). Hint: cd means ‘change directory’!\ncd Documents – this will move you into your ‘Documents’ folder. Note: on Windows this might be called My\\ Documents, in which case it’s cd My\\ Documents! If you have set up your computer in another language this might be called something else, but Power Shell still ‘knows’ which folder should contain your documents.\nmkdir CASA – this will create a CASA folder in your home directory.\ncd CASA – you are now changing into the CASA directory.\necho $pwd (PC) or pwd (Mac) – this should show you the ‘full path’ to your new CASA directory (e.g. /Users/&lt;your username&gt;/Documents/CASA or something like that).\n\nLeave the Terminal or PowerShell window open! You will need it in a moment.\nPerhaps a video will help clarify?"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#configuring-the-image",
    "href": "practicals/Practical-01-Getting_Started.html#configuring-the-image",
    "title": "Practical 1: Getting Started",
    "section": "Configuring the sds2024 Image",
    "text": "Configuring the sds2024 Image\nDuring the ‘install festival’ you should have installed Docker and, time permitting, ‘pulled’ the image appropriate to your system. If you haven’t, then you should do so as a priority now."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#running-docker-1",
    "href": "practicals/Practical-01-Getting_Started.html#running-docker-1",
    "title": "Practical 1: Getting Started",
    "section": "Running Docker",
    "text": "Running Docker\nBy default, the best way to start Docker is from the Terminal or Power Shell.\n\nOn Windows\nUsing the same Power Shell copy and paste the following all on one line:\ndocker run --rm -d --name sds2024 -p 8888:8888 \n  -v \"$(pwd):/home/jovyan/work\" jreades/sds:2024-intel start.sh jupyter lab \n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\n\n\n\n\n\n\nWindows Commands and Docker\n\n\n\n$(pwd) is actually a command, you are asking the Power Shell to use the current working directory* (pwd == print working directory) as the ‘mount point’ for the work directory. The Command Prompt doesn’t support pwd, but the Power Shell should. You can check this by simply typing pwd and hitting enter (⏎) to see if you get an error.\n\n\n\n\nOn macOS\nUsing the same Terminal copy and past the following (change the docker image to jreades/sds:2024-intel if using an older Intel Mac):\ndocker run --rm -d --name sds2024 -p 8888:8888 \\\n   -v \"$(pwd):/home/jovyan/work\" \\\n  jreades/sds:2024-silicon start.sh jupyter lab \\\n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#how-do-i-know-it-worked",
    "href": "practicals/Practical-01-Getting_Started.html#how-do-i-know-it-worked",
    "title": "Practical 1: Getting Started",
    "section": "How do I Know it Worked?",
    "text": "How do I Know it Worked?\nWith Docker running, you will mainly interact with Python through a web page. To check if it’s running, we just have to visit the web page and see what happens: http://localhost:8888/lab/tree/work/. We’ll talk more about exactly what is going on over the next few weeks, but this should show you a page that looks something like this (probably with fewer files listed on the left-hand side):\n\n\n\nScreenshot of Jupyter Lab\n\n\n\n\n\n\n\n\nSee Docker Run (and Run)…\n\n\n\nOnce you have started a Docker container using any of the above, the machine will continue running until you either restart the computer or tell Docker to stop the container. This can consume memory and battery power indefinitely."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#your-new-github-account",
    "href": "practicals/Practical-01-Getting_Started.html#your-new-github-account",
    "title": "Practical 1: Getting Started",
    "section": "Your New GitHub Account",
    "text": "Your New GitHub Account\nIt doesn’t really matter which way you do this, but we recommend that you set up your new GitHub account with both your personal and your UCL email addresses. GitHub ‘knows’ about educational users and will give you access to more features for free if you associate a .ac.uk email address to your account. So choose one email address to start with and then add the other one later.\nFrom a security standpoint you should also enable 2-factor authentication so that you receive a text message when you log in on a new machine and are asked to confirm a code."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#creating-a-private-repository",
    "href": "practicals/Practical-01-Getting_Started.html#creating-a-private-repository",
    "title": "Practical 1: Getting Started",
    "section": "Creating a Private Repository",
    "text": "Creating a Private Repository\nTo create a repository, click on the + at the upper-right corner of the GitHub web page and select New Repository. You might as well call your ‘repo’ fsds or foundations since that’s a lot shorter than foundations_of_spatial_data_science.\n\n\n\n\n\n\nYour ‘Repo’ Name\n\n\n\nFor the purposes of this tutorial (and all subsequent tutorials) I will assume that your repository is called fsds. You can call it whatever you like, in which case you will always need to substitute the name that you chose wherever you see me write fsds.\n\n\nIt’s always helpful to provide some basic information about what’s in the project (e.g. your notes and practicals for the Foundations module). And finally, make sure you:\n\nChange the visibility from Public to Private,\nTick Add a README file,\nChange Add .gitignore from None to template: Python.\n\nClick Create Repository and you should end up on a page that looks like this:\n\n\n\nRepository created\n\n\nYour new repository has been created (on GitHub)!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#exclude-data-files",
    "href": "practicals/Practical-01-Getting_Started.html#exclude-data-files",
    "title": "Practical 1: Getting Started",
    "section": "Exclude Data Files",
    "text": "Exclude Data Files\nWe want to make it hard to accidentally add a large data file to our repository. Git/GitHub isn’t designed for large, binary files (you can’t ‘read’ a Parquet file) and we assume that data is backed up or available elsewhere, but our code is not! So as a first step we want to exclude files that are likely to just be ‘data’:\n\n\n\nFile Type\nExtension\n\n\n\n\nCSV\n.csv\n\n\nExcel\n.xls, .xlsx\n\n\nZip\n.zip\n\n\nGZip\n.gzip\n\n\nFeather\n.feather, .geofeather\n\n\nParquet\n.parquet, .geoparquet\n\n\n\nHere’s another hint: C extensions are already excluded by the .gitignore file, so maybe look to see how that’s done to help you figure out how to exlcude .zip, .gz, and .csv files…"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#exclude-a-data-directory",
    "href": "practicals/Practical-01-Getting_Started.html#exclude-a-data-directory",
    "title": "Practical 1: Getting Started",
    "section": "Exclude a Data Directory",
    "text": "Exclude a Data Directory\nTo make it even less like that we accidentally include data, let’s also exclude a data directory from our repository. As a clue, nearly everything in the Distribution / packaging section of the .gitignore file is a directory to be excluded from Git.\nSo how would you indicate that data is a directory? Once you’re sure, add the data directory!\nWhen you are done, don’t forget to add a ‘commit message’ (e.g. ‘Added data files to .gitignore’) at the bottom and then click Commit changes.\n\n\n\n\n\n\nQuick Answer\n\n\n\n\n\nI don’t want you to get hung up on this one thing in Practical 1, so if you just can’t make sense of what you’re being asked to do here, have a look at the Answers at the bottom of this page."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#check-your-changes",
    "href": "practicals/Practical-01-Getting_Started.html#check-your-changes",
    "title": "Practical 1: Getting Started",
    "section": "Check Your Changes",
    "text": "Check Your Changes\nOnce you have committed your changes, you should be back to the default view of the .gitignore file but there should be a message to the effect of Latest commit &lt;some hexadecimal number&gt; 10 seconds ago and, next to that, a History button.\nClick on ‘History’ and let’s go back in time!\n\n\n\nThe Gitignore history\n\n\nOn the history page you can browse every edit to your file. Whenever you commit a file, this like taking a snapshot at a point in time. Using the ‘History’ you can compare two different snapshots in order to see what has changed. This would help you to work out how you broke something, check that requested changes have been made, or see how an error might have been introduced.\n\n\n\n\n\n\nViewing Your Commit History\n\n\n\nYou can mouseover the buttons to see what they do. Why don’t you try to find See commmit details and check what edits you made to the .gitignore file? You should see at least three plusses in the history view representing three new lines in the .gitignore file."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#working-on-your-markdown",
    "href": "practicals/Practical-01-Getting_Started.html#working-on-your-markdown",
    "title": "Practical 1: Getting Started",
    "section": "Working on Your Markdown",
    "text": "Working on Your Markdown\nWrite your README file using at least the following Markdown features:\n\nA level-1 header (#)\nA level-3 header (###)\nItalic text (_this is italicised_)\nBold text (**this is bold**)\nA link ([link text](url))\nAn image (![Alt text](image_location))\n\nIf you’re unsure how these work, just double-click on this text and you’ll see Markdown in a Jupyter notebook. Here’s some sample text to get you started:\n### Foundations of Spatial Data Science\n\nThis repository contains practicals and notes from the _Foundations_ module. \n\nYou can find the original [here](https://jreades.github.io/fsds/).\nDon’t forget to check out the “Preview” tab!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#commiting-a-change",
    "href": "practicals/Practical-01-Getting_Started.html#commiting-a-change",
    "title": "Practical 1: Getting Started",
    "section": "Commiting a Change",
    "text": "Commiting a Change\nOnce you’re happy with how your text looks and works, it’s time to commit! Scroll down to where you see something like this (you will see your own GitHub username, not mine):\n\n\n\nGitHub Commit\n\n\nYou can just accept the description (e.g. Create README.md) or you can write your own. You can also provide an extended description if you choose. Then click Commit new file and you will see your new README appear."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#configuring-defaults",
    "href": "practicals/Practical-01-Getting_Started.html#configuring-defaults",
    "title": "Practical 1: Getting Started",
    "section": "Configuring Defaults",
    "text": "Configuring Defaults\nThe first thing to do is set up the default username and email for GitHub. These can be changed on a project-by-project basis, but to begin with it’s best to set up the global defaults. Using either the Terminal or Bash enter the following (replacing &lt;...&gt; with your details):\ngit config --global user.email '&lt;your GitHub email address&gt;'\ngit config --global user.name '&lt;your GibHub username&gt;'\n\n\n\n\n\n\nRecall: Convention!\n\n\n\nAs a reminder, &lt;...&gt; is a convention in programming to indicate that you should replace the text between the &lt; and &gt; with something that makes sense in your context. For example, if you see '&lt;your GitHub email address&gt;' you should type, for example, 'j.reades@ucl.ac.uk'. You do not include the &lt; or &gt; characters!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#creating-a-personal-access-token",
    "href": "practicals/Practical-01-Getting_Started.html#creating-a-personal-access-token",
    "title": "Practical 1: Getting Started",
    "section": "Creating a Personal Access Token",
    "text": "Creating a Personal Access Token\nFor copying changes up to/down from GitHub, you must use a Personal Access Token. This is like issuing special passwords that allow only limited access to parts of your GitHub account.\nTo create a Personal Access Token:\n\nVisit your GitHub User Page (e.g. github.com/jreades)\nClick on your user icon (at the top-right corner of the page) and click the Settings link (near the bottom on the right side of the page).\nScroll down the settings page until you get to Developer settings (near the bottom on the left side of the page).\nClick the Developer settings link to reach the ‘apps’ page and then click on the Personal access tokens link.\nSelect Tokens (classic) from the left-hand menu and then click the Generate new token button.\n\n\n\n\n\n\n\nTypes of Personal Tokens\n\n\n\nYou now need to choose the type of token to generate. I personally find the old type of tokens easier to work with because the ‘new’ fine-grained tokens are intended to support complex workflows when all we’re trying to do is allow one computer to push/pull from Git.\n\n\n\nGenerate new token (classic) for general use token and then specify the following:\n\nI’d suggest writing FSDS Token or something similar in the Note section.\nSet the expiration to 90 days\nClick the repo tickbox for ‘Full control of private repositories’.\n\nSave the resulting token somewhere safe as you will need it again! (e.g. a Note on your phone, a password manager, etc.).\n\n\n\n\n\n\n\nKeep your Personal Token Safe\n\n\n\nYou will need it at least twice in this tutorial and may want to use it again on other computers. You can always create a new one, but then you’ll need to update every computer where you access your GitHub repositories."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#cloning-your-repository",
    "href": "practicals/Practical-01-Getting_Started.html#cloning-your-repository",
    "title": "Practical 1: Getting Started",
    "section": "Cloning Your Repository",
    "text": "Cloning Your Repository\nNow we are going to clone (i.e. copy) the repository that you just created on to your own computer. This is surprisingly straightforward provided that you have installed the command line tools.\nOn your private repository page, click on the green button labeled Code visible in the screenshot below:\n\n\n\nScreenshot of cloning interface\n\n\nYou should then copy your HTTPS URL (in my screenshot it’s https://github.com/jreades/i2p.git).\nNow go back to the Terminal or PowerShell that you left open earlier and type the following (replacing &lt;the_url_that_you_copied_from_the_browser&gt; with the URL that you copied from GitHub):\ngit clone &lt;the_url_that_you_copied_from_the_browser&gt;\nThe first time that you do this, you will need to provide login information. Use your GitHub username and the Personal Access Token that you just created. On your computer you should now see a new directory with the same name as your repository. For example: Documents/CASA/fsds."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#storing-credentials-pulling",
    "href": "practicals/Practical-01-Getting_Started.html#storing-credentials-pulling",
    "title": "Practical 1: Getting Started",
    "section": "Storing Credentials & ‘Pulling’",
    "text": "Storing Credentials & ‘Pulling’\nYou can now activate the credtial helper that will store your Personal Access Token (though you should still keep your secure note!):\ncd fsds\ngit config credential.helper store\ngit pull\nWhen you type git pull you should be asked again for your username and password. You should (again) use the Personal Access Token as your password. You should not be asked again for pushing or pulling data into this GitHub repository. If you are not asked for your Personal Access Token then this likely means that your token is already saved and ready to use on all future ‘actions’."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#jupyter---your-computer",
    "href": "practicals/Practical-01-Getting_Started.html#jupyter---your-computer",
    "title": "Practical 1: Getting Started",
    "section": "Jupyter < - > Your Computer",
    "text": "Jupyter &lt; - &gt; Your Computer\nIf you now go back to Jupyter, you should now see that the work directory is actually the same as the CASA directory that you created earlier; you know this because you can now see an fsds directory containing that wasn’t there before.\nThis is because Docker is ‘mounting’ the work directory in the container to the CASA directory on your computer. This means that you can save files in the work directory in Jupyter and they will be saved in the CASA directory on your computer.\n\n\n\n\n\n\nOrganising Your Work\n\n\n\nThere are any number of ways to organise your CASA work, what’s important is that you are logical about things like names and hierarchy. This will make it much easier to access files and notebooks using Docker, Quarto, and Python."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#adding-a-local-file-to-your-repository",
    "href": "practicals/Practical-01-Getting_Started.html#adding-a-local-file-to-your-repository",
    "title": "Practical 1: Getting Started",
    "section": "Adding a Local File to Your Repository",
    "text": "Adding a Local File to Your Repository\nIn order to tie together the different concepts covered above, we are now going add Practical 1 (this practical) to your GitHub repo. The easiest way to do this is to download the notebook by clicking on the Jupyter link on the right side of this page. So the process is:\n\nClick on the Jupyter link to save this file to your computer as a notebook with the extension .ipynb.\nMove the file to your new repository folder (e.g. $HOME/work/Documents/CASA/fsds/).\n\n\n\n\n\n\n\nFile Extensions\n\n\n\nIt is highly likely that your browser automatically added a .txt extension when you saved the Notebook file to your computer. You need to remove that ending to your file name or Jupyter won’t be able to run it. You can rename a file by either doing it directly in the Finder/Windows Explorer, or by moving (bash: mv) the file: mv &lt;notebook_name&gt;.ipynb.txt &lt;notebook_name&gt;.ipynb\n\n\nIn the Terminal/PowerShell we now need add this file to Git so that it knows to keep track of it. Unlike Dropbox or OneDrive, just putting a file in a repo directory does not mean that Git will pay attention to it:\n# Assuming that you are 'in' the 'fsds' directory...\ngit add Practical-01-Getting_Started.ipynb\ngit commit -m \"Adding notebook 1 to repo.\"\n\n\n\n\n\n\nAdd, Commit, Push, Repeat\n\n\n\nUnless you have added and committed a file to Git then it is not version controlled.\nUnless you have pushed your committed files to GitHub they are only backed up locally."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#status-check",
    "href": "practicals/Practical-01-Getting_Started.html#status-check",
    "title": "Practical 1: Getting Started",
    "section": "Status Check",
    "text": "Status Check\nWe now want to check that the file has been successfully added to Git. We do this with a status check in the repository directory (i.e. cd $HOME/work/Documents/CASA/fsds/):\ngit status\nYou should see something like:\nOn branch master\nYour branch is ahead of 'origin/master' by 1 commit.\n  (use \"git push\" to publish your local commits)\nThis is telling you that your local computer is 1 commit (the one that you just completed) ahead of the ‘origin’, which is on GitHub. GitHub doesn’t have to be the origin (nor does the repository have to be one that we created in order to be an origin) but conceptually and practically it’s easier to create new repositories on GitHub and clone them to our computer."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#keep-pushing",
    "href": "practicals/Practical-01-Getting_Started.html#keep-pushing",
    "title": "Practical 1: Getting Started",
    "section": "Keep Pushing",
    "text": "Keep Pushing\nTo synchronise the changes we just made, let’s follow Git’s advice:\ngit push\nYou should see something like (the numbers and details will all differ, but the messages will be the same):\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 306 bytes | 306.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nremote: This repository moved. Please use the new location:\nremote:   https://github.com/jreades/fsds.git\nTo https://github.com/jreades/fsds.git\n   7410d0e..45aa80a  master -&gt; master\nIf you now go over to your browser and visit your GitHub repo page (e.g. https://github.com/jreades/fsds) — pressing the Reload button if you had the page open already — then you should see that the file you added on your computer is also showing up on the GitHub site as well! This means it’s now fully version-controlled and backed-up.\n\n\n\n\n\n\nKeep Pushing\n\n\n\nUnless have pushed your commits to GitHub they are only stored on your computer. So your files can be properly version-controlled, but without a push if you lose your computer you still lose everything!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#more-about-git",
    "href": "practicals/Practical-01-Getting_Started.html#more-about-git",
    "title": "Practical 1: Getting Started",
    "section": "More About Git",
    "text": "More About Git\nFrom here on out you can keep changes made either directly on GitHub or locally on your computer (or any other computer to which you clone your repository) in synch by using git push (to push changes from a local computer up to the origin on GitHub) and git pull (to pull changes available on the origin down to the local computer).\nThat said, you can do a lot more than just push/pull to your own repository and this Twitter thread leads to a lot of useful additional resources to do with Git:\n\nIntroduction to Version Control with Git on the Programming Historian web site is written for digital humanities researchers so it’s intended to be accessible.\nOh My Git is an ‘open source game’ to help you learn Git.\nGit Meets Minesweeper? is apparently a ‘thing’.\nVisual Git Reference if you think visually or just want to check your understanding.\nVersion Control with Git is a Software Carpentries lesson that takes you quickly through the important elements of getting set up and started. It would be a good refresher.\nAltassian’s Documentation provides more detailed explanations of the commands and options.\nLearn Git Branching focusses on a key concept for software collaboration.\nGit Immersion provides a ‘guided tour’ of the fundamentals.\n\n\n\n\n\n\n\nTip\n\n\n\nFor the Group Work every member of your group will need to make contributions to a GitHub repository. This will require learning how to invite others to be contributors, how to merge changes, and how to deal with conflicts of the coding kind."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#other-useful-resources",
    "href": "practicals/Practical-01-Getting_Started.html#other-useful-resources",
    "title": "Practical 1: Getting Started",
    "section": "Other Useful Resources",
    "text": "Other Useful Resources\n\nGitHub Markdown Guide\nCommon Mark\nMarkdown Guide, which helpfully includes do’s and don’ts.\n\nFinally, these are a bit overkill but the bits about setting up and installing git, bash/zsh, and so on may come in handy later:\n\nSetting Up a New Mac\nBeginner’s Guide to Setting Up Windows 10\nSetting up Windows without Linux\nMicrosoft Python Setup Guide"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#gitignore",
    "href": "practicals/Practical-01-Getting_Started.html#gitignore",
    "title": "Practical 1: Getting Started",
    "section": ".Gitignore",
    "text": ".Gitignore\nThe main thing you should notice is the pattern: * means ‘anything’, while / at the end of a line implies a directory. So the following four lines should be added to your .gitignore file:\n*.zip\n*.gz\n*.csv\n*.gzip\n*.feather\n*.geofeather\n*.parquet\n*.geoparquet\ndata/\nThat’s it."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#three-phases-of-coding",
    "href": "lectures/1.3-Writing_Code.html#three-phases-of-coding",
    "title": "Writing Code",
    "section": "Three Phases of Coding",
    "text": "Three Phases of Coding\n\nManaging it\nDocumenting it\nWriting it\n\nNotice that writing comes last."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#git",
    "href": "lectures/1.3-Writing_Code.html#git",
    "title": "Writing Code",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#github",
    "href": "lectures/1.3-Writing_Code.html#github",
    "title": "Writing Code",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#gitgithub-is-for-anything",
    "href": "lectures/1.3-Writing_Code.html#gitgithub-is-for-anything",
    "title": "Writing Code",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course (minus videos and assessments) is on GitHub."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#oh-my-git",
    "href": "lectures/1.3-Writing_Code.html#oh-my-git",
    "title": "Writing Code",
    "section": "Oh My Git!",
    "text": "Oh My Git!"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#additional-stylings",
    "href": "lectures/1.3-Writing_Code.html#additional-stylings",
    "title": "Writing Code",
    "section": "Additional Stylings",
    "text": "Additional Stylings\nFor those who know how to do it, you can also insert bits of real HTML and CSS (the ‘languages’ of web sites) as well.\n\nThis content has HTML formatting attached."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#why-use-it",
    "href": "lectures/1.3-Writing_Code.html#why-use-it",
    "title": "Writing Code",
    "section": "Why Use It?",
    "text": "Why Use It?\nThe features that make it easy to use have real advantages for you:\n\nYou will spend less time wrestling with Microsoft Word and its formatting; this means that…\nYou will spend more time focussing on the important stuff: writing and coding!\nYou will be able to combine Code and Documentation easily because Python/R and Markdown all coexist happily on GitHub."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#markdown-examples",
    "href": "lectures/1.3-Writing_Code.html#markdown-examples",
    "title": "Writing Code",
    "section": "Markdown Examples",
    "text": "Markdown Examples\nSee CommonMark and the Markdown Guide for more :\n\n\n\nFormat\n\n\nOutput\n\n\n\n\nPlain text…\n\n\nPlain text\n\n\n\n\n## A Large Heading\n\n\n\nA Large Heading\n\n\n\n\n\n### A Medium Heading\n\n\n\nA Medium Heading\n\n\n\n\n\n- A list- More list\n\n\n\n\nA list\n\n\nMore list\n\n\n\n\n\n\n1. An ordered list2. More ordered list\n\n\n\n\nAn ordered list\n\n\nMore ordered list\n\n\n\n\n\n\n[A link](http://casa.ucl.ac.uk)\n\n\nA link\n\n\n\n\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#more-markdown",
    "href": "lectures/1.3-Writing_Code.html#more-markdown",
    "title": "Writing Code",
    "section": "More Markdown…",
    "text": "More Markdown…\nHere are some more resources:\n\nGetting Started\nAn online interactive tutorial\nCheatsheet\n\nAnd once you’re ready to get ‘serious’, check out this tutorial on Sustainable Authorship in Plain Text using Pandoc and Markdown from The Programming Historian! That’s what actually underpins Knitr and Quarto, but you can do so much more…"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#literate-programming",
    "href": "lectures/1.3-Writing_Code.html#literate-programming",
    "title": "Writing Code",
    "section": "Literate Programming",
    "text": "Literate Programming\nIdeally, we want to write code in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct.\n\n\n– Donald Knuth, Selected Papers on Computer Science"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#key-tenets",
    "href": "lectures/1.3-Writing_Code.html#key-tenets",
    "title": "Writing Code",
    "section": "Key Tenets",
    "text": "Key Tenets\nWhat we want:\n\nWeaving: the code and its documentation are together in one file.\nTangling: the code can be run directly from this file.\n\nWhy do we want this?"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-notebooks",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-notebooks",
    "title": "Writing Code",
    "section": "Jupyter(Lab) & Notebooks",
    "text": "Jupyter(Lab) & Notebooks\n\nModern Browser + Jupyter == Tangled, Woven code in (m)any languages\nIncluding maths:\n\\[\nf(a) = \\frac{1}{2\\pi i} \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n\\]\n\nNote: you can set equations in Markdown too!"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-python",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-python",
    "title": "Writing Code",
    "section": "JupyterLab + Python",
    "text": "JupyterLab + Python"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#all-kinds-of-features",
    "href": "lectures/1.3-Writing_Code.html#all-kinds-of-features",
    "title": "Writing Code",
    "section": "All Kinds of Features",
    "text": "All Kinds of Features\nJupyterLab is basically a web application:"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#jupyterlab-uses-markdown",
    "href": "lectures/1.3-Writing_Code.html#jupyterlab-uses-markdown",
    "title": "Writing Code",
    "section": "JupyterLab Uses Markdown",
    "text": "JupyterLab Uses Markdown"
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#and-so-does-quarto",
    "href": "lectures/1.3-Writing_Code.html#and-so-does-quarto",
    "title": "Writing Code",
    "section": "And so Does Quarto",
    "text": "And so Does Quarto\n\n\n\n\nQuarto is intended to:\n\nExtend power of RMarkdown/knitr to all languages\nMake it easy to create web sites, documents, and articles.\nMake it easy to integrate code with text and images."
  },
  {
    "objectID": "lectures/1.3-Writing_Code.html#resources",
    "href": "lectures/1.3-Writing_Code.html#resources",
    "title": "Writing Code",
    "section": "Resources",
    "text": "Resources\n\nThe Myth of the ‘Genius Programmer’ (by Google Devs)\n“I mostly had one big, ugly, long, unreadable script”\nStack Overflow\nVersion Control with Git\nSetting up and managing your GitHub user account\nPersonal Access Tokens on Git\nGit Cheat Sheet\nThe Most Undervalued Skill for Data Scientists\nFinding your scientific story by writing backwards\n\n\n\n\n\nWriting Code • Jon Reades & Fulvio Lopane"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#so-key---value",
    "href": "lectures/3.1-Dictionaries.html#so-key---value",
    "title": "Dictionaries",
    "section": "So: Key -> Value",
    "text": "So: Key -&gt; Value\nThe key can be almost anything that is immutable (aka. hashable). So these are all ‘legit’:\nlookup[1]             # Int\nlookup(52.1)          # Float\nlookup['1']           # String\nlookup['Jon Reades']  # String\nk = 'Jon Reades'\nlookup[k]             # String variable\nlookup[(52.1, -0.04)] # Tuple\nBut this is not:\nlookup[['Jon','Reades']] # Error, unhashable type\nThat’s because a list is not immutable.\n\nAgain, just like a real dictionary: you don’t have multiple entries for ‘dog’, otherwise the dictionary wouldn’t work. You might have multiple definitions: which is to say, the key might return multiple values."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "href": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "title": "Dictionaries",
    "section": "Deliberately Similar",
    "text": "Deliberately Similar\nNotice the differences when creating them, and the absence of difference when accessing them.\n\n\nList\ncities = [\n  'San Francisco',\n  'London',\n  'Paris',\n  'Beijing']\n  \n# Prints London\nprint(cities[2]) \n\nDict\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\n# Prints pop of London\nprint(cities['London'])\n\n\nSo why might we prefer the dictionary?\n\nDicts are created using: d = { key: value, key: value }\nDicts are accessed using: d[key]\n\nSo the only difference between lists and dicts is: [...] and {...} when they are created.\nOver the next couple of weeks we’ll see ways that you can store more information in a list and also why lists are sometimes more powerful than you realise… if you can think about your data in an entirely new way. But for simple key/value stuff it’s hard to beat a dictionary!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "href": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "title": "Dictionaries",
    "section": "It’s All About Access",
    "text": "It’s All About Access\nSpecifically: do we need sequential or random access?\n\n\nList\n\n\n\nindex\nvalue\n\n\n\n\n0\nSan Francisco\n\n\n1\nLondon\n\n\n2\nParis\n\n\n3\nBeijing\n\n\n\n\nDict\n\n\n\nkey\nvalue\n\n\n\n\nSan Francisco\n837442\n\n\nLondon\n8673713\n\n\nParis\n2229621\n\n\nBeijing\n21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values",
    "href": "lectures/3.1-Dictionaries.html#getting-values",
    "title": "Dictionaries",
    "section": "Getting Values",
    "text": "Getting Values\nThere are two ways to retrieve values from a dictionary:\n\ncities['Beijing']\ncities.get('Beijing')\n\nWhy have two? Consider:\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nprint(cities['Sao Paulo'])     # Throws KeyError\nprint(cities.get('Sao Paulo')) # Returns None\nprint(cities.get('Sao Paulo','No Data')) # Returns 'No Data'\n\nThe first triggers an error, the second returns None. Errors can be unfriendly: do you want your entire Python program to fail because a single city is missing, or would you rather than it did something a little more sensible such as… skipping the row or returning a sensible default?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "href": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "title": "Dictionaries",
    "section": "Getting Values (cont’d)",
    "text": "Getting Values (cont’d)\nIf we want to think about whether a value is in the dictionary (as opposed to just retrieving it) then notice these options:\nc = cities.get('Sao Paulo')\nif not c:\n  print(\"Sorry, no city by that name.\")\n\nif 'Beijing' in cities:\n  print(\"Found Beijing!\")\n\nThe first example works because cities.get returns None, which is the same as ‘undefined’ for Python. So we can use ‘not’ to imply ‘if c is not defined then do something…’\nThe second example works because we are implicitly treating the keys in the cities dictionary as a list and looking to see if Beijing is one of the values in that list.\nPython often benefits and suffers from TMTOWTDI (There’s More Than One Way To Do It): think of these as being different ways to say the same thing, but depending on where you want to put the emphasis you would choose one or the other."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#setting-values",
    "href": "lectures/3.1-Dictionaries.html#setting-values",
    "title": "Dictionaries",
    "section": "Setting Values",
    "text": "Setting Values\nIt’s the same process to update an existing value or create a new one:\ncities = {}  # Empty dictionary\ncities['Beijing'] = 21716620    # Sets key-&gt;value\ncities['Toronto'] = 2930000     # Sets key-&gt;value\n\nprint(cities['Toronto'])        # Prints 2930000\ndel cities['Toronto']           # Deletes Toronto key (and value)\ncities.pop('Toronto','Default') # Prints 'Default' b/c key not found\nprint(cities)\nThis last command outputs:\n{'Beijing': '0.17'16620}"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#iterating",
    "href": "lectures/3.1-Dictionaries.html#iterating",
    "title": "Dictionaries",
    "section": "Iterating",
    "text": "Iterating\nSimilar to iterating over lists but…\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nfor c in cities:\n  print(c)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nOne really important point to note: here, the cities are printed out in the same order that they were added to the dictionary, but that is not guaranteed! Unlike lists, dictionaries are unordered.\nAlso, how would we print out the population of each city?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#section",
    "href": "lectures/3.1-Dictionaries.html#section",
    "title": "Dictionaries",
    "section": "",
    "text": "Keys\nfor k in cities.keys():\n  print(k)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nValues\nfor v in cities.values():\n  print(v)\nPrints:\n837442\n8673713\n2229621\n21716620\n\nBoth\nfor k,v in cities.items():\n  print(f\"{k} -&gt; {v}\")\nPrints:\nSan Francisco -&gt; 837442\nLondon -&gt; 8673713\nParis -&gt; 837442\nBeijing -&gt; 21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#a-final-note",
    "href": "lectures/3.1-Dictionaries.html#a-final-note",
    "title": "Dictionaries",
    "section": "A Final Note!",
    "text": "A Final Note!\nValues can be almost anything, including a dictionary or list! This opens up some interesting possibilities:\n\n\ncities = {\n  'San Francisco': \n    [37.77, -122.43, 'SFO']\n}\n\ncities = {\n  'San Francisco': {\n    'lat': 37.77,\n    'lon': -122.43,\n    'airport':'SFO'}\n}\nprint(cities['San Francisco']['lat'])\n\nSpoiler: you’re going to encounter this kind of thing a lot.\n\nWhat is this starting to look like? This is basically what JSON is."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "href": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "title": "Dictionaries",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\n\nRemember: in most programming languages dictionaries/hashes are unordered and there is no guarantee that things come out in the same order they went in! They complement ordered lists, they don’t replace them!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#resources",
    "href": "lectures/3.1-Dictionaries.html#resources",
    "title": "Dictionaries",
    "section": "Resources",
    "text": "Resources\n\nDictionaries and sets\nComprehensions\nThe Complete Guide to Dictionaries (by a CASA alum)\n\n\n\n\n\nDictionaries • Jon Reades"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "This page contains all articles from the weekly readings and a few more that may be useful.\n\n\nAlsudais, Abdulkareem. 2021. “Incorrect Data in the Widely Used Inside Airbnb Dataset.” Decision Support Systems 141:113453. https://doi.org/10.1016/j.dss.2020.113453.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/10.1177/0263276419851846.\n\n\nAnderson, C. 2008. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete.” Wired. https://www.wired.com/2008/06/pb-theory/.\n\n\nArribas-Bel, Daniel. 2014. “Accidental, Open and Everywhere: Emerging Data Sources for the Understanding of Cities.” Applied Geography 49. Elsevier:45–53. https://doi.org/10.1016/j.apgeog.2013.09.012.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBrockes, E. 2023. “Airbnb was wild, disruptive and cheap: we loved it. But it wasn’t a love strong enough to last.” The Guardian. https://www.theguardian.com/commentisfree/2023/mar/08/airbnb-wild-disruptive-cheap-lettings-agency.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nClark, J. 2023. “Bidding wars: inside the super-charged fight for rental properties.” The Guardian. https://www.theguardian.com/money/2023/apr/08/bidding-wars-inside-the-super-charged-fight-for-rental-properties.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020d. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\n———. 2020c. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\n———. 2020e. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\n———. 2020b. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\n———. 2020a. Data Feminism. MIT Press. https://bookbook.pubpub.org/data-feminism.\n\n\nDark Matter Labs. 2019. “A Smart Commons: A New Model for INvesting in the Commons.” Medium. https://provocations.darkmatterlabs.org/a-smart-commons-528f4e53cec2.\n\n\nDelmelle, Elizabeth C, and Isabelle Nilsson. 2021. “The Language of Neighborhoods: A Predictive-Analytical Framework Based on Property Advertisement Text and Mortgage Lending Data.” Computers, Environment and Urban Systems 88. Elsevier:101658. https://doi.org/10.1016/j.compenvurbsys.2021.101658.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics‘.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching introductory GIS programming to geographers using an open source Python approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30. https://doi.org/10.1080/03098265.2015.1086981.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nFitzpatrick, B., and B. Collins-Sussman. n.d. “The Myth of the ’Genius Programmer’.” Google. https://www.youtube.com/watch?v=0SARbwvhupQ.\n\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nIqbal, N., and A. Chakrabortty. 2023. “Why are London’s inner-city schools disappearing?” Edited by A. Bransbury. The Guardian. 2023. https://www.theguardian.com/news/audio/2023/apr/26/why-are-london-schools-disappearing-podcast.\n\n\nJolly, J. 2023. “Owners of 100,000 properties held by foreign shell companies unknown despite new UK laws.” The Guardian. https://www.theguardian.com/business/2023/sep/03/owners-of-100000-properties-held-by-foreign-shell-companies-unknown-despite-new-uk-laws.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLansley, Guy. 2016. “Cars and Socio-Economics: Understanding Neighbourhood Variations in Car Characteristics from Administrative Data.” Regional Studies, Regional Science 3 (1). Taylor & Francis:264–85. https://doi.org/10.1080/21681376.2016.1177466.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are statisticians cold-blooded bosses? a new perspective on the ’old’ concept of statistical population.” Teaching Statistics 35 (1). Wiley Online Library:66–71. https://doi.org/10.1111/j.1467-9639.2012.00524.x.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458.\n\n\nMattern, Shannon. 2015. “Mission control: A history of the urban dashboard.” Places Journal. https://doi.org/10.22269/150309.\n\n\n———. 2017. “A City Is Not a Computer.” Places Journal. https://doi.org/10.22269/170207.\n\n\nMiller, Harvey J, and Michael F Goodchild. 2015. “Data-Driven Geography.” GeoJournal 80. Springer:449–61. https://doi.org/10.1007/s10708-014-9602-6.\n\n\nMinton, A. 2023. “New York is breaking free of Airbnb’s clutches. This is how the rest of the world can follow suit.” The Guardian. https://www.theguardian.com/commentisfree/2023/sep/27/new-york-airbnb-renters-cities-law-ban-properties.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nNeate, R. 2023. “‘This is where people with staggering wealth end up’: who will buy Britain’s most expensive house?” The Guardian. https://www.theguardian.com/money/2023/apr/08/britain-most-expensive-house-rutland-gate-mansion-london-super-rich-buyer.\n\n\nO’Sullivan, David, and Steven M Manson. 2015. “Do physicists have geography envy? And what can geographers learn from it?” Annals of the Association of American Geographers 105 (4). Taylor & Francis:704–22. https://doi.org/10.1080/00045608.2015.1039105.\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nReades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian. https://doi.org/10.46430/phen0111.\n\n\nReades, J., H. Yingjie, Emmanouil Tranos, and E. Delmelle. in review. “The City as Text: Understanding and Modeling Cities through the Lens of Text.” NA, in review.\n\n\nRose, Gillian. 1997. “Situating Knowledges: Positionality, Reflexivities and Other Tactics.” Progress in Human Geography 21 (3):305–20. https://doi.org/10.1191/030913297673302122.\n\n\nScheider, Simon, Enkhbold Nyamsuren, Han Kruiger, and Haiqi Xu. 2020. “Why Geographic Data Science Is Not a Science.” Geography Compass 14 (11). Wiley Online Library:e12537.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2P) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSmith, D. 2010. “Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London.” GLA Economics. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nStrauß, Stefan. 2015. “Datafication and the Seductive Power of Uncertainty–a Critical Exploration of Big Data Enthusiasm.” Information 6 (4). MDPI:836–47.\n\n\nTravers, Tony, Sam Sims, and Nicolas Bosetti. 2016. “Housing and Inequality in London.” Centre for London. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nVanderPlas, Jake. 2014. “Is Seattle Really Seeing an Uptick in Cycling?” http://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608. https://doi.org/10.1177/0309132520924722.\n\n\nXiao, Ningchuan. 2016. GIS Algorithms: Theory and Applications for Geographic Information Science & Technology. Research Methods. SAGE. https://doi.org/https://dx.doi.org/10.4135/9781473921498.\n\n\nXie, Tessa. 2024a. “How to Better Communicate as a Data Scientist.” Towards Data Science. https://www.divingintodata.com/p/how-to-better-communicate-as-a-data-scientist-6fc5428d3143.\n\n\n———. 2024b. “The Most Undervalued Skill for Data Scientists.” Towards Data Science. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321.\n\n\nZemanek, H. 1983. “Algorithmic Perfection.” Annals of the History of Computing. AMER FED INFORM PROCESSING SOC.\n\n\nZervas, Georgios, Davide Proserpio, and John W Byers. 2021. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” Marketing Letters 32. Springer:1–16.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "ref/Template.html",
    "href": "ref/Template.html",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "ref/Template.html#what-kind-of-reading-is-it",
    "href": "ref/Template.html#what-kind-of-reading-is-it",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "ref/Template.html#who-is-the-intended-audience",
    "href": "ref/Template.html#who-is-the-intended-audience",
    "title": "Summary of Reading",
    "section": "2 Who is the intended audience?",
    "text": "2 Who is the intended audience?\n\nIs it intended for academics, field experts, the general public? etc.? For example, if you think this is intended for someone who needs to learn new skills or someone who is in a policy-making position how do you know this?"
  },
  {
    "objectID": "ref/Template.html#how-is-the-piece-structured",
    "href": "ref/Template.html#how-is-the-piece-structured",
    "title": "Summary of Reading",
    "section": "3 How is the piece structured?",
    "text": "3 How is the piece structured?\n\nBriefly, how is it organised in terms of headings, sub-headings, sections, etc.? Can you explain this structure responds to the kind of reading and type of audience?"
  },
  {
    "objectID": "ref/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "href": "ref/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "title": "Summary of Reading",
    "section": "4 What are the key ideas, concepts, or theories discussed?",
    "text": "4 What are the key ideas, concepts, or theories discussed?\n\nBriefly identify the specific areas that the contribution engages with. How do you know this?"
  },
  {
    "objectID": "ref/Template.html#what-is-the-overall-contribution",
    "href": "ref/Template.html#what-is-the-overall-contribution",
    "title": "Summary of Reading",
    "section": "5 What is the overall contribution?",
    "text": "5 What is the overall contribution?\n\nWhat does it build on or what gap does it respond to? What are the key findings or conclusions?"
  },
  {
    "objectID": "ref/Template.html#what-issues-or-gaps-remain",
    "href": "ref/Template.html#what-issues-or-gaps-remain",
    "title": "Summary of Reading",
    "section": "6 What issues or gaps remain?",
    "text": "6 What issues or gaps remain?\n\nIf relevant, are there assumptions that might not hold in other contexts? Can you think of other case studies or contexts where the reading would apply and explain why? If you think the contribution is generally valid can you explain why? Are there areas for future work identified?"
  },
  {
    "objectID": "readings/ArribasBel-2018.html",
    "href": "readings/ArribasBel-2018.html",
    "title": "Geography and computers: Past, present, and future",
    "section": "",
    "text": "1. What kind of reading is it?\n“Geography and Computers: Past, Present, and Future” by Dani Arribas-Bel and Jon Reades is an academic paper published in Geography Compass in 2018¹. The paper aims to make a conceptual and theoretical contribution by exploring the historical and contemporary relationship between geography and computational methods, and by proposing future directions for the integration of data science into geographical research².\n\n\n2. Who is the intended audience?\nThe intended audience for this paper includes academics, field experts, and advanced students in geography and related disciplines². The paper’s detailed historical review, theoretical discussions, and focus on recent developments in data science suggest it is meant for those who are already familiar with the basics of geographical study and are interested in the evolving role of computational methods in the field².\n\n\n3. How is the piece structured?\nThe paper is structured as follows:\n\nIntroduction: Sets the context for the discussion by highlighting the rise of big data and its impact on geography.\nHistorical Perspective: Reviews the history of computation in geography to provide context for contemporary debates.\nRecent Developments: Documents recent advancements in data science and their implications for geography.\nFuture Directions: Reflects on how Geographic Data Science might provide a foundation for further development.\nConclusion: Summarizes the main points and discusses the potential future of geography in the context of computational advancements¹².\n\nThis structure is designed to cater to an academic audience, providing a clear and logical flow of information that helps readers navigate through complex theoretical and methodological discussions².\n\n\n4. What are the key ideas, concepts, or theories discussed?\nThe key ideas and concepts discussed include:\n\nHistorical Relationship: The long-standing relationship between geography and computers.\nBig Data: The impact of big data on geographical research and the potential for a new computational paradigm.\nGeographic Data Science: The proposal for a new field that integrates geography and data science to enhance the understanding of spatial phenomena.\nMethodological Shifts: The evolution of geographical methods in response to technological changes in computer hardware and software¹².\n\nThese concepts are central to understanding the transformation of geography in the context of computational advancements and are discussed in detail to illustrate the significance of integrating data science into geographical research².\n\n\n5. What is the overall contribution?\nThe paper builds on the existing body of geographical knowledge by highlighting the historical and contemporary relationship between geography and computational methods. It responds to the gap in understanding how big data and data science can be integrated into geographical research. The key findings include the identification of significant methodological shifts and the proposal for the establishment of Geographic Data Science as a new field¹².\n\n\n6. What issues or gaps remain?\nSome potential issues or gaps include:\n\nAssumptions: The paper assumes that the integration of data science into geography is universally beneficial, which might not hold in all contexts.\nContextual Variability: The impact of computational methods might vary across different regions and sub-disciplines of geography.\nFuture Work: Areas for future work could include exploring the applicability of data science in diverse geographical contexts and integrating qualitative approaches to provide a more holistic understanding¹².\n\n\n\n7. How does this connect to the module/programme?\nThis reading connects to the module by providing a historical and theoretical foundation for understanding the evolution of geographical methods in the context of computational advancements. It prompts reflection on the methodological shifts in the discipline and encourages students to think critically about the applicability and limitations of data science in geography. This reading likely complements other modules focused on research methods, theoretical frameworks, and the history of geographical thought, raising further questions about the integration of different methodological approaches in geography¹².\nIf you have any more questions or need further details, feel free to ask!\n¹: Geography Compass ²: University of Liverpool Repository\nConversation with Copilot 24/09/2024\n\nGeography and computers: Past, present, and future. https://scholar.archive.org/work/j7nnki6bczg25fenio5u36bslq.\nGeography and Computers: Past, present, and future. https://core.ac.uk/download/pdf/161102322.pdf.\nGeography & Computers: Past, present, and future – Geography. https://blogs.kcl.ac.uk/geography/2018/09/09/geography-computers-past-present-and-future/.\nundefined. https://web.archive.org/web/20200322035948/https://kclpure.kcl.ac.uk/portal/files/105003893/Geography_Compass_Self_Archiving_Version.pdf.\n\n\n\n\n\n\nReferences\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-WE.html",
    "href": "practicals/fragments/Practical-X-Grouping_Data-WE.html",
    "title": "Preamble",
    "section": "",
    "text": "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: this group of observations are similar to one another, that group is dissimilar to this group. But what defines similarity and difference? There is no one answer to that question and so there are many different ways to cluster data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-WE.html#load-geodata-for-display",
    "href": "practicals/fragments/Practical-X-Grouping_Data-WE.html#load-geodata-for-display",
    "title": "Preamble",
    "section": "Load GeoData for Display",
    "text": "Load GeoData for Display\n# Load Water GeoPackage\nw_path = os.path.join('data','geo','Water.gpkg')\nif not os.path.exists(w_path):\n    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n    water.to_file(w_path)\n    print(\"Downloaded Water.gpkg file.\")\nelse:\n    water = gpd.read_file(w_path)\n\n# Boroughs GeoPackage\nb_path = os.path.join('data','geo','Boroughs.gpkg')\nif not os.path.exists(b_path):\n    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n    boroughs.to_file(b_path)\n    print(\"Downloaded Boroughs.gpkg file.\")\nelse:\n    boroughs = gpd.read_file(b_path)\n\nUseful Functions for Plotting\ndef plt_ldn(w=water, b=boroughs):\n    fig, ax = plt.subplots(1, figsize=(14, 12))\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n    ax.set_xlim([502000,563000])\n    ax.set_ylim([155000,201500])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return fig, ax\n\ndef default_cmap(n, outliers=False):\n    cmap = mpl.cm.get_cmap('viridis_r', n)\n    colors = cmap(np.linspace(0,1,n))\n    if outliers:\n        gray = np.array([225/256, 225/256, 225/256, 1])\n        colors = np.insert(colors, 0, gray, axis=0)\n    return ListedColormap(colors)\n\n# mappable = ax.collections[-1] if you add the geopandas\n# plot last.\ndef add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n                    boundaries=breaks,\n                    extend=('min' if outliers else 'neither'), \n                    spacing='uniform',\n                    orientation='horizontal',\n                    fraction=0.05, shrink=0.5, pad=0.05)\n    cb.set_label(\"Cluster Number\")"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-WE.html#loading-data",
    "href": "practicals/fragments/Practical-X-Grouping_Data-WE.html#loading-data",
    "title": "Preamble",
    "section": "Loading Data",
    "text": "Loading Data\n\nLoad the Listings Data\nFeel free to download this manually and load it locally rather that loading via the URL:\nvsize=100\nwe = pd.read_csv(os.path.join('data','clean',f'2020-08-24-listing-description-w2v-{vsize}.csv.gz'), compression='gzip', low_memory=False)\nprint(f\"Data frame is {we.shape[0]:,} x {we.shape[1]}\")\nwe.head(3)\nYou should have: Data frame is 74,184 x 101.\n\n\nLink Back to Listings\ndf = pd.read_csv(os.path.join('data','raw','2020-08-24-listings.csv.gz'), compression='gzip',\n                low_memory=False, usecols=['id','longitude','latitude','room_type','price'])\ndf['price'] = df.price.str.replace('$','').str.replace(',','').astype(float)\ndf.drop(df[((df.latitude.isna())|(df.longitude.isna()))].index, axis=0, inplace=True)\ndf.drop(df[((df.latitude &lt; 40)|(df.longitude &gt; 1))].index, axis=0, inplace=True)\ndf['id'] = df.id.astype(int)\ndf = df[df.room_type=='Entire home/apt']\nprint(df.shape)\ndf.set_index('id')\nwedf = pd.merge(df, we, on='id')\ngdf = gpd.GeoDataFrame(wedf, \n      geometry=gpd.points_from_xy(wedf['longitude'], wedf['latitude'], crs='epsg:4326'))\ngdf = gdf.to_crs('epsg:27700').drop(columns=['longitude','latitude','room_type']).set_index('id')\nprint(gdf.shape)\ngdf.plot(column='WordVec 1', scheme='quantiles', k=5, \n         cmap='plasma', figsize=(10,8), markersize=1, alpha=0.5);\n\n\nAggregating Listings by MSOA\nNext, let’s link all this using the MSOA Geography that we created last week and a mix or merge and sjoin!\nmsoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\n# ml == MSOA Listings\nml = gpd.sjoin(gdf, msoas[['MSOA11CD','geometry']], op='within').drop(columns=\n    ['index_right','geometry']\n)\nml.head()\npcts = [0.15, 0.85]\nprice_range = ml[['MSOA11CD','price']].groupby('MSOA11CD').describe(percentiles=pcts).reset_index().set_index('MSOA11CD')\nprice_range.columns = price_range.columns.get_level_values(1)\nprice_range = price_range[[f\"{x*100:0.0f}%\" for x in pcts]]\nprint(price_range.shape)\nprice_range.head()\nmlp = pd.merge(ml, price_range, left_on='MSOA11CD', right_index=True)\nmlp.head()[['price']+[f\"{x*100:0.0f}%\" for x in pcts]]\nmlp['keep'] = (mlp.price &gt;= mlp[f\"{pcts[0]*100:0.0f}%\"]) & (mlp.price &lt;= mlp[f\"{pcts[1]*100:0.0f}%\"])\nprint(f\"Keeping {mlp.keep.sum()} of {mlp.shape[0]} ({mlp.keep.sum()/mlp.shape[0]:0.3f})\")\nmlg = mlp[mlp.keep==True].groupby('MSOA11CD').mean().reset_index().set_index('MSOA11CD')\nprint(mlg.shape)\nmlg.head()\n#rs = RobustScaler(quantile_range=[2.5,97.5])\nYou should see wide ranges of counts by roomt type in the first MSOA alone: the largest number of listings is for Entire home/apt but there are 242 multi-host listings compared to ‘just’ 156 non-multis. I believe this is the City of London though, which is quite unusual for a MSOA and should probably be treated as an outlier in most cases.\ntsne = TSNE(n_components=3, perplexity=40, early_exaggeration=3.0,\n            learning_rate=9.0, n_iter=500, n_iter_without_progress=300,\n            min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0,\n            random_state=42, method='barnes_hut', angle=0.5)\nX = tsne.fit_transform(mlg)\n\nfrom mpl_toolkits.mplot3d import Axes3D\ndtsne = pd.DataFrame({'MSOA11CD':mlg.index,'D1':X[:,0], 'D2':X[:,1], 'D3':X[:,2]})\nf = plt.figure(figsize=(10,10))\nax = Axes3D(f)\nax.scatter(dtsne.D1, dtsne.D2, dtsne.D3, marker='.')"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-WE.html#clustering-data-frame",
    "href": "practicals/fragments/Practical-X-Grouping_Data-WE.html#clustering-data-frame",
    "title": "Preamble",
    "section": "Clustering Data Frame",
    "text": "Clustering Data Frame\ndtsne.head()\ncldf = dtsne.set_index('MSOA11CD').copy()\nrs = pd.merge(msoas[['MSOA11CD','geometry']], cldf, on='MSOA11CD', how='inner').set_index('MSOA11CD')\nrs.head()\ncols_to_plot = np.random.choice(cldf.columns.values, 2, replace=False)\nprint(\"Plotting cols: \" + \", \".join(cols_to_plot))\nc_nm   = 'KMeans' # Clustering name\nk_pref = 4 # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\n\n# Add it to the data frame\ns = pd.Series(kmeans.labels_, index=cldf.index) \nrs[c_nm] = s\n\n# How are the clusters distributed?\nax = sns.histplot(data=rs, x=c_nm)\nplt.xticks(range(rs[c_nm].min(), math.ceil(rs[c_nm].max())+1));\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(rs, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde');\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True)\nOne More Thing…\nThere’s just one little problem: what assumption did I make when I started this k-means cluster analysis? It’s a huge one, and it’s one of the reasons that k-means clustering can be problematic when used naively…\nSTOP. What critical assumption did we make when running this analysis? The ‘Right’ Number of Clusters Again, there’s more than one way to skin this cat. In Geocomputation they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try it for a range of values…\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nx = []\ny = []\n\nfor k in range(2,41):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gcf().suptitle(\"Average Silhouette Scores\");\nk_pref=7\n    \n#############\n# Do the clustering using the main columns\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n\n# We do this for plotting\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(cldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n    \n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\nInterpreting the Results\nSTOP. Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine.\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True)\n‘Representative’ Centroids To get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\nDBScan\nOf course, as we’ve said above k-means is just one way of clustering, DBScan is another. Unlike k-means, we don’t need to specify the number of clusters in advance. Which sounds great, but we still need to specify other parameters (typically, these are known as hyperparameters because they are about specifying parameters that help the aglorithm to find the right solution… or final set of parameters!) and these can have a huge impact on our results!\nFind a Reasonable Value for Epsilon\nBefore we an use DBSCAN it’s useful to find a good value for Epsilon. We can look for the point of maximum ‘curvature’ in a nearest neigbhours plot. Which seems to be in the vicinity of 0.55. Tips on selecting min_pts can be found here.\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(cldf)\ndistances, indices = nbrs.kneighbors(cldf)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances);\naxes = plt.gca()\naxes.set_ylim([0.0,2.5])\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.5, 1.5, 0.01):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf.shape[1]+1).fit(cldf.values)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\nodf['Count'] = odf.Count.astype(float)\nxmin = odf[odf.Cluster=='Cluster 0'].Epsilon.min()\nxmax = odf[(odf.Cluster=='Cluster -1') & (odf.Count &lt; cldf.shape[0]/5)].Epsilon.min()\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster', ax=ax);\nax.vlines(0.97, 0, ax.get_ylim()[1], colors='r', linestyles='dashed', label='Epsilon');\ne = 0.97\ndbs = DBSCAN(eps=e, min_samples=cldf.shape[1]+1).fit(cldf.values)\ns = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, cmap='plasma', categorical=True, legend=True);\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 4\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=rs.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True)\n\nWhich Clustering Approach is Right?\nThe reason that there is no ‘right’ approach to clustering is that it all depends on what you’re trying to accomplish and how you’re reasoning about your problem. The image below highlights the extent to which the different clustering approaches in sklearn can produce different results – and this is only for the non-geographic algorithms!\n\nNote: for geographically-aware clustering you need to look at PySAL.\nTo think about this in a little more detail:\n\nIf I run an online company and I want to classify my customers on the basis of their product purchases, then I probably don’t care much about where they are, only about what they buy, and so my clustering approach doesn’t need to take geography into account. I might well discover that many of my most valuable customers live in a few areas, but that is a finding, not a factor, in my research.\nConversely, if I am looking for cancer clusters then I might well care a lot about geography because I want to make sure that I don’t overlook an important cluster of cases because it’s ‘hidden’ inside an area with lots of people who don’t have cancer. In that case, I want my clusters to take geography into account. That approach might classify an area with a smaller proportion of cancer patients as part of a ‘cancer cluster’ but that’s because it is still significant because of the geography.\n\nSo you can undertake a spatial analysis using either approach, it just depends on the role that you think geography should play in producing the clusters in the first place. We’ll see this in action today!\n\nEnsure Plotting Output\nimport matplotlib as mpl\nmpl.use('TkAgg')\n%matplotlib inline\n\n\nImporting the Libraries\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport requests\nimport zipfile\nimport re\nimport os\nimport pickle as pk\n\nfrom io import BytesIO, StringIO\nfrom os.path import join as pj\nfrom pathlib import Path\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\n\nimport sklearn\nsklv = int(sklearn.__version__.replace(\".\",\"\"))\nif sklv &lt; 210:\n    print(\"SciKit-Learn verion is: \" + sklearn.__version__)\n    print(\"The OPTICS part of this notebook relies on a version &gt;= 0.21.0\")\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn import preprocessing\nfrom sklearn import cluster\n\nimport random\nrandom.seed(42)    # For reproducibility\nnp.random.seed(42) # For reproducibility\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\nsrc = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Census.zip?raw=true'\ndst = os.path.join('analysis','Census.zip')\n\nif not os.path.exists(dst):\n    if not os.path.exists(os.path.dirname(dst)):\n        os.makedirs(os.path.dirname(dst))\n    \n    print(\"Downloading...\")\n    r = requests.get(src, stream=True)\n    \n    with open(dst, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=128):\n            fd.write(chunk)\nelse:\n    print(\"File already downloaded.\")\n    \nprint(\"Done.\")\n\n\n\nLoading the NomisWeb Data\nYou may need to make a few adjustments to the path to get the data loaded on your own computer. But notice what we’re now able to do here: using the zipfile library we can extract a data file (or any other file) from the Zip archive without even having to open it. Saves even more time and disk space!\nz = zipfile.ZipFile(os.path.join('analysis','Census.zip'))\nz.namelist()\nWe’re going to save each data set to a separate data frame to make it easier to work with during cleaning. But note that this code is fairly flexible since we stick each new dataframe in a dictionary (d) where we can retrieve them via an iterator.\nraw   = {}\nclean = {}\ntotal_cols = 0\n\nfor r in range(0, len(z.namelist())):\n    \n    m  = re.search(\"(?:-)([^\\.]+)\", z.namelist()[r])\n    nm = m.group(1)\n    \n    print(\"Processing {0} file: \".format(nm))\n    \n    with z.open(z.namelist()[r]) as f:\n                \n        if z.namelist()[r] == '99521530-Activity.csv': \n            raw[nm] = pd.read_csv(BytesIO(f.read()), header=7, skip_blank_lines=True, skipfooter=7, engine='python')\n        else:\n            raw[nm] = pd.read_csv(BytesIO(f.read()), header=6, skip_blank_lines=True, skipfooter=7, engine='python')\n    \n    print(\"\\tShape of dataframe is {0} rows by {1} columns\".format(raw[nm].shape[0], raw[nm].shape[1]))\n    total_cols += raw[nm].shape[1]\n\nprint(\"There are {0} columns in all.\".format(total_cols))\nnm  = 'Occupation'\nurl = 'https://github.com/jreades/urb-studies-predicting-gentrification/raw/master/data/src/2011/ks610ew.csv.gz'\n\nprint(f\"Processing {nm}\")\ndf = pd.read_csv(url, header=7, skip_blank_lines=True, compression='gzip', low_memory=False)\nmapping = {\n    '1. Managers, directors and senior officials':'Managers',\n    '2. Professional occupations':'Professionals',\n    '3. Associate professional and technical occupations':'Associates',\n    '4. Administrative and secretarial occupations':'Administrative',\n    '5. Skilled trades occupations':'Skilled trades',\n    '6. Caring, leisure and other service occupations':'Caring and Leisure',\n    '7. Sales and customer service occupations':'Customer Service',\n    '8. Process plant and machine operatives':'Operatives',\n    '9. Elementary occupations':'Elementary'\n}\ndf.rename(columns=mapping, inplace=True)\ndf.drop(['2011 super output area - lower layer','All categories: Occupation'], axis=1, inplace=True)\n\nraw[nm] = df\ndf.sample(3, random_state=42)\nnm  = 'Income'\nurl  = 'https://data.london.gov.uk/download/household-income-estimates-small-areas/7c1099d9-327b-4f20-abb8-8c24a3c10c47/modelled-household-income-estimates-lsoa.csv'\n\nprint(f\"Processing {nm}\")\ndf = pd.read_csv(url, encoding='latin-1')[['Code','Median 2011/12']]\ndf['Median Income'] = df['Median 2011/12'].str.replace('£','').str.replace(',','').astype('float')\ndf.drop('Median 2011/12', axis=1, inplace=True)\n\nraw[nm] = df\ndf.sample(3, random_state=42)\nnm  = 'Housing'\nurl = 'https://data.london.gov.uk/download/average-house-prices/9a92fbaf-c04e-498a-9f8c-6c85f280817e/land-registry-house-prices-LSOA.csv'\n\nprint(f\"Processing {nm}\")\ndf = pd.read_csv(url, encoding='latin-1', low_memory=False)\n\ndf['Borough'] = df.Area.str.replace(' [0-9A-Z]{4}$','')\ndf.drop('Area', axis=1, inplace=True)\n\ndf = df[ (df.Year=='Year ending Dec 2011') & (df.Measure=='Median') ][['Code','Value','Borough']]\n\n# Note: not all have a value for this year!\ndf['Median House Price'] = df.Value.str.replace(':','-1').astype(float)\n\nla = df.groupby('Borough')\nla_prices = pd.DataFrame(la['Median House Price'].median())\n\ndf = df.join(la_prices, how='inner', on='Borough', rsuffix='_la')\n\ndf.loc[df['Median House Price'] &lt; 50000,'Median House Price'] =  df[df['Median House Price'] &lt; 50000]['Median House Price_la']\ndf.drop(['Value','Borough','Median House Price_la'], inplace=True, axis=1)\n\nraw[nm] = df\ndf.sample(3, random_state=42)\n\n\nONS Boundary Data\nWe also need to download the LSOA boundary data. A quick Google search on “2011 LSOA boundaries” will lead you to the Data.gov.uk portal. The rest is fairly straightforward: * We want ‘generalised’ because that means that they’ve removed some of the detail from the boundaries so the file will load (and render) more quickly. * We want ‘clipped’ because that means that the boundaries have been clipped to the edges of the land (e.g. the Thames; the ‘Full’ data set splits the Thames down the middle between adjacent LSOAs).\n\nSaving Time\nAgain, in order to get you started more quickly I’ve already created a ‘pack’ for you. However, note that the format of this is a GeoPackage, this is a fairly new file format designed to replace ESRI’s antique Shapefile format, and it allows us to include all kinds of useful information as part of the download as well as doing away with the need to unzip a download first! So here we load the data directly into a geopandas dataframe:\nsrc = 'https://github.com/kingsgeocomp/applied_gsa/raw/master/data/London%20LSOAs.gpkg'\n\ngdf = gpd.read_file(src)\nprint(\"Shape of LSOA file: {0} rows by {1} columns\".format(gdf.shape[0], gdf.shape[1]))\ngdf.columns = [x.lower() for x in gdf.columns.values]\ngdf.set_index('lsoa11cd', drop=True, inplace=True)\ngdf.sample(4)\n\n\nError!\nDepending on your version of GDAL/Fiona, you may not be able to read the GeoPackage file directly. In this case you will need to replace the code above with the code below for downloading and extracting a Shapefile from a Zip archive:\nsrc = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.zip?raw=true'\ndst = os.path.join('analysis','LSOAs.zip')\nzpd = 'analysis'\n\nif not os.path.exists(dst):\n    if not os.path.exists(os.path.dirname(dst)):\n        os.makedirs(os.path.dirname(dst))\n\n    r = requests.get(src, stream=True)\n\n    with open(dst, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=128):\n            fd.write(chunk)\n\nif not os.path.exists(zpd):\n    os.makedirs(os.path.dirname(zpd))\n    \nzp = zipfile.ZipFile(dst, 'r')\nzp.extractall(zpd)\nzp.close()\n\ngdf = gpd.read_file(os.path.join('analysis','lsoas','Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.shp'))\ngdf.crs = {'init' :'epsg:27700'}\nprint(\\\"Shape of LSOA file: {0} rows by {1} columns\\\".format(gdf.shape[0], gdf.shape[1]))\ngdf.set_index('lsoa11cd', drop=True, inplace=True)\ngdf.sample(4)\nYou can probably see why I’m a big fan of GeoPackages when they’re available!\n\n\n\nOther Sources of Data\nIf you’re more interested in US Census data then there’s a nice-looking (though I haven’t used it) wrapper to the Census API. And Spielman and Singleton have done some work on large-scale geodemographic clustering of U.S. Census geographies.\n\n\nDirect Downloads\nThese are already clean, so we can just copy them over.\nfor t in ['Occupation','Housing','Income']:\n    raw[t].rename(columns={'Code':'mnemonic'}, inplace=True)\n    print(raw[t].columns)\n    clean[t] = raw[t]\n\n\nDwellings\nFrom dwellings we’re mainly interested in the housing type since we would expect that housing typologies will be a determinant of the types of people who live in an area. We could look at places with no usual residents as well, or explore the distribution of shared dwellings, but this is a pretty good start.\nt = 'Dwellings'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Whole house or bungalow: Detached', \n    'Whole house or bungalow: Semi-detached',\n    'Whole house or bungalow: Terraced (including end-terrace)',\n    'Flat, maisonette or apartment: Purpose-built block of flats or tenement',\n    'Flat, maisonette or apartment: Part of a converted or shared house (including bed-sits)',\n    'Flat, maisonette or apartment: In a commercial building'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nmapping = {}\nfor c in selection[1:]:\n    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n    nm = m.group(1).strip()\n    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n    mapping[c] = nm\n\nclean[t].rename(columns=mapping, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nAge\nClearly, some areas have more young people, some have older people, and some will be composed of families. A lot of these are going to be tied to ‘lifestage’ and so will help us to understand something about the types of areas in which they live.\nt = 'Age'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Age 0 to 14',\n    'Age 15 to 24',\n    'Age 25 to 44',\n    'Age 45 to 64',\n    'Age 65+'\n]\n\n# Derived columns\nraw[t]['Age 0 to 14']  = raw[t]['Age 0 to 4'] + raw[t]['Age 5 to 7'] + raw[t]['Age 8 to 9'] + raw[t]['Age 10 to 14'] \nraw[t]['Age 15 to 24'] = raw[t]['Age 15'] + raw[t]['Age 16 to 17'] + raw[t]['Age 18 to 19'] + raw[t]['Age 20 to 24']\nraw[t]['Age 25 to 44'] = raw[t]['Age 25 to 29'] + raw[t]['Age 30 to 44']\nraw[t]['Age 45 to 64'] = raw[t]['Age 45 to 59'] + raw[t]['Age 60 to 64']\nraw[t]['Age 65+']      = raw[t]['Age 65 to 74'] + raw[t]['Age 75 to 84'] + raw[t]['Age 85 to 89'] + raw[t]['Age 90 and over']\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nEthnicity\nWe might also think that the balance of ethnic groups might impact a categorisation of LSOAs in London.\nt = 'Ethnicity'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'White', \n    'Mixed/multiple ethnic groups', \n    'Asian/Asian British', \n    'Black/African/Caribbean/Black British', \n    'Other ethnic group'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nRooms\nLet’s next incorporate the amount of space available to each household.\nt = 'Rooms'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Occupancy rating (bedrooms) of -1 or less',\n    'Occupancy rating (rooms) of -1 or less',\n    'Average household size', \n#    'Average number of bedrooms per household',\n#    'Average number of rooms per household',\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nVehicles\nCar ownership and use is also known to be a good predictor of social and economic ‘status’: Guy Lansley’s article on the DLVA’s registration database offers a useful perpective on the usefulness of this approach.\nt = 'Vehicles'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'No cars or vans in household', \n    '1 car or van in household',\n    '2 cars or vans in household', \n    '3 or more cars or vans in household'\n]\n\n# Calculate a new column\nraw[t]['3 or more cars or vans in household'] = raw[t]['3 cars or vans in household'] + raw[t]['4 or more cars or vans in household']\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nTenure\nOwnership structure is another categorisation predictor.\nt = 'Tenure'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Owned', \n    'Social rented', \n    'Private rented',\n    'Shared ownership (part owned and part rented)'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].rename(columns={'Shared ownership (part owned and part rented)':'Shared ownership'}, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nQualifications\nYou can find out a bit more about qualifications here.\nt = 'Qualifications'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Highest level of qualification: Below Level 3 qualifications',\n    'Highest level of qualification: Level 3 and above qualifications',\n    'Highest level of qualification: Other qualifications'\n]\n\n# Derive a new aggregate field for 'didn't complete HS'\nraw[t]['Highest level of qualification: Below Level 3 qualifications'] = \\\n    raw[t]['No qualifications'] + \\\n    raw[t]['Highest level of qualification: Level 1 qualifications'] + \\\n    raw[t]['Highest level of qualification: Level 2 qualifications'] + \\\n    raw[t]['Highest level of qualification: Apprenticeship'] \n\nraw[t]['Highest level of qualification: Level 3 and above qualifications'] = \\\n    raw[t]['Highest level of qualification: Level 3 qualifications'] + \\\n    raw[t]['Highest level of qualification: Level 4 qualifications and above']\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nmapping = {}\nfor c in selection[1:]:\n    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n    nm = m.group(1).strip()\n    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n    mapping[c] = nm\n\nclean[t].rename(columns=mapping, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nActivity\nt = 'Activity'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Economically active: In employment',\n    'Economically active: Unemployed',\n    'Economically active: Full-time student',\n#    'Economically inactive: Retired',\n    'Economically inactive: Looking after home or family',\n    'Economically inactive: Long-term sick or disabled',\n#    'Economically inactive: Other'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nmapping = {}\nfor c in selection[1:]:\n    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n    nm = m.group(1).strip()\n    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n    mapping[c] = nm\n\nclean[t].rename(columns=mapping, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nStandardisation with SKLearn\nLet’s try standardising the data now:\n# Here's how we can rescale and transform data easily\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nrandom.seed(42)\nt   = random.sample(population=list(clean.keys()), k=1)[0]\ncol = random.sample(population=list(clean[t].columns.values[1:]), k=1)[0]\nprint(f\"Looking at {col} column from {t}.\")\nHere’s the ‘original’ distribution:\nplt.rcParams['figure.figsize']=(7,3)\nsns.distplot(clean[t][col], kde=False)\nHere’s the version that has been re-scaled (standardised) using Min/Max rescaling:\nplt.rcParams['figure.figsize']=(7,3)\nsns.distplot(preprocessing.minmax_scale(clean[t][col].values.reshape(-1,1)), kde=False)\nHere’s a version that has been robustly rescaled:\nplt.rcParams['figure.figsize']=(7,3)\nsns.distplot(preprocessing.robust_scale(clean[t][col].values.reshape(-1,1), quantile_range=[5.0, 95.0]), kde=False)\nAnd here’s a version that has been Power Transformed… spot the difference!\nsns.distplot(\n    preprocessing.power_transform(clean[t][col].values.reshape(-1,1), method='yeo-johnson'), kde=False)\nCombining transformation and rescaling:\nsns.distplot(\n    preprocessing.robust_scale(\n        preprocessing.power_transform(\n            clean[t][col].values.reshape(-1,1), method='yeo-johnson'\n        ), quantile_range=[5.0, 95.0] \n    ), \n    kde=False\n)\n# Set up a new dictionary for the transforms\ntransformed = {}\n\ntransformer = preprocessing.PowerTransformer()\nscaler      = preprocessing.RobustScaler(quantile_range=[5.0, 95.0])\n#scaler      = preprocessing.MinMaxScaler()\n\n# Simple way to drop groups of data we don't want...\nsuppress       = set(['Rooms','Vehicles'])\n\nfor k in set(clean.keys()).difference(suppress):\n    print(f\"Transforming {k}\")\n    df = clean[k].copy(deep=True)\n    df.set_index('mnemonic', inplace=True)\n    \n    # For rescale and transforming everything when the operations\n    # apply to each Series separately you can do it as a 1-liner like this:\n    #df[df.columns] = scaler.fit_transform(transformer.fit_transform(df[df.columns]))\n\n    # To calculate within-*column* proportions it's like this:\n    #for c in df.columns.values: \n    #    df[c] = scaler.fit_transform( (df[c]/df[c].max() ).values.reshape(-1, 1) )\n\n    # To calculate within-*group* proportions it's like this:\n    if k in ['Housing','Income','Rooms']:\n        df[df.columns] = scaler.fit_transform( df[df.columns] )\n    else: \n        df['sum'] = df[list(df.columns)].sum(axis=1)\n        for c in df.columns.values:\n            if c == 'sum':\n                df.drop(['sum'], axis=1, inplace=True)\n            else:\n                df[c] = scaler.fit_transform( (df[c]/df['sum']).values.reshape(-1, 1) )\n            \n    #print(df.sample(5, random_state=42))\n    transformed[k] = df\n\n\nCreating the Single Data Set\nNow that we’ve converted everything to percentages, it’s time to bring the data together! We’ll initialise the data frame using the first matching data set, and then iterate over the rest, merging the data frames as we go.\nmatching = list(transformed.keys())\nprint(\"Found the following data sets:\\n\\t\" + \", \".join(matching))\n\n# Initialise the data frame simply by grabbing the\n# very first existing data frame and copying it \n# directly (SCaled Data Frame == scdf)\nscdf  = transformed[matching[0]].copy() \nlsoac = clean[matching[0]].copy() \n\nfor m in range(1, len(matching)):\n    scdf  = scdf.merge(transformed[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')\n    lsoac = lsoac.merge(clean[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')\n\nscdf.to_csv(os.path.join('data','Scaled_and_Transformed.csv.gz'), compression='gzip')\nlsoac.to_csv(os.path.join('data','Cleaned.csv.gz'), compression='gzip')\nprint(\"Shape of full data frame is {0} by {1}\".format(scdf.shape[0], scdf.shape[1]))\nWith luck you still have 4,835 rows, but now you have rather fewer than 88 columns.\nrandom.seed(42)\ncols_to_plot = random.sample(population=list(scdf.columns.values), k=3)\nprint(\"Columns to plot: \" + \", \".join(cols_to_plot))\n# The data as it is now...\nsns.set(style=\"whitegrid\")\nsns.pairplot(lsoac, \n             vars=cols_to_plot, \n             markers=\".\", height=3, diag_kind='kde')\n# The data as it is now...\nsns.set(style=\"whitegrid\")\nsns.pairplot(scdf, \n             vars=cols_to_plot, \n             markers=\".\", height=3, diag_kind='kde')\n\nSTOP. Making sure that you understand how and why this results differns from the same plot above.\n:::\nRight, so you can see that rescaling the dimension hasn’t actually changed the relationships within each dimension, or even between dimensions, but it has changed the overall range so that the the data is broadly re-centered on 0 but we still have the original outliers from the raw data. You could also do IQR standardisation (0.25 and 0.75) with the percentages, but in those cases you would have more outliers and then more extreme values skewing the results of the clustering algorithm.\n\nFreeing Up Memory\nWe now have quite a few variables/datasets in memory, so it’s a good idea to free up some RAM by getting rid of anything we no longer need…\nin_scope  = set([x for x in dir() if not x.startswith('_')])\nto_delete = set(['raw','clean','transformed','col','k','c','lsoac','scdf'])\nz = list(in_scope.intersection(to_delete))\ndel(z)\n\n\nClustering Your Data\nOK, we’re finally here! It’s time to cluster the cleaned, normalised, and standardised data set! We’re going to start with the best-known clustering technique (k-means) and work from there… Don’t take my word for it, here are the 5 Clustering Techniques Every Data Scientist Should Know. This is also a good point to refer back to some of what we’ve been doing (and it’s a good point to potentially disagree with me!) since clustering in high dimensions can be problematic (i.e. the more dimensions the worse the Euclidean distance gets as a cluster metric).\nThe effectiveness of clustering algorithms is usually demonstrated using the ‘iris data’ – it’s available by default with both Seaborn and SciKit-Learn. This data doesn’t usually need normalisation but it’s a good way to start looking at the data across four dimensions and seeing how it varies and why some dimensions are ‘good’ for clustering, while others are ‘not useful’…\n\nUnfortunately, our data is a lot messier and has many more dimensions (&gt;25) than this.\n:::\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions\n\n\nBrief Discussion\nIn the practical I’ve followed the Geocomputation approach of basically converting everything to a share (percentage) and then clustering on that. This is one way to approach this problem, but there are many others. For instance, many people might skip the percentages part and apply robust rescaling (sklearn.preprocessing.RobustScaler) using centering and quantile standardisation (the 5th and 95th, for example) instead. And possibly using a normalising transformation (such as a Power Transform) as well.\nI would also consider using PCA on groups of related variables (e.g. the housing features as a group, the ethnicity features as a group, etc.) and then take the first few eigenvalues from each group and cluster on all of those together. This would remove quite a bit of the correlation between variables and still allow us to perform hierarchical and other types of clustering on the result. It might also do a better job of preserving outliers.\n\n\nCreate an Output Directory and Load the Data\no_dir = os.path.join('outputs','clusters')\nif os.path.isdir(o_dir) is not True:\n    print(\"Creating '{0}' directory.\".format(o_dir))\n    os.makedirs(o_dir)\ndf = pd.read_csv(os.path.join('data','Scaled_and_Transformed.csv.gz'))\ndf.rename(columns={'mnemonic':'lsoacd'}, inplace=True)\ndf.set_index('lsoacd', inplace=True)\ndf.describe()\ndf.sample(3, random_state=42)\n\n\nGrab Borough Boundaries and Water Courses\nNote: if reading these GeoPackages gives you errors then you will need to comment out the following two lines from the plt_ldn function immediately below:\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n# Load Water GeoPackage\nw_path = os.path.join('data','Water.gpkg')\nif not os.path.exists(w_path):\n    water = gpd.read_file('https://github.com/kingsgeocomp/applied_gsa/raw/master/data/Water.gpkg')\n    water.to_file(w_path)\n    print(\"Downloaded Water.gpkg file.\")\nelse:\n    water = gpd.read_file(w_path)\n\n# Boroughs GeoPackage\nb_path = os.path.join('data','Boroughs.gpkg')\nif not os.path.exists(b_path):\n    boroughs = gpd.read_file('https://github.com/kingsgeocomp/applied_gsa/raw/master/data/Boroughs.gpkg')\n    boroughs.to_file(b_path)\n    print(\"Downloaded Boroughs.gpkg file.\")\nelse:\n    boroughs = gpd.read_file(b_path)\n\n\nUseful Functions for Plotting\ndef plt_ldn(w=water, b=boroughs):\n    fig, ax = plt.subplots(1, figsize=(14, 12))\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n    ax.set_xlim([502000,563000])\n    ax.set_ylim([155000,201500])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return fig, ax\n\ndef default_cmap(n, outliers=False):\n    cmap = mpl.cm.get_cmap('viridis_r', n)\n    colors = cmap(np.linspace(0,1,n))\n    if outliers:\n        gray = np.array([225/256, 225/256, 225/256, 1])\n        colors = np.insert(colors, 0, gray, axis=0)\n    return ListedColormap(colors)\n\n# mappable = ax.collections[-1] if you add the geopandas\n# plot last.\ndef add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n                    boundaries=breaks,\n                    extend=('min' if outliers else 'neither'), \n                    spacing='uniform',\n                    orientation='horizontal',\n                    fraction=0.05, shrink=0.5, pad=0.05)\n    cb.set_label(\"Cluster Number\")\n\n\nSelect 4 Columns to Plot\nrandom.seed(42)\ncols_to_plot = random.sample(population=list(df.columns.values), k=4)\nprint(\"Columns to plot: \" + \", \".join(cols_to_plot))\n\n\nStoring Results\nresult_set = None\n\ndef add_2_rs(s, rs=result_set):\n    if rs is None:\n        # Initialise\n        rs = pd.DataFrame()\n    rs[s.name] = s\n    return rs\n\n\nK-Means\n\nImporting the Library\nfrom sklearn.cluster import KMeans\n#help(KMeans)\nThe next few code blocks may take a while to complete, largely because of the pairplot at the end where we ask Seaborn to plot every dimension against every other dimension while colouring the points according to their cluster. I’ve reduced the plotting to just three dimensions, if you want to plot all of them, then just replace the array attached to vars with main_cols, but you have to bear in mind that that is plotting 4,300 points each time it draws a plot… and there are 81 of them! It’ll take a while, but it will do it, and try doing that in Excel or SPSS?\n\n\nA First Cluster Analysis\nc_nm   = 'KMeans' # Clustering name\nk_pref = 6 # Number of clusters\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nkmeans = KMeans(n_clusters=k_pref, n_init=20, random_state=42, n_jobs=-1).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\n\n# Add it to the data frame\ncldf[c_nm] = pd.Series(kmeans.labels_, index=df.index) \n\n# How are the clusters distributed?\ncldf[c_nm].hist(bins=k_pref)\n\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(cldf, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde')\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(0,cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks))\n\nnorm    = mpl.colors.BoundaryNorm(breaks, cmap.N)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n\ndel(cgdf)\n\n\nOne More Thing…\nThere’s just one little problem: what assumption did I make when I started this k-means cluster analysis? It’s a huge one, and it’s one of the reasons that k-means clustering can be problematic when used naively…\n\nSTOP. What critical assumption did we make when running this analysis?\n:::\n\nThe ‘Right’ Number of Clusters\nAgain, there’s more than one way to skin this cat. In Geocomputation they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2...n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try it for the range 3-9.\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\ntext = []\n\nfor k in range(3,10):\n    # Debugging\n    print(\"Cluster count: \" + str(k))\n    \n    #############\n    # Do the clustering using the main columns\n    clusterer = KMeans(n_clusters=k, n_init=15, random_state=42, n_jobs=-1)\n    cluster_labels = clusterer.fit_predict(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, cluster_labels)\n    text = text + [f\"For k={k} the average silhouette_score is: {silhouette_avg:6.4f}\"]\n    \n    # Calculate the silhouette values\n    sample_silhouette_values = silhouette_samples(cldf, cluster_labels)\n    \n    #############\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(9, 5)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1\n    ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \n    y_lower = 10\n    \n    # For each of the clusters...\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        \n        # Set the color ramp\n        #cmap  = cm.get_cmap(\"Spectral\")\n        color = plt.cm.Spectral(i/k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(cluster_labels.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()\n\nprint(\"\\n\".join(text))    \n\ndel(cldf)\n\n\nInterpreting the Results\n\nSTOP. Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine.\n:::\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\n\nFinal Clustering\nLet’s repeat the clustering process one more time using the silhouette score as a guide and then map it.\n#| scrolled: true\nc_nm = 'KMeans'\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nk_pref = 4\nkmeans = KMeans(n_clusters=k_pref, n_init=75, random_state=42).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n\n# We do this for plotting\ncldf[c_nm] = s\n\n# We do this to keep track of the results\nresult_set=add_2_rs(s)\n\n\nMapping Results\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(0,cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks))\n\nnorm    = mpl.colors.BoundaryNorm(breaks, cmap.N)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}-{k_pref}.png\"), dpi=200)\ndel(cgdf)\nTo make sense of whether this is a ‘good’ result, you might want to visit datashine or think back to last year when we examined the NS-SeC data.\nYou could also think of ways of plotting how these groups differ. For instance…\n\n\n‘Representative’ Centroids\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids, cldf)\n\n\nDBScan\nOf course, as we’ve said above k-means is just one way of clustering, DBScan is another. Unlike k-means, we don’t need to specify the number of clusters in advance. Which sounds great, but we still need to specify other parameters (typically, these are known as hyperparameters because they are about specifying parameters that help the aglorithm to find the right solution… or final set of parameters!) and these can have a huge impact on our results!\n\nImporting the Library\nfrom sklearn.cluster import DBSCAN\n#?DSCAN\n\n\nFind a Reasonable Value for Epsilon\nBefore we an use DBSCAN it’s useful to find a good value for Epsilon. We can look for the point of maximum ‘curvature’ in a nearest neigbhours plot. Which seems to be in the vicinity of 0.55. Tips on selecting min_pts can be found here.\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(cldf)\ndistances, indices = nbrs.kneighbors(cldf)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\n\n\nExploration\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\n# There's an argument for making min_samples = len(df.columns)+1\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.15, 1.55, 0.01):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=12, n_jobs=-1).fit(cldf.values)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    for c in range(-1, max_clusters+1):\n        try:\n            row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\nedf.sample(random_state=42)\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)       \nxmin = odf[odf.Cluster=='Cluster 0'].Epsilon.min()\nxmax = odf[(odf.Cluster=='Cluster -1') & (odf.Count &lt; cldf.shape[0]/5)].Epsilon.min()\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.set_xlim([xmin,xmax])\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster')\n\nFinal Clustering\ne = 0.835\ndbs = DBSCAN(eps=e, min_samples=12, n_jobs=-1).fit(cldf.values)\ns = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\ncldf[c_nm] = s\nresult_set=add_2_rs(s)\nprint(s.value_counts())\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks), outliers=True)\nnorm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks, outliers=True)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\ndel(cgdf)\n\n\n‘Representative’ Centroids\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). For algorithms other than k-means it may be better to use medians than means.\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nOPTICS Clustering\nThis is a fairly new addition to sklearn and is similar to DBSCAN in that there are very few (if any) parameters to specify. This means that we’re making fewer assumptions about the nature of any clustering in the data. It also allows us to have outliers that don’t get assigned to any cluster. The focus is mainly on local density, so in some sense it’s more like a geographically aware clustering approach, but applied in the data space, not geographical space.\n\nImporting the Library\nfrom sklearn.cluster import OPTICS\n\n\nFinal Clustering\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.9850\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(df.columns)+1, max_eps=math.ceil(e * 100)/100, n_jobs=-1).fit(cldf.values)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\ncldf[c_nm] = s\nresult_set=add_2_rs(s)\n\n# Distribution\nprint(s.value_counts())\n\nMapping Clustering Results\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks), outliers=True)\nnorm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks, outliers=True)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\ndel(cgdf)\n\n‘Representative’ Centroids\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). For algorithms other than k-Means it may be better to use medians, not means.\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\nSTOP. Aside from the fact that we should probably reduce the number of dimensions on which we’re clustering, what about the process of selecting variables (a.k.a. feature selection) might have led to the result that our results are a bit crap? Hint: how did we decide what to keep and what to drop, and is this a robust approach?\n:::\n\nHDBSCAN\nNot implemented, but you could give it a try after installing the package:\nconda activate &lt;your environment name here&gt;\nconda install -c conda-forge sklearn-contrib-hdbscan\nThen it should be something like:\nimport hdbscan\nclusterer = hdbscan.HDBSCAN()\n# HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n#    gen_min_span_tree=False, leaf_size=40, memory=Memory(cachedir=None),\n#    metric='euclidean', min_cluster_size=5, min_samples=None, p=None)\nclusterer.fit(&lt;data&gt;)\nclusterer.labels_\n\n\nHierarchical Clustering\nProbably not appropriate as it tends to be confused by noise.\n\n\nSelf-Organising Maps\nSOMs offer a third type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n(Re)Installing SOMPY\n\nWARNING. The maintainers of the main SOMPY library are fairly inactive, so we’ve had to write our own version that fixes a few Python3 bugs, but this means that it can’t be installed the ‘usual’ way without also having Git installed. Consequently, I have left the output from SOMPY in place so that you can see what it will produce even if you cannot successfully install SOMPY during this practical\n\nTo work out if there is an issue, check to see if the import statement below gives you errors:\nfrom sompy.sompy import SOMFactory\nIf this import has failed with a warning about being unable to find SOM or something similar, then you will need to re-install SOMPY using a fork that I created on our Kings GSA GitHub account. For that to work, you will need to ensure that you have git installed.\nIf the following Terminal command (which should also work in the Windows Terminal) does not give you an error then git is already installed:\ngit --version\nTo install git on a Mac is fairly simple. Again, from the Terminal issue the following command:\nxcode-select --install\nThis installation may take some time over eduroam since there is a lot to download.\nOnce that’s complete, you can move on to installing SOMPY from our fork. On a Mac this is done on the Terminal with:\nconda activate &lt;your kernel name here&gt;\npip install -e git+git://github.com/kingsgeocomp/SOMPY.git#egg=SOMPY\nconda deactivate\nOn Windows you probably drop the conda part of the command.\n\nTraining the SOM\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(os.path.join(o_dir, f\"{c_nm}-Map.png\"), dpi=200)\n\n\nHere’s What I Got\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(os.path.join(o_dir,f\"{c_nm}-BMU Hit View.png\"), dpi=200)\n\nBMU Hit Map\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 5\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(os.path.join(o_dir,f\"{c_nm}-Hit Map View.png\"), dpi=200)\n\nClustering the BMUs\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\ntry:\n    cldf.drop(c_nm,inplace=True,axis=1)\nexcept KeyError:\n    pass\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\ncldf[c_nm] = s\nresult_set = add_2_rs(s)\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks))\nnorm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\ndel(cgdf)\n\nResult!\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\nRepresentative Centroids\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nWrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-WE.html#wrap-up",
    "href": "practicals/fragments/Practical-X-Grouping_Data-WE.html#wrap-up",
    "title": "Preamble",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#section",
    "href": "lectures/9.2-Linking_Spatial_Data.html#section",
    "title": "Linking Spatial Data",
    "section": "",
    "text": "Data Set 1\n\n\n\nSensorID\nLatitude\nLongitude\n\n\n\n\n1\n51.5070\n-0.1347\n\n\n2\n51.5071\n-0.0042\n\n\n3\n51.5074\n-0.1223\n\n\n4\n51.5073\n-0.1122\n\n\n5\n51.5072\n0.1589\n\n\n\ndf = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Latitude': [51.5070, 51.5071, 51.5074, 51.5073, 51.5073],\n  'Longitude': [-0.1347, -0.0042, -0.1223, -0.1122, 0.1589]\n})\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1\nTemperature\n5ºC\n\n\n1\nHumidity\n15%\n\n\n3\nTemperature\n7ºC\n\n\n4\nTemperature\n7ºC\n\n\n6\nHumidity\n18%\n\n\n\ndf1 = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Parameter': ['Temperature','Humidity','Temperature','Temperature','Humidity'], \n  'Value': ['5ºC', '15%', '7ºC', '7ºC', '18%']\n})\n\n\n\nObviously, we can use non-spatial operations on spatial data sets.\n\n\nThis wouldn’t be a particularly good design for a data structure… why?"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking Spatial Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking Spatial Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking Spatial Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking Spatial Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking Spatial Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking Spatial Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking Spatial Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking Spatial Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\nA. Here’s how I’d do this:\n\nCity = New York (probably a key)\n\nCuisine = Italian (probably a key)\n\nStyle = Family (probably an enumeration/free text)\n\nLocation = Within Distance of X from Request (probably a buffered spatial query)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking Spatial Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking Spatial Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server.\n\n\n\n\n\nSee also Carto and competitors."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking Spatial Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich, but £££ for commercial use.\nMySQL: free, unless you want dedicated support; was feature-poor (though this looks to have changed with MySQL8); heavyweight.\nPostgreSQL: free, unless you want dedicated support; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\nGeoParquet+DuckDB: not as full-featured as Postgres, but evolving quickly and much simpler to configure.\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS\n\n\n\n\n\nLinking Spatial Data • Jon Reades"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#dummy-variables",
    "href": "lectures/8.4-Analysing_Text.html#dummy-variables",
    "title": "Analysing Text",
    "section": "Dummy Variables",
    "text": "Dummy Variables\nThe concept of ‘dummy variables’ in economics/regression is a useful point to start thinking about text:\n\n\n\nTopic\nDummy\n\n\n\n\nNews\n0\n\n\nCulture\n1\n\n\nPolitics\n2\n\n\nEntertainment\n3\n\n\n\n\nWhat’s the problem with this approach when you’re thinking about the topics in a document? You either have to assign each document to one, and only one, topic, or you need a lot of dummies."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#one-hot-encoders",
    "href": "lectures/8.4-Analysing_Text.html#one-hot-encoders",
    "title": "Analysing Text",
    "section": "One-Hot Encoders",
    "text": "One-Hot Encoders\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n1\n1\n0\n1\n\n\nCulture item\n0\n1\n1\n0\n\n\nPolitics item\n1\n0\n0\n1\n\n\nEntertainment item\n1\n1\n1\n1\n\n\n\n\nOne-Hot encoders are not often used this way, but for keyword detection or keyword-based classification this might be appropriate: i.e. this keyword was used in this document!\nSo the big difference is One Hot == \\(n\\) variables, Dummy == \\(n-1\\).\nDefinitely some ‘gotchas’ in deployment: one-hot models shouldn’t have an intercept unless you apply a ‘ridge shrinkage penalty’. Standardisation affects whether or not an intercept is needed."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#the-bag-of-words",
    "href": "lectures/8.4-Analysing_Text.html#the-bag-of-words",
    "title": "Analysing Text",
    "section": "The ‘Bag of Words’",
    "text": "The ‘Bag of Words’\nJust like a one-hot (binarised approach) on preceding slide but now we count occurences:\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n4\n2\n0\n6\n\n\nCulture item\n0\n4\n7\n0\n\n\nPolitics item\n3\n0\n0\n3\n\n\nEntertainment item\n3\n4\n8\n1"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#bow-in-practice",
    "href": "lectures/8.4-Analysing_Text.html#bow-in-practice",
    "title": "Analysing Text",
    "section": "BoW in Practice",
    "text": "BoW in Practice\nEnter, stage left, scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\n# Non-reusable transformer\nvectors = vectorizer.fit_transform(texts)\n\n# Reusable transformer\nvectorizer.fit(texts)\nvectors1 = vectorizer.transform(texts1)\nvectors2 = vectorizer.transform(texts2)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'All vectors: {vectors.toarray()}')"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#tfidf",
    "href": "lectures/8.4-Analysing_Text.html#tfidf",
    "title": "Analysing Text",
    "section": "TF/IDF",
    "text": "TF/IDF\nBuilds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:\n\\[\nW(t,d) = TF(t,d) / log(N/DF_{t})\n\\]\nFor example:\n\nIf the term ‘cat’ appears 3 times in a document of 100 words then Term Frequency given by: \\(TF(t,d)=3/100\\), and\nIf there are 10,000 documents and cat appears in 1,000 documents then Normalised Document Frequency given by: \\(N/DF_{t}=10000/1000\\) so the Inverse Document Frequency is \\(log(10)=1\\),\nSo IDF=1 and TF/IDF=0.03."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#tfidf-in-practice",
    "href": "lectures/8.4-Analysing_Text.html#tfidf-in-practice",
    "title": "Analysing Text",
    "section": "TF/IDF in Practice",
    "text": "TF/IDF in Practice\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Non-reusable form:\nvectors=vectorizer.fit_transform(texts)\n\n# Reusable form:\nvectorizer.fit(texts)\nvectors = vectorizer.transform(texts)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'Full vector: {vectors.toarray()}')\n\nWhat do you notice about how this code differs from the CountVectorizer?"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "href": "lectures/8.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "title": "Analysing Text",
    "section": "Term Co-Occurence Matrix (TCM)",
    "text": "Term Co-Occurence Matrix (TCM)\nThree input texts with a distance weighting (\\(d/2\\), where \\(d&lt;3\\)):\n\nthe cat sat on the mat\nthe cat sat on the fluffy mat\nthe fluffy ginger cat sat on the mat\n\n\n\n\n\nfluffy\nmat\nginger\nsat\non\ncat\nthe\n\n\n\n\nfluffy\n\n1\n1\n\n0.5\n0.5\n2.0\n\n\nmat\n\n\n\n\n0.5\n\n1.5\n\n\nginger\n\n\n\n0.5\n0.5\n1.0\n1.5\n\n\nsat\n\n\n\n\n3.0\n3.0\n2.5\n\n\non\n\n\n\n\n\n1.5\n3.0\n\n\ncat\n\n\n\n\n\n\n2.0\n\n\nthe"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#how-big-is-a-tcm",
    "href": "lectures/8.4-Analysing_Text.html#how-big-is-a-tcm",
    "title": "Analysing Text",
    "section": "How Big is a TCM?",
    "text": "How Big is a TCM?\nThe problem:\n\nA corpus with 10,000 words has a TCM of size \\(10,000^{2}\\) (100,000,000)\nA corpus with 50,000 words has a TCM of size \\(50,000^{2}\\) (2,500,000,000)\n\nCleaning is necessary, but it’s not sufficient to create a tractable TCM on a large corpus.\n\nAlthough usually used in the context of clustering, there’s also a curse of dimensionality here!"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#enter-document-embeddings",
    "href": "lectures/8.4-Analysing_Text.html#enter-document-embeddings",
    "title": "Analysing Text",
    "section": "Enter Document Embeddings",
    "text": "Enter Document Embeddings\nTypically, some kind of 2 or 3-layer neural network that ‘learns’ how to embed the TCM into a lower-dimension representation: from \\(m \\times m\\) to \\(m \\times n, n &lt;&lt; m\\).\nSimilar to PCA in terms of what we’re trying to achieve, but the process is utterly different.\n\nMany different approaches, but GloVe (Stanford), word2vec (Google), fastText (Facebook), and ELMo (Allen) or BERT (Google) are probably the best-known."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#sentiment-analysis",
    "href": "lectures/8.4-Analysing_Text.html#sentiment-analysis",
    "title": "Analysing Text",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nRequires us to deal in great detail with bi- and tri-grams because negation and sarcasm are hard. Also tends to require training/labelled data.\n\nSource."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#clustering",
    "href": "lectures/8.4-Analysing_Text.html#clustering",
    "title": "Analysing Text",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\nCluster\nGeography\nEarth Science\nHistory\nComputer Science\nTotal\n\n\n\n\n1\n126\n310\n104\n11,018\n11,558\n\n\n2\n252\n10,673\n528\n126\n11,579\n\n\n3\n803\n485\n6,730\n135\n8,153\n\n\n4\n100\n109\n6,389\n28\n6,626\n\n\nTotal\n1,281\n11,577\n13,751\n11,307\n37,916"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#topic-modelling",
    "href": "lectures/8.4-Analysing_Text.html#topic-modelling",
    "title": "Analysing Text",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nLearning associations of words (or images or many other things) to hidden ‘topics’ that generate them:"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#word-clouds",
    "href": "lectures/8.4-Analysing_Text.html#word-clouds",
    "title": "Analysing Text",
    "section": "Word Clouds",
    "text": "Word Clouds"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#resources",
    "href": "lectures/8.4-Analysing_Text.html#resources",
    "title": "Analysing Text",
    "section": "Resources",
    "text": "Resources\n\nOne-Hot vs Dummy Encoding\nCategorical encoding using Label-Encoding and One-Hot-Encoder\nCount Vectorization with scikit-learn\nCorpus Analysis with Spacy\nThe TF*IDF Algorithm Explained\nHow to Use TfidfTransformer and TfidfVectorizer\nSciKit Learn Feature Extraction\nYour Guide to LDA\nMachine Learning — Latent Dirichlet Allocation LDA\nA Beginner’s Guide to Latent Dirichlet Allocation(LDA)\nAnalyzing Documents with TF-IDF\n\nBasically any of the lessons on The Programming Historian."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#more-resources",
    "href": "lectures/8.4-Analysing_Text.html#more-resources",
    "title": "Analysing Text",
    "section": "More Resources",
    "text": "More Resources\n\n::: {column width=“50%”} - Introduction to Word Embeddings - The Current Best of Universal Word Embeddings and Sentence Embeddings - Using GloVe Embeddings - Working with Facebook’s FastText Library - Word2Vec and FastText Word Embedding with Gensim - Sentence Embeddings. Fast, please!\n\n::: {column width=“50%”} - PlasticityAI Embedding Models - Clustering text documents using k-means - Topic extraction with Non-negative Matrix Factorization and LDA - Topic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison ::: ::::\n\n\n\n\nAnalysing Text • Jon Reades"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#can-we-describe-text",
    "href": "lectures/8.2-Patterns_in_Text.html#can-we-describe-text",
    "title": "Patterns in Text",
    "section": "Can We Describe Text?",
    "text": "Can We Describe Text?\nConsider the following character sequences:\n\nfoo@bar.com\nhttps://www.ucl.ac.uk/bartlett/casa/\n(555) 102-1111\nE17 5RS\nNow, fair Hippolyta, our nuptial hour / Draws on apace. Four happy days bring in / Another moon. But, oh, methinks how slow / This old moon wanes. She lingers my desires, / Like to a stepdame or a dowager / Long withering out a young man’s revenue. (I.i.)\n\n\nWe need ways to distinguish: Upper and Lower Case, Digits, Space Characters, Other Characters, Repetition, Type… Can you do those with strings alone?"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "href": "lectures/8.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "title": "Patterns in Text",
    "section": "Strings Methods are Not Enough",
    "text": "Strings Methods are Not Enough\n'123foo456'.index('foo') # 2\n'123foo456'.split('foo') # ['123', '456']\n' 123 foo 456 '.strip()  # '123 foo 456'\n'HOW NOW BROWN COW?'.lower() # 'how now brown cow?'\n'How now brown cow?'.replace('brown ','green-')\n# 'How now green-cow?'\nSee: dir(str) for full list of string methods."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#regular-expressions",
    "href": "lectures/8.2-Patterns_in_Text.html#regular-expressions",
    "title": "Patterns in Text",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegexes are a way for talking about patterns observed in text, although their origins are rooted in philosophy and linguistics.\nImplemented in Python as:\nimport re\n# re.search(&lt;regex&gt;, &lt;str&gt;)\ns = '123foo456'\nif re.search('123',s):\n  print(\"Found a match.\")\nelse:\n  print(\"No match.\")\nPrints 'Found a match.'"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#capturing-matches",
    "href": "lectures/8.2-Patterns_in_Text.html#capturing-matches",
    "title": "Patterns in Text",
    "section": "Capturing Matches",
    "text": "Capturing Matches\nm = re.search('123',s)\nprint(m.start())\nprint(m.end())\nprint(m.span())\nprint(m.group())\nOutputs:\n0\n3\n(0,3)\n123\n\nSo, we have None if a search fails, but if it succeeds then we have attributes of the match objection like start, end, span, and group (this last is going to be particularly interesting since it tells us what matched)."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#configuring-matches",
    "href": "lectures/8.2-Patterns_in_Text.html#configuring-matches",
    "title": "Patterns in Text",
    "section": "Configuring Matches",
    "text": "Configuring Matches\ns = '123foo456'\nm = re.search('FOO',s)\nprint(m)\nm = re.search('FOO',s,re.IGNORECASE)\nprint(m)\nOutputs:\nNone\n&lt;re.Match object; span=(3, 6), match='foo'&gt;\nThe third parameter allows us to: match newlines (re.DOTALL), ignore case (re.IGNORECASE), take language into account (re.LOCALE), match across lines (re.MULTILINE), and write patterns across multiple lines (re.VERBOSE). If you need multiple options it’s re.DOTALL | re.IGNORECASE. Bitwise again!"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#more-than-one-match",
    "href": "lectures/8.2-Patterns_in_Text.html#more-than-one-match",
    "title": "Patterns in Text",
    "section": "More Than One Match",
    "text": "More Than One Match\ns = '123foo456foo789'\nlst = re.findall('foo',s)\nprint(lst)\nlst = re.finditer('foo',s)\n[x for x in lst]\nrs  = re.sub('foo',' ',s)\nprint(rs)\nrs  = re.split(' ',rs)\nprint(rs)\nOutputs:\n['foo','foo']\n[&lt;re.Match object; span=(3, 6), match='foo'&gt;, &lt;re.Match object; span=(9, 12), match='foo'&gt;]\n'123 456 789'\n['123', '456', '789']"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "href": "lectures/8.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "title": "Patterns in Text",
    "section": "Regular Expressions Do Much More",
    "text": "Regular Expressions Do Much More\nimport re\nm = re.search(r'\\$((\\d+,){2,}\\d+)',\n        \"'That will be $1,000,000 he said...'\")\nprint(m.group(1)) # '1,000,000'\nThis looks for sequences of 1-or-more digits followed by a comma… and for those sequences to repeat two or more times:\n# Look for a literal '$'\nre.search(r'\\$') \n# Group of &gt;=1 digits followed by a comma...\nre.search(r'(\\d+,)') \n# Repeated two or more times...\nre.search(r'(\\d+,){2,}') \n\n\nAlso notice the r'&lt;regex&gt;' with an r in front of the string. This means ‘raw’ and is often a required modifier for regular expression patterns. Simple ones don’t need it, but from here on out you will."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#character-classes",
    "href": "lectures/8.2-Patterns_in_Text.html#character-classes",
    "title": "Patterns in Text",
    "section": "Character Classes",
    "text": "Character Classes\n\n\n\n\n\n\n\n\nCharacters\nRegex Meta Class Options\n‘Antonyms’\n\n\n\n\na…z\n[a-z], \\w (word-like characters)\n[^a-z], \\W\n\n\nA…Z\n[A-Z], \\w (word-like characters)\n[^A-Z], \\W\n\n\n0…9\n[0-9], \\d (digits)\n[^0-9], \\D\n\n\n' ', \\n, \\t, \\r, \\f, \\v\n\\s\n\\S\n\n\n., [, ], +, $, ^, \\|, {, }, *, (, ), ?\nFor safety always precede character with a \\.\nNone\n\n\n\n\n\n\\w will include _. And \\ is, once again, important as it ‘escapes’ various characters, and options."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#metacharacters",
    "href": "lectures/8.2-Patterns_in_Text.html#metacharacters",
    "title": "Patterns in Text",
    "section": "Metacharacters",
    "text": "Metacharacters\n\n\n\nMetacharacter\nMeaning\nExample\n\n\n\n\n.\nAny character at all\nc.t\n\n\n^\nStart of a string/line\n^start\n\n\n$\nEnd of a string/line\nend$\n\n\n*\n0 or more of something\n-*\n\n\n+\n1 or more of something\n-+\n\n\n?\n0 or 1 of something; also lazy modifier\n,?\n\n\n{m,n}\nRepeat between m and n times\n\\d{1,4}\n\n\n[ ]\nA set of character literals\n[1-5]\n\n\n( )\nGroup/remember this sequence of characters\n(\\d+)\n\n\n|\nOr\n(A|B)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#building-blocks",
    "href": "lectures/8.2-Patterns_in_Text.html#building-blocks",
    "title": "Patterns in Text",
    "section": "Building Blocks",
    "text": "Building Blocks\n\n\n\n\n\n\n\nRegex\nInterpretation\n\n\n\n\nr'\\s*'\n0 or more spaces\n\n\nr'\\d+'\n1 or more digits\n\n\nr'[A-Fa-f0-7]{5}'\nExactly 5 hexadecimal ‘digits’\n\n\nr'\\w+\\.\\d{2,}'\n1 or more ‘wordish’ characters, followed by a full-stop, then 2 or more digits\n\n\nr'^[^@]+@\\w+'\nOne more non-@ characters at the start of a line, followed by a ‘@’ then 1 or more ‘wordish’ characters.\n\n\nr'(uk|eu|fr)$'\nThe characters ‘uk’ or ‘eu’ or ‘fr’ at the end of a line."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#exploring",
    "href": "lectures/8.2-Patterns_in_Text.html#exploring",
    "title": "Patterns in Text",
    "section": "Exploring",
    "text": "Exploring\nRegex101 can be a useful way to build a regex interactively:"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^[^@]+@([a-z0-9\\-]+\\.){1,5}[a-z0-9\\-]+$', s)\n\ns should be replaced with any string you want to check."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-1",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-1",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'\\d{4}-\\d{2}-\\d{2}', s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-2",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-2",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^\\s*$', s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-3",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-3",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^(http|https|ftp):[\\/]{2}([a-zA-Z0-9\\-]+\\.){1,4}[a-zA-Z]{2,5}(:[0-9]+)?\\/?([a-zA-Z0-9\\-\\._\\?\\'\\/\\\\\\+\\&\\%\\$#\\=~]*)',s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-4",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-4",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})',s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#to-help",
    "href": "lectures/8.2-Patterns_in_Text.html#to-help",
    "title": "Patterns in Text",
    "section": "To Help…",
    "text": "To Help…\nre.VERBOSE to the rescue:\nregex = r\"\"\"\n([GIR] 0[A]{2})|    # Girobank \n(\n  (\n    ([A-Z][0-9]{1,2})| # e.g A00...Z99\n      (\n        ([A-Z][A-HJ-Y][0-9]{1,2})|  # e.g. AB54...ZX11\n          (([A-Z][0-9][A-Z])|  # e.g. A0B...Z9Z \n          ([A-Z][A-HJ-Y][0-9][A-Z]?))  # e.g. WC1 or WC1H\n        )\n      )\n    \\s?[0-9][A-Z]{2} # e.g. 5RX\n  )\n\"\"\"\nre.match(regex,s,re.VERBOSE|re.IGNORECASE) # Can also use: re.X|re.I\n\nThis is the government’s own regex but is probably not 100% accurate."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "href": "lectures/8.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "title": "Patterns in Text",
    "section": "Applications of Regular Expressions",
    "text": "Applications of Regular Expressions\nIf our problem follows some set of articulable rules about permissible sequences of characters then we can probably validate it using a regex:\n\n\n\n\n\n\n\nExamples\nMore Examples\n\n\n\n\nEmail\nPassword\n\n\nPostcode\nPhone number\n\n\nDate\nCredit cards\n\n\nWeb scraping\nSyntax highlighting\n\n\nSentence structure\nData wrangling\n\n\nSearching for/withinfiles/content\nLexical analysis/Language detection\n\n\n\n\nThese are all good problems…"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#resources",
    "href": "lectures/8.2-Patterns_in_Text.html#resources",
    "title": "Patterns in Text",
    "section": "Resources",
    "text": "Resources\n\nRegex 101\nPython Documentation\nReal Python: Regular Expressions 1\nReal Python: Regular Expressions 2\nData Camp RegEx Tutorial\nIntroduction to Regex\nUnderstanding RegExes in Python\nDemystifying RegExes in Python\nPython RegExes\nMastering String Methods in Python\n\nThanks to Yogesh Chavan and Nicola Pietroluongo for examples.\n\n\n\n\nPatterns in Text • Jon Reades"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#getting-spatial-with-boroughs",
    "href": "lectures/7.4-ESDA.html#getting-spatial-with-boroughs",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Getting Spatial (with Boroughs)",
    "text": "Getting Spatial (with Boroughs)\nimport geopandas as gpd\nurl = 'https://bit.ly/3neINBV'\nboros = gpd.read_file(url, driver='GPKG')\nboros.plot(color='none', edgecolor='red');"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#convex-hull",
    "href": "lectures/7.4-ESDA.html#convex-hull",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Convex Hull",
    "text": "Convex Hull\nboros['hulls'] = boros.geometry.convex_hull\nboros = boros.set_geometry('hulls')\nboros.plot(column='NAME', categorical=True, alpha=0.5);"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#dissolve",
    "href": "lectures/7.4-ESDA.html#dissolve",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Dissolve",
    "text": "Dissolve\nboros['region'] = 'London'\nboros = boros.set_geometry('geometry') # Set back to original geom\nldn   = boros.dissolve(by='region')    # And dissolve to a single poly\n\nf,ax = plt.subplots(figsize=(10,8))    # New plot\nldn.plot(ax=ax)                        # Add London layer to axis"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#simplify",
    "href": "lectures/7.4-ESDA.html#simplify",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Simplify",
    "text": "Simplify\nldn.simplify(500).plot()"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#buffer",
    "href": "lectures/7.4-ESDA.html#buffer",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Buffer",
    "text": "Buffer\nldn.buffer(500).plot()"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#buffer-simplify",
    "href": "lectures/7.4-ESDA.html#buffer-simplify",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Buffer & Simplify",
    "text": "Buffer & Simplify\nldn.buffer(1000).simplify(1000).plot()"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#difference",
    "href": "lectures/7.4-ESDA.html#difference",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Difference",
    "text": "Difference\nAnd some nice chaining…\nldn.buffer(3000).simplify(2500).difference(ldn.geometry).plot()"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#legendgrams",
    "href": "lectures/7.4-ESDA.html#legendgrams",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Legendgrams",
    "text": "Legendgrams\n\n\n\nCode begins on next slide."
  },
  {
    "objectID": "lectures/7.4-ESDA.html#implementing-legendgrams",
    "href": "lectures/7.4-ESDA.html#implementing-legendgrams",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing Legendgrams",
    "text": "Implementing Legendgrams\nimport pysal as ps\n# https://github.com/pysal/mapclassify\nimport mapclassify as mc\n# https://jiffyclub.github.io/palettable/\nimport palettable.matplotlib as palmpl\nfrom legendgram import legendgram\n\nf,ax = plt.subplots(figsize=(10,8))\ngdf.plot(column='price', scheme='Quantiles', cmap='magma', k=5, ax=ax)\nq = mc.Quantiles(gdf.price.array, k=5)\n\n# https://github.com/pysal/legendgram/blob/master/legendgram/legendgram.py\nlegendgram(f, ax, \n               gdf.price, q.bins, pal=palmpl.Magma_5,\n               legend_size=(.4,.2), # legend size in fractions of the axis\n               loc = 'upper left', # mpl-style legend loc\n               clip = (0,500), # clip range of the histogram\n               frameon=True)\n\nNote that the number of colours need to match k, which is 5 in this case.\nIt should be possible to set up the colormap and bins such that they can be passed to both GeoPandas and Legendgram."
  },
  {
    "objectID": "lectures/7.4-ESDA.html#knn-weights",
    "href": "lectures/7.4-ESDA.html#knn-weights",
    "title": "Exploratory Spatial Data Analysis",
    "section": "KNN Weights",
    "text": "KNN Weights"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#implementing-knn",
    "href": "lectures/7.4-ESDA.html#implementing-knn",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing KNN",
    "text": "Implementing KNN\nfrom pysal.lib import weights\nw = weights.KNN.from_dataframe(gdf, k=3)\ngdf['w_price'] = weights.lag_spatial(w, gdf.price)\ngdf[['name','price','w_price']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nname\nprice\nw_price\n\n\n\n\n83\nSouthfields Home\n85.0\n263.0\n\n\n53\nFlat in Islington, Central London\n55.0\n190.0\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\n470.0\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\n186.0\n\n\n44\nAvni Kensington Hotel\n430.0\n821.0"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#spatial-lag-of-distance-band",
    "href": "lectures/7.4-ESDA.html#spatial-lag-of-distance-band",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Spatial Lag of Distance Band",
    "text": "Spatial Lag of Distance Band"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#implementing-db",
    "href": "lectures/7.4-ESDA.html#implementing-db",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing DB",
    "text": "Implementing DB\nw2 = weights.DistanceBand.from_dataframe(gdf, threshold=2000, alpha=-0.25)\ngdf['price_std'] = (gdf.price - gdf.price.mean()) / gdf.price.std()\ngdf['w_price_std'] = weights.lag_spatial(w2, gdf.price_std)\ngdf[['name','price_std','w_price_std']].sample(5, random_state=42)\n\n\n\n\nname\nprice_std\nw_price_std\n\n\n\n\n83\nSouthfields Home\n-0.27\n0.00\n\n\n53\nFlat in Islington, Central London\n-0.51\n-0.58\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n0.83\n0.46\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n-0.07\n-0.82\n\n\n44\nAvni Kensington Hotel\n2.52\n3.25"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#morans-i",
    "href": "lectures/7.4-ESDA.html#morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Moran’s I",
    "text": "Moran’s I\nmi = esda.Moran(gdf['price'], w)\nprint(f\"{mi.I:0.4f}\")\nprint(f\"{mi.p_sim:0.4f}\")\nmoran_scatterplot(mi)"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#local-morans-i",
    "href": "lectures/7.4-ESDA.html#local-morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#implementing-local-morans-i",
    "href": "lectures/7.4-ESDA.html#implementing-local-morans-i",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Implementing Local Moran’s I",
    "text": "Implementing Local Moran’s I\nlisa = esda.Moran_Local(gdf.price, w)\n# Break observations into significant or not\ngdf['sig'] = lisa.p_sim &lt; 0.05\n# Store the quadrant they belong to\ngdf['quad'] = lisa.q\ngdf[['name','price','sig','quad']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nname\nprice\nsig\nquad\n\n\n\n\n83\nSouthfields Home\n85.0\nFalse\n3\n\n\n53\nFlat in Islington, Central London\n55.0\nFalse\n3\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\nFalse\n1\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\nFalse\n3\n\n\n44\nAvni Kensington Hotel\n430.0\nFalse\n1"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#full-lisa",
    "href": "lectures/7.4-ESDA.html#full-lisa",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Full LISA",
    "text": "Full LISA\nplot_local_autocorrelation(lisa, gdf, 'price')"
  },
  {
    "objectID": "lectures/7.4-ESDA.html#resources",
    "href": "lectures/7.4-ESDA.html#resources",
    "title": "Exploratory Spatial Data Analysis",
    "section": "Resources",
    "text": "Resources\nThere’s so much more to find, but:\n\nPandas Reference\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nVisualising Missing Data\nChoosing Map Colours\n\n\n\n\n\nExploratory Spatial Data Analysis • Jon Reades"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reading-writing",
    "href": "lectures/7.2-Geopandas.html#reading-writing",
    "title": "Geopandas",
    "section": "Reading & Writing",
    "text": "Reading & Writing\nSupported file formats:\n\n\n\nType\nExtension\nNotes\n\n\n\n\nShape\n.shp (etc.)\nMaximum compatibility\n\n\nGeoPackage\n.gpkg\nGood default choice\n\n\nGeoJSON\n.geojson\nFor web mapping\n\n\nZip\n.zip\nFor use with Shapefiles\n\n\nWKT\n.txt\nPlain-text & SQL\n\n\nGeoParquet\n.geoparquet\nGood for large data sets & SQL\n\n\n\nAdditionally, it is possible to read only subsets of the data using row, column, geometry, and bbox filters."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reading-remote-files",
    "href": "lectures/7.2-Geopandas.html#reading-remote-files",
    "title": "Geopandas",
    "section": "Reading (Remote Files)",
    "text": "Reading (Remote Files)\nAgain, depending on file size you may want to save these locally, but…\nimport geopandas as gpd\ngpkg_src = 'https://bit.ly/2K4JcsB'\nworld = gpd.read_file(gpkg_src)\n# The ';' suppresses matplotlib output\nworld.plot(facecolor='white', edgecolor='darkblue');"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#writing-local-files",
    "href": "lectures/7.2-Geopandas.html#writing-local-files",
    "title": "Geopandas",
    "section": "Writing (Local Files)",
    "text": "Writing (Local Files)\nWrite any OGR-supported vector drivers.\nworld.to_file('world.gpkg', driver='GPKG')\nworld.to_file('world.shp', driver='ESRI Shapefile')\nworld.to_file('world.geojson', driver='GeoJSON')\n\nIf you forget to specify the driver it writes shapefiles by default. This is mainly an issue if you try to write a GeoPackage or GeoJSON file but then end up writing a shapefile to a directory called world.gpkg!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#data-structures",
    "href": "lectures/7.2-Geopandas.html#data-structures",
    "title": "Geopandas",
    "section": "Data Structures",
    "text": "Data Structures\nGeoPandas does all this by adding just two new classes:\n\nGeoDataFrame\nGeoSeries\n\nIn principle, a GeoSeries can contain multiple geo-data types, but in practice you’ll want to be one of the following shapely classes:\n\nPoints / Multi-Points\nLines / Multi-Lines\nPolygons / Multi-Polygons"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#consider",
    "href": "lectures/7.2-Geopandas.html#consider",
    "title": "Geopandas",
    "section": "Consider",
    "text": "Consider\nRecall that we can ask if a particular object is an instance of any given class:\nimport pandas as pd\nprint(isinstance(world, str))\nprint(isinstance(world, pd.DataFrame))\nprint(isinstance(world, gpd.GeoDataFrame))\nPrints: False, True, True.\nimport pandas as pd\nprint(isinstance(world.geometry, str))\nprint(isinstance(world.geometry, pd.Series))\nprint(isinstance(world.geometry, gpd.GeoSeries))\nAlso prints: False, True, True.\n\nSo converting from Pandas to GeoPandas works well because GeoPandas knows all about Pandas.\nYou can use a GeoDataFrame anywhere you’d use a DataFrame with no loss of functionality! Same for a GeoSeries, though in this case a GeoSeries cannot perform the same statistical operations."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#projections",
    "href": "lectures/7.2-Geopandas.html#projections",
    "title": "Geopandas",
    "section": "Projections",
    "text": "Projections\nDepending on your data source, you may or may not have projection information attached to your GeoDataFrame:\nprint(world.crs)\noutputs epsg:4326, but:\nworld.crs\noutputs:\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#finding-projections",
    "href": "lectures/7.2-Geopandas.html#finding-projections",
    "title": "Geopandas",
    "section": "Finding Projections",
    "text": "Finding Projections\nYou’ll have already covered this in GIS, but you can find nearly any EPSG you might need at epsg.io. By far the most commonly-used here are:\n\nEPSG:4326 for the World Geodetic System 84 used in GPS.\nEPSG:27700 for OSGB 1936/British National Grid used in the UK.\n\nNote: recall that large territories (such as Canada, China and Russia) may well have multiple projections at the state of provincial level."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reprojection",
    "href": "lectures/7.2-Geopandas.html#reprojection",
    "title": "Geopandas",
    "section": "Reprojection",
    "text": "Reprojection\nFor data sets without projection information (i.e. often anything loaded from a shapefile) you must gdf.set_crs(&lt;spec&gt;). For all others you should gdf.to_crs(&lt;spec&gt;).\nworld2 = world.to_crs('ESRI:54030')\nworld2.plot();\n\n\nUnlike a shapefile, you can have more than one geometry column, each with a different projection. However, only one will be plotted (the one named geometry or specified via set_geometry())."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#the-spatial-index",
    "href": "lectures/7.2-Geopandas.html#the-spatial-index",
    "title": "Geopandas",
    "section": "The Spatial Index",
    "text": "The Spatial Index\nWe can use GeoSeries’ spatial index directly to perform simple spatial queries:\nimport matplotlib.pyplot as plt\nwslice = world.cx[-50:50, -20:20] # cx = coordinate index\nax = wslice.plot()\nplt.axvline(-50, linestyle='--', color='red')\nplt.axvline(50, linestyle='--', color='red')\nplt.axhline(-20, linestyle='--', color='red')\nplt.axhline(20, linestyle='--', color='red');"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#attributes",
    "href": "lectures/7.2-Geopandas.html#attributes",
    "title": "Geopandas",
    "section": "Attributes",
    "text": "Attributes\nA GeoSeries has attributes like any other Series, but also includes some spatially-specifc ones:\n\narea — if a polygon\nbounds — for each feature\ntotal_bounds — for each GeoSeries\ngeom_type — if you don’t already know\nis_valid — if you need to test"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#methods",
    "href": "lectures/7.2-Geopandas.html#methods",
    "title": "Geopandas",
    "section": "Methods",
    "text": "Methods\nAdditional GeoSeries methods icnlude:\n\ndistance() — returns Series measuring distances to some other feature (called as: &lt;GeoSeries&gt;.distance(&lt;feature&gt;))\ncentroid — returns GeoSeries of strict centroids (called as: &lt;GeoSeries&gt;.centroid)\nrepresentative_point() — returns GeoSeries of points within features\nto_crs() and plot(), which you’ve already seen.\n\n\nNote that centroid is not called with parentheses. Technically it’s more like an attribute than a method."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#relationship-tests",
    "href": "lectures/7.2-Geopandas.html#relationship-tests",
    "title": "Geopandas",
    "section": "Relationship Tests",
    "text": "Relationship Tests\nSimple geographical tests:\n\ngeom_almost_equals() — tries to deal with rounding issues when comparing two features.\ncontains() — is shape contained within some other features.\nintersects() — does shape intersect some other features."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#converting-non-spatial-data-1",
    "href": "lectures/7.2-Geopandas.html#converting-non-spatial-data-1",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 1",
    "text": "Converting Non-Spatial Data 1\nLat/Long and Northing/Easting benefit from a helper function gpd.points_from_xy():\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url)\n\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.plot()\n\nYou can also use list comprehensions ([x for x in list]) and zip to combine two lists but then need to specify the CRS as a separate step!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#csv-to-points-in-3-lines",
    "href": "lectures/7.2-Geopandas.html#csv-to-points-in-3-lines",
    "title": "Geopandas",
    "section": "CSV to Points in 3 Lines!",
    "text": "CSV to Points in 3 Lines!\n\n\nNotice that the default plot from a GeoDataFrame is… a map!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#converting-non-spatial-data-2",
    "href": "lectures/7.2-Geopandas.html#converting-non-spatial-data-2",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 2",
    "text": "Converting Non-Spatial Data 2\nOther feature types need to be in some kind of regular format such as Well-Known Text (WKT), GeoJSON, or something readable as a Shapely geometry.\nfrom shapely import wkt\n\n# Notice coordinate pairs and last point is same as first one\nbbox = 'POLYGON((5000000.0 2500000.0, 5000000.0 -2500000.0, -5000000.0 -2500000.0, -5000000.0 2500000.0, 5000000.0 2500000.0))'\n\n# Create GeoPandas from dict just like Pandas\nbgdf = gpd.GeoDataFrame({'id':[0], 'coordinates':bbox})\n\n# Turn it into a geometry\nbgdf['geometry'] = bgdf.coordinates.apply(wkt.loads)\nbgdf = bgdf.set_crs('ESRI:54030')\nbgdf.plot() # Not very interesting but...\n\nThese are more rarely used for our purposes but knowing that they exist is useful."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#from-text-to-bounding-box",
    "href": "lectures/7.2-Geopandas.html#from-text-to-bounding-box",
    "title": "Geopandas",
    "section": "From Text to Bounding Box",
    "text": "From Text to Bounding Box\nscale = int(float('1e7'))\nf,ax=plt.subplots(figsize=(8,4))\nworld2.plot(ax=ax)\nbgdf.plot(ax=ax, color='none', edgecolor='r')\nax.set_xlim([-0.75*scale, +0.75*scale])\nax.set_ylim([-3*scale/10, +3*scale/10])"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#resources",
    "href": "lectures/7.2-Geopandas.html#resources",
    "title": "Geopandas",
    "section": "Resources",
    "text": "Resources\n\nI Hate Coordinate Systems\nGeoPandas on ReadTheDocs\nDani’s GDS Course\nDani’s Web Mapping Course\n\n\n\n\n\nGeopandas • Jon Reades"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#why-pandas",
    "href": "lectures/6.4-Pandas.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy, numpy, and sklearn) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-data-frame",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-index",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-series",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-slice",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#using-pandas",
    "href": "lectures/6.4-Pandas.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nHere’s code to read a (remote) CSV file:\nimport pandas as pd      # import package\n# Bitly: https://raw.githubusercontent.com/jreades/fsds/master/data/2019-sample-crime.csv\nurl='https://bit.ly/39SJpfp'\ndf = pd.read_csv(url)       # load a (remote) CSV\nprint(type(df))             # not simple data type\nprint(df.columns.to_list()) # column names\nprint(df.columns.values)    # Also works but deprecated\nOutput:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n['ID' 'Case Number' 'Date' 'Primary Type' 'Description'\n 'Location Description' 'Arrest' 'Domestic' 'Year' 'Latitude' 'Longitude']"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#summarise-a-data-frame",
    "href": "lectures/6.4-Pandas.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\ndf.describe() # Information about each Series\ndf.info()     # Information about each Series and the df\ndf.info is more about data types and memory usage. df.describe is for summarising information about the distribution of values in every series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#familiar",
    "href": "lectures/6.4-Pandas.html#familiar",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\nThis should be looking eerily familiar:\nprint(type(df['Latitude']))          # type for column\nprint(type(df['Latitude'].array))    # type for values\nprint(df['Latitude'].array[:5])     # first five values\nprint(f\"1: {df['Latitude'].mean()}\") # summarise a series/column\nprint(f\"2: {df.Latitude.mean()}\")    # if no spaces in name\nProduces:\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'numpy.ndarray'&gt;\n[41.75130706 41.90399688 41.88032861 41.92438396 41.75579713]\n1: 41.84550008439\n2: 41.84550008439\n\nNotice that we’ve got two ways of accessing a pandas Series:\n\nThe dictionary-like way: df['Latitude']; this works for all columns, always.\nThe method-like way: df.Latitude; this works for ‘reading’ columns without spaces in their names."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#jupyter-formatting",
    "href": "lectures/6.4-Pandas.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter in ‘fancy’ ways:"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#familiar-1",
    "href": "lectures/6.4-Pandas.html#familiar-1",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                       # First 3 rows of df\ndf[['ID','Date','Year']].tail(3) # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nOn one level, this is what we’ve been building towards! We’ve got head and tail which we saw in the Command Line lecture. We’ve got random sampling with seeds which we saw in the Randomness lecture. We’ve even got LoLs, which we saw way back in the Lists of Lists lecture!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#data-frames-vs-series",
    "href": "lectures/6.4-Pandas.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nPandas operates on two principles:\n\nAny operation on a Data Frame returns a Data Frame.\nAny operation on a Series returns a Series.\n\n\nWe’ll see in a moment why this is useful!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#shallow-copies",
    "href": "lectures/6.4-Pandas.html#shallow-copies",
    "title": "Pandas",
    "section": "‘Shallow’ Copies",
    "text": "‘Shallow’ Copies\nMore subtly, operations on a Series or Data Frame return a shallow copy, which is like a ‘view’ in a database…\n\nThe original is unchanged unless you specify inplace=True (where supported).\nAttempts to change a subset of the data frame will often trigger a SettingWithCopyWarning warning.\n\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy()).\n\n\nDataQuest has a nice overview of how SettingWithCopyWarning is triggered and what to do about it."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#putting-these-ideas-together",
    "href": "lectures/6.4-Pandas.html#putting-these-ideas-together",
    "title": "Pandas",
    "section": "Putting These Ideas Together",
    "text": "Putting These Ideas Together\n# Returns a series but not a column\ndf.Latitude - 1 \n# Saves returned series as a new column\ndf['lat'] = df.Latitude - 1\n# Returns a new data frame w/o 'lat' \ndf.drop(columns=['lat']) \n# Modifies df directly\ndf.drop(columns=['lat'], inplace=True) \n# Try to modify a view of df (triggers warning)\ndf[df['Primary Type']=='BURGLARY'].Latitude = 41.7"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#chaining",
    "href": "lectures/6.4-Pandas.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\ndf.sort_values(by=['Year','ID'], ascending=False).sample(frac=0.5).head(20).median()"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#selection",
    "href": "lectures/6.4-Pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n# Returns a selection (Boolean series)\ndf['Primary Type']=='ASSAULT'\n\n# All rows where Primary Type is ASSAULT\ndf[ df['Primary Type']=='ASSAULT' ]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['Primary Type']=='ASSAULT'][['Longitude','Latitude']].mean()\n\n# Two conditions with a bit-wise AND\ndf[\n  (df['Primary Type']=='ASSAULT') &\n  (df['Description']=='AGGRAVATED: HANDGUN')\n]\n\n# Two conditions with a bit-wise OR\ndf[\n  (df['Primary Type']=='ASSAULT') |\n  (df['Primary Type']=='THEFT')\n]"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#dealing-with-types",
    "href": "lectures/6.4-Pandas.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\n\n\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns (including arrays)\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#changing-the-type",
    "href": "lectures/6.4-Pandas.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\nprint(df['Primary Type'].unique())   # Find unique values\nprint(df['Primary Type'].dtype.name) # Confirm is 'object'\ndf['Primary Type'] = df['Primary Type'].astype('category')\nprint(df['Primary Type'].dtype.name) # Confirm is 'category'\nprint(df['Primary Type'].describe()) # Category column info\nOutputs:\n['BURGLARY' 'DECEPTIVE PRACTICE' 'BATTERY'...]\nobject   # &lt; before `as type`\ncategory # &lt; after `as type`\ncount       100\nunique       15\ntop       THEFT\nfreq         28\nName: Primary Type, dtype: object # category==special class of object"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#datetime-data",
    "href": "lectures/6.4-Pandas.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\nprint(df.Date.dtype.name)\n# object\ndf.Date.to_list()[:3]\n# ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nThis shows that Date is currently a string of dates+times.\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.array, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nprint(df.dt.dtype.name)\n# datetime64[ns]\ndf.dt.to_list()[:3]\n# [Timestamp('2019-04-20 11:00:00'), Timestamp('2019-12-02 10:35:00'), Timestamp('2019-10-06 04:50:00')]\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#datetime-formats",
    "href": "lectures/6.4-Pandas.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM\n\n\n\nSo that is why:\npd.to_datetime(df.Date.array, format=\"%m/%d/%Y %H:%M:%S %p\")\nNote the other things happening here:\n\npd.to_datetime(...) is not a method, it’s a function from the pandas package.\ndf.Date.array (and df.Date.to_numpy() and df.Data.tolist()) gives access to the data directly, whereas df.Date gives access to the Series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#deprecation-warning",
    "href": "lectures/6.4-Pandas.html#deprecation-warning",
    "title": "Pandas",
    "section": "Deprecation Warning!",
    "text": "Deprecation Warning!\nFrom time to time, real-world software projects will change the way things work. Pandas is just such a project!\n\n\n\n\n\n\nWarning\n\n\nWe recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array. See API Documenation.\n\n\n\nSo while Series.values still works, and will continue to work for some time, you are being advised to start using Series.array or Series.to_numpy() instead. Meaning, we should consider using df.Date.array."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#tidying-up",
    "href": "lectures/6.4-Pandas.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many options and subtleties…\n# Fix categories\nmapping = {}\n\n# df['Primary Type'].unique().to_list() also works\nfor x in df['Primary Type'].cat.categories.to_list():\n  mapping[x]=x.title()\n\n# And update\ndf['Primary Type'] = df['Primary Type'].cat.rename_categories(mapping)\nHow would you work out what this code does? 1\nTo deal with pricing information treated as a string:\ndf2['price'].str.replace('$','').astype(float)\nMany more examples accessible via Google!\n\nAnother thing you might notice here: adding .cat allows us to access category methods for the Series; adding .str allows us to access string methods for the Series.\n\nThere are at least two ways: 1) print out mapping; 2) before running the code comment out the ‘update’ line and print out x and x.title(); 3) search for title python."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#dropping-rows-and-columns",
    "href": "lectures/6.4-Pandas.html#dropping-rows-and-columns",
    "title": "Pandas",
    "section": "Dropping Rows and Columns",
    "text": "Dropping Rows and Columns\nThere are multiple ways to drop ‘stuff’:\ndf2 = df.copy()\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf2.drop(index=range(5,10), inplace=True) # Row 'numbers' or index values\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf.drop(columns=['Year'], inplace=True)   # Column name(s)\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\nThere is also df.dropna() which can apply to rows or columns with NULL or np.nan values.\nI often prefer df = df[df.index &gt; 15] (negative selection) to df.drop(index=range(0,14)) (positive selection).\n\nWhy might you want the default to not be in_place?"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#accessing-data-by-location",
    "href": "lectures/6.4-Pandas.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index:\n\ndf.loc returns the rows labeled 0, 1, and 2 ([0..2]), whereas\ndf.iloc returns the range 0..2 ([0..2))!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#indexes",
    "href": "lectures/6.4-Pandas.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you print out the head:\ndf.head(3)\nThe left-most column (without) a name is the index.\ndf.set_index('ID', inplace=True)\ndf.head(3)\nNow we see:\n         Case Number                    Date  ...  Longitude                  dt\nID                                            ...\n11667185    JC237601  04/20/2019 11:00:00 PM  ... -87.603468 2019-04-20 11:00:00\n11909178    JC532226  12/02/2019 10:35:00 AM  ... -87.643230 2019-12-02 10:35:00\n11852571    JC462365  10/06/2019 04:50:00 PM  ... -87.758473 2019-10-06 04:50:00\n\nSo ID is now the index and is not accessible as a column: df.ID will now throw an error because it’s not longer part of the Column Index."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#indexes-contd",
    "href": "lectures/6.4-Pandas.html#indexes-contd",
    "title": "Pandas",
    "section": "Indexes (cont’d)",
    "text": "Indexes (cont’d)\nNotice the change to the data frame:\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nAnd now:\nprint(df.loc[11667185,:])\nprint(df.loc[11667185:11852571,'Case Number':'Date'])\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#saving",
    "href": "lectures/6.4-Pandas.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, here are some of the more popular ones:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(&lt;path&gt;)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(&lt;path&gt;)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_feather(&lt;path&gt;)\nDirectly usable by R. Requires pyarrow to be installed to access the options.\n\n\ndf.to_parquet(&lt;path&gt;)\nDirectly usable by many languages. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(&lt;path&gt;))\nWrite a LaTeX-formatted table to a file. Display requires booktabs. Could do copy+paste with print(df.to_latex()).\n\n\ndf.to_markdown(&lt;path&gt;)\nWrite a Markdown-formatted table to a file. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is detected automatically (e.g. df.to_csv('file.csv.gz')) but you can also specify it (e.g. df.to_csv('file.csv.gz', compression='gzip')).1\nFor instance, a bit.ly link to a Gzipped file requires compression='gzip' because there’s nothing in the link itself to tell Pandas what to expect."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#resources",
    "href": "lectures/6.4-Pandas.html#resources",
    "title": "Pandas",
    "section": "Resources",
    "text": "Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply\n\n\n\n\n\nPandas • Jon Reades"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#reproducibility-good-or-bad",
    "href": "lectures/6.2-Randomness.html#reproducibility-good-or-bad",
    "title": "Randomness",
    "section": "Reproducibility: Good or Bad?",
    "text": "Reproducibility: Good or Bad?\nDepends on the problem:\n\nBanking and encryption?\nSampling and testing?\nReproducing research/documentation?\n\n\nOK, technically, even encryption needs to be reproducible to allow for decryption, but you sure don’t want it to be easy."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#not-very-good-encryption",
    "href": "lectures/6.2-Randomness.html#not-very-good-encryption",
    "title": "Randomness",
    "section": "Not Very Good Encryption",
    "text": "Not Very Good Encryption\n\n\n\nCyphertext\nOutput\n\n\n\n\nROT0\nTo be or not to be, That is the question\n\n\nROT1\nUp cf ps opu up cf, Uibu jt uif rvftujpo\n\n\nROT2\nVq dg qt pqv vq dg, Vjcv ku vjg swguvkqp\n\n\n…\n…\n\n\nROT9\nCx kn xa wxc cx kn, Cqjc rb cqn zdnbcrxw\n\n\n\nROT is known as the Caesar Cypher, but since the transformation is simple (A..Z+=x) decryption is easy now. How can we make this harder?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#python-is-random",
    "href": "lectures/6.2-Randomness.html#python-is-random",
    "title": "Randomness",
    "section": "Python is Random",
    "text": "Python is Random\nimport random\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nSee also: random.randrange, random.choice, random.sample, random.random, random.gauss, etc."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#and-repeat",
    "href": "lectures/6.2-Randomness.html#and-repeat",
    "title": "Randomness",
    "section": "And Repeat…",
    "text": "And Repeat…\nimport random\nsize = 10\nresults = [0] * size\n\ntests = 100000\nwhile tests &gt; 0:\n    results[random.randint(0,len(results)-1)] += 1\n    tests -= 1\n\nfor i in range(0,len(results)):\n    print(f\"{i} -&gt; {results[i]}\")\n\nWhat will this return?\nWill it hold for more than 10 numbers?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat",
    "href": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat\nimport random \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nsize = 1000\ndata = [0] * size\n\ntests = 10000000\nwhile tests &gt; 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nfig = plt.figure()\nplt.bar(np.arange(0,len(data)), data)\nfig.savefig('Random.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "href": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat-1",
    "title": "Randomness",
    "section": "Aaaaaaaaaaand Repeat",
    "text": "Aaaaaaaaaaand Repeat"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#setting-a-seed",
    "href": "lectures/6.2-Randomness.html#setting-a-seed",
    "title": "Randomness",
    "section": "Setting a Seed",
    "text": "Setting a Seed\nTwo main libraries where seeds are set:\nimport random\nrandom.seed(42)\n\nimport numpy as np\nnp.random.seed(42)\n\nWhy do you often see 42 used as a seed?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#seeds-and-state",
    "href": "lectures/6.2-Randomness.html#seeds-and-state",
    "title": "Randomness",
    "section": "Seeds and State",
    "text": "Seeds and State\nimport random\nrandom.seed(42)\nst = random.getstate()\nfor r in range(0,3):\n    random.setstate(st)\n    print(f\"Repetition {r}:\")\n    ints = []\n    for i in range(0,10):\n        ints.append(random.randint(0,10))\n    print(f\"\\t{ints}\")"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#hashing",
    "href": "lectures/6.2-Randomness.html#hashing",
    "title": "Randomness",
    "section": "Hashing",
    "text": "Hashing\nChecking for changes (usally in a security context).\nimport hashlib # Can take a 'salt' (similar to a 'seed')\n\nr1 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r1 is: {r1.hexdigest()}\")\n\nr2 = hashlib.md5('CASA Intro to Programming '.encode())\nprint(f\"The hashed equivalent of r2 is: {r2.hexdigest()}\")\n\nr3 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r3 is: {r3.hexdigest()}\")\nOutputs:\n\"The hashed equivalent of r1 is: acd601db5552408851070043947683ef\"\n\"The hashed equivalent of r2 is: 4458e89e9eb806f1ac60acfdf45d85b6\"\n\"The hashed equivalent of r3 is: acd601db5552408851070043947683ef\"\n\nThis is like generating a ‘fingerprint’ of an application or file. In fact, it’s what is going on behind the scenes when you download something to install on macOS or Windows and you’re told that the installer is being ‘verified’ before it will run: the computer is generating a hash of the application’s codebase, and sending that to the Apple Store or Windows Store or direct to the developer in order to check that the file hasn’t been tampered with."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#and-note",
    "href": "lectures/6.2-Randomness.html#and-note",
    "title": "Randomness",
    "section": "And Note…",
    "text": "And Note…\nimport requests\nnight = requests.get(\"http://www.gutenberg.org/ebooks/1514.txt.utf-8\")\nprint(f\"The text is {night.text[30:70]}\")\nprint(f\"The text is {len(night.text):,} characters long\")\nhash = hashlib.md5(night.text.encode())\nprint(f\"This can be hashed into: {hash.hexdigest()}\")\nOutputs:\n\"The text is A Midsummer Night's Dream by Shakespeare\"\n\"The text is 112,127 characters long\"\n\"This can be hashed into: cce0d35b8b2c4dafcbde3deb983fec0a\"\n\nCan be applied to anything: even one byte’s difference (e.g. in a application) can lead to a different hash output.\nBut notice that hashes are always the same length. This property is quite useful for databases and verifying the integrity of applications (MD5 Checksums)."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#jupyterlab-password",
    "href": "lectures/6.2-Randomness.html#jupyterlab-password",
    "title": "Randomness",
    "section": "JupyterLab Password",
    "text": "JupyterLab Password\nYou may have noticed this in Docker:\n'sha1:5b1c205a53e14e:0ce169b9834984347d62b20b9a82f6513355f72d'\nHow this was generated:\nimport uuid, hashlib\nsalt = uuid.uuid4().hex[:16] # Truncate salt\npassword = 'casa2021'        # Set password\n\n# Here we combine the password and salt to \n# 'add complexity' to the hash\nhashed_password = hashlib.sha1(password.encode() + \n                  salt.encode()).hexdigest()\nprint(':'.join(['sha1',salt,hashed_password]))\n\n\nThen you can replace the JUPYTER_PWD parameter in the start-up string for Docker if you want to set a password.\n\n\nDon’t set your passwords this way."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#encryption-security",
    "href": "lectures/6.2-Randomness.html#encryption-security",
    "title": "Randomness",
    "section": "Encryption & Security",
    "text": "Encryption & Security\nSimple hashing algorithms are not normally secure enough for full encryption. Genuine security training takes a whole degree + years of experience.\nAreas to look at if you get involved in applications:\n\nPublic and Private Key Encryption (esp. OpenSSL)\nPrivileges used by Applications (esp. Docker)\nRevocable Tokens (e.g. for APIs)\nInjection Attacks (esp. for SQL using NULL-byte and similar)\n\n\n\n\n\nRandomness • Jon Reades"
  },
  {
    "objectID": "lectures/5.4-Errors.html#helpfully",
    "href": "lectures/5.4-Errors.html#helpfully",
    "title": "Errors",
    "section": "Helpfully…",
    "text": "Helpfully…\nPython will always try to tell you what it thinks went wrong: “I didn’t understand what you meant by this…” or “I’m sorry, I can’t let you do that Dave…”\nThe challenges are:\n\nPython tends to give you a lot of information about the error: this can be very helpful for programmers dealing with complex problems and totally overwhelming for beginners.\nThat what Python thinks the problem is doesn’t always line up with where the problem actually is. In cases of syntax, for instance, the problem could be an unclosed parenthesis three lines earlier!"
  },
  {
    "objectID": "lectures/5.4-Errors.html#challenge-1",
    "href": "lectures/5.4-Errors.html#challenge-1",
    "title": "Errors",
    "section": "Challenge 1",
    "text": "Challenge 1\nThat the ‘error’ isn’t always the error…\ntotal = 0\nprint(\"About to start loop\"\nfor i in range(1,10):\n  total += i\nprint(total)\nThis outputs:\nprint(\"About to start loop\"\n... for i in range(1,10):\n  File \"&lt;stdin&gt;\", line 2\n    for i in range(1,10):\n                        ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "lectures/5.4-Errors.html#errors-have-types",
    "href": "lectures/5.4-Errors.html#errors-have-types",
    "title": "Errors",
    "section": "Errors Have Types",
    "text": "Errors Have Types\nIn the same way that variables have types, so do errors:\n\nModuleNotFoundError\nIndexError\nKeyError\nOSError\n…\n\nWe can add our own messages:\nraise Exception(\"Sorry, I can't let you do that, Dave.\")\n\n\nWhy might different types of errors be useful?\nWe might reasonably want to distinguish between errors that we could reasonably expect or that are not serious, from those that we did not expect or that call the results of the program into question.\n\n\nLots more built-in error types in the Python documentation and imported packages will provide their own as well."
  },
  {
    "objectID": "lectures/5.4-Errors.html#custom-errors",
    "href": "lectures/5.4-Errors.html#custom-errors",
    "title": "Errors",
    "section": "Custom Errors",
    "text": "Custom Errors\nWe can create our own types (classes) of error:\nclass CustomError(Exception):\n  pass # We do nothing except create a new type\nThis can then be triggered with:\nraise CustomError(\"Our custom error\")\nAnd (very importantly) this can be caught with:\nexcept CustomError: \n  #... do something with CustomError ...\nThis means that exceptions could accept custom arguments, perform tidying-up or rollback operations, etc.\n\nSome of the intricacies of errors can seem quite confusing. What’s the point of having a finally (which we’ll get to in a moment), for instance? Well, if your application is connected to a database then finally gives your application a chance to disconnect cleanly (freeing up resources for the database) and even to rollback incomplete changes (e.g. a new user whose details were only partially inputted when the application crashed)."
  },
  {
    "objectID": "lectures/5.4-Errors.html#so-errors-can-be-trapped",
    "href": "lectures/5.4-Errors.html#so-errors-can-be-trapped",
    "title": "Errors",
    "section": "So Errors can be Trapped",
    "text": "So Errors can be Trapped\nPython calls errors exceptions, so this leads to:\ntry:\n  #... some code that might fail...\nexcept &lt;Named_Error_Type&gt;:\n  #... what do it if it fails for a specific reason...\nexcept:\n  #... what to do if it fails for any other reason...\nfinally:\n  #... always do this, even if it fails...\nYou can use any or all of these together: you can have multiple named excepts to handle different types of errors from a single block of code; you do not have to have a catch-all except or a finally.\n\nSo it makes sense to think: “Well, let’s try this and see what happens. If we have a problem of this type then it’s not serious and we should carry on. But if we have a problem that type then we need to stop what we’re doing right away.”"
  },
  {
    "objectID": "lectures/5.4-Errors.html#trapping-errors",
    "href": "lectures/5.4-Errors.html#trapping-errors",
    "title": "Errors",
    "section": "Trapping Errors",
    "text": "Trapping Errors\nThis code fails:\nx,y = 10,0\nprint(x/y)\nAnd it generates this error:\n&gt; Traceback (most recent call last):\n&gt;   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n&gt; ZeroDivisionError: division by zero"
  },
  {
    "objectID": "lectures/5.4-Errors.html#trapping-errors-contd",
    "href": "lectures/5.4-Errors.html#trapping-errors-contd",
    "title": "Errors",
    "section": "Trapping Errors (cont’d)",
    "text": "Trapping Errors (cont’d)\nBut if you ‘trap’ the error using except then:\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\nexcept:\n  print(\"Something has gone very wrong.\")\nfinally: \n  print(\"Division is fun!\")\nThis will print\n&gt; You can't divide by zero!\n&gt; Division is fun!\n\n\nNote: if we need to access the actual exception: except ZeroDivisionError as e:"
  },
  {
    "objectID": "lectures/5.4-Errors.html#raising-hell",
    "href": "lectures/5.4-Errors.html#raising-hell",
    "title": "Errors",
    "section": "Raising Hell",
    "text": "Raising Hell\nYou can trigger your own exceptions using raise.\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")"
  },
  {
    "objectID": "lectures/5.4-Errors.html#understanding-multiple-errors",
    "href": "lectures/5.4-Errors.html#understanding-multiple-errors",
    "title": "Errors",
    "section": "Understanding Multiple Errors",
    "text": "Understanding Multiple Errors\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")\n\nThe code we try triggers the ZeroDivisionError block.\nThis prints \"You can't divide by zero!\"\nWe then raise a new exception that is not caught.\nThe finally code executes because it always does before Python exits.\nPython exits with the message from our newly raised Exception.\n\nThus: ‘During handling of above (ZeroDivisionError) another exception (our Exception) occurred…’"
  },
  {
    "objectID": "lectures/5.4-Errors.html#test-based-development",
    "href": "lectures/5.4-Errors.html#test-based-development",
    "title": "Errors",
    "section": "Test-Based Development",
    "text": "Test-Based Development\nWe can actually think of exceptions as a way to develop our code.\nHere’s some ‘pseudo-code’:\n# Testing the 'addition' operator\ntest(1+1, 2)           # Should equal 2\ntest(1+'1', TypeError) # Should equal TypeError\ntest('1'+'1', '11')    # Should equal '11'\ntest(-1+1, 0)          # Should equal 0 \nOur test(A,B) function takes an input (A) and the expected output (B) and then compares them. The test returns True if A==B and False otherwise."
  },
  {
    "objectID": "lectures/5.4-Errors.html#unit-tests",
    "href": "lectures/5.4-Errors.html#unit-tests",
    "title": "Errors",
    "section": "Unit Tests",
    "text": "Unit Tests\nEach test is a Unit Test because it tests one thing and one thing only. So if you had three functions to ‘do stuff’ then you’d need at least three unit tests.\nA Unit Test may be composed of one or more assertions. Our pseudo-code on the previous slide contained 4 assertions.\nA Unit Test does not mean that your code is correct or will perform properly under all circumstances. It means that your code returns the expected value for a specified input.\nPython considers this approach so important that it’s built in."
  },
  {
    "objectID": "lectures/5.4-Errors.html#approach-1",
    "href": "lectures/5.4-Errors.html#approach-1",
    "title": "Errors",
    "section": "Approach 1",
    "text": "Approach 1\nThis is an explict assertion to test fun:\nimport unittest\n\ndef fun(x):\n  return x + 1\n\nclass MyTest(unittest.TestCase):\n  def test(self):\n    self.assertEqual(fun(3), 4)\n    print(\"Assertion 1 passed.\")\n    self.assertEqual(fun(3), 5)\n    print(\"Assertion 2 passed.\")\n\nm = MyTest()\nm.test()\nThe critical output is:\nAssertionError: 4 != 5"
  },
  {
    "objectID": "lectures/5.4-Errors.html#approach-2",
    "href": "lectures/5.4-Errors.html#approach-2",
    "title": "Errors",
    "section": "Approach 2",
    "text": "Approach 2\nThis approach uses the ‘docstring’ (the bits between \"\"\") to test the results of the function. This is intended to encourage good documentation of functions using examples:\ndef square(x):\n    \"\"\"Return the square of x.\n\n    &gt;&gt;&gt; square(2)\n    4\n    &gt;&gt;&gt; square(-2)\n    4\n    &gt;&gt;&gt; square(-1)\n    2\n    \"\"\"\n    return x * x\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()\n\n\nNotice the __name__ (what could that possibly mean???) is tested to see if it’s '__main__'. The doctest code only runs when this code is executed in '__main__'."
  },
  {
    "objectID": "lectures/5.4-Errors.html#collaboration-continuous-integration",
    "href": "lectures/5.4-Errors.html#collaboration-continuous-integration",
    "title": "Errors",
    "section": "Collaboration & Continuous Integration",
    "text": "Collaboration & Continuous Integration\nThe Unit Test approach is often used on collaborative projects, especially in the Open Source world. PySAL, for instance, asks for unit tests with every new feature or integration.\nThe running of all tests for multiple components is called ‘integration testing’.\nA commit, merge, or pull on GitHub can trigger the unit testing process for the entire software ‘stack’. This is known as Continuous Integration because you are always checking that the code works as expected, rather than leaving testing to the end.\n\nThis is heavily used by PySAL and other robust FOSS projects since TravisCI is free for FOSS projects!"
  },
  {
    "objectID": "lectures/5.4-Errors.html#resources",
    "href": "lectures/5.4-Errors.html#resources",
    "title": "Errors",
    "section": "Resources",
    "text": "Resources\n\n\n\nHandling exceptions\nReporting errors\nPython Custom Exceptions\nWriting and Using Custom Exceptions in Python\nPython Documentation\nHow to Define Custom Exception Classes\n\n\n\nUnit Testing in Python\nUnderstanding Unit Testing\nTesting Your Code\nGetting Started with Testing in Python\nPython’s unittest Library\nVideo: Unit Testing Your Code\n\n\n\n\n\n\nErrors • Jon Reades"
  },
  {
    "objectID": "lectures/5.1-Methods.html#todays-question",
    "href": "lectures/5.1-Methods.html#todays-question",
    "title": "Methods",
    "section": "Today’s Question",
    "text": "Today’s Question\nWe know that a function looks like this:\n&lt;function name&gt;( &lt;input&gt; )\nAnd we know that a function in a package looks like this:\n&lt;package name&gt;.&lt;function name&gt;( &lt;input&gt; )\nSo is list a package?\nmy_list.append( &lt;value&gt; )"
  },
  {
    "objectID": "lectures/5.1-Methods.html#well",
    "href": "lectures/5.1-Methods.html#well",
    "title": "Methods",
    "section": "Well…",
    "text": "Well…\nmy_list.append( &lt;value&gt; ) is a function.\nmy_list.append( &lt;value&gt; ) is a special type of function called a method."
  },
  {
    "objectID": "lectures/5.1-Methods.html#whats-a-method-then",
    "href": "lectures/5.1-Methods.html#whats-a-method-then",
    "title": "Methods",
    "section": "What’s a Method Then?",
    "text": "What’s a Method Then?\nPackages group useful constants and functions together in one place.\nMethods group constants and functions together in one place with data.\nSo my_list.append(...) is called a list method:\n\nIt only knows how to append things to lists.\nIt is only available as a function when you have an insantiated list (e.g. [] or [1,'dog',3.5]).\nIt is bound to variables (aka. objects) of class list."
  },
  {
    "objectID": "lectures/5.1-Methods.html#proof",
    "href": "lectures/5.1-Methods.html#proof",
    "title": "Methods",
    "section": "Proof!",
    "text": "Proof!\nmy_list = [] # \nhelp(my_list)\nThis will give you:\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  Built-in mutable sequence.\n |  If no argument is given, the constructor creates a new empty list.\n |  Methods defined here:\n | ...\n |  append(self, object, /)\n |      Append object to the end of the list.\n |\n |  clear(self, /)\n |      Remove all items from list.\n |\n |  copy(self, /)\n |      Return a shallow copy of the list.\n | ...\n\nIt’s not obvious here, but you can also create lists by writing list()."
  },
  {
    "objectID": "lectures/5.1-Methods.html#its-all-methods",
    "href": "lectures/5.1-Methods.html#its-all-methods",
    "title": "Methods",
    "section": "It’s all Methods",
    "text": "It’s all Methods\nmsg = 'Hello World'\ndir(msg)\n['__add__', '__class__', ..., 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', ... 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\nAnd then we can inquire about these methods:\nhelp(msg.capitalize)\nHelp on built-in function capitalize:\n\ncapitalize() method of builtins.str instance\n    Return a capitalized version of the string.\n\n    More specifically, make the first character have upper case and the rest lower\n    case."
  },
  {
    "objectID": "lectures/5.1-Methods.html#final-fundamental-concepts",
    "href": "lectures/5.1-Methods.html#final-fundamental-concepts",
    "title": "Methods",
    "section": "Final Fundamental Concepts",
    "text": "Final Fundamental Concepts\nFrom here on out, nearly all of what you learn will be new applications, not new concepts and terminology.\n\n\n\n\n\n\n\n\nTerm\nMeans\nExample\n\n\n\n\nClass\nThe template for a ‘thing’.\nRecipe for a pizza.\n\n\nObject\nThe instantiated ‘thing’.\nA pizza I can eat!\n\n\nMethod\nFunctions defined for the class and available to the object.\nThings I can do with a pizza (eat, cook, make).\n\n\nConstructor\nThe special method that builds new objects of that class.\nHow to start a new pizza!\n\n\nSelf\nA reference to the current object.\nThe pizza in front of me!\n\n\n\n\n\n\n\nMethods • Jon Reades"
  },
  {
    "objectID": "lectures/4.1-Functions.html#what-does-a-function-look-like",
    "href": "lectures/4.1-Functions.html#what-does-a-function-look-like",
    "title": "Functions",
    "section": "What Does a Function Look Like?",
    "text": "What Does a Function Look Like?\nlen(&lt;some_list&gt;) is a function\nSo len(...) encapsulates the process of figuring out how long something with ‘countable units’ actually is, whether it’s a string or a list.\nprint(&lt;some_value&gt;) is also a function\nBecause print(...) encapsulates the process of sending output to the command line, a file, or even a database or API!\n\nlen(123) is a Type Error.\nlen(‘123’) is not.\nCan you think why?"
  },
  {
    "objectID": "lectures/4.1-Functions.html#so-what-does-a-function-look-like",
    "href": "lectures/4.1-Functions.html#so-what-does-a-function-look-like",
    "title": "Functions",
    "section": "So What Does a Function Look like?",
    "text": "So What Does a Function Look like?\nAll function ‘calls’ looking something like this:\nfunction_name(...)\nWhere the ‘...’ are the inputs to the function; it could be one variable, 25 variables, a list, even another function!\nAnd if the function ‘returns’ something it will look like this:\nreturn_data = function_name(...)"
  },
  {
    "objectID": "lectures/4.1-Functions.html#in-action",
    "href": "lectures/4.1-Functions.html#in-action",
    "title": "Functions",
    "section": "In Action!",
    "text": "In Action!\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data)) # 8.666666666666666\ndata2 = [200000,2500000,-4,1400000,70,900000]\nprint(calc_mean(data2)) # 833344.3333333334"
  },
  {
    "objectID": "lectures/4.1-Functions.html#but-notice",
    "href": "lectures/4.1-Functions.html#but-notice",
    "title": "Functions",
    "section": "But Notice!",
    "text": "But Notice!\ndata    = [1,25,-4,14,7,9]\ntotal   = 1\nnumbers = []\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\nprint(calc_mean(data))\n\n# Why haven't these changed????\nprint(total)\nprint(numbers)\n\nFunctions encapsulate information: the total and numbers used within the function are different from the variables with the same name that we created outside the function. When we get to libraries and packages you’ll understand why, but the key concept here is ‘namespace’ and that these variables might have the same name but they sit in different namespaces."
  },
  {
    "objectID": "lectures/4.1-Functions.html#simple-function",
    "href": "lectures/4.1-Functions.html#simple-function",
    "title": "Functions",
    "section": "Simple Function",
    "text": "Simple Function\nBy ‘simple’ I don’t mean easy, I mean it does one thing only:\ndef hello():\n  print(\"Hello world!\")\nWe then run it with:\nhello()\nAnd that produces:\nHello world!"
  },
  {
    "objectID": "lectures/4.1-Functions.html#passing-in-information",
    "href": "lectures/4.1-Functions.html#passing-in-information",
    "title": "Functions",
    "section": "Passing in Information",
    "text": "Passing in Information\nWe can pass information to a function if we tell the function what to expect:\ndef hello(name:str):\n  print(f\"Hello {name}!\")\nNow we can do this:\nhello(\"new programmers\")\nAnd that produces:\nHello new programmers!"
  },
  {
    "objectID": "lectures/4.1-Functions.html#getting-information-out",
    "href": "lectures/4.1-Functions.html#getting-information-out",
    "title": "Functions",
    "section": "Getting Information Out",
    "text": "Getting Information Out\nWe can also get information out of them!\ndef hello(name:str) -&gt; str:\n  return f\"Hello {name}!\"\nNow we can do this:\noutput = hello(\"new programmers\")\nprint(output.title())\n# Same as: print(hello(\"new programmers\").title())\nAnd this produces:\n'Hello New Programmers!'"
  },
  {
    "objectID": "lectures/4.1-Functions.html#writing-a-function",
    "href": "lectures/4.1-Functions.html#writing-a-function",
    "title": "Functions",
    "section": "Writing a Function",
    "text": "Writing a Function\ndef &lt;function_name&gt;(&lt;var_name&gt;: &lt;var_type&gt;) -&gt; &lt;var_type&gt;:\n  ...\n  return &lt;var&gt;\nThis can also be written:\ndef &lt;function_name&gt;(&lt;var_name&gt;):\n  ...\n  return &lt;var&gt;\nPython is ‘friendly’ in the sense that all of the &lt;var_type&gt; information is optional, but it will help you (and Python) to know what you were expecting to see happen."
  },
  {
    "objectID": "lectures/4.1-Functions.html#complicating-things",
    "href": "lectures/4.1-Functions.html#complicating-things",
    "title": "Functions",
    "section": "Complicating Things…",
    "text": "Complicating Things…\nds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz': [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ndef get_city_info(city:str, field:str, city_lookup:str='name', data:dict=ds2) -&gt; str:\n  return str(data[field][ data[city_lookup].index(city) ])\n\ncity = 'New York'\nprint(f\"The latitude of {city} is {get_city_info(city,'lat')}\")\n# The latitude of New York is 40.71"
  },
  {
    "objectID": "lectures/4.1-Functions.html#resources",
    "href": "lectures/4.1-Functions.html#resources",
    "title": "Functions",
    "section": "Resources",
    "text": "Resources\n\n\n\nWhat is a function?\nPython functions\nBuilt-in functions\nDefine your own functions\nTypes of functions\nDefining a function\n\n\n\nFunction arguments\nArgument lists\nKeyword arguments\nReturn values\nDecorators\nVariable Scopes\nRobust Python with Type Hints\n\n\n\n\n\n\nFunctions • Jon Reades"
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#as-in",
    "href": "lectures/3.4-The_Command_Line.html#as-in",
    "title": "The Command Line",
    "section": "As in…",
    "text": "As in…\n\nWhy are you torturing me with this arcane knowledge?\nWhy do I need to do this when we have slick IDEs now?"
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#the-answer",
    "href": "lectures/3.4-The_Command_Line.html#the-answer",
    "title": "The Command Line",
    "section": "The Answer?",
    "text": "The Answer?\nNo matter how long you try to avoid it, eventually you’ll find things that can only be solved (or that can be much more quickly solved) using the Command Line Interface (CLI).\nThings like:\n\nInteracting with git is actually easier on the Command Line.\nMaking the most of developer-oriented tools (e.g. docker, GDAL, proj4/6).\nPeeking and poking at (large) files efficiently…\nAutomating things that would be hard/annoying to do manually…\n\nA lot of this ties back to data and servers.\n\nTrue story: 25 years ago I used to process more than 40GB of compressed plain-text data every day from my Titanium PowerBook. But that’s because it was all running on a server in New Jersey while I was in Manhattan. Everything was done using the Command Line and SSH (secure shell).\nMore recently, processing OSM data for the entire UK was possible on my MacBook Air using GDAL and bash scripts but not possible using R/RStudio directly. Basically, the work took so long (&gt; 13 hours) that RStudio thought the script had died and tried to kill it."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#anatomy-of-a-command",
    "href": "lectures/3.4-The_Command_Line.html#anatomy-of-a-command",
    "title": "The Command Line",
    "section": "Anatomy of a Command",
    "text": "Anatomy of a Command\ncurl -L http://bit.ly/2vrUFKi | \n  head -3 |  \n  awk -F\",\" '{ print $2, $4, $6; }' &gt; results.txt\nThis command does four things in one ‘line’ on the CLI:\n\ncurl downloads the file and passes the contents to…\nhead which takes the first three rows and passes those to…\nawk which splits the rows on \",\" and takes the 2nd, 4th, and 6th fields and directs them into…\nA file called results.txt\n\n\nNote that results.txt is created if it doesn’t already exist, or overwritten if it does.\nIf you wanted to append to an existing file you would use &gt;&gt; instead of &gt;."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#interacting-with-files",
    "href": "lectures/3.4-The_Command_Line.html#interacting-with-files",
    "title": "The Command Line",
    "section": "Interacting with Files",
    "text": "Interacting with Files\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nls\nList\nls .\n\n\ncd\nChange Directory\ncd $HOME or cd ~\n\n\npwd\nPrint Working Directory\npwd\n\n\nmv\nRename/Move file a to b\nmv a.txt b.txt\n\n\nfind\nFind files matching some criteria\nfind . -name \"*.md\"\n\n\n\n\nNotice that most commands on the Command Line involve typing mnemonics (the shortest possible combination of letters that is unique memorable)."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#common-shortcuts",
    "href": "lectures/3.4-The_Command_Line.html#common-shortcuts",
    "title": "The Command Line",
    "section": "Common Shortcuts",
    "text": "Common Shortcuts\n\n\n\n\n\n\n\n\nShortcut\nMeans\nExample\n\n\n\n\n.\nThe current working directory\nls .\n\n\n..\nThe directory above the current working one\ncd ..\n\n\n~1\nThe current user’s home directory.\ncd ~\n\n\n/\nThe ‘root’ directory for the entire computer\nls /\n\n\n\"*\"\nA ‘wildcard’ meaning any number of characters in a filename\nfind . -name \"*.md\"\n\n\n\"?\"\nA ‘wildcard’ meaning one character in a filename\nfind . -name \"2.?-*.md\"\n\n\n\n\nThe main reason we care about all this is that all data is stored somewhere and all code executes somewhere. So we want a way to traverse the device efficiently when looking in directories, creating new files, writing different types of data to different places, and so forth. These shortcuts therefore crop up all over the place ‘in the wild’–if you don’t know what they’re telling you then you’ll wonder why your code doesn’t run or you can’t find the data you saved!\n\nThis may be easier to remember and write as cd $HOME, which does the same thing."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "href": "lectures/3.4-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "title": "The Command Line",
    "section": "A Simulated Walk Across My Laptop",
    "text": "A Simulated Walk Across My Laptop\ncd /\npwd\n&gt; /\nls\n&gt; Applications  Library  System  Users Volumes ...\ncd $HOME\npwd\n&gt; /Users/casa\nls\n&gt; Applications  Desktop  Dropbox  ...\ncd Dropbox\npwd\n&gt; /Users/casa/Dropbox\nls\n&gt; CASA  Lectures  Practicals ...\n\nModern computers (especially if you’ve grown up around iPhones and Android phones/tablets) are really good at hiding this fact, but that’s because people using phones or tablets really don’t want to be thinking about where their data is being stored, they just want to click save. But when you start coding then you need to start caring a lot more about where something is happening."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#finding-things-in-files",
    "href": "lectures/3.4-The_Command_Line.html#finding-things-in-files",
    "title": "The Command Line",
    "section": "Finding Things in Files",
    "text": "Finding Things in Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nless\nPeek at contents of a text file\nless file.txt\n\n\ngrep\nFind lines matching a ‘pattern’ in a file\ngrep 'pattern' file.txt\n\n\nhead\nPeek at first x rows of a text file\nhead -n 10 file.txt\n\n\ntail\nPeek at last x rows of a text file\ntail -n 10 file.txt\n\n\nwc\nCount things (rows, words, etc.)\nwc -l file.txt\n\n\nsed/awk\nComplicated, but powerful, things\nawk -F\",\" '{ print $1, $3; }' file.csv\n\n\n\n\nThe really crucial thing about all of these utilities is that they don’t load the entire file into memory. So you can ‘peek’ into a 15GB text file instantly without waiting four hours for it to load into memory (and then crash your machine). It’s kind of like the anti-Excel."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#time-to-escape",
    "href": "lectures/3.4-The_Command_Line.html#time-to-escape",
    "title": "The Command Line",
    "section": "Time to Escape!",
    "text": "Time to Escape!\nSome characters are ‘special’ and need to be escaped. You’ll encounter these both in the shell (a.k.a. command line) and in Python:\n\n\n\n\n\n\n\n\nEscape\nDoes\nExample\n\n\n\n\n\\\nAllows spaces in file names\nless My\\ File\\ with\\ Spaces.txt\n\n\n\\t\nCreates/matches a tab character\n\\tThe start of a paragraph...\n\n\n\\n\nCreates/matches a newline character\nThe end of a row/para...\\n\n\n\n\\r\nCreates/matches a carriage return\nThe end of a row/para...\\r\\n\n\n\n\\$\nLiteral dollar sign (since $ often marks a variable)\nIt costs \\$1,000,000\n\n\n\\!\nLiteral exclamation mark (since ! can mean a number of things)\nDon't forget me\\!\n\n\n\nThis also becomes relevant when you’re dealing with quotes:\n\"\"This is a problem,\" she said.\"\nvs. \n\"\\\"This is a problem,\\\" she said.\"\n\nThe carriage return is only ever encountered on files that have been opened on Windows machines."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#compressingdecompressing-files",
    "href": "lectures/3.4-The_Command_Line.html#compressingdecompressing-files",
    "title": "The Command Line",
    "section": "Compressing/Decompressing Files",
    "text": "Compressing/Decompressing Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\ngzip\nCompress/Decompress files\ngzip file.txt\n\n\ngunzip\nDecompress files\ngunzip file.txt.gz1\n\n\n\nThis can also be done using ‘switches’ passed to gzip: gzip -cd (where -d means ‘decompress’)."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#chaining-commands",
    "href": "lectures/3.4-The_Command_Line.html#chaining-commands",
    "title": "The Command Line",
    "section": "Chaining Commands",
    "text": "Chaining Commands\nThe CLI becomes much useful with command chaining:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\"\nThe ‘pipe’ (|) takes output from command and ‘pipes’ (aka. passes) it to another.\n\nThis will give you an ‘answer’ much, much, much faster than trying to open the whole file in, say, Excel, Numbers, or even Python."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#redirecting-output",
    "href": "lectures/3.4-The_Command_Line.html#redirecting-output",
    "title": "The Command Line",
    "section": "Redirecting Output",
    "text": "Redirecting Output\nWe can redirect outputs in to new files with &gt;, and inputs out of existing files using &lt;:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\" &gt; matches.txt\nSo the output from the previous commands goes into matches.txt as plain-text. The reverse &lt; is only used in very special circumstances so you probably won’t encounter it very often."
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#practical-applications",
    "href": "lectures/3.4-The_Command_Line.html#practical-applications",
    "title": "The Command Line",
    "section": "Practical Applications",
    "text": "Practical Applications\nMost developers will use one or more of these on a daily basis:"
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#a-complex-example",
    "href": "lectures/3.4-The_Command_Line.html#a-complex-example",
    "title": "The Command Line",
    "section": "A (Complex) Example",
    "text": "A (Complex) Example\nI do not expect you to understand this, but I do want you to understand why this is important:\ndocker run -v conda:/home/jovyan/work --rm ${DOCKER_NM} start.sh \\\n   conda env export -n ${ENV_NM} | sed '1d;$d' | sed '$d' \\\n   | perl -p -e 's/^([^=]+=)([^=]+)=.+$/$1$2/m' \\\n   | grep -Ev '\\- _|cpp|backports|\\- lib|\\- tk|\\- xorg' &gt; conda/environment_py.yml\n\nThis is how I generated the YAML file used by Anaconda Python installers: it is running a command on a virtual machine, collecting the output, filtering out lines by both row number and textual pattern, and directing this all in the environment_py.yml file. This can be run as part of my ‘build’ of the programming environment. It’s all automated!"
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#getting-help",
    "href": "lectures/3.4-The_Command_Line.html#getting-help",
    "title": "The Command Line",
    "section": "Getting Help",
    "text": "Getting Help\nThe Software Carpentry people have a whole set of lessons around working with ‘the shell’ (a.k.a. Command Line) that might help you.\n\nThe UNIX Shell.\nMIT’s ‘Missing Semester’ on Vim\n\nIndeed all of MIT’s Missing Semester content could be useful!"
  },
  {
    "objectID": "lectures/3.4-The_Command_Line.html#useful-videos",
    "href": "lectures/3.4-The_Command_Line.html#useful-videos",
    "title": "The Command Line",
    "section": "Useful Videos",
    "text": "Useful Videos\n\n\nThe Shell/Terminal in general:\n\nAbsolute BEGINNER Guide to the Mac OS Terminal\nLinux Bash Shell for Beginners: Tutorial 1\nBeginner’s Guide to the Bash Terminal\nShell Novice\nHow to use the Command Line\n\n\nSome specific commands:\n\nCat\nGzip/Tar (also a good point about spaces in a file name!)\nGrep\nFind\n\n\nAnd lots more here on using the file system and shell commands\n\n\n\n\nThe Command Line • Jon Reades"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-sense-of-this",
    "href": "lectures/3.2-LOLs.html#making-sense-of-this",
    "title": "LoLs",
    "section": "Making Sense of This",
    "text": "Making Sense of This\nWe can ‘unpack’ my_list in stages in order to make sense of it:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nfor i in my_list:\n  print(i)\nWhat do you think this will print?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "href": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "title": "LoLs",
    "section": "Debugging Our Thinking",
    "text": "Debugging Our Thinking\nLet’s make it a little more obvious:\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [7, 8, 9]\n\nmy_list = [a, b, c]\n\nfor i in my_list:\n  print(i) # Prints a, b, c in turn..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#the-next-step",
    "href": "lectures/3.2-LOLs.html#the-next-step",
    "title": "LoLs",
    "section": "The Next Step",
    "text": "The Next Step\nWe could then try this:\nfor i in my_list:\n  print(f\" &gt;&gt; {i}\")\n  for j in i: # Remember that i is a list!\n    print(j)\nThis produces:\n &gt;&gt; [1, 2, 3]\n1\n2\n3\n &gt;&gt; [4, 5, 6]\n4\n..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#putting-it-together",
    "href": "lectures/3.2-LOLs.html#putting-it-together",
    "title": "LoLs",
    "section": "Putting It Together",
    "text": "Putting It Together\nSome observations:\n\nWe can access i in my_list using either for i in my_list (every element in turn) or my_list[i] (one element only).\nWe can access j in list i using for j in i (every element in turn) or i[j] (one element only).\n\nDoes that mean we can also do this:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\ni,j = 0,1\nprint(my_list[i][j])"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "href": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "title": "LoLs",
    "section": "Let’s Talk It Out!",
    "text": "Let’s Talk It Out!\nSo if we write:\ni,j = 0,1\nprint(my_list[i][j])\nThen:\n\nmy_list[i] returns [1,2,3] (because i==0 and the first list is [1,2,3]), and\nmy_list[i][j] returns 2 (because j==1 and the [1,2,3][1]==2).\n\nSimilarly, my_list[2] grabs the third list ([7,8,9]) and then my_list[2][2] tells Python to get the third item in that third list (i.e. 9).\n\nHow you print the number 5 from this list-of-lists?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-this-useful",
    "href": "lectures/3.2-LOLs.html#making-this-useful",
    "title": "LoLs",
    "section": "Making This Useful",
    "text": "Making This Useful\nIf I rewrite the list this way perhaps it looks a little more useful?\nmy_cities = [\n  ['London', 51.5072, 0.1275, +0], \n  ['New York', 40.7127, 74.0059, -5], \n  ['Tokyo', 35.6833, 139.6833, +8]\n]\nNow we have something that is starting to look like data!"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "href": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "title": "LoLs",
    "section": "Down the Rabbit Hole",
    "text": "Down the Rabbit Hole"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lols-of-lols",
    "href": "lectures/3.2-LOLs.html#lols-of-lols",
    "title": "LoLs",
    "section": "LOLs of LOLs",
    "text": "LOLs of LOLs\nThis is also a legitimate list in Python.\nmy_cities = [\n  ['London', [51.5072, 0.1275], +0], \n  ['New York', [40.7127, 74.0059], -5], \n  ['Tokyo', [35.6833, 139.6833], +8]\n]\nprint(my_cities[0][0])\n&gt; London\nprint(my_cities[0][1][0])\n&gt; 51.5072\n\nWhy might it be a better choice of data structure than the earlier version?"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#conditions-consequences",
    "href": "lectures/2.4-Python_the_Basics-2.html#conditions-consequences",
    "title": "Python, the Basics (Part 2)",
    "section": "Conditions & Consequences",
    "text": "Conditions & Consequences\nThe simplest condition only considers one outcome:\nif &lt;condition is true&gt;:\n    #...do something...\nBut you’ll often needs something a little more sophisticated:\nif &lt;condition is true&gt;:\n    #...do something...\nelif &lt;some other condition is true&gt;:\n    #...do something else...\nelse:\n    #...if no conditions are true...\n\n#...code continues...\n\nBut no matter how complex, conditions always ultimately evaluate to True or False."
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#for-example",
    "href": "lectures/2.4-Python_the_Basics-2.html#for-example",
    "title": "Python, the Basics (Part 2)",
    "section": "For Example",
    "text": "For Example\nif x &lt; y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x &lt; y:\n  print(\"x is less than y\")\nelif x &gt; y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#conditional-syntax",
    "href": "lectures/2.4-Python_the_Basics-2.html#conditional-syntax",
    "title": "Python, the Basics (Part 2)",
    "section": "Conditional Syntax",
    "text": "Conditional Syntax\nThe most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic."
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#all-of-them-together-input",
    "href": "lectures/2.4-Python_the_Basics-2.html#all-of-them-together-input",
    "title": "Python, the Basics (Part 2)",
    "section": "All of Them Together (Input)!",
    "text": "All of Them Together (Input)!\nif hours &gt;= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#all-of-them-together-output",
    "href": "lectures/2.4-Python_the_Basics-2.html#all-of-them-together-output",
    "title": "Python, the Basics (Part 2)",
    "section": "All of Them Together (Output)!",
    "text": "All of Them Together (Output)!\nOutput from the Python interpreter:\n&gt;&gt;&gt; if hours &gt;= 0:\n... print(\"Hours were worked.\")\n  File \"&lt;stdin&gt;\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n&gt;&gt;&gt; else\n  File \"&lt;stdin&gt;\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt;     print \"No hours were worked.\")\n  File \"&lt;stdin&gt;\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#thats-better",
    "href": "lectures/2.4-Python_the_Basics-2.html#thats-better",
    "title": "Python, the Basics (Part 2)",
    "section": "That’s Better!",
    "text": "That’s Better!\nIt’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours &gt; 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#make-your-life-easy-well-easier",
    "href": "lectures/2.4-Python_the_Basics-2.html#make-your-life-easy-well-easier",
    "title": "Python, the Basics (Part 2)",
    "section": "Make Your Life Easy (Well, Easier)",
    "text": "Make Your Life Easy (Well, Easier)\nAlways comment your code:\n\nSo that you know what is going on.\nSo that you know why it is going on.\nSo that others can read your code.\nTo help you plan your code\n\n\nYou are reminding your future self what your code was for and helping to give it structure (explaining==thinking!)."
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#different-comment-styles",
    "href": "lectures/2.4-Python_the_Basics-2.html#different-comment-styles",
    "title": "Python, the Basics (Part 2)",
    "section": "Different Comment Styles",
    "text": "Different Comment Styles\n# This is a short comment\nprint(\"Foo\")\nprint(\"Bar\") # Also a short comment\n\n# ------- New Section --------\n# You can have comments span multiple\n# lines just by adding more '#' at the \n# start of the line.\n\n# You can keep code from running\n# print(\"Baz\")"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#comments-follow-indentation",
    "href": "lectures/2.4-Python_the_Basics-2.html#comments-follow-indentation",
    "title": "Python, the Basics (Part 2)",
    "section": "Comments Follow Indentation",
    "text": "Comments Follow Indentation\n# Function for processing occupational data\n# from the 2001 and 2011 Censuses.\ndef occ_data(df):\n  #  Columns of interest\n  cols = ['Managerial','Professional','Technical']\n    \n  # Integrate results into single dataset -- \n  # right now we don't replicate Jordan's approach of\n  # grouping them into 'knowledge worker' and 'other'. \n  for i in df.iterrows():\n    # For each column...\n    for j in cols:\n      # Do something\n      ..."
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#easier-multi-line-comments",
    "href": "lectures/2.4-Python_the_Basics-2.html#easier-multi-line-comments",
    "title": "Python, the Basics (Part 2)",
    "section": "Easier Multi-Line Comments",
    "text": "Easier Multi-Line Comments\nThe below are not real comments, but they can help when you have a really long comment that you want to make. They are also used to help explain what a function does (called a docstring).\n\"\"\"\nSo I was thinking that what we need here is \na way to handle the case where the data is\nincomplete or contains an observation that we\nweren't expecting (e.g. \"N/A\" instead of \"0\").\n\"\"\""
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#tips",
    "href": "lectures/2.4-Python_the_Basics-2.html#tips",
    "title": "Python, the Basics (Part 2)",
    "section": "Tips",
    "text": "Tips\nSome useful tips for commenting your code:\n\nInclude general information at the top of your programming file.\nAssume the person reading the code is a coder themselves.\nGood commenting is sparse in the sense that it is used judiciously, and concise without being gnomic.\nUse comments to track the logic of your code (especially in conditionals and loops)"
  },
  {
    "objectID": "lectures/2.4-Python_the_Basics-2.html#more-resources",
    "href": "lectures/2.4-Python_the_Basics-2.html#more-resources",
    "title": "Python, the Basics (Part 2)",
    "section": "More Resources",
    "text": "More Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\nConditional structures\nIf Statements\nIf-Else Statements\nIf-Elif\nWhitespace and comments\nUsing print()\nConditional syntax\nConditional operators\nConditional assignment\n\n\n\n\n\nPython, the Basics • Jon Reades"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#what-about-computational-thinking",
    "href": "lectures/2.2-Principles_of_Programming.html#what-about-computational-thinking",
    "title": "Principles of Programming",
    "section": "What About Computational Thinking?",
    "text": "What About Computational Thinking?\n\nComputational thinking is not thinking like a Computer Scientist. It is about recognising how to code can help us to understand, and manipulate, the world."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#key-features",
    "href": "lectures/2.2-Principles_of_Programming.html#key-features",
    "title": "Principles of Programming",
    "section": "Key Features",
    "text": "Key Features\nAspects of computational thinking include:\n\nRecognising how one problem connects to other problems.\nRecognising when and how to make things simpler and faster\nRecognising how different ways of tackling a problem gives you power to tackle new problems.\n\nSee this keynote by Lorena Barba (2014); esp. from 52:00 onwards.\n\nYou already do a lot of this when you generalise from your readings to your ideas/understanding!"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#why-are-these-virtues",
    "href": "lectures/2.2-Principles_of_Programming.html#why-are-these-virtues",
    "title": "Principles of Programming",
    "section": "Why Are These Virtues?",
    "text": "Why Are These Virtues?\nAccording to Larry Wall the three virtues of the programmer are:\n\nLaziness\nImpatience\nHubris\n\nThese are not to be taken literally (see Larry Wall’s “Three Virtues of a Programmer” are Utter Bull💩).\n\nAutomate the boring stuff, focus on the interesting bits! And it’s not about quantity of code, it’s about quality.\nUse code to save time, but don’t just jump head-first into problems.\nWhen something isn’t working well you want to make it work better/faster/more efficiently…"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#four-quotes-to-remember",
    "href": "lectures/2.2-Principles_of_Programming.html#four-quotes-to-remember",
    "title": "Principles of Programming",
    "section": "Four Quotes to Remember",
    "text": "Four Quotes to Remember\n\nComputers make very fast, very accurate mistakes.\nA computer program does what you tell it to do, not what you want it to do.\nOnly half of programming is coding. The other 90% is debugging.\nWeeks of coding can save you hours of planning."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#and-one-more",
    "href": "lectures/2.2-Principles_of_Programming.html#and-one-more",
    "title": "Principles of Programming",
    "section": "And One More…",
    "text": "And One More…"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#following-a-recipe-is-easy-right",
    "href": "lectures/2.2-Principles_of_Programming.html#following-a-recipe-is-easy-right",
    "title": "Principles of Programming",
    "section": "Following a Recipe is Easy, Right?",
    "text": "Following a Recipe is Easy, Right?\n\nSource"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#calculating-the-mean",
    "href": "lectures/2.2-Principles_of_Programming.html#calculating-the-mean",
    "title": "Principles of Programming",
    "section": "Calculating the Mean",
    "text": "Calculating the Mean\nGiven these numbers, what’s the average?\n1, 4, 7, 6, 4, 2"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#as-a-recipe",
    "href": "lectures/2.2-Principles_of_Programming.html#as-a-recipe",
    "title": "Principles of Programming",
    "section": "As a Recipe",
    "text": "As a Recipe\n\nTake a list of numbers\nStart a count of numbers in the list at 0\nStart a sum of numbers in the list at 0\nTake a number from the list:\n\nAdd 1 to the count\nAdd the value of the number to the sum\n\nRepeat step #4 until no numbers are left in the list.\nDivide the sum by the count\nReport this number back to the user"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#as-python",
    "href": "lectures/2.2-Principles_of_Programming.html#as-python",
    "title": "Principles of Programming",
    "section": "As Python",
    "text": "As Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#why-we-still-havent-solved-it",
    "href": "lectures/2.2-Principles_of_Programming.html#why-we-still-havent-solved-it",
    "title": "Principles of Programming",
    "section": "Why We Still Haven’t ‘Solved It’",
    "text": "Why We Still Haven’t ‘Solved It’\n\nSource"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#languages",
    "href": "lectures/2.2-Principles_of_Programming.html#languages",
    "title": "Principles of Programming",
    "section": "Languages",
    "text": "Languages\nComputer languages come with all of the ‘baggage’ of human languages; they have:\n\nA vocabulary (reserved words)\nA grammar (syntax)\nRules about the kinds of things you can say (grammar)\nStyles and idiosyncrasies all their own (history)\n\nIn this module we will use the Python programming language. We could also teach this same content in R."
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#python",
    "href": "lectures/2.2-Principles_of_Programming.html#python",
    "title": "Principles of Programming",
    "section": "Python",
    "text": "Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count += 1 # An alternative to count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#r",
    "href": "lectures/2.2-Principles_of_Programming.html#r",
    "title": "Principles of Programming",
    "section": "R",
    "text": "R\nnumbers = c(1, 4, 7, 6, 4, 2)\ntotal   = 0\ncount   = 0\nfor (num in numbers) {\n  total = total + num\n  count = count + 1\n}\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.2-Principles_of_Programming.html#finally-style",
    "href": "lectures/2.2-Principles_of_Programming.html#finally-style",
    "title": "Principles of Programming",
    "section": "Finally: Style",
    "text": "Finally: Style\nAlthough all programmers develop their own style (see: writing in any language), Python encourages coders to use a consistent style so that others can pick up your code and make sense of what’s going on (see: comments!).\nTwo useful resources:\n\nThe Hitchhiker’s Guide to Python\nA summary of Python code style conventions"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#space-adds-complexity",
    "href": "lectures/12.4-Clustering_and_Geography.html#space-adds-complexity",
    "title": "Clustering and Geography",
    "section": "Space Adds Complexity",
    "text": "Space Adds Complexity\nWe now have to consider two more types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#trade-offs-again",
    "href": "lectures/12.4-Clustering_and_Geography.html#trade-offs-again",
    "title": "Clustering and Geography",
    "section": "Trade-offs (Again)…",
    "text": "Trade-offs (Again)…\nConsider:\n\nClustering algorithms are inherently spatial.\nClustering algorithms do not take space geography into account.\n\nDoes this matter?\n\nAll clustering algorithms are about inter-observation and intra-cluster distances so they have some conceptualisation of ‘space’.\nSpatially-aware clustering algorithms exist but are generally much more computationally-intensive than ‘regular ones’."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#different-approaches",
    "href": "lectures/12.4-Clustering_and_Geography.html#different-approaches",
    "title": "Clustering and Geography",
    "section": "Different Approaches",
    "text": "Different Approaches\n\n\n\nAlgorithm\nPros\nCons\nGeographically Aware?\n\n\n\n\nk-Means\nFast. Deterministic.\nEvery observation to a cluster.\nN.\n\n\nDBSCAN\nAllows for clusters and outliers.\nSlower. Choice of \\(\\epsilon\\) critical. Can end up with all outliers.\nN, but implicit in \\(\\epsilon\\).\n\n\nOPTICS\nFewer parameters than DBSCAN.\nEven slower.\nN, but implicit in \\(\\epsilon\\).\n\n\nHierarchical/ HDBSCAN\nCan cut at any number of clusters.\nNo ‘ideal’ solution.\nY, with connectivity parameter.\n\n\nADBSCAN\nScales. Confidence levels.\nMay need large data set to be useful. Choice of \\(\\epsilon\\) critical.\nY.\n\n\nMax-p\nCoherent regions returned.\nVery slow if model poorly specified.\nY."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "href": "lectures/12.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "title": "Clustering and Geography",
    "section": "Setting the Relevant Distance",
    "text": "Setting the Relevant Distance\nMany clustering algorithms rely on a distance specification (usually \\(\\epsilon\\)). So to set this threshold:\n\nIn high-dimensional spaces this threshold will need to be large.\nIn high-dimensional spaces the scale will be meaningless (i.e. not have a real-world meaning, only an abstract one).\nIn 2- or 3-dimensional (geographical) space this threshold could be meaningful (i.e. a value in metres could work)."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "href": "lectures/12.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "title": "Clustering and Geography",
    "section": "Choosing a Distance Metric",
    "text": "Choosing a Distance Metric\n\n\n\n\n\n\n\n\nn Dimensions\nHow to Set\nExamples\n\n\n\n\n2 or 3\nTheory/Empirical Data\nWalking speed; Commute distance\n\n\n2 or 3\nK/L Measures\nPlot with Simulation for CIs to identify significant ‘knees’.\n\n\n3\nMarked Point Pattern?\n\n\n\n&gt; 3\nkNN\nCalculate average kNN distance based on some expectation of connectivity.\n\n\n\n\nRemember: inter-observation distance increases with dimensionality!"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian",
    "title": "Clustering and Geography",
    "section": "Experian",
    "text": "Experian\nSpecialist in consumer segmentation and geodemographics (bit.ly/2jMRhAW).\n\nMarket cap: £14.3 billion.\nMosaic: “synthesises of 850 million pieces of information… to create a segmentation that allocates 49 million individuals and 26 million households into one of 15 Groups and 66 detailed Types.””\nMore than 450 variables used.\n\nMost retail companies will have their own segmentation scheme. Competitors: CACI, Nielsen, etc."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian-groups",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian-groups",
    "title": "Clustering and Geography",
    "section": "Experian Groups",
    "text": "Experian Groups"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian-mapping",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian-mapping",
    "title": "Clustering and Geography",
    "section": "Experian Mapping",
    "text": "Experian Mapping"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#output-area-classification",
    "href": "lectures/12.4-Clustering_and_Geography.html#output-area-classification",
    "title": "Clustering and Geography",
    "section": "Output Area Classification",
    "text": "Output Area Classification\nOAC set up as ‘open source’ alternative to Mosaic:\n\nWell documented (UCL Geography a major contributor)\nDoesn’t require a license or payment\nCan be tweaked/extended/reweighted by users as needed\n\n\n\n\n\nClustering • Jon Reades"
  },
  {
    "objectID": "lectures/12.2-Classification.html#spot-the-difference",
    "href": "lectures/12.2-Classification.html#spot-the-difference",
    "title": "Classification",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nOn Maps\n\nGroup observations by ‘class’.\nTypically based on 1-D distribution.\nClasses are assigned by user choice.\n\n\nOn Labels\n\nLabel observations by ‘class’.\nTypically based on model outputs.\nLabels are assigned by user feedback.\n\n\n\nIn this session we are primarily concerned with the first column — classification as a modelling process is better considered a data science/modelling problem that is beyond the scope of this module."
  },
  {
    "objectID": "lectures/12.2-Classification.html#map-classification-choices",
    "href": "lectures/12.2-Classification.html#map-classification-choices",
    "title": "Classification",
    "section": "Map Classification Choices",
    "text": "Map Classification Choices\n\nAssign classes manually.\nSplit range evenly.\nSplit data evenly\nSplit data according to distribution\nSplit data according to their similarity to each other.\n\n\n\nAccording to some logic/theory/regulatory or policy fact or objective.\nEqual intervals for cases without heavy skew\nQuantiles or HeadTailBreaks for cases with heavy skew\nSD for cases with normal distribution; BoxPlot for others.\nNatural breaks/FIsher Jenks for cases where distribution is discontinuous"
  },
  {
    "objectID": "lectures/12.2-Classification.html#mapclassify",
    "href": "lectures/12.2-Classification.html#mapclassify",
    "title": "Classification",
    "section": "Mapclassify",
    "text": "Mapclassify\nMapclassify (part of PySAL) provides a wide range of classifiers:\n\n\n\nNo Parameters\nk Parameter\n\n\n\n\nBoxPlot\nUserDefined\n\n\nStdMean\nPercentiles\n\n\nMaxP\nQuantiles\n\n\nHeadTailBreaks\nNatural Breaks\n\n\nEqualInterval\nMaximum Breaks\n\n\n\nJenksCaspall/Sampled/Forced\n\n\n\nFisherJenks/Sampled\n\n\n\nk will a user-specified number of classes or binning criterion."
  },
  {
    "objectID": "lectures/12.2-Classification.html#raw",
    "href": "lectures/12.2-Classification.html#raw",
    "title": "Classification",
    "section": "Raw",
    "text": "Raw"
  },
  {
    "objectID": "lectures/12.2-Classification.html#user-defined",
    "href": "lectures/12.2-Classification.html#user-defined",
    "title": "Classification",
    "section": "User Defined",
    "text": "User Defined\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, 125000.00]\n0\n\n\n( 125000.00, 250000.00]\n4\n\n\n( 250000.00, 925000.00]\n865\n\n\n( 925000.00, 1500000.00]\n85\n\n\n(1500000.00, 4500000.00]\n29"
  },
  {
    "objectID": "lectures/12.2-Classification.html#box-plot",
    "href": "lectures/12.2-Classification.html#box-plot",
    "title": "Classification",
    "section": "Box Plot",
    "text": "Box Plot\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -31429.25]\n0\n\n\n( -31429.25, 391267.00]\n246\n\n\n( 391267.00, 495010.00]\n246\n\n\n( 495010.00, 673064.50]\n245\n\n\n( 673064.50, 1095760.75]\n175\n\n\n(1095760.75, 4416659.00]\n70"
  },
  {
    "objectID": "lectures/12.2-Classification.html#standard-deviations",
    "href": "lectures/12.2-Classification.html#standard-deviations",
    "title": "Classification",
    "section": "Standard Deviations",
    "text": "Standard Deviations\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -171366.63]\n0\n\n\n(-171366.63, 216174.43]\n0\n\n\n( 216174.43, 991256.55]\n892\n\n\n( 991256.55, 1378797.61]\n53\n\n\n(1378797.61, 4416659.00]\n38"
  },
  {
    "objectID": "lectures/12.2-Classification.html#max-p",
    "href": "lectures/12.2-Classification.html#max-p",
    "title": "Classification",
    "section": "Max P",
    "text": "Max P\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346594.00]\n142\n\n\n( 346594.00, 461577.00]\n279\n\n\n( 461577.00, 529197.00]\n140\n\n\n( 529197.00, 530662.00]\n3\n\n\n( 530662.00, 613465.00]\n115\n\n\n( 613465.00, 842387.00]\n167\n\n\n( 842387.00, 4416659.00]\n137"
  },
  {
    "objectID": "lectures/12.2-Classification.html#head-tail-breaks",
    "href": "lectures/12.2-Classification.html#head-tail-breaks",
    "title": "Classification",
    "section": "Head Tail Breaks",
    "text": "Head Tail Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 603715.49]\n670\n\n\n( 603715.49, 976290.79]\n218\n\n\n( 976290.79, 1508985.73]\n66\n\n\n(1508985.73, 2257581.55]\n16\n\n\n(2257581.55, 2826007.08]\n9\n\n\n(2826007.08, 3553496.25]\n3\n\n\n(3553496.25, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#equal-interval",
    "href": "lectures/12.2-Classification.html#equal-interval",
    "title": "Classification",
    "section": "Equal Interval",
    "text": "Equal Interval\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 825125.00]\n842\n\n\n( 825125.00, 1423714.00]\n108\n\n\n(1423714.00, 2022303.00]\n17\n\n\n(2022303.00, 2620892.00]\n10\n\n\n(2620892.00, 3219481.00]\n4\n\n\n(3219481.00, 3818070.00]\n1\n\n\n(3818070.00, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#quantiles",
    "href": "lectures/12.2-Classification.html#quantiles",
    "title": "Classification",
    "section": "Quantiles",
    "text": "Quantiles\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346009.00]\n140\n\n\n( 346009.00, 405677.86]\n140\n\n\n( 405677.86, 461959.29]\n140\n\n\n( 461959.29, 529612.86]\n141\n\n\n( 529612.86, 639488.86]\n140\n\n\n( 639488.86, 827691.43]\n140\n\n\n( 827691.43, 4416659.00]\n141"
  },
  {
    "objectID": "lectures/12.2-Classification.html#natural-breaks",
    "href": "lectures/12.2-Classification.html#natural-breaks",
    "title": "Classification",
    "section": "Natural Breaks",
    "text": "Natural Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 433543.00]\n356\n\n\n( 433543.00, 605879.00]\n316\n\n\n( 605879.00, 842387.00]\n174\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/12.2-Classification.html#maximum-breaks",
    "href": "lectures/12.2-Classification.html#maximum-breaks",
    "title": "Classification",
    "section": "Maximum Breaks",
    "text": "Maximum Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 1688895.00]\n961\n\n\n(1688895.00, 1926265.50]\n4\n\n\n(1926265.50, 2278155.50]\n5\n\n\n(2278155.50, 2929865.50]\n9\n\n\n(2929865.50, 3349991.00]\n2\n\n\n(3349991.00, 3959682.50]\n1\n\n\n(3959682.50, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#fisher-jenks",
    "href": "lectures/12.2-Classification.html#fisher-jenks",
    "title": "Classification",
    "section": "Fisher Jenks",
    "text": "Fisher Jenks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 435961.00]\n363\n\n\n( 435961.00, 607480.00]\n310\n\n\n( 607480.00, 842387.00]\n173\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/12.2-Classification.html#jenks-caspall",
    "href": "lectures/12.2-Classification.html#jenks-caspall",
    "title": "Classification",
    "section": "Jenks Caspall",
    "text": "Jenks Caspall\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 365741.00]\n188\n\n\n( 365741.00, 441979.00]\n187\n\n\n( 441979.00, 520791.00]\n167\n\n\n( 520791.00, 638474.00]\n160\n\n\n( 638474.00, 890055.00]\n156\n\n\n( 890055.00, 1626454.00]\n103\n\n\n(1626454.00, 4416659.00]\n22"
  },
  {
    "objectID": "lectures/12.2-Classification.html#summary",
    "href": "lectures/12.2-Classification.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\n\nThe choice of classification scheme should be data- and distribution-led. This is simply a demonstration of how different schemes can shape your understanding of the data."
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips",
    "href": "lectures/12.2-Classification.html#code-useful-tips",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the classes:\nkl = 7\ncls = [mapclassify.BoxPlot, ...,  mapclassify.JenksCaspall]\nSetting up the loop:\nfor cl in cls:\n    try: \n        m = cl(ppd.Value, k=kl)\n    except TypeError:\n        m = cl(ppd.Value)\n    \n    f = plt.figure()\n    gs = f.add_gridspec(nrows=2, ncols=1, height_ratios=[1,4])\n\n    ax1 = f.add_subplot(gs[0,0])\n    ...\n\n    ax2 = f.add_subplot(gs[1,0])\n    ..."
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips-1",
    "href": "lectures/12.2-Classification.html#code-useful-tips-1",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the distribution:\n    ax1 = f.add_subplot(gs[0,0])\n    sns.kdeplot(ppd.Value, ax=ax1, color='r')\n    ax1.ticklabel_format(style='plain', axis='x') \n\n    y = ax1.get_ylim()[1]\n    for b in m.bins:\n        ax1.vlines(b, 0, y, linestyles='dotted')"
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips-2",
    "href": "lectures/12.2-Classification.html#code-useful-tips-2",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nAdjusting the legend text:\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k,v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\nSetting up the map:\n    ax2 = f.add_subplot(gs[1,0])\n    ppd.assign(cl=m.yb).plot(column='cl', k=len(m.bins), categorical=True, legend=True, ax=ax2)\n    \n    mapping = dict([(i,s) for i,s in enumerate(m.get_legend_classes())])\n    ax2.set_axis_off()\n    replace_legend_items(ax2.get_legend(), mapping)\n\n\n\n\nClassification • Jon Reades"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#curse-and-blessing",
    "href": "lectures/11.3-Dimensionality.html#curse-and-blessing",
    "title": "Dimensionality",
    "section": "Curse and Blessing…",
    "text": "Curse and Blessing…\n\nMore dimensions means more information.\nMore dimensions makes for easier seperation.\nMore dimensions inflates distance.\nMore dimensions increases the risk of overfitting.\n\n\nOr as Analytics India Magazine puts it:\n\nHigh-dimensional spaces have geometrical properties that are counter-intuitive and far from the properties observed in two- or three-dimensional spaces.\nData analysis tools are often designed with intuitive properties and low-dimensional spaces in mind."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#pca",
    "href": "lectures/11.3-Dimensionality.html#pca",
    "title": "Dimensionality",
    "section": "PCA",
    "text": "PCA\nWorkhorse dimensionality reduction method: simple, fast, and effective. Can be thought of as freely rotating axes to align with directions of maximum variance. I like this summary:\n\nPCA (Principal Components Analysis) gives us our ‘ideal’ set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, the second has higher variance than the third, and so on), uncorrelated (all components are orthogonal), and low in number (we can throw away the lower ranked components as they usually contain little signal).\n\nBut I particularly liked this exposition in Towards Data Science."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice",
    "href": "lectures/11.3-Dimensionality.html#in-practice",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\npca.transform(data)\nSee also: Kernel PCA for non-linear problems."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#rtfm",
    "href": "lectures/11.3-Dimensionality.html#rtfm",
    "title": "Dimensionality",
    "section": "RT(F)M",
    "text": "RT(F)M\nWhy was I banging on about transformations? Well, what does this assume about the data?\n\nLinear dimensionality reduction using Singular Value Decomposition projects data into to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\n\nI found a nice explanation of PCA using dinner conversation over several bottles of wine as an example on Stats.StackExhcange.com. There are many good illustrations of this process on stats.stackexchange.com."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#other-considerations",
    "href": "lectures/11.3-Dimensionality.html#other-considerations",
    "title": "Dimensionality",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nPCA is a form of unsupervised learning that does not take output labels into account. Other approaches (such as Linear Discriminant Analysis [note: not Latent Dirichlet Allocation]) consider the output as part of the transformation. PCA is also deterministic.\n\nSee this discussion."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#t-sne",
    "href": "lectures/11.3-Dimensionality.html#t-sne",
    "title": "Dimensionality",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbour Embedding is best understood as a visualisation technique, not an analytical one. This is because it is probabilistic and not deterministic."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice-1",
    "href": "lectures/11.3-Dimensionality.html#in-practice-1",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.manifold import TSNE\nembedded = TSNE(n_components=d).fit_transform(x)\nThe choice of perplexity and n_iter matter, and so does the metric. In practice you will need to experiment with these.\n\nt-SNE is also much harder computationally than PCA and it may be preferrable on very high-D data sets to apply PCA first and then t-SNE to the reduced data set! The output could then be fed to a clustering algorithm to make predictions about where new observations belong, but do not confuse that with meaning."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#umap",
    "href": "lectures/11.3-Dimensionality.html#umap",
    "title": "Dimensionality",
    "section": "UMAP",
    "text": "UMAP\nNon-linear dimensionality reduction that tries to preserve both local and global structure. Puts it between PCA and t-SNE.\n\n\n\nsee examples on umap-learn.readthedocs.io\n\n\nNote that the library is called umap-learn."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice-2",
    "href": "lectures/11.3-Dimensionality.html#in-practice-2",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nimport umap\ntransformer = umap.UMAP(n_components=d)\nembedded = transformer.fit_transform(x)\nThe choice of n_neighbors, min_dist, and metric matter. In practice you may need to experiment with these."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#gotcha",
    "href": "lectures/11.3-Dimensionality.html#gotcha",
    "title": "Dimensionality",
    "section": "Gotcha!",
    "text": "Gotcha!\nt-SNE (less so UMAP) requires very careful handling:\n\nHyperparameters matter a lot\nCluster size means nothing\nCluster distances mean nothing\nClusters may mean nothing (low neighbour count/perplexity)\nOutputs are stochastic (not deterministic)\n\nBoth likely require repeated testing and experimentation."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#other-approaches",
    "href": "lectures/11.3-Dimensionality.html#other-approaches",
    "title": "Dimensionality",
    "section": "Other Approaches",
    "text": "Other Approaches\n\nFeature selection, including forwards/backwards (sklearn.feature_selection here)\nDecomposition (sklearn.decomposition here, especiall SVD)\nOther types of manifold learning (sklearn.manifold here)\nRandom projection (sklearn.random_projection here)\nSupport Vector Machines (sklearn.svm here)\nEnsemble Methods (such as Random Forests: sklearn.ensemble.ExtraTreesClassifier and sklearn.ensemble.ExtraTreesRegressor here and here)"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#resources",
    "href": "lectures/11.3-Dimensionality.html#resources",
    "title": "Dimensionality",
    "section": "Resources",
    "text": "Resources\n\n\n\nRethinking ‘distance’ in New York City Medium URL\nFive Boroughs for the 21\\(^{st}\\) Century Medium URL\nCurse of Dimensionality\nThe Curse of Dimensionality\nUnderstanding Curse of Dimensionality\nCurse of Dimensionality – A ‘Curse’ to Machine Learning\nImportance of Feature Scaling\nUnderstanding PCA\nIntroduction to t-SNE in Python\n\n\n\nHow to Use t-SNE Effectively\nHow to tune the Hyperparameters of t-SNE\nUnderstanding UMAP (Compares to t-SNE)\nHow UMAP Works\n3 New Techniques for Data-Dimensionality Reduction in ML\nUMAP for Dimensionality Reduction (Video)\nA Bluffer’s Guide to Dimensionality Reduction (Video)\n\n\n\n\n\n\nDimensionality • Jon Reades"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#the-data-generating-process",
    "href": "lectures/11.1-Data_Space.html#the-data-generating-process",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\nSource."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#the-data-generating-process-1",
    "href": "lectures/11.1-Data_Space.html#the-data-generating-process-1",
    "title": "The Data Space",
    "section": "The Data Generating Process",
    "text": "The Data Generating Process\n\nSource."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#cashier-income-as-dgp",
    "href": "lectures/11.1-Data_Space.html#cashier-income-as-dgp",
    "title": "The Data Space",
    "section": "Cashier Income as DGP",
    "text": "Cashier Income as DGP\nQuestion: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#house-prices-as-dgp",
    "href": "lectures/11.1-Data_Space.html#house-prices-as-dgp",
    "title": "The Data Space",
    "section": "House Prices as DGP",
    "text": "House Prices as DGP\n\nSource.\n\nWhat does this suggest about the DGP?"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "href": "lectures/11.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "title": "The Data Space",
    "section": "Is Bill Gates as Rich as He is Tall?",
    "text": "Is Bill Gates as Rich as He is Tall?\nInstinctively, we know that Bill Gates’ wealth is much further from ‘normal’ than is his height. But how?\n\nHow can we compare income and height if they share no common units?\nHow can we compare the biodiversity of sites in the tropics with those of sub-Arctic areas given that there are different numbers of species to begin with?\n\nWe need:\n\nWays to make different dimensions comparable, and\nWays to remove unit effects from distance measures."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-1d",
    "href": "lectures/11.1-Data_Space.html#distance-in-1d",
    "title": "The Data Space",
    "section": "Distance in 1D",
    "text": "Distance in 1D\n\\[\nd(i,j) = |(i_{1}-j_{1})|\n\\]"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-2d",
    "href": "lectures/11.1-Data_Space.html#distance-in-2d",
    "title": "The Data Space",
    "section": "Distance in 2D",
    "text": "Distance in 2D\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}}\n\\]"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-3d-or-more",
    "href": "lectures/11.1-Data_Space.html#distance-in-3d-or-more",
    "title": "The Data Space",
    "section": "Distance in 3D… or More",
    "text": "Distance in 3D… or More\nWe can keep adding dimensions…\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}+(i_{3}-j_{3})^{2}}\n\\]\nYou continue adding dimensions indefinitely, but from here on out you are dealing with hyperspaces!"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#thinking-in-data-space",
    "href": "lectures/11.1-Data_Space.html#thinking-in-data-space",
    "title": "The Data Space",
    "section": "Thinking in Data Space",
    "text": "Thinking in Data Space\nWe can write the coordinates of an observation with 3 attributes (e.g. height, weight, income) as:\n\\[\nx_{i} = { {x_{i1}, x_{i2}, x_{i3} } }\n\\]\nSomething with 8 attributes (e.g. height, weight, income, age, year of birth, …) ‘occupies’ an 8-dimensional space…"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#two-propositions",
    "href": "lectures/11.1-Data_Space.html#two-propositions",
    "title": "The Data Space",
    "section": "Two Propositions",
    "text": "Two Propositions\n\nThat geographical space is no different from any other dimension in a data set.\nThat geographical space is still special when it comes to thinking about relationships."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#implication",
    "href": "lectures/11.1-Data_Space.html#implication",
    "title": "The Data Space",
    "section": "Implication",
    "text": "Implication\nIf you can shift from thinking in columns of data, to thinking of a data space then you’ll have a much easier time dealing with dimensionality reduction and clustering."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#resources",
    "href": "lectures/11.1-Data_Space.html#resources",
    "title": "The Data Space",
    "section": "Resources",
    "text": "Resources\n\nAre Statisticians Cold-Blooded Bosses?\nBeyond 3D: Thinking in Higher Dimensions\nVisualizing beyond 3 Dimensions\nThe things you’ll find in higher dimensions (also useful for dimensionality reduction)\nWhat’s a Tensor? (heavy on the Physics relevance, but a lot of useful terminology and abstraction)\n\n\n\n\n\nThe Data Space • Jon Reades"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#choices-choices",
    "href": "lectures/10.1-Visualising_Data.html#choices-choices",
    "title": "Visualising Data",
    "section": "Choices, Choices…",
    "text": "Choices, Choices…\n\nmatplotlib: the ‘big beast’ of visualisation in Python. Similar to MATLAB. Highly customisable. Very complex.\nseaborn: a layer that sits over top of matplotlib and makes it easier to produce good-quality graphics.\nbokeh: web-based visualisation tool that can integrate with Jupyter or output to static HTML files.\nplotly: another web-based visualisation tool that can integrate with Jupyter.\n\nMore emerging all the time: Vega/Altair, HoloViews, etc.\n\n\nIf you’re really wedded to ggplot, plotnine is a clone of ggplot’s interface (Grammer of Graphics) in Python. A brief overview of visualisation libraries could be helpful."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#seaborn",
    "href": "lectures/10.1-Visualising_Data.html#seaborn",
    "title": "Visualising Data",
    "section": "Seaborn",
    "text": "Seaborn\nDesigned to provide ggplot-like quality output using matplotlib:\n\nImprove on default colourmaps and colour defaults.\nIntegration with pandas data frames (Note: not geopandas!).\nOffers more plot types out of the box.\nStill offers access to matplotlib’s back-end."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#plot-types",
    "href": "lectures/10.1-Visualising_Data.html#plot-types",
    "title": "Visualising Data",
    "section": "Plot Types",
    "text": "Plot Types\n\n\n\nPartial Overview of Seaborn Plots\n\n\n\n\nFor the fuller overview see Overview of seaborn plotting functions and the full API reference."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#in-practice",
    "href": "lectures/10.1-Visualising_Data.html#in-practice",
    "title": "Visualising Data",
    "section": "In Practice",
    "text": "In Practice\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\",\n             hue=\"region\", style=\"event\",\n             data=fmri)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#in-practice-2",
    "href": "lectures/10.1-Visualising_Data.html#in-practice-2",
    "title": "Visualising Data",
    "section": "In Practice 2",
    "text": "In Practice 2\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\ndf = sns.load_dataset(\"penguins\")\n\nax = sns.swarmplot(data=df, x=\"body_mass_g\", y=\"sex\", hue=\"species\")\nax.set(ylabel=\"\")"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#configuring-seaborn",
    "href": "lectures/10.1-Visualising_Data.html#configuring-seaborn",
    "title": "Visualising Data",
    "section": "Configuring Seaborn",
    "text": "Configuring Seaborn\nSeaborn ‘themes’ act as shortcuts for setting multiple matplotlib parameters:\n\n\n\nSeaborn Command\nAccomplishes\n\n\n\n\nset_theme(...)\nSet multiple theme parameters in one step.\n\n\naxes_style(...)\nReturn a parameter dict for the aesthetic style of the plots.\n\n\nset_style(...)\nSet the aesthetic style of the plots.\n\n\nplotting_context(...)\nReturn a parameter dict to scale elements of the figure.\n\n\nset_context(...)\nSet the plotting context parameters.\n\n\n\nYou can also access:\n\nPalettes: colormaps can be generated using sns.color_palette(...) and set using sns.set_palette(...).\nAxes Styles: includes darkgrid, whitegrid, dark, white, ticks."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#anatomy-of-a-figure",
    "href": "lectures/10.1-Visualising_Data.html#anatomy-of-a-figure",
    "title": "Visualising Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\nSource."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#writing-a-figure",
    "href": "lectures/10.1-Visualising_Data.html#writing-a-figure",
    "title": "Visualising Data",
    "section": "Writing a Figure",
    "text": "Writing a Figure\nThere are multiple ways to access/write elements of a plot:\n\nFigure: high-level features (e.g. title, padding, etc.). Can be accessed via plt.gcf() (get current figure) or upon creation (e.g. f, ax = plt.subplots(1,1) or f = plt.figure()).\nAxes: axis-level features (e.g. labels, tics, spines, limits, etc.). Can be accessed via plt.gca() (get current axes) or upon creation (e.g. f, ax = plt.subplots(1,1) or ax = f.add_subplot(1,1,1)).\n\nAnnotations, artists, and other features are typically written into the axes using the coordinate space of the figure (e.g. decimal degrees for lat/long, metres for BNG, etc.)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#adding-a-3rd-dimension",
    "href": "lectures/10.1-Visualising_Data.html#adding-a-3rd-dimension",
    "title": "Visualising Data",
    "section": "Adding a 3rd Dimension",
    "text": "Adding a 3rd Dimension\nThis ‘feature’ is less well-developed but does work:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax  = plt.axes(projection='3d')\n# OR\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\n# THEN\nax.contour3D(X, Y, Z, ...)\nax.plot_surface(x, y, z, ...)\nax.plot3D(xline, yline, zline, ...)\nax.scatter3D(x, y, z, ...)\n# ax.plot_surface and ax.plot_wire also give you 3D renderings\nYou can then set the elevation and azimuth using: ax.view_init(&lt;elevation&gt;, &lt;azimuth&gt;)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#saving-outputs",
    "href": "lectures/10.1-Visualising_Data.html#saving-outputs",
    "title": "Visualising Data",
    "section": "Saving Outputs",
    "text": "Saving Outputs\nStraightforward via save figure function, but lots of options!\nplt.savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n    orientation='portrait', papertype=None, format=None,\n    transparent=False, bbox_inches=None, pad_inches=0.1,\n    frameon=None, metadata=None)\nThe format can be largely determined by the file extension in the fname (file name) and the supported formats depends on what you’ve installed! You can find out what’s available to you using: plt.gcf().canvas.get_supported_filetypes()."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#jupyter",
    "href": "lectures/10.1-Visualising_Data.html#jupyter",
    "title": "Visualising Data",
    "section": "Jupyter",
    "text": "Jupyter\nBy default, Jupyter’s output is static matplotlib, but we can extend this in three ways:\n\nMake the static plot zoomable and pannable using %matplotlib widget (declare this at the top of your notebook).\nMake the plot more directly interactive using ipywidgets (import interact and related libs as needed).\nUse a browser-based visualisation tool such as bokeh, plotly, altair/vega, holoviews, or even d3 (format may be very, very different from what you are ‘used to’ in Python)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#widgets",
    "href": "lectures/10.1-Visualising_Data.html#widgets",
    "title": "Visualising Data",
    "section": "Widgets",
    "text": "Widgets\n%matplotlib widget\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = hackney.plot(edgecolor='k', facecolor='none')\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#interact",
    "href": "lectures/10.1-Visualising_Data.html#interact",
    "title": "Visualising Data",
    "section": "Interact()",
    "text": "Interact()\nTaking an example from Dani’s work:\nfrom ipywidgets import interact\n# Alternatives: interactive, fixed, interact_manual\ninteract(\n    &lt;function&gt;, # Function to make interactive\n    &lt;param0&gt;,   # e.g. Data to use\n    &lt;param1&gt;,   # e.g. Range start/end/step\n    &lt;param2&gt;    # e.g. Fixed value\n);"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#bokeh",
    "href": "lectures/10.1-Visualising_Data.html#bokeh",
    "title": "Visualising Data",
    "section": "Bokeh",
    "text": "Bokeh"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#automation",
    "href": "lectures/10.1-Visualising_Data.html#automation",
    "title": "Visualising Data",
    "section": "Automation",
    "text": "Automation\nPlots built on top of matploblib can, to some extent, be automated using functions. For example, to draw circles and place text:\ndef circle(ax, x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, \n                    linewidth=1, edgecolor='black', \n                    facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, \n                                  foreground='w')])\n    ax.add_artist(circle)\n\ndef text(ax, x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n         ha='center', va='top', weight='bold', color='blue')"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!1",
    "text": "Don’t Underestimate Text!1\nClick to cycle through the examples:\n\n\n\n\n\n\n\nShamelssly taken from Datawrapper"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#resources",
    "href": "lectures/10.1-Visualising_Data.html#resources",
    "title": "Visualising Data",
    "section": "Resources",
    "text": "Resources\n\n\n\nMatplotlib Cheatsheets\nIntroduction to PyPlot (includes lots of parameter information)\nVisualisation with Seaborn\nSeaborn Tutorial\nElite Data Science Seaborn Tutorial\nDatacamp Seaborn Tutorial\nThree-Dimensional Plotting in Matplotlib\nAn easy introduction to 3D plotting with Matplotlib\n\n\n\nUsing text effectively in data viz\nChoosing fonts for charts and tables\nBokeh Gallery\nBokeh User Guide\nProgramming Historian: Visualizing Data with Bokeh and Pandas\nReal Python: Data Viz with Bokeh\nData Viz with Bokeh (Pt. 1)\nUsing Interact\nText in Data Visualizations\n\n\n\n\n\n\nVisualising Data • Jon Reades"
  },
  {
    "objectID": "lectures/1.4-Writing.html#the-pipeline",
    "href": "lectures/1.4-Writing.html#the-pipeline",
    "title": "Writing",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nCode\nAnalyse\nUnderstand\nCommunicate\nReport\n\n\nHere are five things that we might imagine any data scientists, spatial or otherwise, does.\nDo we think they’re in the right order?\nI can tell you right now that this sequence of steps is how to get a mark of between 45 and 63 in your dissertation."
  },
  {
    "objectID": "lectures/1.4-Writing.html#the-pipeline-1",
    "href": "lectures/1.4-Writing.html#the-pipeline-1",
    "title": "Writing",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nUnderstand\nCode\nReport\nAnalyse\nCommunicate\n\n\nHere’s a slightly better order, here’s why:\n\nYou can’t answer a question – from your boss, from an academic, from your friends even – if you don’t understand it. So before you run off and start writing some code, the first thing you need to do is understand the problem you’re trying to solve. That problem should not be technical, it should be practical.\nOnce you understand the problem you can start trying to code a solution.\nThe code will allow you to produce reports. These reports might be to do with data quality, they might be diagnostics from a Random Forest Machine Learning algorithm. At each stage in the development of your results you should be generating reports that help you to better-understand your problem and work out if your code is working.\nOnce the reports have given you confidence in your findings now you can actually write the analysis. Your analysis might lead you to realise that you need to go back and write more code and produce more reports, but that’s normal.\nFinally, you need to work out how to communicate your analysis. If you understand the problem then you’ll find this process rewarding. If you don’t then you’ll find it frustrating and want to brush it off quickly.\n\nSo the fact these are in a list is still rather misleading because at each point you get feedback effects, and it’s also a loop."
  },
  {
    "objectID": "lectures/1.4-Writing.html#another-way",
    "href": "lectures/1.4-Writing.html#another-way",
    "title": "Writing",
    "section": "Another Way…",
    "text": "Another Way…\nAnother way to think about all this is to write backwards:"
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-understand",
    "href": "lectures/1.4-Writing.html#how-to-understand",
    "title": "Writing",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?"
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-understand-1",
    "href": "lectures/1.4-Writing.html#how-to-understand-1",
    "title": "Writing",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?\n\n\nWhy are we doing this?"
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-report",
    "href": "lectures/1.4-Writing.html#how-to-report",
    "title": "Writing",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number."
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-report-1",
    "href": "lectures/1.4-Writing.html#how-to-report-1",
    "title": "Writing",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number.\n\n\nThese are the takeaways."
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-analyse",
    "href": "lectures/1.4-Writing.html#how-to-analyse",
    "title": "Writing",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods we can use."
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-analyse-1",
    "href": "lectures/1.4-Writing.html#how-to-analyse-1",
    "title": "Writing",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods I can use.\n\n\nThis is the method that matches the need."
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-communicate",
    "href": "lectures/1.4-Writing.html#how-to-communicate",
    "title": "Writing",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?"
  },
  {
    "objectID": "lectures/1.4-Writing.html#how-to-communicate-1",
    "href": "lectures/1.4-Writing.html#how-to-communicate-1",
    "title": "Writing",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?\n\n\nWho needs to know?"
  },
  {
    "objectID": "lectures/1.4-Writing.html#writing-thinking",
    "href": "lectures/1.4-Writing.html#writing-thinking",
    "title": "Writing",
    "section": "Writing == Thinking",
    "text": "Writing == Thinking\n\nIf writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial. ~ Paul Graham\n\n\nWriting is thinking. To write well is to think clearly. That’s why it’s so hard. ~ David McCullough"
  },
  {
    "objectID": "lectures/1.4-Writing.html#clarity-understanding",
    "href": "lectures/1.4-Writing.html#clarity-understanding",
    "title": "Writing",
    "section": "Clarity == Understanding",
    "text": "Clarity == Understanding\n\nIf you can’t explain an idea to an 8 year old, you don’t understand it. ~ Richard Feynman"
  },
  {
    "objectID": "lectures/1.4-Writing.html#i-dont-care",
    "href": "lectures/1.4-Writing.html#i-dont-care",
    "title": "Writing",
    "section": "I Don’t Care",
    "text": "I Don’t Care\n\nFocus on the ‘so what’."
  },
  {
    "objectID": "lectures/1.4-Writing.html#tell-me-a-story",
    "href": "lectures/1.4-Writing.html#tell-me-a-story",
    "title": "Writing",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nDon’t bury the lede.\n\n\nAll data science is, ultimately, a story. A story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways."
  },
  {
    "objectID": "lectures/1.4-Writing.html#resources",
    "href": "lectures/1.4-Writing.html#resources",
    "title": "Writing",
    "section": "Resources",
    "text": "Resources\n\nThe Most Undervalued Skill for Data Scientists\nOne Mindset Shift That Will Make You a Better Data Scientist\nSword, H. (2017). Air & light & time & space: How successful academics write. Harvard University Press.1\nGreene, A. E. (2013). Writing science in plain English. University of Chicago Press.2\n\n\n\n\n\nWriting • Jon Reades and Fulvio Lopane\n\n\nAvailable for free via JStor.Not seemingly available for free, but I found a nice little summary (with typos) here."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#who-we-are",
    "href": "lectures/1.1-Getting_Oriented.html#who-we-are",
    "title": "Getting Oriented",
    "section": "Who We Are?",
    "text": "Who We Are?\n\nA few fun facts about your module leads…\n\n\nJon:\n\nHas a first degree in literature.\nHas never studied computer science and has only ever taken night classes in programming.\nWorked for a dot.com start-up in New York before transferring to London.\nHas taught undergraduate geography at King’s, from which this module evolved, and has taught this module for five years.\nWhen not working on teaching, being head of department, and squeezing in some reseach, Jon can be found looking after his two young daughters, mountain biking with Andy or hiking in Scotland.\n\nFulvio:\n\nAs a child wanted to be a geographer, but something went wrong and ended up being an engineer.\nHe learned informatics without a computer, but writing code with paper and pen.\nDesigned seismic adjustments to power plants before dedicating to spatial data science.\nDid a PhD in Multi-Objective Spatial Optimisation.\nWhen not at CASA, Fulvio can be found nerding out with anthropology or history books, old maps, or -on weekends- football matches."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#information",
    "href": "lectures/1.1-Getting_Oriented.html#information",
    "title": "Getting Oriented",
    "section": "Information",
    "text": "Information\nFoundations is distributed across two web sites:\n\nThe micro-site: jreades.github.io/fsds/ – lectures, practicals, readings, and information about the assessments.\nMoodle: moodle.ucl.ac.uk/course/view.php?id=44186 – recorded sessions, booking drop-in hours, group messaging, ‘answer sheets’, and submission of assessments, as well as other formal components."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#where-does-fsds-fit",
    "href": "lectures/1.1-Getting_Oriented.html#where-does-fsds-fit",
    "title": "Getting Oriented",
    "section": "Where Does FSDS Fit?",
    "text": "Where Does FSDS Fit?\n\n\nGeographic Information Systems (GIS)\n\nFoundations of spatial analysis\nWorking with geo-data\n\nQuantitative Methods (QM)\n\nFoundations of statistical analysis\nWorking with data\n\n\nFoundations of Spatial Data Science (FSDS)\n\nFoundations of applied spatial and statistical analysis\nIntegrating and applying concepts from GIS & QM to a problem\nDeveloping programming and practical analysis skills\nSeeing the ‘data science’ pipeline from end to end"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#what-are-we-trying-to-do",
    "href": "lectures/1.1-Getting_Oriented.html#what-are-we-trying-to-do",
    "title": "Getting Oriented",
    "section": "What Are We Trying to Do?",
    "text": "What Are We Trying to Do?\nThis class hopes to achieve four things:\n\nTo teach you the basics of how to code in Python.\nTo teach you the basics of how to think in Python.\nTo teach you how to engage with data critically.\nTo help you integrate concepts taught across Term 1 and prepare you to apply them in Term 2.\n\nThese skills are intended to be transferrable to post-degree employment or research."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#overall-structure",
    "href": "lectures/1.1-Getting_Oriented.html#overall-structure",
    "title": "Getting Oriented",
    "section": "Overall Structure",
    "text": "Overall Structure\n\nPart 1: Foundations: Weeks 1–5 to cover the ‘basics’ and set out a data science workflow.\nPart 2: Data: Weeks 6–10 look at the same data through three lenses.\nPart 3: Bonus: Weeks 11–12 additional content if you want it.\n\n\n1-5 means tackling the ‘basics’ of Python, foundational concepts in programming, and practicing with the ‘tools of the trade’ for programmers.\n6-10 means different types of data (numeric, spatial and textual) with a view to understanding how such data can be cleaned, processed, and aggregated for use in a subsequent analysis. It is commonly held that 80% of ‘data science’ involves data cleaning, so this is a critical phase in developing an understanding of data. We also look at selection and visualisation.\n11-12 is about classification, dimensionality reduction, and clustering. These concepts will have been encountered in other modules, so the intention is that the student will see how these fit into the ‘bigger picture’ of applied spatial analysis."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#week-to-week",
    "href": "lectures/1.1-Getting_Oriented.html#week-to-week",
    "title": "Getting Oriented",
    "section": "Week-to-Week",
    "text": "Week-to-Week\nThe specific activities for each week can be found on the microsite: jreades.github.io/fsds/. These include:\n\nPreparation: readings, pre-recorded lectures, quizzes/feedback.\nIn-Person: discussing readings and lectures; responding to assessment requirements; discussing issues arising from the previous week’s practical, and some ‘live coding’.\nPracticals: working through a weekly ‘programming notebook’ with support from your PGTAs.\n\n\n\n\n\n\n\nBring Your Computer\n\n\nPlease remember to bring your own computer to the practical sessions! The tools we use are not installed on cluster systems."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#assessments",
    "href": "lectures/1.1-Getting_Oriented.html#assessments",
    "title": "Getting Oriented",
    "section": "Assessments",
    "text": "Assessments\n\nTimed, Open Book Exam (30% of module grade): A quiz requiring a mix of numeric and textual answers to short data analysis questions for which you must write the code.\nGroup Report (60% of module grade; 2,500 words max): A structured, small-group submission in which students respond to set questions and develop an exploratory analysis of the assigned data set.\nSelf-Evaluation (10% of module grade): A structured individual reflection combined with numerical scoring of peers on their contribution to the group’s outcomes.\n\n\nAssessment logic:\n\nTeach and test the most challenging aspects of data science ‘work’ without mastery of Python.\nDiscover transferrability of skills and tools across projects, disciplines, and industries.\nBuild on content from QM (e.g. setting quantitative research questions) and GIS (e.g. spatial statistics).\nDevelop experience with non-academic research formats and writing.\n\nSo,\n\nIs a Moodule quiz due Friday, 22 November 2024 (after Reading Week) and it will focus on the effective use of the pandas library.\nIs a Quarto document due Tuesday, 17 December 2024 (immediately after the end of term) that combines the analysis and outputs in one document with a set of specified questions upon which randomly-selected groups will receive feedback throughout the term.\nIs a reflection and ranking exercise due Friday, 20 December 2024 (the day after the Quarto submission).\n\nWe will talk more about these over the course of the term."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#consequences",
    "href": "lectures/1.1-Getting_Oriented.html#consequences",
    "title": "Getting Oriented",
    "section": "Consequences…",
    "text": "Consequences…\nSo…\n\nIf you only code during the practical session then you will not learn how to code.\nIf you cram the night before then you will not learn how to code.\nIf you practice for 45 minutes a day then you will learn how to code.\n\nDon’t take my word for it, Prat et al. (2020) in Nature link language learning to programming language learning!\n\nThis said, we do hope to convince you that:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#actual-feedback",
    "href": "lectures/1.1-Getting_Oriented.html#actual-feedback",
    "title": "Getting Oriented",
    "section": "Actual Feedback…",
    "text": "Actual Feedback…\n\nI was really struggling with the concepts of lists, dictionaries and iterations (I basically could not do any of Practical 3 without panicking) and I was telling  that it felt like Workshop 3 was all in a foreign language - I was so lost. \n But both yesterday and today, I have been over all the content, recordings and even code camp again and I’ve just had a penny drop moment, I could cry woohooo!!!!!! \nI really appreciate all the effort you’ve put into recording the concepts ahead of lectures and the way you’ve structured the module, although it is very fast-moving you have provided all the resources for us to do well."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#more-feedback",
    "href": "lectures/1.1-Getting_Oriented.html#more-feedback",
    "title": "Getting Oriented",
    "section": "More Feedback",
    "text": "More Feedback\n\nI just wanted to update you on my progress. Since flipping the content round following your advice, I have been feeling much much better. I followed what you were doing in the workshop and also have completed the practical in about half the time than I usually do. Thanks so much for responding and for your effort with this module."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-old-challenges",
    "href": "lectures/1.1-Getting_Oriented.html#the-old-challenges",
    "title": "Getting Oriented",
    "section": "The (Old) Challenges",
    "text": "The (Old) Challenges\n\nDifferent style of learning from what you might be used to (“I didn’t anticipate, or rather factor into my schedule, the amount of out-of-hours practice that was required to stay up to date.”).\nDoing stats and programming at the same time and connecting this all back to the bigger picture.\nDelayed gratification (you have to walk before you can run).\nEasy to fall behind, but hard to catch up (“the pace is relentless”)."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-new-challenges",
    "href": "lectures/1.1-Getting_Oriented.html#the-new-challenges",
    "title": "Getting Oriented",
    "section": "The (New) Challenges",
    "text": "The (New) Challenges\n\nChatGPT (you’re going to learn when not to trust it)\nClose reading 101 (you’re going to be asked to really read)\nPublic speaking (yes, you’re going to have do a bit of this too)\n\n\nThis is a new one for us too. We don’t want to pretend that ChatGPT doesn’t exist. It’s how you will do your work. Unquestionably. But it is also a trap. This year we’re hoping to show you that."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#the-rewards",
    "href": "lectures/1.1-Getting_Oriented.html#the-rewards",
    "title": "Getting Oriented",
    "section": "The Rewards",
    "text": "The Rewards\n\nSkills that are highly transferrable and highly sought-after professionally.\nProblem-solving and practical skills that are valued by the private and public sectors.\nA whole new way of seeing the world and interacting with it.\nLots of support along the way… if you remember to ask for it!\n\nSee this thread on moving from academia to data science."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#lots-of-help-out-there",
    "href": "lectures/1.1-Getting_Oriented.html#lots-of-help-out-there",
    "title": "Getting Oriented",
    "section": "Lots of Help ‘Out There’",
    "text": "Lots of Help ‘Out There’\nYou will learn cope best if you treat this like learning a new language:\n\nStart simple and work up.\nGoogle is your friend (really).\nTalk with your friends (i.e. in-person or on Slack).\nImmerse yourself and practice regularly.\nPrint out the readings and annotate them.\nSubscribe to a ‘magazine’ or two (e.g. Medium or Pocket).\nLearn how to ask questions (i.e. Search Stack Overflow)."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#study-aids",
    "href": "lectures/1.1-Getting_Oriented.html#study-aids",
    "title": "Getting Oriented",
    "section": "Study Aids",
    "text": "Study Aids\nWhen you need an answer right now:\n\nGoogle\nStack Overflow\nSlack\n\nWhen you want to learn more:\n\nMedium\nPocket\n\n\nGoogle will become more useful as you learn more and this is definitely one class in which “I Googled it” is a good answer.\nAs of early September 2020, Stack Overflow contains over 1.5 million Python questions alone! Chances are someone else has had your question before."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#before-you-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#before-you-ask-for-help",
    "title": "Getting Oriented",
    "section": "Before You Ask for Help",
    "text": "Before You Ask for Help\nFrom the Computer Science Wiki:\n\nDraw a picture of the problem\nExplain the problem to a rubber duck, teddy bear or whatever (really!)\nForget about a computer; how would you solve this with a pencil and paper?\nThink out loud\nExplain the problem to a friend\n\nTo which we would add:\n\nUse print(variable) statements liberally in your code!\n\n\nWe’ll cover this last bit as we get more used to coding!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#where-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#where-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "Where to Ask for Help",
    "text": "Where to Ask for Help\nThere is no shame in asking for help. None. We are here to support your learning and we have chosen a range of tools to support that:\n\nSlack: use public #fsds channel for help with coding, practical, and related course questions.\nDrop-in Hours: use Booking Form\nOut-of-Hours: use email to raise personal circumstances and related issues for focussed support.\nEmergencies: contact Professional Services for support as-needed to preserve privacy.\n\n\nWe’ll talk about Slack more later, but we think that this is the best way to get help when you need it. Slack enables us to support you as a community of learners across computer / tablet / phone.\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time, as well as how to read and how to think."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#when-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#when-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "When to Ask for Help",
    "text": "When to Ask for Help\n\nWhen you get warning messages from your computer’s Operating System.\nWhen you cannot get the coding environment to run at all.\nWhen even simple commands return line after line of error code.\nWhen you have no clue what is going on or why.\nWhen you have been wrestling with a coding question for more than 20 minutes (but see: How to Ask for Help!)\n\n\nIn order to learn you do need to struggle, but only up to a point! So we don’t think that giving you the answer to a coding question as soon as you get stuck is a good way for you to learn. At the same time, I remain sad to this day that one of the most insightful students I’ve ever taught in a lecture context dropped out of our module because they were having trouble with their computer and thought it was their fault nothing was working right. By we had realised what was going on it was too late: they were so far behind that they didn’t feel able to catch up. We’d rather that you asked and we said “Close, but try it again” than you didn’t ask and checked out thinking that you couldn’t ‘do’ programming."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#how-to-ask-for-help",
    "href": "lectures/1.1-Getting_Oriented.html#how-to-ask-for-help",
    "title": "Getting Oriented",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nIn addition to what we have provided, we like the “How to ask programming questions” page provided by ProPublica:\n\nDo some research first.\nBe specific.\nRepeat.\nDocument and share.\n\nIf you find yourself wanting to ask a question on Stack Exchange then they also have a guide, and there are plenty of checklists.\n\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’."
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#learn-from-your-mistakes",
    "href": "lectures/1.1-Getting_Oriented.html#learn-from-your-mistakes",
    "title": "Getting Oriented",
    "section": "Learn from Your Mistakes",
    "text": "Learn from Your Mistakes"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#one-more-thing",
    "href": "lectures/1.1-Getting_Oriented.html#one-more-thing",
    "title": "Getting Oriented",
    "section": "One More Thing…",
    "text": "One More Thing…\nYou will get things wrong. We will get things wrong.\nWe will assume that you are trying your best. Please assume the same about us!\nIt’s going to be messy, but we’re really excited about it!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#and-finally",
    "href": "lectures/1.1-Getting_Oriented.html#and-finally",
    "title": "Getting Oriented",
    "section": "And Finally…",
    "text": "And Finally…\n\n\n\n\n\n\nAuto-Updates\n\n\nDo not allow your computer to auto-update during term. Inevitably, major upgrades will break developer tools. Do this by choice only when you have time. MacOS Sonoma is out 26 September, do not install it!\n\n\n\n\nMany students allowed their computer to update to Big Sur last year and it broke their entire computing environment. Some did this shortly before a submission was due. Do not do this!"
  },
  {
    "objectID": "lectures/1.1-Getting_Oriented.html#additional-resources",
    "href": "lectures/1.1-Getting_Oriented.html#additional-resources",
    "title": "Getting Oriented",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSee the GIS&T Body of Knowledge (BoK) for quick overview of concepts, techniques, and tools: gistbok.ucgis.org.\nA degree of ‘plagiarism’ is acceptable in code since that’s how we learn; however, mindless copy+pasting of Stack Overflow code leads to sphagetti and, often, incorrect results or difficult-to-squash bugs. Think of it like paraphrasing.\nTo distinguish between plagiarism and paraphrasing here’s a nice tutorial that you can also use to help you with your ‘regular’ writing.\n\n\n\n\n\nGetting Oriented • Jon Reades & Fulvio Lopane\n\n\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817."
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "We all need help from time to time, and while we will always do our best to support you because we know that this module is hard for students who are new to programming, the best way to ‘get help’ will also always be taking steps to ‘help yourself’ first."
  },
  {
    "objectID": "help.html#how-to-help-yourself",
    "href": "help.html#how-to-help-yourself",
    "title": "Getting Help",
    "section": "How to Help Yourself",
    "text": "How to Help Yourself\nHere are at least six things that you can do to ‘help yourself’:\n\nUse the dedicated #fsds channel on Slack–this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours.\nDo the readings–regardless of whether we ask you questions in class about them (or not), the readings are designed to support the module’s learning outcomes, so if you are struggling with a concept or an idea then please look to the week’s readings! You should also review the full bibliography while developing your thinking for the final project.\nUse Google–this is one course where saying “I googled it…” will be taken as a good sign! Probalby the biggest difference between a good programmer and a new programmer is that the good one knows which terms to type into Google to get the answer that they need right away.\nUse Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the right answer right away, but whether you’re a beginner or an expert Stack Overflow is your friend.\nMake use of Drop-in Hours (see also below)–there is no extra credit for struggling in silence, and we can’t help you if we don’t know that you’re lost! That doesn’t mean that we can simply ‘give’ you the answers to challenging questions, but we will do everything that we can to support your learning. Many of the same advice applies here as on our other Getting Help page (with the Soft Skills).\nPractice like it’s a language–set yourself little problems or tasks and see if you can apply what you’ve learned in class to a problem in a different class, or a friend’s problem, or just something you’re curious about! In the same way that practicing your Chinese or French with native speakers will help you to learn those languages, so will practicing your Python.\nSign up for online classes–realistically, you will have a lot on your plate, but if you want or need more practice with Python and are strugging to come up with your own problems to work through then there is a wealth of options out there. You might also find that a different explanation or challenge resonates and gives you new insight into how to code.\n\nRemember: when you are learning to code there is no such thing as a stupid question. Sometimes students have lazy questions when they are frustrated and just want to know ‘the answer’, but anyone finding themselves stuck on a particular problem has a 100% chance that someone else in the class has the same problem as well but hasn’t quite worked up the courage to ask. So please: ask."
  },
  {
    "objectID": "help.html#you-can-book-me",
    "href": "help.html#you-can-book-me",
    "title": "Getting Help",
    "section": "You Can Book Me",
    "text": "You Can Book Me"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html",
    "href": "extra/Live-02-Foundations_1.html",
    "title": "Context",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 2: Foundations (Part 1)&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with the 'Basics'&lt;/h2&gt;\nWe’re going to start out with the live coding and practical sessions potentially looking quite different, but that’s because I don’t think you all need to spend 60-90 minutes reviewing lists and variables. Instead, the angle in Weeks 1-3 is about getting you organised, starting to explore the data in a way that’s more immediately flexible than using code, and familiarising you with the data we’ll be using across the rest of term.\nIn the live coding sessions we will be using Crime data from 2022 for Chicago. In the practical sessions we’ll be using Inside Airbnb data for London."
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-1.-creating-and-cloning-a-repo",
    "href": "extra/Live-02-Foundations_1.html#task-1.-creating-and-cloning-a-repo",
    "title": "Context",
    "section": "Task 1. Creating and Cloning a Repo",
    "text": "Task 1. Creating and Cloning a Repo\n\nSuggest fsds as private repo\nClone to local machine\nOpen and look around."
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-2.-adding-files-commit",
    "href": "extra/Live-02-Foundations_1.html#task-2.-adding-files-commit",
    "title": "Context",
    "section": "Task 2. Adding Files & Commit",
    "text": "Task 2. Adding Files & Commit\n\ngit status\nAdd a notebook file downloaded from i2p\n\ngit add\ngit commit\n\nNow let’s look on GitHub.\n\nNow let’s update the README\n\nTry to make a change locally and push it\n\nMake a change to README\nTry to push\ngit diff\n\nMerge and resolve.\nPush again"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-3.-more-on-docker",
    "href": "extra/Live-02-Foundations_1.html#task-3.-more-on-docker",
    "title": "Context",
    "section": "Task 3. More on Docker",
    "text": "Task 3. More on Docker\nCan expand this after asking students if they have questions about how Docker works. It is available directly on the jreades.github.io/sds_env/docker/#understanding-docker page\n\nUnderstanding Docker Section\n\nStarting up\nLogging in\nTurning on extensions\nLooking at what the extensions do\n\nEspecially the toc view\nVariable inspector\nGitPlus (untested)\n\nCreating a notebook\nUsing the Terminal"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-4.-investigating-a-file",
    "href": "extra/Live-02-Foundations_1.html#task-4.-investigating-a-file",
    "title": "Context",
    "section": "Task 4. Investigating a File",
    "text": "Task 4. Investigating a File\n\nDo all of this through JupyterLab/Docker\n\n\nTask 4.1. Download the File\n\nThe full file\nThe sample file\n\nIdeally download the files to ~/Downloads/\n\n\nTask 4.2. Check Names of Files\nls ~/Downloads/*.gz\n\nExplain the ~.\nExplain the *.\nExplain the .gz.\n\n\n\nTask 4.3. How big are the files?\ndu -sh *.gz\n\nAsk them to explain the *.gz.\nWhat is du (Anyone want to guess? You can Google the answer right now!)\nHow do we figure out what du does?\n\nMany command line tools have a --help option, so you can run du --help to see the options.\nA full Unix/Linux/macOS installation also has a ‘manual’ option: man du…\n\nOk, how the hell do I navigate this?\n\narrow keys to move up and down one line at a time\nspace bar to move down one page at a time\nq to quit\n/ and then a term to search\n\n\n\n\n\n\nTask 4.4. Moving the file around\nMove the file to the cloned repo.\nmv ~/Downloads/&lt;fn&gt;.gz ~/Documents/&lt;repo&gt;/\ncd ~/Documents/&lt;repo&gt;/\nmkdir data\nmv &lt;fn&gt;.gz data/\n\nHow would we make sense of mkdir?\nWhat does -p option do?\nHow did you find it?\n\n\n\nTask 4.5. How many rows are in the data set?\n\nShow them what open . does on a Mac.\nDecompress a copy of the file (just double-click as compressed file will remain)\nOpen the copy in Excel.\nWhat are we looking at?\n\nDiscuss\nNotice any issues with the data?\n\nDate is actually datetime, and some is in 24-hour, others in 12-hour format\nWhat data types can we see? Think about what you’re learning in QM? (We’ve got floats [both ratio–meaningful zero–and interval], integers [discrete ids that are not case numbers], booleans, categorical…)\n\n\n\nNow back to the file… - How do we count things in a file? - wc - How do we count lines in a file? - Look it up! man wc\nwc -l *-Crime.csv\n\n\nTask 4.6. What are the names of the columns?\nObviously we can look in Excel, but what commands could you use to look at the first line of the file?\n\nWe’ve seen at least two ways to do this in my talk, but can you remember them?\n\nless 2022-Crime-data.csv\nand\nhead -n 1 2022-Crime-data.csv\nWith less, talk through the commands for stepping forwards, backwards, one page down, and quitting. Notice that these are the same as for the manpage help we used above with du.\n\nHow would you get the last 5 lines?\n\ntail -n 5 2022-Crime-data.csv\n\nHow would you get the 50th to 60th lines?\n\nhead -n 60 2022-Crime-data.csv | tail -n 10\n\n\nTask 4.7. How many ‘Narcotics’ charges are there?\nLead them through the fact that case matters:\n\nWhy doesn’t this work?\n\ngrep \"Narcotics\" 2022-Crime-data.csv\n\nHow could we get matches for Narcotics?\n\nOption #1: use “NARCOTICS”\nOption #2: use -i or --ignore-case\nWe get the latter by looking at man grep\n\n\ngrep -i \"Narcotics\" 2022-Crime-data.csv\nThere should be 7 rows.\n\nCan you figure out how many Narcotics charges there are in the file?\n\nYou’ve seen how to use the ‘pipe’ to pass output from head to tail.\nYou’ve seen how to find the rows that match using grep\nYou’ve seen how to count the number of lines\nYou need to put these pieces all together.\n\n\ngrep -i \"Narcotics\" 2022-Crime-data.csv | wc -l\nThere should be 7 rows.\n\n\nTask 4.8. How many charges mention values over $500?\nDemonstrate value of escape commands. Let’s have a look at the output from:\ngrep \"$500\" 2022-Crime-data.csv | wc -l\nNow let’s try:\ngrep '$500' 2022-Crime-data.csv | wc -l\nAnd finally:\ngrep \"\\$500\" 2022-Crime-data.csv | wc -l\n\n\nTask 4.9. More Complex Command Chaining\nOther fun things we can do…\n\nHow do we know what types of primary crimes there are in the data?\n\nawk -F',' '{print $4}' 2022-Crime-data.csv | sort | uniq\n\nOr how about what types of descriptions there are of Narcotics charges?\n\ngrep -i 'Narcotics' 2022-Crime-data.csv | awk -F',' '{print $5}' | sort | uniq\n\nOr how may there are of Possession?\n\ngrep -i 'Narcotics' 2022-Crime-data.csv | grep 'POSS' | wc -l"
  },
  {
    "objectID": "assessments/models.html",
    "href": "assessments/models.html",
    "title": "Model Content",
    "section": "",
    "text": "Although the following examples are all much longer than permitted under the assessment format, they are exemplary in their communication of the data and key findings in a manner that is clear, straightforward, and well-illustrated:\nThe last of these is bit more ‘academic’ in tone but still intended to be very accessible to a lay-reader (i.e. non-expert).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Model Content"
    ]
  },
  {
    "objectID": "assessments/models.html#possible-topics",
    "href": "assessments/models.html#possible-topics",
    "title": "Model Content",
    "section": "Possible Topics",
    "text": "Possible Topics\nThe exact nature of the group’s response to the final questions in the assessment is up to you, but you should reference existing policies, where relevant, and feel free to make recommendations based on the analysis undertaken.\nBelow are some indicative topics and you should feel free to approach the lecturers if you wish to strike out because some other aspect of the question and data interest you:\n\nImpact of Airbnb on local area rental markets — this would require some assumptions about listings and lettings based on available data but as long as these are clearly stated this would be a strong approach; there are good examples of models used in other cities that it may be possible to draw on, or adapt to, London. You may want to consider things like the type of listing and the issues around the Short- and Long-Term Rental markets.\nImpact of Airbnb on London’s Tourism Economy — this would look at the distribution of London’s tourism venues and, possibly, hotels alongside Airbnb listings in order to evaluate the extent to which tourism ‘dollars’ might be spent in ways that positively impact less tourist-oriented areas if we assume (again, detail the assumptions) that some percentage of a tourist’s dollars are spent locally in an area. Again, there may be models developed elsewhere that could be adapted for the London context.\nOpportunities and Risks arising from Covid-19 — it should/may be possible to assess the impact of Covid-19 on London’s short- and long-term rental markets by looking at entry to/exit from the Airbnb marketplace by comparing more than one snapshot of London data. Again, this will require some reasonable assumptions to be drawn (are all flats withdrawn from Airbnb going back on to the Long-Term Rental Sector?) but these can be documented and justified.\nOpportunities for Place- or Listing-Branding — identifying key terms and features/amenities used to market listings by area and using these to identify opportunities for investment or branding. This would benefit from the use of NLP approaches and, potentially, word embeddings to identify distinctive patterns of word use as well as, potentially, One-Hot encoding to identify specific amenities that appear associated in some way with particular areas.\nThe Challenge of Ghost Hotels — evaluating ways to automatically identify ghost hotels from the InsideAirbnb data and then, potentially, assessing their extent and impact on local areas where they dominate either ‘proper’ hotel provision or other types of listings. You will need to consider the way that Airbnb randomly shuffles listings to prevent exactly this type of application and textual similarity via NLP is an obvious application.\nThe Professionalisation of Airbnb — this could be treated either as a regulatory challenge (is Airbnb not benefiting locals) or an investment opportunity (is this a way to ‘scale’ or develop new service offers for small hosts) depending on your interests. You will need to consider the different types of hosts and evaluate ways of distinguishing between them (e.g. number of listings, spatial extent, etc.).\nImpact Profiles — a geodemographic classification of London neighbourhoods based on how they have, or have not, been impacted by Airbnb. This would require you to think about how to develop a classification/clustering of London neighbourhoods and use data to develop ‘pen portraits’ of each so that policy-makers could better-understand the range of environments in which Airbnb operates and why a 1-size-fits-all regulatory approach may be insufficient. Again, this could be argued from either standpoint or even both simultaneously: these areas are already so heavily impacted that regulation is too little, too late, while these other areas are ‘at risk’.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Model Content"
    ]
  },
  {
    "objectID": "assessments/models.html#partial-bibliography",
    "href": "assessments/models.html#partial-bibliography",
    "title": "Model Content",
    "section": "Partial Bibliography",
    "text": "Partial Bibliography\nYou will also want to review the partial bibliography available here; this is by no means complete and you will likely find other relevant work ‘out there’ but you may find it useful for spurring your thinking on what to study and how to study it. You might also want to have a look at guidance for London:\n\nKeyNest (2019), Understanding Airbnb regulations in London, KeyNest; URL\nAirbnb (n.d.), I rent out my home in London. What short-term rental laws apply?, Airbnb; URL\nHostmaker (2018), Important Airbnb regulations and laws you should know about in London, Hostmaker; URL Now Houst.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Model Content"
    ]
  },
  {
    "objectID": "assessments/group.html",
    "href": "assessments/group.html",
    "title": "Rationale",
    "section": "",
    "text": "This is a collaborative project (worth 60%) due Tuesday, 17 December 2024 @ 18:00 that you will undertake in a small group of no more than five students. The project is intended to resemble real-world data science ways of working: you will be part of a small team, you will need to figure out how to work effectively together, you will need to jointly produce an output in which you all have confidence. You will be submitting a reproducible analysis (written for Quarto+Python) that we will be able to run on our own computers in order to generate a PDF output.\nThe focus of this assessment is the student’s ability to make use of concepts and methods covered in class as part of an analytical process to support decision-making in a non-academic context. It is not necessary that you employ every technique covered in class. It is necessary that you justify your choice of approach with reference to relevant academic and ‘grey’ literature, as well as the computational, statistical, and analytical objectives of your submission. It is perfectly possible to complete this assessment without the use of advanced analytical topics (e.g. clustering, NLP, or global/local/LISA autocorrelation methods); however, it is unlikely that you would be able to complete this assessment to a high standard without some graphs and some maps chosen for their ability to advance your argument.\nThe assessment may be completed without substantially new modelling or coding by drawing on the code written in practicals to develop an analysis based on the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature), but it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was taught by selectively deploying more advanced programming techniques.\nThe reproducible analysis must be a runnable QMD (Quarto Markdown Document) file that addresses the set questions provided in class. The QMD file will be assessed on two components:",
    "crumbs": [
      "Elements",
      "Group Work",
      "Rationale"
    ]
  },
  {
    "objectID": "assessments/group.html#supporting-documents",
    "href": "assessments/group.html#supporting-documents",
    "title": "Rationale",
    "section": "Supporting Documents",
    "text": "Supporting Documents\n\nA template should be used, though you are free to modify this template as needed. You can see both PDF and HTML output, but please only submit the PDF!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Rationale"
    ]
  },
  {
    "objectID": "assessments/code.html",
    "href": "assessments/code.html",
    "title": "Reproducible Analysis",
    "section": "",
    "text": "The Reproducible Analysis is worth 40% of the Group Assessment.\nThe Reproducible Analysis must be written using Python in a Quarto Markdown Document (QMD file). You are free to draw on concepts and methods covered in both Quantitative Methods and GIS, but must still write the code in Python (e.g. adapting something from R in the GIS module to Python).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "assessments/code.html#format",
    "href": "assessments/code.html#format",
    "title": "Reproducible Analysis",
    "section": "Format",
    "text": "Format\nYou will be submitting a runnable markdown document (.qmd file) that includes: your group’s name and all student ids. We will supply a template closer to the deadline.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "assessments/code.html#how-we-measure-reproducibility",
    "href": "assessments/code.html#how-we-measure-reproducibility",
    "title": "Reproducible Analysis",
    "section": "How We Measure Reproducibility",
    "text": "How We Measure Reproducibility\nWe will assess reproducibility by selecting “Restart Kernel and Run All” using the jreades/sds:2024 Docker environment. If you have made use of another Docker image then you must clearly signpost this at the start of your notebook so that we know to select a different image. We will not install libraries ‘by-hand’ in an ad hoc manner order to test the reproducibility of your work.\n\n\n\n\n\n\nReproducibility\n\n\n\nTo ensure reproducibility, markers must be able to run quarto render &lt;your file.qmd&gt; and reproduce your entire analysis. This includes downloading and extracting data (we will provide a function for the main data set), cleaning, transformation, clustering… charts, tables, etc.\n\n\n\n\n\n\n\n\nFor Large Workflows\n\n\n\nIf you need to provide supplementary or partially-processed data (see section below) then you can provide this via Dropbox, OneDrive (public sharing link), or some other robust cloud solution that will be accessible from the marker’s system.\n\n\nIf you have made use of one or more libraries that are not part of the Docker image then you can install these using ! pip install; however, if you take this approach then you should also ‘place nice’ by checking first to see if the library is already installed using try... except code that you can find on Stack Overflow and elsewhere (you will need to look this up).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "assessments/code.html#data-and-resources-used",
    "href": "assessments/code.html#data-and-resources-used",
    "title": "Reproducible Analysis",
    "section": "Data and Resources Used",
    "text": "Data and Resources Used\nIt is also up to you to ensure that all relevant data are available via a valid URL for downloading and running. You may host your data anywhere you like, but please bear in mind that the markers will be based in the U.K. so some servers may be inaccessible. For very small data sets we’d recommend a GitHub repo, but for larger ones a Dropbox or OneDrive link would be more appropriate (you will need to check that the link you’ve created gives permission to anyone to download).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "assessments/code.html#time-consuming-code",
    "href": "assessments/code.html#time-consuming-code",
    "title": "Reproducible Analysis",
    "section": "Time-Consuming Code",
    "text": "Time-Consuming Code\nIf your analysis has a particularly time-consuming stage (e.g. Named-Entity Recognition or Part-of-Speech tagging) then you can provide partially-processed data: comment out the code up to the point where you have generated the ‘expensive’ data set but leave it in the markdown document. That way we can see how you generated the data without it being part of the reproducibility stage.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "assessments/code.html#other-requirements",
    "href": "assessments/code.html#other-requirements",
    "title": "Reproducible Analysis",
    "section": "Other Requirements",
    "text": "Other Requirements\nYou must maintain a copy of the submission in GitHub so that we can review contributions if necessary.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "assessments/code.html#supporting-documents",
    "href": "assessments/code.html#supporting-documents",
    "title": "Reproducible Analysis",
    "section": "Supporting Documents",
    "text": "Supporting Documents\n\nA template shou ld be used, though you are free to modify this template as needed. You can see both PDF and HTML output, but please only submit the PDF!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Reproducible Analysis"
    ]
  },
  {
    "objectID": "sessions/to_dos.html",
    "href": "sessions/to_dos.html",
    "title": "Foundations of Spatial Data Science",
    "section": "",
    "text": "jupyter: python3 project: format: html: theme: - minty - css/web.scss code-copy: true code-link: true #cover-image: cover.png toc: true toc-title: “On this page” toc-depth: 2 toc_float: collapsed: false smooth_scroll: true"
  },
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "In order to get you started on your data science ‘journey’ you will need to follow the guidance provided on the pages we’ve linked to below from the CASA Computing Environment web site.\n\n\nBefore trying to do anything else please complete the basic health check, which also includes our recommendations if you are considering buying a new computer when you start your studies. Once you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment. We also provide information about Code Camp which is a self-paced introduction to the fundamentals of programming in Python.\n\n\n\nOnce you’ve ticked off the Requirements, you can start installing the tools that you will use to write and run both code and documentation. You will find it simplest to follow the Install ‘Fest’ Guidance.\n\n\n\n\n\n\nWhen Installation Fails\n\n\n\nIn an emergency, some of the earlier material in the module will work with the ‘no install’ option or Anaconda Python, but this will make your life significantly harder. We are unable to provide support for these.\n\n\n\n\n\nOver the years, based on student experience and feedback we have collected a range of advice that is not purely technical in nature. This section covers managing distractions, an introduction to how to read (in the academic sense of reading journal articles and books for meaning and relevance) as well as how to think (in the sense of why the modules are the way they are and the importance of reflection), and how to ask for help (because that’s what we’re to do, but first you need to help yourself!)."
  },
  {
    "objectID": "setup/index.html#sec-requirements",
    "href": "setup/index.html#sec-requirements",
    "title": "Getting Started",
    "section": "",
    "text": "Before trying to do anything else please complete the basic health check, which also includes our recommendations if you are considering buying a new computer when you start your studies. Once you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment. We also provide information about Code Camp which is a self-paced introduction to the fundamentals of programming in Python."
  },
  {
    "objectID": "setup/index.html#setup",
    "href": "setup/index.html#setup",
    "title": "Getting Started",
    "section": "",
    "text": "Once you’ve ticked off the Requirements, you can start installing the tools that you will use to write and run both code and documentation. You will find it simplest to follow the Install ‘Fest’ Guidance.\n\n\n\n\n\n\nWhen Installation Fails\n\n\n\nIn an emergency, some of the earlier material in the module will work with the ‘no install’ option or Anaconda Python, but this will make your life significantly harder. We are unable to provide support for these."
  },
  {
    "objectID": "setup/index.html#soft-skills",
    "href": "setup/index.html#soft-skills",
    "title": "Getting Started",
    "section": "",
    "text": "Over the years, based on student experience and feedback we have collected a range of advice that is not purely technical in nature. This section covers managing distractions, an introduction to how to read (in the academic sense of reading journal articles and books for meaning and relevance) as well as how to think (in the sense of why the modules are the way they are and the importance of reflection), and how to ask for help (because that’s what we’re to do, but first you need to help yourself!)."
  },
  {
    "objectID": "assessments/briefing.html",
    "href": "assessments/briefing.html",
    "title": "Written Content",
    "section": "",
    "text": "The Content is worth 60% of the Group Assessment. You will be writing for a non-technical audience: imagine that you are writing an outline proposal to undertake a piece of research for the Mayor of London.\nConsequently, through set questions the Content will establish the suitability of data taken from the Inside Airbnb web site for London ifor policy-making and, specifically, for the regulation of Short-Term Lets (STL) in London.\nThe responses to the set questions may be written without substantially new modelling or coding through the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature), but it is likely that a better mark will be obtained by demonstrating the capacity to go beyond exactly what was taught in class.\nStudents may use data from more than one time period if they wish, but this is not required.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Written Content"
    ]
  },
  {
    "objectID": "assessments/briefing.html#format",
    "href": "assessments/briefing.html#format",
    "title": "Written Content",
    "section": "Format",
    "text": "Format\nThis is not an essay, and students who submit a traditional essay format will see their overall mark impacted as a result. You must preserve the question/response format, and your responses should stress intelligibility to an intelligent, but non-technical audience. This doesn’t mean that you don’t need citations, but you should not employ academic writing styles.\nExamples of the kind of tone and presentation format expected in response to the set questions are provided below and there will be opportunities to discuss the submission during Term. You should refer to the models provided for insight into how to write for the target audience.\n\n\n\n\n\n\nWrite for Your Audience\n\n\n\nWhat makes writing a good briefing hard—and not just about writing good code—is finding the right balance of technical detail and high-level explanation: you can’t just say ‘here are the five types of accommodation we found…’, but you also can’t say ‘we tested clustering solutions in the range 3–50 and found the optimal result at k=12…’ You should have a look at the examples.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Written Content"
    ]
  },
  {
    "objectID": "assessments/briefing.html#word-count",
    "href": "assessments/briefing.html#word-count",
    "title": "Written Content",
    "section": "Word Count",
    "text": "Word Count\nThe word limit for this assessment is 2,500 words.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Written Content"
    ]
  },
  {
    "objectID": "assessments/briefing.html#preparing-your-submission",
    "href": "assessments/briefing.html#preparing-your-submission",
    "title": "Written Content",
    "section": "Preparing Your Submission",
    "text": "Preparing Your Submission\nYou are expected to use sustainable authorship tools for this submission. You may be asked to provide evidence of this. A template will be provided, and you should also look at the Quarto Guide and, in particular, the PDF options in order to customise your project.\nYou must develop and maintain the submission in GitHub so that we can review contributions if necessary.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Written Content"
    ]
  },
  {
    "objectID": "assessments/briefing.html#supporting-documents",
    "href": "assessments/briefing.html#supporting-documents",
    "title": "Written Content",
    "section": "Supporting Documents",
    "text": "Supporting Documents\n\nA template shou ld be used, though you are free to modify this template as needed. You can see both PDF and HTML output, but please only submit the PDF!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Written Content"
    ]
  },
  {
    "objectID": "assessments/conflicts.html",
    "href": "assessments/conflicts.html",
    "title": "Dealing with Conflict",
    "section": "",
    "text": "Working on a group project through Git and GitHub will invetiably result in version control conflicts: two (or more) students will have made changes that need to be reconciled using Git’s conflict resolution mechanisms. Depending on the nature of the conflict, these can be trivial or very, very hard to resolve. If you think about it, the obvious conflicts will broadly fall into three classes:\nMaking this all a lot harder is that a ‘change’ could be anything from a direct edit to a section of a line, to deleting a file or folder! There’s a lot more to this, and there are undoubtedly lots of good examples of about (e.g. Example 1, Example 2).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#spotting-a-conflict",
    "href": "assessments/conflicts.html#spotting-a-conflict",
    "title": "Dealing with Conflict",
    "section": "Spotting a Conflict",
    "text": "Spotting a Conflict\nYou will most likely discover a conflict when pushing local changes or pulling remote changes. So let’s imagine that you’ve done something like this:\ngit add [file]\ngit commit -m \"[your message]\"\ngit push\nYou then see the message:\nfailed to push some refs to [url].\nUpdates were rejected because the remote contains work that you do not have locally.  \nSince you can’t push, what do you do?",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-1-hope-for-the-best",
    "href": "assessments/conflicts.html#option-1-hope-for-the-best",
    "title": "Dealing with Conflict",
    "section": "Option 1: Hope for the Best",
    "text": "Option 1: Hope for the Best\nIf you’re lucky then some conflicts can be resolved simply with a git pull. This works when there are remote changes that have zero impact on your local changes. Pulling the remote changes will update your local copy and then you can just turn around and push your local changes back to the remote (usually: GitHub).\nWhat do you do if that doesn’t work?",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-2-resolve-conflicts-locally",
    "href": "assessments/conflicts.html#option-2-resolve-conflicts-locally",
    "title": "Dealing with Conflict",
    "section": "Option 2: Resolve Conflicts Locally",
    "text": "Option 2: Resolve Conflicts Locally\nFetch the latest changes from the remote repository:\ngit fetch origin\nMerge the remote version of the repository (that you just fetched) with the local version:\ngit merge origin/main (or git merge origin/master)\n\n\n\n\n\n\nWhat Branch?\n\n\n\nThe command above assumes that you are working in the main branch, which is the most likely branch unless your group is very, very advanced. But how do you know what branch you’re working on?\nRunning git branch --show-current will give you the currently active branch. The other way (git branch) shows you a list of local branches, the one with a * next to it is the one you’re currently using. To exit the list of branches just hit the letter q for ‘Quit’.\n\n\nYou will likely then see a message similar to:\nCONFLICT (content): Merge conflict in your-file.md\n\nFormat of a Conflict\nWhen you have a conflict, the format of a conflict is (roughly) as follows2:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:file.txt\nHello world\n=======\nGoodbye\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 77976da35a11db4580b80ae27e8d65caf5208086:file.txt\nThe first section (from &lt;&lt;&lt;&lt; HEAD to ====) is what you have in your local file. The second section (from ==== to &gt;&gt;&gt;&gt; &lt;hexadecimal number&gt;) is the change that is coming from the remote repository that Git wants to merge on to your local file but can’t because of the changes that you’ve made locally.\nOpen the conflicting files and manually edit them. You will see annotations like this indicating where there are conflicts:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; origin/main\nChange #1\n=======\nChange #2\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\nResolve the conflicts and remove the annotations (the &lt;&lt;&lt;..., ===... and &gt;&gt;&gt;... lines):\nChange #1\nChange #2\nCommit your changes and push the merged results back to the remote repository.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-3-open-a-pull-request",
    "href": "assessments/conflicts.html#option-3-open-a-pull-request",
    "title": "Dealing with Conflict",
    "section": "Option 3: Open a Pull Request",
    "text": "Option 3: Open a Pull Request\nTo resolve, create a new branch:\ngit checkout -b new-branch-name\nCheck that you are in the new branch:\ngit branch -v\nCheck that your local changes are committed on to the new branch and then push the new branch to the remote repository:\ngit push --set-upstream origin new-branch-name\nYou then create a Pull Request on Github (you will see a green button pop up “Compare & Pull Request”):\n\nGit will say “Can’t automatically merge”. That’s OK. Click “Create pull request”.\nGit will “Check for ability to merge automatically” and will then say:\n\n“This branch has conflicts that must be resolved”\nClick “Resolve conflicts”\nGit will show you text like this that shows the conflicting changes in each branch and the resolution process is the same as above:\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; new-branch-name\nYet another one\n=======\nAnother one\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n\nNow that you’ve resolved the conflict, click “Mark as resolved” and “Commit merge”.\nThen, “Merge pull request” and “Confirm merge”.\n\nWhen you go back to the main page of your repository, you will see the new changes.\nOne more thing! On your computer, make sure to exit your branch and return to the main (or master) branch.\ngit checkout main\nThen, pull your latest changes from the Github repository.\ngit pull\nCongratulations (🎉), you have successfully resolved a merge conflict!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#footnotes",
    "href": "assessments/conflicts.html#footnotes",
    "title": "Dealing with Conflict",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example: you’ve deleted lines 6-9 and changed line 10, the other person has modified lines 17-18 of the same file, so Git needs to determine that 17-18 of the other change maps on to 14-15 of your change.↩︎\nTaken from: https://stackoverflow.com/a/7901901↩︎",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "Context",
    "section": "",
    "text": "The overall assessment package is intended to test students’ comprehension of, and ability to integrate, technical skills with a broader understanding of, and reflection upon, computational approaches to urban research and spatial data science.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#assessment-elements",
    "href": "assessments/index.html#assessment-elements",
    "title": "Context",
    "section": "Assessment Elements",
    "text": "Assessment Elements\nThe assessments are grounded in a mixture of critical reflection and group work that map on to real-world data science challenges, including:\n\nTimed Open Book Examination (30%) to ensure that you are acquiring the fundamentals of data analysis in Python, and simulate the real-world challenge of having to write code to a deadline (to be completed between 9-11am or 11-1pm on Friday, 22 November 2024);\nCollaboratively evaluating and analysing a data set (60%) as a group you will be determining the suitability of a data set for tackling an analytical ‘problem’ using a mix of coding, analysis, and presentation skills in a reproducible format Tuesday, 17 December 2024 @ 18:00);\nReflecting on the process (10%) to better-understand why a project succeeded/failed so as to improve future outcomes and recognise the contributions of individual members of the group to the success of the project (due Friday, 20 December 2024 @ 18:00).",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#rationale",
    "href": "assessments/index.html#rationale",
    "title": "Context",
    "section": "Rationale",
    "text": "Rationale\nCollectively, these assessments seek to provide multiple opportunities to ‘shine’ both individually and as part of a group. You do not need to be the best programmer in the class in order to do well on these assessments. Indeed, focussing only on the programming is likely to result in a low mark because it missed the context in which data science and data analysis ‘work’. As a budding data scientist/analyst your job is just as much to understand your audience and their needs: you will work with clients who can’t really articulate what they want or why, so good project management often involves putting yourself in your client’s shoes and working out how to translate what they say they want into what they actually need.\nYou will therefore do poorly on the assessments if you do not do the readings, watch the pre-recorded lectures, or participate in discussions (both online and in-person during practicals and classes). These provide you with context for the work that is being done when you start typing and running code in a Jupyter Notebook. Code is the how. Context is the why.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/restrictions.html",
    "href": "assessments/restrictions.html",
    "title": "Additional Guidance",
    "section": "",
    "text": "Unless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used must be generated by Python code cells included in the markdown file. You may not modify or create figures in another application since this undermines the reproducibility of the analysis.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "assessments/restrictions.html#figures-tables",
    "href": "assessments/restrictions.html#figures-tables",
    "title": "Additional Guidance",
    "section": "",
    "text": "Unless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used must be generated by Python code cells included in the markdown file. You may not modify or create figures in another application since this undermines the reproducibility of the analysis.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "assessments/restrictions.html#word-counts",
    "href": "assessments/restrictions.html#word-counts",
    "title": "Additional Guidance",
    "section": "Word Counts",
    "text": "Word Counts\nEach figure or table counts for 150 words, and so students should give careful consideration to the trade-offs involved: more figures may serve to illustrate your points but leave you with much less space to synthesise and present and argument.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "assessments/restrictions.html#ab-figures",
    "href": "assessments/restrictions.html#ab-figures",
    "title": "Additional Guidance",
    "section": "A/B Figures",
    "text": "A/B Figures\nA figure with A/B/C elements will count as one figure, but only where the parts parts are conceptually related (e.g. before/after; non-spatial/spatial distribution; type 1 and type 2; etc.). The output from PySAL’s LISA analysis library, for instance, is pre-formatted as 3 figures. Seaborn’s jointplot will only be considered to be one plot even though it is technically three because the distribution plots in the margin are related to the scatter plot that is the focus of the plot.\nIn principle, a briefing with 16 figures would have no space for any text or interpretation; this choice is deliberate because its purpose is to focus your attention on which charts and tables best-communicate your findings. In practice, using A/B/C figure layouts then you are looking at up to 48 separate figures before hitting the limit, though you would at this point be producing an infographic and not a briefing.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "assessments/restrictions.html#referencing",
    "href": "assessments/restrictions.html#referencing",
    "title": "Additional Guidance",
    "section": "Referencing",
    "text": "Referencing\nYou will need to reference various documents as part of this submission. It is possible to write these ‘by hand’, but we will award higher reproducibility marks to submissions making use of BibTeX and Markdown referencing in Quarto. The Docker image that we have provided has Quarto pre-installed and this will simplify generation of the output because the Python kernel (which will execute your code) has all of the libraries you’ve been using available.\n\n\n\n\n\n\nReferencing in Quarto\n\n\n\nIf you want to make use of BibTeX to automate referencing You’ll need BibDesk (Mac) or JabRef (Mac/Windows) or similar (Zotero shuould also work) to edit the BibTeX file (outside of Docker). You would ordinarily add/edit/remove works outside of Docker, but by placing your BibTeX file in the same folder as your Quarto document and putting all of that in Git/GitHub you will enable the built-in version to generate the references for you and ensure that your Quarto document can download the file when we run the QMD on our side.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "assessments/restrictions.html#the-templates",
    "href": "assessments/restrictions.html#the-templates",
    "title": "Additional Guidance",
    "section": "The Template(s)",
    "text": "The Template(s)\nTo get you started, we’ve created a set of templates that you’ll want to place together in your Git/GitHub repo:\n\nThe template file (.qmd) that we have provided: Group_Work.qmd.\nThe CSL file (.csl) that governs how references are done: harvard-cite-them-right.csl.\nThe BibTeX file (.bib) that will contain any references you use: bio.bib (you may also make use of the one used for the module to simplify the creation/management of your bibliography)\n\n\n\n\n\n\n\nReally Look at the Template!\n\n\n\nYou’ll notice that the template specifies three fonts (mainfont, monofont, sansfont). These are installed in Docker, and I’ve added links in the template to show how you can download them for your own computer.\nYou are welcome to change the fonts used and can see what’s available in the existing Docker image by asking matplotlib or using the Terminal… which will help you to learn about font-management.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "assessments/restrictions.html#rendering",
    "href": "assessments/restrictions.html#rendering",
    "title": "Additional Guidance",
    "section": "Rendering",
    "text": "Rendering\nThese three files together will enable you to ‘render’ the Quarto Markdown Document as a PDF for submission. To do this, ensure that all three files above are in the same folder (e.g. ~/git/fsds-group-project/), in which case you’d see something like:\ncd ~/&lt; your local copy of the repo &gt;\nls . # Should show a .qmd file, a .csl file, and a .bib file\nquarto render Group_&lt;simple_version_of_your_group_name&gt;_Submission.qmd\nThis should render the PDF with a look something like this example output.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Additional Guidance"
    ]
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#check-in",
    "href": "extra/Live-03-Foundations_2_Intro.html#check-in",
    "title": "Foundations of Spatial Data Science",
    "section": "1. Check In",
    "text": "1. Check In\n\nSEATS QR Code:"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#organise-your-group",
    "href": "extra/Live-03-Foundations_2_Intro.html#organise-your-group",
    "title": "Foundations of Spatial Data Science",
    "section": "2. Organise (Your Group)",
    "text": "2. Organise (Your Group)\n\nGroup Sign-upforms.office.com/e/2ij0sWHnpR"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#comment-on-progress",
    "href": "extra/Live-03-Foundations_2_Intro.html#comment-on-progress",
    "title": "Foundations of Spatial Data Science",
    "section": "2. Comment (On Progress)",
    "text": "2. Comment (On Progress)\n\nwww.menti.com/al8qfmocpxkg"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#on-with-the-show",
    "href": "extra/Live-03-Foundations_2_Intro.html#on-with-the-show",
    "title": "Foundations of Spatial Data Science",
    "section": "3. On with the show…",
    "text": "3. On with the show…\n\nDon’t forget to start Lecture Capture!\n\n\n\n\n\nJon Reades"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Jon Reades1\nThe Foundations of Spatial Data Science (FSDS) module is an optional element of CASA’s MSc programmes and is intended provide an introduction to doing data science in Python for students who are new to programming or whose previous exposure to coding is fairly limited. The module seeks to enable students to access, understand, and communicate data in a spatial context. FSDS is not about pushing buttons, but about using logic, programming, and your growing analytical skills to tackle real-world problems in a creative, reproducible, and open manner.\nFSDS is not easy: in order to make the most of the module—and the foundation that it provides both for Term 2 modules on the MSc and for post-Masters employment—you will need to work hard. This does not mean cramming before each practical, it means practicing between practicals, doing the readings, and really watching the videos. UCL expectations for a Masters-level module is 150 hours of study time: there are 4 hours of timetabled activity per week, and about an hour of videos to watch before each workshop, leaving up to 100 hours of ‘self-study’. By implication, you should expect to spend about 1.25 hours/day studying for this module: reading, coding, and (above all) practicing.\nIn exchange for your hard work, there is a pressing need for analysts, planners, and geographers able to think computationally using programming, analytics, and data manipulation skills that are anchored in the needs of policy-makers, businesses, and non-profits. There is a severe skills shortage in this domain across all sectors and, consequently, significant opportunity for those who can ‘make sense’ of data+code."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWhile this module in indebted to both feedback from students and colleagues over the years, several people played a particularly outsize role in my thinking and deserve special acknowledgement:\n\nDani for help with Docker, geopandas, and any number of other new tools with which I’ve had to familiarise myself.\nAndy for somehow knowing about all kinds of new web apps that I could use to support the module.\nThe Geocomp team at King’s College London, who supported my hare-brained scheme to teach Geography undergrads to code and offered all manner of useful feedback on what we could/could not feasibly cover."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Bartlett Centre for Advanced Spatial Analysis↩︎"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#principles",
    "href": "lectures/1.2-Tools_of_the_Trade.html#principles",
    "title": "Tools of the Trade",
    "section": "Principles",
    "text": "Principles\n\nSoftware should be free (as far as practicable).\nSoftware should be open (as far as practicable).\nSoftware should run on all platforms.\nSoftware should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#tools-to-make-your-life-easier",
    "href": "lectures/1.2-Tools_of_the_Trade.html#tools-to-make-your-life-easier",
    "title": "Tools of the Trade",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nOneDrive (or Dropbox): keep your stuff backed up in the cloud.\nSlack: get help (or just tips and tricks) from peers and staff\nDocker: virtualisation platforms to ensure you don’t ‘hose’ your computer.\nPython: how we do ‘data science’.\nGitHub: manage your code, your data, even your essays/reports.\nMarkdown: focus on the right things while you write and treat your essays like code!\nQuarto: convert Markdown+Python to pretty documents/web pages.\nLLMs: assistance in sketching out code snippets/validation."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#onedrivedropbox",
    "href": "lectures/1.2-Tools_of_the_Trade.html#onedrivedropbox",
    "title": "Tools of the Trade",
    "section": "OneDrive/Dropbox",
    "text": "OneDrive/Dropbox\n\n\n\n\n\n\nOneDrive and Dropbox are ‘cloud-based file synchronisation tools’: files placed in the special folder are automatically uploaded to servers, and automatically downloaded to any other computer on which you have set up the service. Changes are also synchronised ‘every time’ you save the file."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use OneDrive and/or Dropbox for four reasons:\n\nYou can access your files anywhere in the world via the Desktop or Web.\nYou have an backup of all of your work, even if your computer has a complete meltdown.\nYou have limited ‘versioning’ support, so if you accidentally overwrite an essay or file, you can recover a previous version.\nThis is how we collaborate, and it’s how many businesses work as well."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nThat you keep all files that aren’t in GitHub in your Dropbox or OneDrive folder. This applies to all your CASA MSc work but could be especially useful for ensuring that data files used as part of your group work are readily accessible!"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#slack",
    "href": "lectures/1.2-Tools_of_the_Trade.html#slack",
    "title": "Tools of the Trade",
    "section": "Slack",
    "text": "Slack\n\nSlack is a “messaging app for teams” that is designed to reduce email, organise conversations & topics of discussion, and pull in relevant data from a host of other services in a flexible, fully-searchable way."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-1",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-1",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use Slack for four reasons:\n\nMoodle is clunky and formal—it works well for one-to-many communication, but not so much for ‘chat’.\nSlack offers a searchable history1—you will have access to this archive for as long as you need it.\nYou (and we) can access Slack on every major OS (OSX, Windows, iOS, Android, and Windows Phone) and via a browser quickly.\nSlack is used in the ‘real world’ by everyone from Apple to PayPal and the JPL. This is how developers work.\n\nUp to a point, we don’t pay for the permanent history."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-1",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-1",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nInstall the Slack client on your phone and on your personal computer and start using it as the way to ask questions, share answers, and generally keep ‘up to date’ on things across the entire MSc.\nWorkspace: https://casa-students-2024-25.slack.com\nP.S. Unless a question is personal it should normally be asked in the appropriate module channel."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#docker",
    "href": "lectures/1.2-Tools_of_the_Trade.html#docker",
    "title": "Tools of the Trade",
    "section": "Docker",
    "text": "Docker\n\nDocker “makes development efficient and predictable” because it is “used through the development lifecycle for fast, easy and portable application development”."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-2",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-2",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nDocker is a ‘virtualisation platform’ that allows you to run a second (virtual) computer on your personal computer. We use it for four reasons:\n\nEasier installation than Anaconda Python and everyone has the same versions of every library.\nNo spillover effects since each container is isolated.\nEasy to tidy up when you’re done or add new containers when you start something new (e.g. PostgreSQL).\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-2",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-2",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Docker because configuring a development machine is hard, this makes it simple. If a Docker image works for us then we know 1 it works for you.\nDocker Desktop with either:\n\njreades/sds:2024-intel (Windows and Older Macs)\njreades/sds:2024-silicon (Newer Macs)\n\nNot always true, alas."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#anaconda-python",
    "href": "lectures/1.2-Tools_of_the_Trade.html#anaconda-python",
    "title": "Tools of the Trade",
    "section": "Anaconda Python",
    "text": "Anaconda Python\n\nAnaconda Python is a ‘flavour’ of Python that comes packaged with useful tools for configuring and management. If virtualisation is too resource-intensive for your computer (e.g. because it’s older or doesn’t have enough RAM) then installing Python directly is the next-best option."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-3",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-3",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe use Anaconda Python for three reasons:\n\nIt is easy to create and configure virtual environments (each research project has its own environment).\nUse of channels allows installation of cutting-edge libraries not yet packaged for ‘regular’ Python (install from GitHub, etc.)\nWidely supported by developers with builds for most Operating Systems and a focus on data science applications.\n\nIt’s what we use on the Docker image as well."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-3",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-3",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nIf Docker doesn’t work on your computer, then this is how we will get you up and running because it’s (fairly) robust and ‘standard issue’. However, we can’t guarantee you’ll get the same versions of every package as installed on the virtualised systems so differences may emerge.\nYou’ll need to download the ‘Individual Edition’ installer1\nUnless your computer is very, very old."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#large-language-models-llms",
    "href": "lectures/1.2-Tools_of_the_Trade.html#large-language-models-llms",
    "title": "Tools of the Trade",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\n\nChatGPT from OpenAI (an increasingly ‘ironic’ name) is simply the most famous of a growing number of Large Language Models that draw on information found on the web and in open texts to perform sophisticated summarisation tasks."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-4",
    "href": "lectures/1.2-Tools_of_the_Trade.html#why-use-it-4",
    "title": "Tools of the Trade",
    "section": "Why Use It?",
    "text": "Why Use It?\nMany programmers are starting to use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#danger-will-robinson",
    "href": "lectures/1.2-Tools_of_the_Trade.html#danger-will-robinson",
    "title": "Tools of the Trade",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nHere’s what we’ve noticed about ChatGPT’s output so far:\n\nOver-use of flowery language (e.g. ‘intricate methodologies’, ‘exhaustive reviews’, …).\nLack of overall structure/coherence (i.e. individual paragraphs seem reasonable but the whole doesn’t ‘work’ as a single output).\nPoor referencing, including the use of non-existent or irrelevant references (i.e. papers that don’t actually exist, or that seem superficially useful but are not relevant in practice).\nThe more specific the information, the less likely it is to be correct (e.g. Who is Jon Reades? vs. What are the main areas where NLP is useful in built environment research?)"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#over-confidence-in-action",
    "href": "lectures/1.2-Tools_of_the_Trade.html#over-confidence-in-action",
    "title": "Tools of the Trade",
    "section": "Over-Confidence in Action",
    "text": "Over-Confidence in Action\nWho is Jon Reades?1\n\nJon Reades is a Senior Lecturer in the Department of Geography at University College London (UCL). He is also the director of the MSc in Smart Cities and Urban Analytics program at UCL.\nReades’ research focuses on the intersection of cities, data, and technology, with a particular emphasis on the use of spatial data and analytics to understand urban processes and support decision-making in cities. He has published widely on topics such as spatial analysis, urban data science, and the use of smart technologies in urban planning and governance.\nIn addition to his academic work, Reades is actively involved in a number of professional organizations and initiatives related to urban analytics and smart cities. He is a co-founder of the Urban Dynamics Institute at UCL and has served on the editorial board of several academic journals in the fields of geography and urban studies.\n\nAnd, by the way, there’s only one of me!"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#read-the-fine-print",
    "href": "lectures/1.2-Tools_of_the_Trade.html#read-the-fine-print",
    "title": "Tools of the Trade",
    "section": "Read the Fine Print",
    "text": "Read the Fine Print"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-4",
    "href": "lectures/1.2-Tools_of_the_Trade.html#we-recommend-4",
    "title": "Tools of the Trade",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose of the code.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write."
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#recap",
    "href": "lectures/1.2-Tools_of_the_Trade.html#recap",
    "title": "Tools of the Trade",
    "section": "Recap",
    "text": "Recap\n\nWith Docker we have a way to create a coding environment that is isolated from the computer and highly portable across machines.\nWith OneDrive and/or Dropbox we have a place to store, backup, and share files (size limits apply).\nWith Slack we have a place to ask for/provide help.\nWith LLMs we have a personal ‘tutor’ who can help us to learn more quickly and effectively.\n\nLet’s turn to the rest in part 2: Writing Code!"
  },
  {
    "objectID": "lectures/1.2-Tools_of_the_Trade.html#resources",
    "href": "lectures/1.2-Tools_of_the_Trade.html#resources",
    "title": "Tools of the Trade",
    "section": "Resources",
    "text": "Resources\n\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nProgram-Aided Language Models\nChain of Thought Prompting\nChatGPT is a blurry JPEG of the Internet 1\nWhy Meta’s latest large language model survived only three days online 2\nGit for Decolonisation3\n\n\n\n\n\nTools of the Trade • Jon Reades & Fulvio Lopane\n\n\nProbably the best ‘lay person’s’ explanation of how LLMs work/fall apart you’ll ever read.And this one was trained on scientific articles!Part art, part activism, part tech!"
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#many-models",
    "href": "lectures/1.5-Group_Working.html#many-models",
    "title": "Group Working",
    "section": "Many Models…",
    "text": "Many Models…\n\nArtist Collectives — shared responsibilities for ideas and outputs; a lot of freedom and fuzziness in roles, but propensity for struggles over direction and power.\nTheatre — every member has a specific role; there is freedom within the role and clear lines of responsibility for delivery, but can blow up spectacularly.\nCo-Creation — emphasis on participation and recognition of diverse strengths; problem- and communication-focused; lots of effort and uncertainty, but results can be much more meaningful and durable.\n…\n\n\nThe arts world is particularly innovative when it comes to project work because teams are always being formed and reformed around individual outputs (a play, an artistic collaboration, a theatre production)…\nFields such as consultancy and software development have their own norms, but they are for the most part less experimental in their structure.\nHowever, the point is that there is no one-size-fits-all pattern for a successful piece of group work."
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#challenges",
    "href": "lectures/1.5-Group_Working.html#challenges",
    "title": "Group Working",
    "section": "Challenges",
    "text": "Challenges\n\nWe make superficial assessments of the flaws/strengths of others.\nWe make a range of assumptions about the motivations of others.\nWe make a range of assumptions about the situations of others.\nWe respond differently to stimuli and stresses.\nWe have a hard time talking about any of this.\n\n\nThese are just some of the challenges to setting up your group.\nIn particular, when we meet a new group, it’s often easy to be impressed by someone who is self-confident and well-spoken: they must know what they’re doing! Group dynamics can be like a popularity contest in which the person who is easy-going or ‘fun’ is making a wonderful contribution, while the one who asks difficult questions is ‘not constructive’ and ‘not contributing’. Someone who is ‘quiet’ doesn’t seem to have much to say…"
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#identifying-your-strengths",
    "href": "lectures/1.5-Group_Working.html#identifying-your-strengths",
    "title": "Group Working",
    "section": "Identifying Your Strengths",
    "text": "Identifying Your Strengths\n\n\nDominance\n\nResults-oriented\nInnovative\nCompetitive\nDirect\n\nInfluence\n\nPeople-oriented\nEnthusiastic\nOptimistic\nCreative\n\n\nSteadiness\n\nSincere\nDependable\nPatient\nModest\n\nConscientiousness\n\nAccurate\nCautious\nPrecise\nAnalytical\n\n\n\nHere is one way of moving beyond the stereotype that the person doing the talking is also doing the thinking. The DISC model is apparently connected back to Wonder Woman by her creator W.M Marston. Like Wonder Woman, this is a model that reflects its cultural context: it was created for American teams and if you aren’t American then you might find that these four personality types don’t ‘fit’ very well. But it’s still a good place to start talking about preferences, behaviours, and the way they impact others."
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#too-much-of-a-good-thing",
    "href": "lectures/1.5-Group_Working.html#too-much-of-a-good-thing",
    "title": "Group Working",
    "section": "Too Much of a Good Thing",
    "text": "Too Much of a Good Thing\n\n\nDominance\n\nFails to involve others\nImpatient\nOffensive\n\nInfluence\n\nToo social\nEasily distracted\nOverly optimistic\n\n\nSteadiness\n\nIndirect\nAvoids conflict\nDelays difficult decisions\n\nConscientiousness\n\nPerfectionist\nAvoids unsystematic people\nDelays decisions over risks\n\n\n\nAnything that is a strength in one context can become a weakness in another.\nFor instance, a dominant personality might assume that the fact no one else is objecting means that there is support for their proposal; however, it could just be that the rest of the group wants to avoid conflict even though they think there are significant risks.\nOr someone who is motivated by the social component might spend so long bringing people together to talk about directions (and constantly adding new ideas to the mix) that very little progress is made on the actual project!"
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#consequences",
    "href": "lectures/1.5-Group_Working.html#consequences",
    "title": "Group Working",
    "section": "Consequences1",
    "text": "Consequences1\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#every-good-plan",
    "href": "lectures/1.5-Group_Working.html#every-good-plan",
    "title": "Group Working",
    "section": "Every Good Plan1…",
    "text": "Every Good Plan1…\n\n\n\n\n\nSource: Zentao"
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#nothing-ever-goes-to-plan",
    "href": "lectures/1.5-Group_Working.html#nothing-ever-goes-to-plan",
    "title": "Group Working",
    "section": "Nothing Ever Goes to Plan",
    "text": "Nothing Ever Goes to Plan\nFrom relational to practical…\n\nAgile principles1: iterative delivery of successful projects focussed on individuals and interactions; working software, customer collaboration; and responding to change.\nScrum methodology2: organised around stand ups, sprints, sprint review, and retrospectives.\nKanban methodology: use of ‘sticky notes’ in columns to organise and prioritise visually.\nXP methdology: one person writes/codes while a partner researches/solves/feeds back.\n\n\nOne challenge that teams face in the world of software is failure due to complexity and change.\nYou may have seen ‘Waterfall charts’ of the sort produced by project management software where one task cascades into the next until the project is delivered at the end. This is recipe for failure. The Mythical Man Month comprehensively debunked this approach in 1975.\nThe general conclusion was that “Adding more people to a project that is late makes it later.” The reason this happens is that you cannot break a complex task into discrete parts, send someone away to work on it in isolation, and then have them report back at the end with a finished product. I have seen this happen in previous years’ group work too: one person did a lit review while someone else was writing code but they didn’t line up at the end!\nI saw this in graphic design too: a client writes a brief for exactly what they want. You deliver something exactly like what they asked for. And they say that’s not what they meant.\nThe Agile philosophy emerged in response to these kinds of failures, with the approach that the product should be finished as quickly as possible, but as provisionally as possible. So you always have a working application, even if it only does a fraction of what you intend for it to do in the long run.\nThis has been elevated by companies like Facebook to slogans along the lines of ‘Fail Fast’, but the point is to recognise as quickly as possible when something isn’t working or isn’t headed in the direction you planned.\nFor the group work at the heart of this module, the point of the iterative process that begins in Week 6 is to help you develop competency in using the tools required to make your submission, to start you thinking along the lines that are required for the more complex questions, and to tease out points of failure in the group’s ability to work together long before you are under the tight deadline at the end of term!\n\nSource: SimpleLearnSource: Northeastern University"
  },
  {
    "objectID": "lectures/1.5-Group_Working.html#resources",
    "href": "lectures/1.5-Group_Working.html#resources",
    "title": "Group Working",
    "section": "Resources",
    "text": "Resources\n\nBartlett Guide to Group Work\nThe Five Dysfunctions of a Team\nThe Mythical Man Month\n\n\n\n\n\nGroup Working • Jon Reades and Leah Lovett"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#what-is-it",
    "href": "lectures/11.2-Transformation.html#what-is-it",
    "title": "Transformation",
    "section": "What is it?",
    "text": "What is it?\nData transformation just means changing the raw data in some way to make it more tractable for analysis.\nFor example:\n\nCentering the mean on zero is an obvious example.\nBut we can also do quite complicated things (with caution!) in order to get a distribution that we can work with using statistical tests that have certain expectations about the data we’re feeding them.\n\n\nSo, even though we add or subtract, multiply or divide, square or log the data, because we are doing the same thing to every observation the underlying relationships between the data are ‘unchanged’."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#transformation-in-1d",
    "href": "lectures/11.2-Transformation.html#transformation-in-1d",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D\n\n\n\\[\nx-\\bar{x}\n\\]\n\n\n\nInput\nOutput\n\n\n\n\n12\n-2\n\n\n13\n-1\n\n\n14\n0\n\n\n15\n+1\n\n\n16\n+2\n\n\n\n\n\nHow is this any different?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#so",
    "href": "lectures/11.2-Transformation.html#so",
    "title": "Transformation",
    "section": "So…",
    "text": "So…\n\nTransformations are mathematical operations applied to every observation in a data set that preserve some of the relationships between them."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#for-example",
    "href": "lectures/11.2-Transformation.html#for-example",
    "title": "Transformation",
    "section": "For Example",
    "text": "For Example\nIf we subtract the mean from everyone’s height then we can immediately tell if someone is taller or shorter than we would expect.\nIf we subtract the mean from everyone’s income then we cannot immediately tell if someone is earning more or less that we would expect.\nSo what is a useful transformation in one context, may not be in another!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#fleshing-this-out",
    "href": "lectures/11.2-Transformation.html#fleshing-this-out",
    "title": "Transformation",
    "section": "Fleshing This Out",
    "text": "Fleshing This Out\nQuestion: How can you tell if you did better than everyone else on the Quiz or on the Final Report?\n\nAnswer: Just subtracting the mean is not enough because the distributions are not the same. For that we also need to standardise the data in some way.\n\\[\nz = \\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nDivide through by the distribution!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#z-score-standardisation",
    "href": "lectures/11.2-Transformation.html#z-score-standardisation",
    "title": "Transformation",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\\[\n\\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nprint(scaler.mean_)\nscaler.transform(x)\n\nThe important thing to note is that if transform data that has not been fit and you get values outside the range used for fitting then you can no longer assume a standard normal."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#interquartile-standardisation",
    "href": "lectures/11.2-Transformation.html#interquartile-standardisation",
    "title": "Transformation",
    "section": "Interquartile Standardisation",
    "text": "Interquartile Standardisation\n\\[\n\\dfrac{x_{i}-x_{Q2}}{x_{Q3}-x_{Q1}}\n\\]\nfrom sklearn.preprocessing import RobustScaler\ntrf = RobustScaler(\n        quantile_range=(25.0,75.0)).fit(x)\ntrf.transform(x)"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#interdecile-standardisation",
    "href": "lectures/11.2-Transformation.html#interdecile-standardisation",
    "title": "Transformation",
    "section": "Interdecile Standardisation",
    "text": "Interdecile Standardisation\n\\[\n\\dfrac{x_{i}-x_{50^{th}}}{x_{90^{th}}-x_{10^{th}}}\n\\]\nprint(\"You've got this...\")\n\nWhy standardise:\n\nWe understand the properties of normal-ish distributions, and can simulate them easily.\nMore ‘power’ in the statistical tools available.\nMany analyses assume that ‘error’ is random and symmetric (homoscedastic, not skewed)."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#group-standardisation",
    "href": "lectures/11.2-Transformation.html#group-standardisation",
    "title": "Transformation",
    "section": "Group Standardisation",
    "text": "Group Standardisation\n\\[\nx'_{a,i} = \\dfrac{x_{ai}}{\\sum_{g} r_{N,g} P_{a,g}}\n\\]\n\nDetails:\n\n\\(x_{a,i}\\) = Value of attribute i in area a.\n\\(P_{a,g}\\) = Population of group g in area a.\n\\(r_{N,g}\\) = National ratio N of group g\n\\(\\sum\\) = Sum for all groups.\n\\(x'_{a,i}\\) = Standardised value of i in area a."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#proportional-normalisation",
    "href": "lectures/11.2-Transformation.html#proportional-normalisation",
    "title": "Transformation",
    "section": "Proportional Normalisation",
    "text": "Proportional Normalisation\n\\[\n\\dfrac{x_{i}}{\\sum{x}_{i=1}^{n}}\n\\]\nimport numpy as np\nx/np.sum(x)\n\nNumpy has a fair few options for implementing this, but note that log means natural log, not log10!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#range-normalisation",
    "href": "lectures/11.2-Transformation.html#range-normalisation",
    "title": "Transformation",
    "section": "Range Normalisation",
    "text": "Range Normalisation\n\\[\n\\dfrac{x_{i}-x_{min}}{x_{max}-x_{min}}\n\\]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(x)\nprint(scaler.data_max_)\nscaler.transform(x)\n\nNormalisation helps in several ways:\n\nScaling is important for comparability\nClustering is particularly sensitive to scale"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#log-transformation",
    "href": "lectures/11.2-Transformation.html#log-transformation",
    "title": "Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nRecall: logs are the inverse of exponentiation!\n\nSo if \\(10^{3} = 1,000\\) then \\(log_{10}(1,000) = 3\\).\nAnd if \\(10^{0} = 1\\) then \\(log_{10}(1) = 0\\)\n\nimport numpy as np\nxhat = np.log(x)\n\n\nWhy is this so common? Esp. in social sciences?\n\n\nNote that numpy uses the natural log in base \\(e\\)."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#why-log-transform",
    "href": "lectures/11.2-Transformation.html#why-log-transform",
    "title": "Transformation",
    "section": "Why Log Transform?",
    "text": "Why Log Transform?\nLet’s assume that \\(x = \\{10, 100, 1000, 10000\\}\\), consider what happens if:\n\nThe formula for the mean is \\(\\frac{\\sum{x}}{n}\\).\nThe formula for variance is \\(\\frac{(x-\\bar{x})^{2}}{n}\\).\n\nThe Natural Log (\\(e\\)) has certain advantages over other logs and should probably be your default choice for log transformations."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#other-transforms",
    "href": "lectures/11.2-Transformation.html#other-transforms",
    "title": "Transformation",
    "section": "Other Transforms…",
    "text": "Other Transforms…\n\nQuantile (maps the PDF of each feature to a uniform distribution)\nSquare Root (often with count data)\nArcsine/Angular (with percentages, proportions, text)\nRank (with care on extreme distributions)\nBox-Cox and Yeo-Johnson (arbitrary power transformations)\n\n\n\nTo report measures of central tendency it’s usually helpful to convert back to the original units.\nThe more extreme the transformation the less meaningful measures of dispersion.\nCorrelation can be significantly affected in either direction.\nCount data can be tricky because you should not have negative values (especially \\(-\\infty\\))."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#when-transforms-dont-help",
    "href": "lectures/11.2-Transformation.html#when-transforms-dont-help",
    "title": "Transformation",
    "section": "When Transforms Don’t Help",
    "text": "When Transforms Don’t Help\nArbitrarily transforming data isn’t a panacea. ‘Robust’ tests can be another approach when all else fails and two common approaches are:\n\nTrimming: cutting off, say, the top and bottom 5% of scores would start to remove skew and offer a more useful view of the central tendency of the data.\nBootstrapping: taking many sub-samples (usually of \\(n-1\\) data points or similar) we can build a picture of how certain metrics vary."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#one-last-note",
    "href": "lectures/11.2-Transformation.html#one-last-note",
    "title": "Transformation",
    "section": "One Last Note",
    "text": "One Last Note\n\nThe term normalization is used in many contexts, with distinct, but related, meanings. Basically, normalizing means transforming so as to render normal. When data are seen as vectors, normalizing means transforming the vector so that it has unit norm. When data are though of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be normal, normalizing means transforming to unit variance.\n\nSource: Stack Exchange"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#whats-wrong-with-this-map",
    "href": "lectures/11.2-Transformation.html#whats-wrong-with-this-map",
    "title": "Transformation",
    "section": "What’s Wrong with this Map?",
    "text": "What’s Wrong with this Map?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#thats-better",
    "href": "lectures/11.2-Transformation.html#thats-better",
    "title": "Transformation",
    "section": "That’s Better!",
    "text": "That’s Better!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#whats-a-projection",
    "href": "lectures/11.2-Transformation.html#whats-a-projection",
    "title": "Transformation",
    "section": "What’s a Projection?",
    "text": "What’s a Projection?\n\nSource"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#resources",
    "href": "lectures/11.2-Transformation.html#resources",
    "title": "Transformation",
    "section": "Resources",
    "text": "Resources\n\nNormalisation vs Standardisation – Quantitative analysis\nTransforming Data with R\nData Transformation and Normality Testing\nIntroduction to Logarithms\nWhat is ‘e’ and where does it come from?\nLogarithms - What is e?\nsklearn API reference\nCompare effects of different scalers on data with outliers\nThe things you’ll find in higher dimensions (useful brief discussion of manifolds)\n\n\n\n\n\nTransformation • Jon Reades"
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#useful-but-limited",
    "href": "lectures/12.1-Grouping_Data.html#useful-but-limited",
    "title": "Grouping Data",
    "section": "Useful, But Limited?",
    "text": "Useful, But Limited?\n\n\n\nMethod\nAchieves\n\n\n\n\ncount()\nTotal number of items\n\n\nfirst(), last()\nFirst and last item\n\n\nmean(), median()\nMean and median\n\n\nmin(), max()\nMinimum and maximum\n\n\nstd(), var()\nStandard deviation and variance\n\n\nmad()\nMean absolute deviation\n\n\nprod()\nProduct of all items\n\n\nsum()\nSum of all items\n\n\n\n\nHere are a bunch of pandas functions that have to do with aggregating data in some way. Some of these you’ll have seen before, some you may not. However, up to this point if you wanted to to know the median price of each type of Airbnb listing, or the sum of each type of vehicle sold, you’d have had to select out one listing type or vehicle type, call median or sum, and then remember the result. Let’s change that."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#grouping-operations",
    "href": "lectures/12.1-Grouping_Data.html#grouping-operations",
    "title": "Grouping Data",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#in-practice",
    "href": "lectures/12.1-Grouping_Data.html#in-practice",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(&lt;fields&gt;).&lt;function&gt;\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "href": "lectures/12.1-Grouping_Data.html#grouping-by-arbitrary-mappings",
    "title": "Grouping Data",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#pivot-tables",
    "href": "lectures/12.1-Grouping_Data.html#pivot-tables",
    "title": "Grouping Data",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(&lt;series&gt;, &lt;bins&gt;) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#in-practice-1",
    "href": "lectures/12.1-Grouping_Data.html#in-practice-1",
    "title": "Grouping Data",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#deriving-measures-of-diversity",
    "href": "lectures/12.1-Grouping_Data.html#deriving-measures-of-diversity",
    "title": "Grouping Data",
    "section": "Deriving Measures of Diversity",
    "text": "Deriving Measures of Diversity\n\nOne of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few… Location Quotient (LQ), Herfindah-Hirschman Index (HHI), Shanon Entropy.\n\n\nI like easy measures."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#location-quotient",
    "href": "lectures/12.1-Grouping_Data.html#location-quotient",
    "title": "Grouping Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R. \\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[&lt; 1\\]\n\n\nLow Regional Share\n\\[&gt; 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#herfindahl-hirschman-index",
    "href": "lectures/12.1-Grouping_Data.html#herfindahl-hirschman-index",
    "title": "Grouping Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry: \\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[&lt; 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#shannon-entropy",
    "href": "lectures/12.1-Grouping_Data.html#shannon-entropy",
    "title": "Grouping Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure: \\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "lectures/12.1-Grouping_Data.html#resources",
    "href": "lectures/12.1-Grouping_Data.html#resources",
    "title": "Grouping Data",
    "section": "Resources",
    "text": "Resources\n\nStop aggregating away the signal in your data\n\n\n\n\n\nGrouping Data • Jon Reades"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#spot-the-difference",
    "href": "lectures/12.3-Clustering.html#spot-the-difference",
    "title": "Clustering",
    "section": "Spot the Difference",
    "text": "Spot the Difference\n\n\nClassification\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms (A) present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in 1–2 dimensions\n\n\nClustering\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms A present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in &lt; 9 dimensions\n\n\n\nClustering algorithms can suffer from the ‘curse of dimensionality’ such that high-dimensional spaces cluster poorly without either dimensionality reduction or the use of specialist algorithms such as Spherical k-Means."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#the-first-geodemographic-classification",
    "href": "lectures/12.3-Clustering.html#the-first-geodemographic-classification",
    "title": "Clustering",
    "section": "The First Geodemographic Classification?",
    "text": "The First Geodemographic Classification?\n\nSource: booth.lse.ac.uk/map/"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#more-than-100-years-later",
    "href": "lectures/12.3-Clustering.html#more-than-100-years-later",
    "title": "Clustering",
    "section": "More than 100 Years Later",
    "text": "More than 100 Years Later\n\nSource: vis.oobrien.com/booth/"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "href": "lectures/12.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "title": "Clustering",
    "section": "Intimately Linked to Rise of The State",
    "text": "Intimately Linked to Rise of The State\n\nGeodemographics only possible in context of a State – without a Census it simply wouldn’t work… until now?\nClearly tied to social and economic ‘control’ and intervention: regeneration, poverty & exclusion, crime, etc.\nPresumes that areas are the relevant unit of analysis; in geodemographics these are usually called neighbourhoods… which should ring a few bells.\nIn practice, we are in the realm of ‘homophily’, a.k.a. Tobler’s First Law of Geography"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#where-is-it-used",
    "href": "lectures/12.3-Clustering.html#where-is-it-used",
    "title": "Clustering",
    "section": "Where is it used?",
    "text": "Where is it used?\nAnything involving grouping individuals, households, or areas into larger ‘groups’…\n\nStrategic marketing (above the line, targeted, etc.)\nRetail analysis (store location, demand modelling, etc.)\nPublic sector planning (resource allocation, service development, etc.)\n\nCould see it as a subset of customer segmentation."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#problem-domains",
    "href": "lectures/12.3-Clustering.html#problem-domains",
    "title": "Clustering",
    "section": "Problem Domains",
    "text": "Problem Domains\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nSupervised\nRegression\nClassification\n\n\nUnsupervised\nDimensionality Reduction\nClustering\n\n\n\n\n\nIn classification we ‘know’ the answer (we test against labels).\nIn clustering we don’t ‘know’ the answer (we look for clusters)."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#measuring-fit",
    "href": "lectures/12.3-Clustering.html#measuring-fit",
    "title": "Clustering",
    "section": "Measuring ‘Fit’",
    "text": "Measuring ‘Fit’\n\nUsually working towards an ‘objective criterion’ for quality… these are known as cohesion and separation measures."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#how-your-data-looks",
    "href": "lectures/12.3-Clustering.html#how-your-data-looks",
    "title": "Clustering",
    "section": "How Your Data Looks…",
    "text": "How Your Data Looks…\nClustering is one area where standardisation (and, frequently, normalisation) are essential:\n\nYou don’t (normally) want scale in any one dimension to matter more than scale in another.\nYou don’t want differences between values in one dimension to matter more than differences in another.\nYou don’t want skew in one dimension to matter more than skew in another.\n\nYou also want uncorrelated variables… why?"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#first-steps",
    "href": "lectures/12.3-Clustering.html#first-steps",
    "title": "Clustering",
    "section": "First Steps",
    "text": "First Steps\nYou will normally want a continuous variable… so these types of data are especially problematic:\n\nDummies / One-Hot Encoded\nCategorical / Ordinal\nPossible solutions: k-modes, CCA, etc."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#performance",
    "href": "lectures/12.3-Clustering.html#performance",
    "title": "Clustering",
    "section": "Performance",
    "text": "Performance\nTypically about trade-offs between:\n\n\n\nAccuracy\nGeneralisation"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#trade-offs",
    "href": "lectures/12.3-Clustering.html#trade-offs",
    "title": "Clustering",
    "section": "Trade-Offs",
    "text": "Trade-Offs\nNeed to balance:\n\nAbility to cluster at speed.\nAbility to replicate results.\nAbility to cope with fuzzy/indeterminate boundaries.\nAbility to cope with curse of dimensionality.\nUnderlying representation of group membership…"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#visualising-the-trade-offs",
    "href": "lectures/12.3-Clustering.html#visualising-the-trade-offs",
    "title": "Clustering",
    "section": "Visualising the Trade-Offs",
    "text": "Visualising the Trade-Offs\n\n\n\nNotice the limitations to k-means: it may be fast but it’s got problems if your data is non-linear/non-Gaussian.\nAnd this doesn’t even include options like HDBSCAN, HAC/Hierarchical Clustering, and many more!\n\n\nDetails on scikit-learn.org."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#putting-it-all-into-context",
    "href": "lectures/12.3-Clustering.html#putting-it-all-into-context",
    "title": "Clustering",
    "section": "Putting it All into Context",
    "text": "Putting it All into Context\n\n\n\n\n\nClustering • Jon Reades"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#waves",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#waves",
    "title": "Computers in Urban Studies",
    "section": "3 Waves?",
    "text": "3 Waves?\n\n\n“A computer in every institution”\n“A computer in every office”\n“A computer in every thing”\n\n\n\nWave 1\n\nRoughly the 1950s–70s\nComputers as ‘super-human’ calculators for data\nData + models as theory-testing tool\n\nRetrospectively: the 1st quantitative revolution. But can see this as incorrect and focus on the theoretical aspect.\nWave 2\n\nRoughly the 1980s–2000s\nComputers as tools for thinking about spatial relationships\nExplicit modelling of local spatial effects\n\nRetrospectively: the GIS revolution. But I personally see this as incorrect because GIS is Wave 1.\nShift from computers as processors of data to integrated, pervasive systems that spew out data on everything.\nWave 3\n\nRoughly the mid-2000s–?\nComputers as tools for generating data (pace ABM researchers)\nGeodata being continuously produced as byproduct of other activities\nShift from researching attributes to behaviours (pace Hägerstrand)\n\nRetrospectively: the big data revolution or 2nd quantitative revolution."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#all-waves-still-going",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#all-waves-still-going",
    "title": "Computers in Urban Studies",
    "section": "All Waves Still Going!",
    "text": "All Waves Still Going!\nWave 1: Computers help me do it faster\n\nGIS is ‘just’ the industrial revolution impacting cartography.\n\nWave 2: Computers help me to think\n\nGeocomputation & local stats are qualitatively & meaningfully different.\n\nWave 3: Computers help me to learn\n\nNot ‘just’ about the ‘bigness’ of data, though that is important.\n\n\nWave 2 is about implementing ideas such as recursion and iteration – these could, in theory, have been tackled in Wave 1, but in practice that’s not what people were doing.\nWave 3 is about more explicitly allowing computers to learn about data so that we can extract insight from these models – these could also, in theory, have been tackled in Wave 2 but in practice that’s not what people were doing.\nI’m not totally happy about my description of Wave 3 and will try to dig into this in a little more detail but suggestions welcome!"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#anticipated-by-hägerstrand-1967",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#anticipated-by-hägerstrand-1967",
    "title": "Computers in Urban Studies",
    "section": "Anticipated by Hägerstrand (1967)",
    "text": "Anticipated by Hägerstrand (1967)\n\nI think that the computer can do three different and useful things for us. The first and simplest operation is… descriptive mapping the second… is the analytical one The third kind of service is… to run process models by which we might try to reproduce observed or create hypothetical chains of events of a geographical nature.\nT. Hägerstrand (1967), ‘The Computer and the Geographer’, TiBG"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#but-persistent-critiques",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#but-persistent-critiques",
    "title": "Computers in Urban Studies",
    "section": "But Persistent Critiques",
    "text": "But Persistent Critiques\n\nThere is a clear disparity between the sophisticated theoretical and methodological framework which we are using and our ability to say anything really meaningful about events as they unfold around us. There are too many anomalies… There is an ecological problem, an urban problem… and yet we seem incapable of saying anything of any depth or profundity about any of them. When we do say something it appears trite and rather ludicrous.\nHarvey (1972, p.6)"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#data-science",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#data-science",
    "title": "Computers in Urban Studies",
    "section": "Data science?",
    "text": "Data science?\n\n\n\n\n\nIndustry-led\nSpatially ignorant (often)\nDisciplinarily greedy (often)"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#what-is-different",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#what-is-different",
    "title": "Computers in Urban Studies",
    "section": "What is Different?",
    "text": "What is Different?\nAccording to Donoho (2017) ‘data science’ differs from plain old ‘statistics’ through an interest in:\n\nData gathering, preparation, and exploration;\nData representation and transformation;\nComputing with data;\nData visualisation and presentation;\nData modelling; and\nA reflexive ‘science of data science’."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#in-practice",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#in-practice",
    "title": "Computers in Urban Studies",
    "section": "In Practice…",
    "text": "In Practice…\nI think there are several distinguishing features that I encounter in day-to-day (geography) work:\n\nData-driven methods development & deployment\nExplicit tuning/meta-parameterisation\nExplicit feature optimisation/engineering\nExplicit training/testing from ‘one shot’ data\n‘Black boxes’ feature prominently & ‘online learning’ emerging quickly\n\nData science as process and pipeline, not just input to research."
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#do-we-need-it",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#do-we-need-it",
    "title": "Computers in Urban Studies",
    "section": "Do We Need It?",
    "text": "Do We Need It?\n\nSource: xkcd"
  },
  {
    "objectID": "lectures/2.1-Computers_in_Planning_and_Geography.html#resources",
    "href": "lectures/2.1-Computers_in_Planning_and_Geography.html#resources",
    "title": "Computers in Urban Studies",
    "section": "Resources",
    "text": "Resources\n\nThe Data of Urban Spaces — a nice collection of pieces on Towards Data Science\n\n\n\n\n\nComputers in Urban Studies • Jon Reades"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#variables-have-types",
    "href": "lectures/2.3-Python_the_Basics-1.html#variables-have-types",
    "title": "Python, the Basics",
    "section": "Variables Have Types…",
    "text": "Variables Have Types…\n\n\n\nName\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#but-we-can-change-that",
    "href": "lectures/2.3-Python_the_Basics-1.html#but-we-can-change-that",
    "title": "Python, the Basics",
    "section": "… But We Can Change That",
    "text": "… But We Can Change That\nMessage starts as a string:\nmsg = '42'\ntype(msg) # str\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg) # change to int\nAnd back to a string:\nmsg = str(msg)\ntype(msg) # back to str\nAnd notice:\nprint(str(int('42'))) # string from int from string\n\nThat last line of code is intended to start familiarising you with Python syntax: programmers rarely do one operation per line of code if they can do more than one, so you’ll often see nested parentheses like this and you need to learn how to read this kind of code starting from the inner-most parentheses (the int()) and working outwards from there to str() and finally print()."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#and-they-are-all-objects",
    "href": "lectures/2.3-Python_the_Basics-1.html#and-they-are-all-objects",
    "title": "Python, the Basics",
    "section": "…And They Are All Objects",
    "text": "…And They Are All Objects\nOne to remember for the session on objects and classes:\nisinstance(msg,object)     # True\nisinstance(answer,object)  # True\nisinstance(pi,object)      # True\nisinstance(c,object)       # True\nisinstance(correct,object) # True\n\nLike str(), int(), and print(), you see here another command — or ‘function’ in Python terminology — called ininstance. An ‘instance’ of something is just a way of asking ‘is it a type of’: so the first line asks if string msg is a kind of object, and the answer to that is: True. But it’s the same for answer (an integer), pi (a floating point number), and so on. They are all a kind of object, and we’ll see why that is a useful answer in a few weeks’ time."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#variables-have-names",
    "href": "lectures/2.3-Python_the_Basics-1.html#variables-have-names",
    "title": "Python, the Basics",
    "section": "Variables Have Names…",
    "text": "Variables Have Names…\nRules for variable names:\n\nThey cannot start with a number: item3 is valid, 3items is not.\nWhite space and symbols are not allowed, but _ is allowed: my_variable is valid, my-variable, my$variable, and my variable are not.\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used; but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#but-not-all-words-are-allowed",
    "href": "lectures/2.3-Python_the_Basics-1.html#but-not-all-words-are-allowed",
    "title": "Python, the Basics",
    "section": "…But Not All Words Are Allowed",
    "text": "…But Not All Words Are Allowed\nDo not try to use any of these as variable names. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#simple-operations-on-variables",
    "href": "lectures/2.3-Python_the_Basics-1.html#simple-operations-on-variables",
    "title": "Python, the Basics",
    "section": "Simple Operations on Variables",
    "text": "Simple Operations on Variables\nLet’s start with x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n15\n\n\nDifference\nx - y\n5\n\n\nProduct\nx * y\n50\n\n\nQuotient\nx / y\n2.0\n\n\n‘Floored’ Quotient\nx // y\n2\n\n\nRemainder\nx % y\n0\n\n\nPower\npow(x,y) or x**y\n100000\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#strings-are-different",
    "href": "lectures/2.3-Python_the_Basics-1.html#strings-are-different",
    "title": "Python, the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nWhen you do things with strings the answers can look a little different. Let’s start with x=\"Hello\" and y=\"You\" and z=2…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n'HelloYou'\n\n\nDifference\nx - y\nTypeError\n\n\nProduct\nx * z\nHelloHello\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#using-strings-to-output-information",
    "href": "lectures/2.3-Python_the_Basics-1.html#using-strings-to-output-information",
    "title": "Python, the Basics",
    "section": "Using Strings to Output Information",
    "text": "Using Strings to Output Information\nPython has no fewer than three ways output information:\n\nstring concatenation using +;\nstring formatting using &lt;str&gt;.format(&lt;variables&gt;); and\nf-strings using f\"{variable_1} some text {variable_n}\".\n\nThere are pros and cons to each:\nx = 24\ny = 'Something'\nprint(\"The value of \" + y + \" is \" + str(x))\nprint(\"The value of {0} is {1}\".format(y, x))\nprint(f\"The value of {y} is {x}\")\n\nObviously, a really common requirement that programmers have is ‘output a nicely formatted string containing information about the variables in my program’.\nI rather like f-strings because they can actually contain any code you like (you could, for instance, write `f”The square root of {y} is {x**(1/2)}” and it would work. However, concatenation is the easiest to learn."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#operators-assemble",
    "href": "lectures/2.3-Python_the_Basics-1.html#operators-assemble",
    "title": "Python, the Basics",
    "section": "Operators Assemble!",
    "text": "Operators Assemble!\nAlways pay attention to precedence:\nx, y = 10, 5\nx + y * 2         # == 20\n(x + y) * 2       # == 30\nx + y * 2 / 3     # == 13.3333333333334\nx + y * (2/3)     # == 13.3333333333332\n(x + y) * (2/3)   # == 10.0\nAnd here’s a subtle one:\n(x * y) ** 2/3    # == 833.333333333334\n(x * y) ** (2/3)  # == 13.5720880829745\nThe full list is here.\n\nIf you’re a little rusty on exponents, that last example is the cube root of (x*y)**2. But the key point is that formatting is not what matters here, it’s operator precedence: one has parentheses, the other does not, so they are evaluated differently. To make the first one more clear we might use (x * y)**2 / 3 or even ((x*y)**2)/3.\nAlso notice that with the two floats in the first block you do not always get the same result from operations that should give the same answer. Non-terminating decimals (e.g. 1/3) will always be rounded, unpredictably, by the computer because it doesn’t have infinite memory. The process of rounding means that you need to be very careful comparing floats (more on this later)."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#comparing-variables",
    "href": "lectures/2.3-Python_the_Basics-1.html#comparing-variables",
    "title": "Python, the Basics",
    "section": "Comparing Variables",
    "text": "Comparing Variables\nFor numeric variables comparisons are easy.\n\n\n\nOperator\nInput\nResult\n\n\n\n\n==\n10 == 5\nFalse\n\n\n!=\n10 != 5\nTrue\n\n\n&lt;, &lt;=\n10 &lt; 5\nFalse\n\n\n&gt;, &gt;=\n10 &gt; 5\nTrue\n\n\n\n\nThe result of any (successful) comparison is a Boolean (True/False). We can save the output of this comparison to a new variable (e.g. z = x &gt; y).\nThis last example has to do with the way that strings are compared."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#strings-are-different-1",
    "href": "lectures/2.3-Python_the_Basics-1.html#strings-are-different-1",
    "title": "Python, the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nBut notice:\nw, x, y, z = '4a','4a','365','42'\nw == x  # True\nw != y  # True\nx &gt; y   # True\nx &gt; z   # True\nWhy is 4a greater than both 365 and 42?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#danger-will-robinson",
    "href": "lectures/2.3-Python_the_Basics-1.html#danger-will-robinson",
    "title": "Python, the Basics",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nNotice the very subtle visual difference between = and ==!\nx = 5\ny = 10\nx = y   # Probably a mistake: setting x to the value of y\nx == y  # True, because x and z are now both set to 10\nRemember this!\n\nConfusing these two operators is the most common source of mistakes early on when learning to code in Python! One (=) does assignment, the other (==) does comparison."
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#common-mistakes",
    "href": "lectures/2.3-Python_the_Basics-1.html#common-mistakes",
    "title": "Python, the Basics",
    "section": "Common Mistakes",
    "text": "Common Mistakes\nHere’s the output from some attempts at comparison:\nx, y = '42', 42\nx==y   # False\nx&gt;y    # An error!\nThis last line produces:\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: '&gt;' not supported between instances of 'str' and 'int'\nIf we want to compare them then we’ll need to change their type:\nx &gt; str(y)   # False\nint(x) &lt;= y  # True\nx &gt; = str(y) # Also True\n\nA really common mistake is to think that string (str) \"42\" is the same as the integer (int) 42.\nNotice that in the first example we can say that 42 is clearly not the same as ‘42’, but we can’t say whether it’s more or less because that’s non-sensical in this context. So this is a computer being totally logical but not always sensible.\nAlso notice the syntax for this: we have str(&lt;something&gt;) and int(&lt;something&gt;) to convert between types. These are functions, which we’ll spend a lot more time on next week!\nWhy might it be (fractionally) faster to compare integers than strings?"
  },
  {
    "objectID": "lectures/2.3-Python_the_Basics-1.html#more-resources",
    "href": "lectures/2.3-Python_the_Basics-1.html#more-resources",
    "title": "Python, the Basics",
    "section": "More Resources",
    "text": "More Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\nTypes of Data\nVariables and expressions\nStrings\nThe string type\nCommon string methods\nFormatting strings\nSplitting and joining\nNumeric types\nThe bool type\nStoring Data in Variables\n\n\n\n\n\nPython, the Basics • Jon Reades"
  },
  {
    "objectID": "lectures/2.5-Lists.html#whats-in-a-list",
    "href": "lectures/2.5-Lists.html#whats-in-a-list",
    "title": "Lists",
    "section": "What’s in a List?",
    "text": "What’s in a List?\nIn the same way that a paper shopping list holds many ‘types’ of shopping in one place, a Python list holds many ‘types’ of data in one place.\nmyList = [1, 3, 5, 7]     # homogenous list\nmyList = [1, \"dog\", 7.01] # heterogenous list \nmyList = []               # empty list\nPython lists are always recognisable by their “square brackets”: [...]"
  },
  {
    "objectID": "lectures/2.5-Lists.html#whats-in-a-list-part-2",
    "href": "lectures/2.5-Lists.html#whats-in-a-list-part-2",
    "title": "Lists",
    "section": "What’s in a List? (Part 2)",
    "text": "What’s in a List? (Part 2)\nIn fact, when I say lists can hold many types of data, I should have said that they can hold any type of data:\nx = 3\ny = \"Foo\"\nz = [\"A\", \"list\", 42]\n\na = [x, y, z] # Holds x, y, *and* list z\nThe output of print(a) is:\n[3, 'Foo', ['A', 'list', 42]]\n\nWe’re going to come back to this a lot later, but for now notice that a list can hold lists!"
  },
  {
    "objectID": "lectures/2.5-Lists.html#using-list-indexes",
    "href": "lectures/2.5-Lists.html#using-list-indexes",
    "title": "Lists",
    "section": "Using List Indexes",
    "text": "Using List Indexes\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nLists are ‘indexed’ numerically from the zero-th element:\n\n\n\n\n\n\n\n\n\n\ngeographers [\n0\n1\n2\n]\n\n\n\n\n\nMassey 1\nHarvey 2\nRose 3\n\n\n\n\nprint(geographers[1]) # Harvey\nprint(geographers[2]) # Rose\nprint(geographers[3]) # Error: List index out of range\n\nAnd notice this error: Python tells you waht the problem is. The issue is understanding what the message means if you don’t know the vocabulary.\n\nhttps://en.wikipedia.org/wiki/Doreen_Massey_(geographer)https://en.wikipedia.org/wiki/David_Harveyhttps://en.wikipedia.org/wiki/Gillian_Rose_(geographer)"
  },
  {
    "objectID": "lectures/2.5-Lists.html#interpolation",
    "href": "lectures/2.5-Lists.html#interpolation",
    "title": "Lists",
    "section": "Interpolation",
    "text": "Interpolation\nWe can also use variables as list indexes:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 0\nprint(geographers[i]) # Massey\nAnything that evaluates (i.e. resolves) to a number can be used as an index:\ni = 1\nprint(geographers[i+1]) # Rose\nprint(geographers[ (i-2+1)*2 ]) # Massey"
  },
  {
    "objectID": "lectures/2.5-Lists.html#countdown",
    "href": "lectures/2.5-Lists.html#countdown",
    "title": "Lists",
    "section": "Countdown!",
    "text": "Countdown!\nWe can ‘count’ backwards from the end of the list using negative numbers:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[-1] ) # Rose\nprint( geographers[-2] ) # Harvey"
  },
  {
    "objectID": "lectures/2.5-Lists.html#does-not-compute",
    "href": "lectures/2.5-Lists.html#does-not-compute",
    "title": "Lists",
    "section": "Does Not Compute!",
    "text": "Does Not Compute!\nErrors can be scary… but informative!\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[4] )\n\nIndexError: list index out of range\n\nAnd then try:\nprint( geographers[1.25] )\n\nTypeError: list indices must be integers or slices, not float\n\nNotice that Python gives us important hints about the source of the problem!"
  },
  {
    "objectID": "lectures/2.5-Lists.html#slicing-dicing-lists",
    "href": "lectures/2.5-Lists.html#slicing-dicing-lists",
    "title": "Lists",
    "section": "Slicing & Dicing Lists",
    "text": "Slicing & Dicing Lists\nYou can access more than one element at a time using a slice:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[0:2] ) # ['Massey','Harvey']\nprint( geographers[1:] )  # ['Harvey', 'Rose']\nprint( geographers[-2:] ) # ['Harvey', 'Rose']\nThe syntax for a slice is: list[ &lt;start_idx&gt;, &lt;end_idx&gt; ], but end_idx is not included in the slice. And notice:\nprint( geographers[1:2] ) # ['Harvey']\nprint( geographers[1] )   #   Harvey\n\nIt’s really subtle, but notice that a slice always returns a list, even if it’s just a list containing one thing. So geographers[1]=='Harvey' but geographers[1:2]==['Harvey']. Not the same thing!"
  },
  {
    "objectID": "lectures/2.5-Lists.html#test-yourself",
    "href": "lectures/2.5-Lists.html#test-yourself",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat do you think this will produce?\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 2\nprint( geographers[ (i-3)**2-4:-1 ] )\nSee if you can work out in your head before typing it!"
  },
  {
    "objectID": "lectures/2.5-Lists.html#wheres-wally",
    "href": "lectures/2.5-Lists.html#wheres-wally",
    "title": "Lists",
    "section": "Where’s Wally?",
    "text": "Where’s Wally?\nlist.index(...) tells you where something can be found in a list:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index(\"Harvey\") # 1\ngeographers.index(\"Massey\") # 0\nCombining ideas that will become very useful later:\nprint(geographers[ geographers.index(\"Massey\") ])\nWhat do you think this prints? Why does it work at all?\n\nThis last example looks a little strange, but what if I had a separate list with first names, or Wikipedia links, or other information about these geographers? Because list.index(x) returns an integer we can use it as an index for accessing another list."
  },
  {
    "objectID": "lectures/2.5-Lists.html#wheres-wally-part-2",
    "href": "lectures/2.5-Lists.html#wheres-wally-part-2",
    "title": "Lists",
    "section": "Where’s Wally (Part 2)",
    "text": "Where’s Wally (Part 2)\nlist.index(...) has one flaw:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index('Batty')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: 'Batty' is not in list\nIf ‘throwing an error’ is overkill, then here’s another way:\nif 'Batty' in geographers:\n    print(\"Found Mike!\")\nelse:\n    print(\"Not a geographer!\")"
  },
  {
    "objectID": "lectures/2.5-Lists.html#sorting",
    "href": "lectures/2.5-Lists.html#sorting",
    "title": "Lists",
    "section": "Sorting",
    "text": "Sorting\nWe can sort lists in alpha-numerical order:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.sort()\nprint(geographers) # ['Harvey', 'Massey', 'Rose']\nAnd we can reverse-sort too:\ngeographers.sort(reverse=True)\nprint(geographers) # ['Rose', 'Massey', 'Harvey']"
  },
  {
    "objectID": "lectures/2.5-Lists.html#lists-are-mutable",
    "href": "lectures/2.5-Lists.html#lists-are-mutable",
    "title": "Lists",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\nMutable == “liable or subject to change or alteration”\nLet’s replace Rose with Jefferson1 in the list.\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers[2] = \"Jefferson\"\nprint(geographers) # ['Massey','Harvey','Jefferson']\nthe-women-cartographers-who-mapped-art-and-science-in-the-20th-century"
  },
  {
    "objectID": "lectures/2.5-Lists.html#addingremoving-items",
    "href": "lectures/2.5-Lists.html#addingremoving-items",
    "title": "Lists",
    "section": "Adding/Removing Items",
    "text": "Adding/Removing Items\nWhen we insert() items into, or pop() items out of, a list we normally need to specify the index.\ngeographers = [\"Massey\", \"Harvey\", \"Jefferson\"]\ngeographers.insert(0,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.insert(3,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'von Humboldt', 'Jefferson']\nAnd in ‘reverse’:\ngeographers.pop(3) # 'von Humboldt'\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\n\nNotice also that insert modifies the list and returns nothing, while pop modifies the list and returns the value that you ‘popped’."
  },
  {
    "objectID": "lectures/2.5-Lists.html#test-yourself-1",
    "href": "lectures/2.5-Lists.html#test-yourself-1",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nThere are two ways to remove David Harvey from the list of geographers without writing this:\ngeographers = ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.pop(2) # Do not use this answer!\n\nYou can adapt an example we saw earlier in ‘Finding Things’.\nYou can use Google to see if there are list operations we’ve not covered.\n\n\nHints: remove and del are both options for 2."
  },
  {
    "objectID": "lectures/2.5-Lists.html#concatenating",
    "href": "lectures/2.5-Lists.html#concatenating",
    "title": "Lists",
    "section": "Concatenating",
    "text": "Concatenating\nWe combine lists using addition:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = female_geographers + male_geographers\nprint(all_geographers)    # ['Rose', ..., 'Hägerstrand']\nprint(all_geographers[0]) # Rose"
  },
  {
    "objectID": "lectures/2.5-Lists.html#appending",
    "href": "lectures/2.5-Lists.html#appending",
    "title": "Lists",
    "section": "Appending",
    "text": "Appending\nNote that this is not the same!\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers   = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\n all_geographers.append(male_geographers)\nprint(all_geographers) # [['Rose',...], [..., 'Hägerstrand']]\nprint(all_geographers[0]) # ['Rose', ..., 'Jefferson']\nWhat do you think has happened here?"
  },
  {
    "objectID": "lectures/2.5-Lists.html#test-yourself-2",
    "href": "lectures/2.5-Lists.html#test-yourself-2",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nmale_geographers.append('Batty')\nprint(male_geographers)\nWhat do you think this will produce? And why do you think that append appears to do something different in these two examples?"
  },
  {
    "objectID": "lectures/2.5-Lists.html#how-many-geographers-do-i-know",
    "href": "lectures/2.5-Lists.html#how-many-geographers-do-i-know",
    "title": "Lists",
    "section": "How many geographers do I know?",
    "text": "How many geographers do I know?\nlen(...) gives you the length of ‘countable’ things:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nlen(geographers) # 3\nBut…\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nprint( len(all_geographers) ) # 2"
  },
  {
    "objectID": "lectures/2.5-Lists.html#whos-on-the-list",
    "href": "lectures/2.5-Lists.html#whos-on-the-list",
    "title": "Lists",
    "section": "Who’s on the List?",
    "text": "Who’s on the List?\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers) # True\nprint(\"Batty\" in geographers)  # False\nBut…\ngeographers.index('Batty')\nis a ValueError that causes your Python code to fail.\n\nWhy might you choose one of these over the other?"
  },
  {
    "objectID": "lectures/2.5-Lists.html#test-yourself-3",
    "href": "lectures/2.5-Lists.html#test-yourself-3",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nHow would you change this code:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers)\nprint(\"Batty\" in geographers)\nSo that it prints:\nFalse\nTrue\nYou will have seen the answer to this in Code Camp, but you can also Google it†!\n\n\n† I’d suggest looking first at Stack Overflow answers in most cases."
  },
  {
    "objectID": "lectures/2.5-Lists.html#tuples-not-actually-a-list",
    "href": "lectures/2.5-Lists.html#tuples-not-actually-a-list",
    "title": "Lists",
    "section": "Tuples: Not Actually a List",
    "text": "Tuples: Not Actually a List\nBecause they come up a lot in geo-data, it’s worth knowing about tuples, which are basically immutable lists:\nt = (52.124021, -0.0012012)\nprint(type(t)) # &lt;class 'tuple'&gt;\nprint(t)       # (52.124021, -0.0012012)\nprint(t[0])    # 52.124021\nBut this…\nt[0] = 25.1203210\nwill throw an error:\n\nTypeError: ‘tuple’ object does not support item assignment"
  },
  {
    "objectID": "lectures/2.5-Lists.html#resources",
    "href": "lectures/2.5-Lists.html#resources",
    "title": "Lists",
    "section": "Resources",
    "text": "Resources\n\nLists in Python\nTuples in Python\nRange and lists\nSequence types\nThe Complete Guide to Lists (by a CASA alum!)\n\n\n\n\n\nLists • Jon Reades"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nConsider how these two data structures differ:\ncities = [\n  {'name': 'London', 'loc': [51.5072, 0.1275], 'tz': +0}, \n  {'name': 'New York', 'loc': [40.7127, 74.0059], 'tz': -5}, \n  {'name': 'Tokyo', 'loc': [35.6833, 139.6833], 'tz': +8}\n]\nOr:\ncities = {\n  'London': {'loc': [51.5072, 0.1275], 'tz': +0}, \n  'New York': {'loc': [40.7127, 74.0059], 'tz': -5}, \n  'Tokyo': {'loc': [35.6833, 139.6833], 'tz': +8}\n}\n\nWhy don’t you copy the code and then see how to access different fields/values? What might be the pros and cons of each?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#implications",
    "href": "lectures/3.3-DOLs_to_Data.html#implications",
    "title": "Data Structures",
    "section": "Implications",
    "text": "Implications\n\nSo we can mix and match dictionaries and lists in whatever way we need to store… ‘data’. The question is then: what’s the right way to store our data?\n\n\nAnswer: the way that makes the most sense to a human while also being the most robust for coding."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nHow do these data structures differ?\n\n\nOption 1\nds1 = [\n  ['lat','lon','name','tz'],\n  [51.51,0.13,'London',+0],\n  [40.71,74.01,'New York',-5],\n  [35.69,139.68,'Tokyo',+8]\n]\n\nOption 2\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\n\nTo understand why I’m asking this question, here are two example questions I’d like you to try to answer:\n\nWhat’s the average latitude of these three cities?\nWhat’s the time zone of Tokyo?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "href": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "title": "Data Structures",
    "section": "Thinking it Through",
    "text": "Thinking it Through\nWhy does this work for both computers and people?\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nWe are doing away with the idea that the order of columns matters (humans don’t care that a city’s name is in the first column, and a city’s latitude in the second). We just want to find the column. But because we have a dictionary-of-lists we can ensure that the row order is preserved. Let’s see this in action."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#examples",
    "href": "lectures/3.3-DOLs_to_Data.html#examples",
    "title": "Data Structures",
    "section": "Examples",
    "text": "Examples\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nprint(ds2['name'][0]) # London\nprint(ds2['lat'][0])  # 51.51\nprint(ds2['tz'][0])   # 0\nSo 0 always returns information about London, and 2 always returns information about Tokyo. But it’s also easy to ask for the latitude (ds2['lat'][0]) or time zone (ds2['tz'][0]) value once you know that 0 is London!\n\nBut there’s another advantage that’s not quite so obvious: for the computer because everything of type ‘lat’ is a float, everything of type ‘tz’ is an integer, and everything of type ‘name’ is a string, it’s a lot easier to work with each column as data."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "href": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "title": "Data Structures",
    "section": "How is that easier???",
    "text": "How is that easier???\nRemember that we can use any immutable ‘thing’ as a key. This means…\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ncity_nm = 'Tokyo'\ncity_idx = ds2['name'].index(city_nm)\n\nprint(f\"The time zone of {city_nm} is {ds2['tz'][city_idx]}\")\nWe can re-write this into a single line as:\ncity_nm = 'New York'\nprint(f\"The time zone of {city_nm} is {ds2['tz'][ ds2['name'].index(city_nm)]}\")\n\nThis achieves several useful things:\n\nIt is fast: faster than iterating over a list-of-lists or dictionary-of-dictionaries. In other words, there is no iteration at all!\nAll data in a list is of the same type so we can easily add checks to make sure that it’s valid.\nWe can also easily calculate an average/max/min/median and so on (as we’ll see later) without even having to look at any other columns!\nWe can add more columns instantly and the process of finding something is just as fast as it is now. And adding more rows doesn’t make it much slower either!\n\nAlso, notice how in these two examples we don’t try to write the second example in one go: first, we work it out as a set of steps: how do we figure out what ‘row’ (position in the list) Tokyo is in? Now that we’ve got that, how do we retrieve the time zone value for Tokyo? We know that code works, now let’s do variable substitution, as we would if we were doing maths: we can replace the city_idx in the time zone lookup with ds2['name'].index('Tokyo')."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#resources",
    "href": "lectures/3.3-DOLs_to_Data.html#resources",
    "title": "Data Structures",
    "section": "Resources",
    "text": "Resources\n\n8 Data Structures Every Data Scientist Should Know (by a CASA alum)\n\n\n\n\n\nData Structures • Jon Reades"
  },
  {
    "objectID": "lectures/3.5-Git.html#how-it-works",
    "href": "lectures/3.5-Git.html#how-it-works",
    "title": "Getting to Grips with Git",
    "section": "How It Works",
    "text": "How It Works\nThe natural way normal people think about managing versions of a document is to save a copy with a new name that somehow shows which version is most recent.\nThe natural way developers used to think about managing versions of a document is to have a master copy somewhere. Everyone asks the server for the master copy, makes some changes, and then checks those changes back in.\nThis is not how Git works.\n\nThe way normal people approach this problem assumes that, usually, only one or two people are making changes. But how do you coordinate with 20 other people to find out who has the most recent copy then collect all 21 people’s changes?\nThe way developers used to approach this problem assumes that someone is in final charge. That a company or organisation runs a server which will decide whose changes are allowed, and whose are not."
  },
  {
    "objectID": "lectures/3.5-Git.html#how-git-works",
    "href": "lectures/3.5-Git.html#how-git-works",
    "title": "Getting to Grips with Git",
    "section": "How Git Works",
    "text": "How Git Works\nGit is distributed, meaning that every computer where git is installed has its own master copy.\nSo every computer has a full history of any git project (aka. repository or ‘repo’). Indeed, you don’t have to synchronise your repo with any other computer or server at all! 1\n\nIn order to make this useful, you need ways to synchronise changes between computers that all think they’re right.\n\nI’d suggest that this is leaving the benefit of free backups on the table for no good reason!"
  },
  {
    "objectID": "lectures/3.5-Git.html#github",
    "href": "lectures/3.5-Git.html#github",
    "title": "Getting to Grips with Git",
    "section": "GitHub",
    "text": "GitHub\nGitHub is nothing special to Git, just another Git server with which to negotiate changes. Do not think of GitHub as the ‘master’ copy. There isn’t one.\nThere are, however, upstream and remote repositories.\n\nAn ‘upstream’ repository is where there’s a ‘gatekeeper’: e.g. the people who run PySAL have a repo that is considered the ‘gatekeeper’ for PySAL.\nA remote repository is any repository with which your copy synchronises. So the remote repository can be ‘upstream’ or it can just be another computer you run, or you GitHub account."
  },
  {
    "objectID": "lectures/3.5-Git.html#a-dropbox-analogy",
    "href": "lectures/3.5-Git.html#a-dropbox-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Dropbox Analogy",
    "text": "A Dropbox Analogy\n\nThink of JupyterLab as being like Word or Excel: an application that allows you to read/write/edit notebook files.\nThink of GitHub as being like Dropbox: a place somewhere in the cloud that files on your home machine can be backed up.\n\nBut Dropbox doesn’t have the .gitignore file!\n\nGitHub offers a lot of ‘value added’ features (like simple text editing) on top of the basic service of ‘storing files’."
  },
  {
    "objectID": "lectures/3.5-Git.html#getting-started",
    "href": "lectures/3.5-Git.html#getting-started",
    "title": "Getting to Grips with Git",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\nTerm\nMeans\n\n\n\n\nRepository (Repo)\nA project or achive stored in Git.\n\n\ninit\nTo create a new repo on your computer.\n\n\nclone\nTo make a full copy of a repo somewhere else.\n\n\n\nThis creates a local repo that is unsynchronised with anything else:\nmkdir test\ncd test\ngit init\nWhereas this creates a local clone that is fully synchronised with GitHub:\ncd .. # To move out of 'test'\ngit clone https://github.com/jreades/fsds.git"
  },
  {
    "objectID": "lectures/3.5-Git.html#working-on-a-file",
    "href": "lectures/3.5-Git.html#working-on-a-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a File",
    "text": "Working on a File\n\n\n\nTerm\nMeans\n\n\n\n\nadd\nAdd a file to a repo.\n\n\nmv\nMove/Rename a file in a repo.\n\n\nrm\nRemove a file from a repo.\n\n\n\nFor example:\ncd test # Back into the new Repo\ntouch README.md # Create empty file called README.md\ngit add README.md # Add it to the repository\ngit mv README.md fileA.md # Rename it (move it)\ngit rm fileA.md # Remove it... which is an Error!\nThis produces:\nerror: the following file has changes staged in the index:\n    fileA.md\n(use --cached to keep the file, or -f to force removal)\n\nThis is telling you that you can force remove (git rm -f fileA.md) if you really want, but you’d probably be better off commiting the changes that have been ‘staged’… more on this in a second!\nAlso: no one else knows about these changes yet!"
  },
  {
    "objectID": "lectures/3.5-Git.html#looking-at-the-history",
    "href": "lectures/3.5-Git.html#looking-at-the-history",
    "title": "Getting to Grips with Git",
    "section": "Looking at the History",
    "text": "Looking at the History\n\n\n\nTerm\nMeans\n\n\n\n\ndiff\nShow changes between commits.\n\n\nstatus\nShow status of files in repo.\n\n\nlog\nShow history of commits.\n\n\n\nFor example:\ncd ../test/ # In case you weren't already there\ngit status  # What's the status\nThis produces:\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   fileA.md\n\nSo again, git is giving us hints as to the options: ‘changes to be committed’ vs. ‘unstage’ the changes. We can also see what files are to be committed (i.e. have changed)."
  },
  {
    "objectID": "lectures/3.5-Git.html#working-on-a-project-or-file",
    "href": "lectures/3.5-Git.html#working-on-a-project-or-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a Project or File",
    "text": "Working on a Project or File\n\n\n\nTerm\nMeans\n\n\n\n\ncommit\nTo record changes to the repo.\n\n\nbranch\nCreate or delete branches.\n\n\ncheckout\nJump to a different branch.\n\n\n\nFor example:\ngit commit -m \"Added and then renamed the README.\"\ngit status\nYou should see:\n[master (root-commit) e7a0b25] Added and then renamed the README Markdown file.\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 fileA.md\n# ... and then this:\nOn branch master\nnothing to commit, working tree clean\nMake a note of the number after ‘root-commit’!"
  },
  {
    "objectID": "lectures/3.5-Git.html#recovery",
    "href": "lectures/3.5-Git.html#recovery",
    "title": "Getting to Grips with Git",
    "section": "Recovery",
    "text": "Recovery\ngit rm fileA.md\ngit status\ngit commit -m \"Removed file.\"\nls \ngit checkout &lt;number you wrote down earlier&gt;\nls \n\nSo every operation on a file is recorded in the repository: adding, renaming, deleting, and so on. And we can roll back any change at any time. For plain-text files (such as Markdown, Python and R scripts) these changes are recorded at the level of each line of code: so you can jump around through your entire history of a project and trace exactly when and what changes you (or anyone else) made."
  },
  {
    "objectID": "lectures/3.5-Git.html#a-rock-climbing-analogy",
    "href": "lectures/3.5-Git.html#a-rock-climbing-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Rock-Climbing Analogy",
    "text": "A Rock-Climbing Analogy"
  },
  {
    "objectID": "lectures/3.5-Git.html#collaborating-on-a-project",
    "href": "lectures/3.5-Git.html#collaborating-on-a-project",
    "title": "Getting to Grips with Git",
    "section": "Collaborating on a Project",
    "text": "Collaborating on a Project\n\n\n\nTerm\nMeans\n\n\n\n\npull\nTo request changes on a repo from another computer.\n\n\npush\nTo send changes on a repo to another computer.\n\n\n\nFor example:\ngit push"
  },
  {
    "objectID": "lectures/3.5-Git.html#a-note-on-workflow",
    "href": "lectures/3.5-Git.html#a-note-on-workflow",
    "title": "Getting to Grips with Git",
    "section": "A Note on Workflow",
    "text": "A Note on Workflow\nSo your workflow should be:\n\nSave edits to Jupyter notebook.\nRun git add &lt;filename.ipynb&gt; to record changes to the notebook (obviously replace &lt;filename.ipynb&gt; completely with the notebook filename.\nRun git commit -m \"Adding notes based on lecture\" (or whatever message is appropriate: -m means ‘message’).\nThen run git push to push the changes to GitHub.\n\nIf any of those commands indicate that there are no changes being recorded/pushed then it might be that you’re not editing the file that you think you are (this happens to me!).\nOn the GitHub web site you may need to force reload the view of the repository: Shift + Reload button usually does it in most browsers. You may also need to wait 5 to 10 seconds for the changes to become ‘visible’ before reloading. It’s not quite instantaeous."
  },
  {
    "objectID": "lectures/3.5-Git.html#this-is-not-easy",
    "href": "lectures/3.5-Git.html#this-is-not-easy",
    "title": "Getting to Grips with Git",
    "section": "This is not easy1",
    "text": "This is not easy1\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/3.5-Git.html#resources",
    "href": "lectures/3.5-Git.html#resources",
    "title": "Getting to Grips with Git",
    "section": "Resources",
    "text": "Resources\n\nUnderstanding Git (Part 1) – Explain it Like I’m Five\nTrying Git\nVisualising Git\nGit Novice\nConfusing Git Terminology\nGit Cheat Sheet: Commands and Best Practices\nAndy’s R-focussed Tutorial\n\n\nI now have everything in Git repos: articles, research, presentations, modules… the uses are basically endless once you start using Markdown heavily (even if you don’t do much coding).\n\n\n\n\n\nGetting to Grips with Git • Jon Reades"
  },
  {
    "objectID": "lectures/4.3-Packages.html#popular-packages",
    "href": "lectures/4.3-Packages.html#popular-packages",
    "title": "Packages",
    "section": "Popular Packages",
    "text": "Popular Packages\n\n\n\nPackage\nDoes\n\n\n\n\nos\nOperating System stuff (paths, processes, etc.)\n\n\ncsv\nReading and Writing CSV files.\n\n\nmath\nMaths constants and functions.\n\n\nstatistics\nSimple statistical tests & measures.\n\n\nurllib\nURL Reading (e.g. web pages).\n\n\nnumpy\nNumerical Python (scientific computing).\n\n\nscipy\nScientific Python (more scientific computing)\n\n\nsklearn\nMachine learning, clustering, etc."
  },
  {
    "objectID": "lectures/4.3-Packages.html#importing-a-package",
    "href": "lectures/4.3-Packages.html#importing-a-package",
    "title": "Packages",
    "section": "Importing a Package",
    "text": "Importing a Package\nIf a package is installed, then it’s as simple as:\nimport &lt;packagename&gt;\nYou normally do this at the start of a program so that it’s easy to see what the program requires to run:\nimport math\nprint(math.pi) # Prints 3.141592653589793"
  },
  {
    "objectID": "lectures/4.3-Packages.html#what-can-a-package-do",
    "href": "lectures/4.3-Packages.html#what-can-a-package-do",
    "title": "Packages",
    "section": "What Can a Package Do?",
    "text": "What Can a Package Do?\nThere are many ways to find this out:\n\nRead the documentation\nSearch Google, and\nSearch StackOverflow\n\nThere’s even a web site python.readthedocs.io.\nBut we can also ask the package:\nimport math\ndir(math)\n# ['__doc__', '__file__', '__name__', '__package__', ..., \n# 'log', 'log10', 'log1p', 'modf', 'pi', ...]\nhelp(math.log10)\n\nSo, remember what I said in the talk about functions: that the numbers and total variables in the function aren’t the same as the ones outside the function? I said the answer had to do with something called the namespace. Here is the math namespace."
  },
  {
    "objectID": "lectures/4.3-Packages.html#so",
    "href": "lectures/4.3-Packages.html#so",
    "title": "Packages",
    "section": "So…",
    "text": "So…\n\ndir(&lt;package name&gt;) lists all ‘things’ that &lt;package&gt; contains.\nBy convention, things that start with __ are ‘private’ (you shouldn’t change them) and things that start and end with __ are metadata (e.g. __version__).\nEverything else you can interrogate with help(&lt;package name&gt;.&lt;thing in package&gt;).\n\nWith help(math.log10) you get an answer like this:\nHelp on built-in function log10 in module math:\n\nlog10(x, /)\n    Return the base 10 logarithm of x.\nWith help(math.pi) you get an answer Help on float object…\n\nThis tells you that pi is a float, it doesn’t tell you what Pi is (an irrational number). So here’s another case where the computer gives you a technically correct but not always helpful answer. In the context of the math package, Pi is a float constant."
  },
  {
    "objectID": "lectures/4.3-Packages.html#why-namespaces-matter",
    "href": "lectures/4.3-Packages.html#why-namespaces-matter",
    "title": "Packages",
    "section": "Why Namespaces Matter",
    "text": "Why Namespaces Matter\nConsider this:\nimport math\npi = 4\nprint(math.pi)\nprint(pi)\nSo math.pi and pi are not the same variable!\n\nThe latter is implicitly main.pi but Python doesn’t have a ‘main’ programme namespace. Everything is ‘main’ unless you put it in a package."
  },
  {
    "objectID": "lectures/4.3-Packages.html#more-laziness-aliases",
    "href": "lectures/4.3-Packages.html#more-laziness-aliases",
    "title": "Packages",
    "section": "More Laziness: Aliases",
    "text": "More Laziness: Aliases\nProgrammers hate typing more than they have to:\nimport math\nr = 5\narea = math.pi * r**2\nln = math.log(area)\nprint(ln)\nSo we can use an alias instead:\nimport math as m\nr = 5\narea = m.pi * r**2\nln = m.log(area)\nprint(ln)\nYou will see this used a lot with more complex libraries like Pandas (pd), Geopandas (gpd), and PySAL (ps)."
  },
  {
    "objectID": "lectures/4.3-Packages.html#importing-part-of-a-package",
    "href": "lectures/4.3-Packages.html#importing-part-of-a-package",
    "title": "Packages",
    "section": "Importing Part of a Package",
    "text": "Importing Part of a Package\nSometimes even that is too much typing… or sometimes we only really want one or two things from a much larger package. In that case we can select these specifically:\nfrom math import pi, log10\nprint(pi)\nhelp(log10)\nThis import pi and log10 from math into the ‘main’ namespace."
  },
  {
    "objectID": "lectures/4.3-Packages.html#gotcha",
    "href": "lectures/4.3-Packages.html#gotcha",
    "title": "Packages",
    "section": "Gotcha!",
    "text": "Gotcha!\nNotice the subtle differences here:\n\n\nApproach 1\npi = 3.1415\nprint(pi)      # 3.1415\n\nimport math as m\nprint(m.pi)    # 3.141592...\nprint(pi)      # 3.1415\nprint(math.pi) # Error!\n\nApproach 2\npi = 3.1415\nprint(pi) # 3.1415\n\nfrom math import pi\nprint(pi)      # 3.141592...\nprint(m.pi)    # Error!\nprint(math.pi) # Error!"
  },
  {
    "objectID": "lectures/4.3-Packages.html#packages-make-your-life-easier",
    "href": "lectures/4.3-Packages.html#packages-make-your-life-easier",
    "title": "Packages",
    "section": "Packages Make Your Life Easier",
    "text": "Packages Make Your Life Easier"
  },
  {
    "objectID": "lectures/4.3-Packages.html#resources",
    "href": "lectures/4.3-Packages.html#resources",
    "title": "Packages",
    "section": "Resources",
    "text": "Resources\nA bit of a mish-mash of different explanations:\n\n\n\nLearnPython.org\nRealPython: Namespaces and Scope\nReal Python: Modules and Packages\nProgramiz\nPythonCourse.eu\nTutorialsTeacher.com\nTutorialsPoint.com\n\n\n\nHow to Make a Package in Python\nHow to Create Your First Python Package\nCreate Python Packages for Your Python Code [This is more for distributing your own packages via Pip]\n\n\n\n\n\n\nPackages • Jon Reades"
  },
  {
    "objectID": "lectures/5.3-Design.html#tree-of-life",
    "href": "lectures/5.3-Design.html#tree-of-life",
    "title": "Object-Oriented Design",
    "section": "Tree of Life",
    "text": "Tree of Life"
  },
  {
    "objectID": "lectures/5.3-Design.html#tree-of-vehicles",
    "href": "lectures/5.3-Design.html#tree-of-vehicles",
    "title": "Object-Oriented Design",
    "section": "Tree of Vehicles",
    "text": "Tree of Vehicles\nMost people would call this a class hierarchy or diagram.\n\nThere is no natural order here: where do e-bikes, unicycles, and rickshaws go?\n\nIndeed, we could map out vehicles based on the number of axles, their source of power, their driver positioning, etc., etc. The point here is that the designer must make choices that are influenced by (and will influence) the design of the application for which this class hierarchy is buing developed. An automobile manufacturer might make different choices from a government trying to implement a tax policy."
  },
  {
    "objectID": "lectures/5.3-Design.html#classes-vs-packages",
    "href": "lectures/5.3-Design.html#classes-vs-packages",
    "title": "Object-Oriented Design",
    "section": "Classes vs Packages",
    "text": "Classes vs Packages\n\nFunctionally, a class and a package are indistinguishable, but a class produces objects that use methods and instance or class variables, whereas a package is a group of functions and constants that may, or may not, include classes.\n\nUgh, now try to keep this straight in your head."
  },
  {
    "objectID": "lectures/5.3-Design.html#key-takeaways",
    "href": "lectures/5.3-Design.html#key-takeaways",
    "title": "Object-Oriented Design",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nYou’ve been using Classes and Methods since you started.\nYou can ‘package up’ useful code into functions, and useful functions into packages.\nTogether, packages and classes will turbo-charge your programming skills.\nYou can stand on the shoulders of giants!"
  },
  {
    "objectID": "lectures/5.3-Design.html#resources",
    "href": "lectures/5.3-Design.html#resources",
    "title": "Object-Oriented Design",
    "section": "Resources",
    "text": "Resources\n\nWhat is object-oriented programming?\nPython object-oriented programming\nObject-oriented programming refresher\nUnderstanding inheritance\nAbstract base classes\nPython - Object-Oriented\n\n\n\n\n\nObject-Oriented Design • Jon Reades"
  },
  {
    "objectID": "lectures/6.1-Logic.html#do-you-want",
    "href": "lectures/6.1-Logic.html#do-you-want",
    "title": "Logic",
    "section": "Do You Want?",
    "text": "Do You Want?\n\n\nConsider the following three things I’ve said to my toddler:\n\nDo you want to go to the park and have ice cream?\nDo you want cereal or toast for breakfast?\nDo not touch that again or else!\n\nUnlike my toddler, the computer does listen, but my toddler has more common sense!"
  },
  {
    "objectID": "lectures/6.1-Logic.html#in-code-now",
    "href": "lectures/6.1-Logic.html#in-code-now",
    "title": "Logic",
    "section": "In Code Now",
    "text": "In Code Now\n(x,y) = True,False\nif x:\n  print(\"x\")\n\nif x and y:\n  print(\"x and y\")\n\nif x or y:\n  print(\"x or y\")\n\nif x and not y:\n  print(\"x and not y\")\n\nif not(x and y):\n  print(\"not x and y\")\n\nif x is the simplest: if True then print \"x\". There is no logic, we just do it or we don’t depending on whether x is True.\nif x and y is the ‘park and ice cream question’: there’s no ice cream without going to the park. So both things have to be True for this to work out happily and print(\"x and y\").\nif x or y is the ‘cereal or toast for breakfast’ question: my daughter is having breakfast either way, so if she answers ‘Yes’ (True) to either then it doesn’t matter that the other one is False, she’s still had breakfast, or we’re still printing “x or y”. And note, if she was really hungry and said yes to both then that would also be breakfast. So if they are both True that’s also fine.\nif x and not y: is the negation of one term. So ‘if you behave and don’t touch that again then you will get a treat’ is what we’re looking for here. But you can also negate the entire thing: not(x and y) is also valid logic. So this turns a False on if x and y into a True."
  },
  {
    "objectID": "lectures/6.1-Logic.html#combining-logic-with-operators",
    "href": "lectures/6.1-Logic.html#combining-logic-with-operators",
    "title": "Logic",
    "section": "Combining Logic With Operators",
    "text": "Combining Logic With Operators\nRemember that operators like &lt;= and == also produce True/False answers:\nx = y = 5\nz = 3\nif x==y:\n  print(\"x==y\")\n\nif x==y and x==z:\n  print(\"x==y and x==z\")\n\nif x==y or x==z:\n  print(\"x==y or x==z\")\n\nif x==y and not x==z:\n  print(\"x==y and not x==z\")"
  },
  {
    "objectID": "lectures/6.1-Logic.html#a-special-case",
    "href": "lectures/6.1-Logic.html#a-special-case",
    "title": "Logic",
    "section": "A Special Case",
    "text": "A Special Case\nThere is a second set of logical operators that apply in very specific circumstances. These are called ‘bitwise’ operators and apply to data specified in bits.\n\n\n\nRegular Operator\nBitwise Equivalent\n\n\n\n\nand\n&\n\n\nor\n|\n\n\nnot\n~\n\n\n\nLet’s see (briefly) how these work…"
  },
  {
    "objectID": "lectures/6.1-Logic.html#working-with-bits",
    "href": "lectures/6.1-Logic.html#working-with-bits",
    "title": "Logic",
    "section": "Working With Bits",
    "text": "Working With Bits\nx,y = 38,3\nprint(f\"{x:b}\") # `:b` means byte-format\nprint(f\"{y:b}\")\nThis gives us that x is '100110' and y is '11', so now:\nprint(f\"{x & y:b}\")  # 10\nprint(f\"{x | y:b}\")  # 100111\nprint(f\"{x & ~y:b}\") # 100100"
  },
  {
    "objectID": "lectures/6.1-Logic.html#perhaps-easier-to-see-this-way",
    "href": "lectures/6.1-Logic.html#perhaps-easier-to-see-this-way",
    "title": "Logic",
    "section": "Perhaps Easier to See This Way?",
    "text": "Perhaps Easier to See This Way?\n\n\n\nOperator\n1\n2\n3\n4\n5\n6\n\n\n\n\nx\n1\n0\n0\n1\n1\n0\n\n\ny\n0\n0\n0\n0\n1\n1\n\n\nx & y\n0\n0\n0\n0\n1\n0\n\n\nx | y\n1\n0\n0\n1\n1\n1\n\n\n~y\n1\n1\n1\n1\n0\n0\n\n\nx & ~y\n1\n0\n0\n1\n0\n0\n\n\n\nBitwise operations are very, very fast and so are a good way to, say, find things in large data sets. You’ve been warned.\n\nThey are how pandas and numpy manage indexes and queries against data frames."
  },
  {
    "objectID": "lectures/6.1-Logic.html#nulls-none-vs.-nan",
    "href": "lectures/6.1-Logic.html#nulls-none-vs.-nan",
    "title": "Logic",
    "section": "Nulls: None vs. NaN",
    "text": "Nulls: None vs. NaN\nBeware of using logic with things that are not what they appear:\n\nNone is Python’s way of saying that something has no value at all (not 0 or \"\"… but None). It is a class.\nNaN (Not a Number) is a special numeric data type provided by the numpy package to deal with things like -ve and +ve infinity and similar ‘issues’.\n\nnp.nan should be used whenever you are dealing with data (e.g. see Pandas!)."
  },
  {
    "objectID": "lectures/6.1-Logic.html#none-vs.-nan",
    "href": "lectures/6.1-Logic.html#none-vs.-nan",
    "title": "Logic",
    "section": "None vs. NaN",
    "text": "None vs. NaN\nimport numpy as np\nprint(type(np.nan))     # float\nprint(type(None))       # NoneType\nCritically:\nprint(\"\"==None)         # False\nprint(None==None)       # True\nprint(np.nan==None)     # False\nprint(np.nan==np.nan)   # False!\nprint(np.nan is np.nan) # True\nprint(np.isnan(np.nan)) # True"
  },
  {
    "objectID": "lectures/6.1-Logic.html#in-not-in",
    "href": "lectures/6.1-Logic.html#in-not-in",
    "title": "Logic",
    "section": "In / Not In",
    "text": "In / Not In\nWe’ve touched on these before:\ng = ['Harvey','Rose','Batty','Jefferson']\n\nif 'Batty' in g:\n  print(\"In the group!\")\n\nif 'Marx' not in g:\n  print(\"Not in the group!\")\nThe set data type also supports in, and not in together with all of the set maths (union, intersect, etc.).\n\nThis is a good place for a recap though."
  },
  {
    "objectID": "lectures/6.1-Logic.html#sets",
    "href": "lectures/6.1-Logic.html#sets",
    "title": "Logic",
    "section": "Sets",
    "text": "Sets\nMembership maths:\ns1 = {'cherry','orange','banana','tomato'} # Or s1(...)\ns2 = {'potato','celery','carrot','tomato'} # Or s2(...)\nprint('potato' in s1)      # False\nprint(s1.difference(s2))   # {'banana', ...}\nprint(s1.intersection(s2)) # {'tomato'}\nprint(s1.union(s2))        # {'orange', ...}\n\nThese more advanced functions are only for sets, not lists or dicts."
  },
  {
    "objectID": "lectures/6.1-Logic.html#resources",
    "href": "lectures/6.1-Logic.html#resources",
    "title": "Logic",
    "section": "Resources",
    "text": "Resources\n\n\n\nLogical operators: And, or, not\nComparison operators\nBitwise operators\nComparison operators\n\n\n\nBoolean operators\nOperator precedence\nNaN and None in Python\nHandling Missing Data\n\n\n\n\n\n\nLogic • Jon Reades"
  },
  {
    "objectID": "lectures/6.3-Files.html#from-files-to-data",
    "href": "lectures/6.3-Files.html#from-files-to-data",
    "title": "File Formats",
    "section": "From Files to Data",
    "text": "From Files to Data\nIn order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data set have row or column names? (a.k.a. headers & metadata)\nIs the metadata in a separate file or embedded in the file?"
  },
  {
    "objectID": "lectures/6.3-Files.html#structure-of-a-tabular-data-file",
    "href": "lectures/6.3-Files.html#structure-of-a-tabular-data-file",
    "title": "File Formats",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names make it a lot easier to find and refer to data (e.g. the ‘East of England row’ or the ‘Total column’) but they are not data and don’t belong in the data set itself.\nUsually, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ’carriage return (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nUsually, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,) which gives rise to CSV (Comma-Separate Values), but it could be tabs (\\t) or anything else too (; or | or EOF).\n\nHow would we choose a good field separator?\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/6.3-Files.html#most-common-formats",
    "href": "lectures/6.3-Files.html#most-common-formats",
    "title": "File Formats",
    "section": "Most Common Formats",
    "text": "Most Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/6.3-Files.html#mapping-data-types",
    "href": "lectures/6.3-Files.html#mapping-data-types",
    "title": "File Formats",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often see the term ‘mapping’ used in connection to data that is not spatial, what do they mean? A map is the term used in some programming languages for a dict! So it’s about key : value pairs again.\nHere’s a mapping\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/6.3-Files.html#testing-a-mapping",
    "href": "lectures/6.3-Files.html#testing-a-mapping",
    "title": "File Formats",
    "section": "Testing a Mapping",
    "text": "Testing a Mapping\nWorking out an appropriate mapping (representation of the data) is hugely time-consuming.\n\nIt’s commonly held that 80% of data science is data cleaning.\n\nThe Unix utilities (grep, awk, tail, head) can be very useful for quickly exploring the data in order to develop a basic understanding of the data and to catch obvious errors.\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/6.3-Files.html#label-these",
    "href": "lectures/6.3-Files.html#label-these",
    "title": "File Formats",
    "section": "Label These",
    "text": "Label These\n\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/6.3-Files.html#things-that-can-go-wrong",
    "href": "lectures/6.3-Files.html#things-that-can-go-wrong",
    "title": "File Formats",
    "section": "Things That Can Go Wrong…",
    "text": "Things That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/6.3-Files.html#arrow-and-parquet",
    "href": "lectures/6.3-Files.html#arrow-and-parquet",
    "title": "File Formats",
    "section": "Arrow and Parquet",
    "text": "Arrow and Parquet\n\nArrow is an in-memory columnar format for data. Data is stored in a structured way in RAM making it blazingly fast for operations.\nParquet is a highly-compressed columnar file format for data. Data is stored in a structured way on your hard drive.\nFeather is a raw storage format for Arrow.\n\nTL;DR: for most applications Parquet will give nice, small files on disk and the benefits of columnar file storage; for computationally intensive applications where disk space and interoperability with other systems isn’t an issue then Feather might work."
  },
  {
    "objectID": "lectures/6.3-Files.html#what-about-the-duck",
    "href": "lectures/6.3-Files.html#what-about-the-duck",
    "title": "File Formats",
    "section": "What About the Duck?",
    "text": "What About the Duck?\n\n\n\n\n\nServerless SQL queries against Parquet files\nQueries returned as Pandas data frames\nSelect and filter before loading\nFast conversion between CSV and Parquet via Arrow"
  },
  {
    "objectID": "lectures/6.3-Files.html#for-later-reference",
    "href": "lectures/6.3-Files.html#for-later-reference",
    "title": "File Formats",
    "section": "For (Later) Reference",
    "text": "For (Later) Reference\n# Notice the engine and dtype_backend options\ndf = pandas.read_csv(fname, engine='pyarrow', \n                dtype_backend='pyarrow')\n\n# And for parquet files\ndf = pandas.read_parquet(fname, columns=[...])\n\n# And for DuckDB we can actually joing two\n# files before they even get to Python!\nq = f'''\n  SELECT * FROM \n    read_parquet('epc-ppd-2022-*.parquet') as ppd, \n    read_parquet('epc-ldd-2022-*.parquet') as ldd,\n  WHERE ppd.uid=ldd.uid\n'''\ndf = duckdb.query(q).df()\nP.S. There’s also a command-line tool for DuckDB so you don’t even need Python."
  },
  {
    "objectID": "lectures/6.3-Files.html#resources",
    "href": "lectures/6.3-Files.html#resources",
    "title": "File Formats",
    "section": "Resources",
    "text": "Resources\n\n\n\nUnderstanding Directories and Subdirectories\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\n\n\n\nText vs. binary mode\nText files\npetl\npandas 2.0 and the Arrow revolution (Part 1)\nWhat parquet files are my preferred API for bulk open data\nDuckDB Documentation\n\n\n\n\n\n\nFile Formats • Jon Reades"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#two-cultures",
    "href": "lectures/7.1-Mapping.html#two-cultures",
    "title": "Computational Mapping",
    "section": "Two Cultures",
    "text": "Two Cultures\n\n\nPurely Computational\nBoth analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\nMostly Computational\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#the-challenge",
    "href": "lectures/7.1-Mapping.html#the-challenge",
    "title": "Computational Mapping",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#constituency-cards",
    "href": "lectures/7.1-Mapping.html#constituency-cards",
    "title": "Computational Mapping",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nClone and reproduce: github.com/alasdairrae/wpc and explanation: cconstituency cards."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/7.1-Mapping.html#short-term-lets-in-scotland",
    "title": "Computational Mapping",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#every-building-in-america",
    "href": "lectures/7.1-Mapping.html#every-building-in-america",
    "title": "Computational Mapping",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#think-it-through",
    "href": "lectures/7.1-Mapping.html#think-it-through",
    "title": "Computational Mapping",
    "section": "Think it Through!",
    "text": "Think it Through!"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/7.1-Mapping.html#a-deceptively-simple-problem",
    "title": "Computational Mapping",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#classification",
    "href": "lectures/7.1-Mapping.html#classification",
    "title": "Computational Mapping",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment-1",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment-1",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment-2",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment-2",
    "title": "Computational Mapping",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#consider",
    "href": "lectures/7.1-Mapping.html#consider",
    "title": "Computational Mapping",
    "section": "Consider",
    "text": "Consider\nWe want to:\n\nGroup features with similar values together.\nShow these in a way that doesn’t mislead the viewer.\n\nBut we have the following problems:\n\nToo many classes confuse the viewer.\nToo few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#choices-choices",
    "href": "lectures/7.1-Mapping.html#choices-choices",
    "title": "Computational Mapping",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#look-at-the-data",
    "href": "lectures/7.1-Mapping.html#look-at-the-data",
    "title": "Computational Mapping",
    "section": "Look at the Data!",
    "text": "Look at the Data!\nDifferent colour and break schemes not only give us different views of the data, they give us different understandings of the data! Each scheme changes how the data looks and, consequently, how we perceive the distribution."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/7.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Computational Mapping",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#resources",
    "href": "lectures/7.1-Mapping.html#resources",
    "title": "Computational Mapping",
    "section": "Resources",
    "text": "Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation\nWorking with Spatial Data in Python\nWeb Mapping Notes\n\n\n\n\n\nComputational Mapping • Jon Reades"
  },
  {
    "objectID": "lectures/7.3-EDA.html#epicyclic-feedback",
    "href": "lectures/7.3-EDA.html#epicyclic-feedback",
    "title": "Exploratory Data Analysis",
    "section": "Epicyclic Feedback",
    "text": "Epicyclic Feedback\nPeng and Matsui, The Art of Data Science, p.8\n\n\n\n\n\n\n\n\n\n\nSet Expectations\nCollect Information\nRevise Expectations\n\n\n\n\nQuestion\nQuestion is of interest to audience\nLiterature search/experts\nSharpen question\n\n\nEDA\nData are appropriate for question\nMake exploratory plots\nRefine question or collect more data\n\n\nModelling\nPrimary model answers question\nFit secondary models / analysis\nRevise model to include more predictors\n\n\nInterpretation\nInterpretation provides specific and meaningful answer\nInterpret analyses with focus on effect and uncertainty\nRevise EDA and/or models to provide more specific answers\n\n\nCommunication\nProcess & results are complete and meaningful\nSeek feedback\nRevises anlyses or approach to presentation"
  },
  {
    "objectID": "lectures/7.3-EDA.html#approaching-eda",
    "href": "lectures/7.3-EDA.html#approaching-eda",
    "title": "Exploratory Data Analysis",
    "section": "Approaching EDA",
    "text": "Approaching EDA\nThere’s no hard and fast way of doing EDA, but as a general rule you’re looking to:\n\nClean\nCanonicalise\nClean More\nVisualise & Describe\nReview\nClean Some More\n…\n\nThe ‘joke’ is that 80% of Data Science is data cleaning.\n\nCleaning Part 1: testing validity of records (possibly while tracking rejected records for subsequent analysis)\nCanonicalisation: controling for variation (e.g. typos, capitalisation, formatting, leading/trailing whitespace, different types of NULL values, etc.) and in a spatial context deal with projection and geo-data issues.\nCleaning Part 2: further testing of records (e.g. deciding what to do with NaNs, missing values, outside of study area, etc.)\nVisualise & Describe: covered in QM but we’ll take a high-level look at this."
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-related-take",
    "href": "lectures/7.3-EDA.html#a-related-take",
    "title": "Exploratory Data Analysis",
    "section": "A Related Take",
    "text": "A Related Take\nEDA—Don’t ask how, ask what:\n\nDescriptive Statistics: get a high-level understanding of your dataset.\nMissing values: come to terms with how bad your dataset is.\nDistributions and Outliers: and why countries that insist on using different units make our jobs so much harder.\nCorrelations: and why sometimes even the most obvious patterns still require some investigating."
  },
  {
    "objectID": "lectures/7.3-EDA.html#another-take",
    "href": "lectures/7.3-EDA.html#another-take",
    "title": "Exploratory Data Analysis",
    "section": "Another Take",
    "text": "Another Take\nHere’s another view of how to do EDA:\n\nPreview data randomly and substantially\nCheck totals such as number of entries and column types\nCheck nulls such as at row and column levels\nCheck duplicates: do IDs recurr, did the servers fail\nPlot distribution of numeric data (univariate and pairwise joint distribution)\nPlot count distribution of categorical data\nAnalyse time series of numeric data by daily, monthly and yearly frequencies"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-is-it",
    "href": "lectures/7.3-EDA.html#what-is-it",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-is-it-1",
    "href": "lectures/7.3-EDA.html#what-is-it-1",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-is-it-2",
    "href": "lectures/7.3-EDA.html#what-is-it-2",
    "title": "Exploratory Data Analysis",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.3-EDA.html#start-with-a-chart",
    "href": "lectures/7.3-EDA.html#start-with-a-chart",
    "title": "Exploratory Data Analysis",
    "section": "Start with a Chart",
    "text": "Start with a Chart\nThe problem of relying on statistics alone was amply illustrated by Anscombe’s Quartet (1973)…\n\nWe are not very good at looking at spreadsheets.\nWe are very good at spotting patterns visually.\n\nSometimes, we are too good; that’s where the stats comes in. Think of it as the ‘tiger in the jungle’ problem.."
  },
  {
    "objectID": "lectures/7.3-EDA.html#anscombes-quartet",
    "href": "lectures/7.3-EDA.html#anscombes-quartet",
    "title": "Exploratory Data Analysis",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\n\nX1\nY1\nX2\nY2\nX3\nY3\nX4\nY4\n\n\n\n\n10.0\n8.04\n10.0\n9.14\n10.0\n7.46\n10.0\n6.58\n\n\n8.0\n6.95\n8.0\n8.14\n8.0\n6.77\n8.0\n5.76\n\n\n13.0\n7.58\n13.0\n8.74\n13.0\n12.74\n13.0\n7.71\n\n\n9.0\n8.81\n9.0\n8.77\n9.0\n7.11\n9.0\n8.84\n\n\n11.0\n8.33\n11.0\n9.26\n11.0\n7.81\n11.0\n8.47\n\n\n14.0\n9.96\n14.0\n8.10\n14.0\n8.84\n14.0\n7.04\n\n\n6.0\n7.24\n6.0\n6.13\n6.0\n6.08\n6.0\n5.25\n\n\n4.0\n4.26\n4.0\n3.10\n4.0\n5.39\n4.0\n12.5\n\n\n12.0\n10.84\n12.0\n9.13\n12.0\n8.15\n12.0\n5.56\n\n\n7.0\n4.82\n7.0\n7.26\n7.0\n6.42\n7.0\n7.91\n\n\n5.0\n5.68\n5.0\n4.74\n5.0\n5.73\n5.0\n6.89"
  },
  {
    "objectID": "lectures/7.3-EDA.html#summary-statistics-for-the-quartet",
    "href": "lectures/7.3-EDA.html#summary-statistics-for-the-quartet",
    "title": "Exploratory Data Analysis",
    "section": "Summary Statistics for the Quartet",
    "text": "Summary Statistics for the Quartet\n\n\n\nProperty\nValue\n\n\n\n\nMean of x\n9.0\n\n\nVariance of x\n11.0\n\n\nMean of y\n7.5\n\n\nVariance of y\n4.12\n\n\nCorrelation between x and y\n0.816\n\n\nLinear Model\ny = 3 + 0.5x"
  },
  {
    "objectID": "lectures/7.3-EDA.html#but-what-do-they-look-like",
    "href": "lectures/7.3-EDA.html#but-what-do-they-look-like",
    "title": "Exploratory Data Analysis",
    "section": "But What do They Look Like?",
    "text": "But What do They Look Like?"
  },
  {
    "objectID": "lectures/7.3-EDA.html#the-tiger-that-isnt",
    "href": "lectures/7.3-EDA.html#the-tiger-that-isnt",
    "title": "Exploratory Data Analysis",
    "section": "The Tiger that Isn’t",
    "text": "The Tiger that Isn’t\n\nI would argue that the basic purpose of charts and of statistics as a whole is to help us untangle signal from noise. We are ‘programmed’ to see signals, so we need to set the standard for ‘it’s a tiger!’ quite high in research & in policy-making.\n\n\nOr, as Albert Einstein reportedly said: “If I can’t picture it, I can’t understand it.”"
  },
  {
    "objectID": "lectures/7.3-EDA.html#think-it-through",
    "href": "lectures/7.3-EDA.html#think-it-through",
    "title": "Exploratory Data Analysis",
    "section": "Think it Through",
    "text": "Think it Through\nYou can make a lot of progress in your research without any advanced statistics!\n\nA ‘picture’ isn’t just worth 1,000 words, it could be a whole dissertation!\nThe right chart makes your case eloquently and succinctly.\n\nAlways ask yourself:\n\nWhat am I trying to say?\nHow can I say it most effectively?\nIs there anything I’m overlooking in the data?\n\nA good chart is a good way to start!"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-makes-a-good-plot",
    "href": "lectures/7.3-EDA.html#what-makes-a-good-plot",
    "title": "Exploratory Data Analysis",
    "section": "What Makes a Good Plot?",
    "text": "What Makes a Good Plot?\nA good chart or table:\n\nServes a purpose — it is clear how it advances the argument in a way that could not be done in the text alone.\nContains only what is relevant — zeroes in on what the reader needs and is not needlessly cluttered.\nUses precision that is meaningful — doesn’t clutter the chart with needless numbers.\n\n\nFar too many charts or tables could be easily written up in a single sentence.\nFar too many charts or tables contain redundancy, clutter, and ‘flair’.\nDon’t report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale."
  },
  {
    "objectID": "lectures/7.3-EDA.html#for-example",
    "href": "lectures/7.3-EDA.html#for-example",
    "title": "Exploratory Data Analysis",
    "section": "For Example…",
    "text": "For Example…\nHow much precision is necessary in measuring degrees at the equator?\n\n\n\nDecimal Places\nDegrees\nDistance\n\n\n\n\n0\n1\n111km\n\n\n1\n0.1\n11.1km\n\n\n2\n0.01\n1.11km\n\n\n3\n0.001\n111m\n\n\n4\n0.0001\n11.1m\n\n\n5\n0.00001\n1.11m\n\n\n6\n0.000001\n11.1cm\n\n\n7\n0.0000001\n1.11cm\n\n\n8\n0.00000001\n1.11mm"
  },
  {
    "objectID": "lectures/7.3-EDA.html#goals-by-world-cup-final",
    "href": "lectures/7.3-EDA.html#goals-by-world-cup-final",
    "title": "Exploratory Data Analysis",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/7.3-EDA.html#goals-by-world-cup-final-1",
    "href": "lectures/7.3-EDA.html#goals-by-world-cup-final-1",
    "title": "Exploratory Data Analysis",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/7.3-EDA.html#average-goals-by-world-cup-final",
    "href": "lectures/7.3-EDA.html#average-goals-by-world-cup-final",
    "title": "Exploratory Data Analysis",
    "section": "Average Goals by World Cup Final",
    "text": "Average Goals by World Cup Final\n\n\nIn 1982 the number of teams went from 16 to 24, and in 1998 it went from 24 to 32!"
  },
  {
    "objectID": "lectures/7.3-EDA.html#how-far-from-equality",
    "href": "lectures/7.3-EDA.html#how-far-from-equality",
    "title": "Exploratory Data Analysis",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/7.3-EDA.html#how-far-from-equality-1",
    "href": "lectures/7.3-EDA.html#how-far-from-equality-1",
    "title": "Exploratory Data Analysis",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/7.3-EDA.html#the-purpose-of-a-chart",
    "href": "lectures/7.3-EDA.html#the-purpose-of-a-chart",
    "title": "Exploratory Data Analysis",
    "section": "The Purpose of a Chart",
    "text": "The Purpose of a Chart\nThe purpose of a graph is to show that there are relationships within the data set that are not trivial/expected.\nChoose the chart to highlight relationships, or the lack thereof:\n\nThink of a chart or table as part of your ‘argument’ – if you can’t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably don’t need it.\nIdentify & prioritise the relationships in the data.\nChoose a chart type/chart symbology that gives emphasis to the most important relationships."
  },
  {
    "objectID": "lectures/7.3-EDA.html#not-everyone-likes-tables",
    "href": "lectures/7.3-EDA.html#not-everyone-likes-tables",
    "title": "Exploratory Data Analysis",
    "section": "Not Everyone Likes Tables",
    "text": "Not Everyone Likes Tables\n\nGetting information from a table is like extracting sunlight from a cucumber. Arthur & Henry Fahrquhar 1891)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#real-numbers",
    "href": "lectures/7.3-EDA.html#real-numbers",
    "title": "Exploratory Data Analysis",
    "section": "Real Numbers",
    "text": "Real Numbers\nConsider the difference in emphasis between:\n\n11316149\n11,316,149\n11.3 million\n11 x 10\\(^{6}\\)\n22%\n22.2559%\n\nAlways keep in mind the purpose of the number."
  },
  {
    "objectID": "lectures/7.3-EDA.html#theres-still-a-role-for-tables",
    "href": "lectures/7.3-EDA.html#theres-still-a-role-for-tables",
    "title": "Exploratory Data Analysis",
    "section": "There’s Still a Role for Tables",
    "text": "There’s Still a Role for Tables\nWhy a table is sometimes better than a chart:\n\nYou need to present data values with greater detail\nYou need to enable readers to draw comparisons between data values\nYou need to present the same data in multiple ways (e.g. raw number and percentage)\nYou want to show many dimensions for a small number of observations\n\n\ne.g. percentage of people falling into each ethnic or income category for a small number of wards or boroughs."
  },
  {
    "objectID": "lectures/7.3-EDA.html#undergraduate-tables-failing-grade",
    "href": "lectures/7.3-EDA.html#undergraduate-tables-failing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Undergraduate Tables (Failing Grade)",
    "text": "Undergraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#undergraduate-tables-passing-grade",
    "href": "lectures/7.3-EDA.html#undergraduate-tables-passing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Undergraduate Tables (Passing Grade)",
    "text": "Undergraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#postgraduate-tables-failing-grade",
    "href": "lectures/7.3-EDA.html#postgraduate-tables-failing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Postgraduate Tables (Failing Grade)",
    "text": "Postgraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#postgraduate-tables-passing-grade",
    "href": "lectures/7.3-EDA.html#postgraduate-tables-passing-grade",
    "title": "Exploratory Data Analysis",
    "section": "Postgraduate Tables (Passing Grade)",
    "text": "Postgraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#design-for-tables",
    "href": "lectures/7.3-EDA.html#design-for-tables",
    "title": "Exploratory Data Analysis",
    "section": "Design for Tables",
    "text": "Design for Tables\nPrinciples:\n\nReduce the number of lines to a minimum (and you should almost never need vertical lines).\nUse ‘white-space’ to create visual space between groups of unrelated (or less related) elements.\nRemove redundancy (if you find yourself typing ‘millions’ or ‘GBP’ or ‘Male’ repeatedly then you’ve got redundancy).\nEnsure that meta-data is clearly separate from, but attached to, the graph (i.e. source, title, etc.)."
  },
  {
    "objectID": "lectures/7.3-EDA.html#getting-started",
    "href": "lectures/7.3-EDA.html#getting-started",
    "title": "Exploratory Data Analysis",
    "section": "Getting Started",
    "text": "Getting Started\nYou can follow along by loading the Inside Airbnb sample:\nimport pandas as pd\nimport geopandas as gpd\nurl='https://bit.ly/3I0XDrq'\ndf = pd.read_csv(url)\ndf.set_index('id', inplace=True)\ndf['price'] = df.price.str.replace('$','',regex=False).astype('float')\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.to_file('Airbnb_Sample.gpkg', driver='GPKG')\n\nNote that this (re)loads the sampled Airbnb data from GitHub every time you run it. For a large data set on someone else’e server you might want to save and (re)load it locally. A simple helper function would then just check if the file already existed locally before trying to download the file again: that would allow you to work while offline and speed up your code substantially too!"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-can-we-do-series",
    "href": "lectures/7.3-EDA.html#what-can-we-do-series",
    "title": "Exploratory Data Analysis",
    "section": "What Can We Do? (Series)",
    "text": "What Can We Do? (Series)\nThis is by no means all that we can do…\n\nSeries-level Methods.\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(f\"Host count is {gdf.host_name.count()}\")\nprint(f\"Mean is {gdf.price.mean():.0f}\")\nprint(f\"Max price is {gdf.price.max()}\")\nprint(f\"Min price is {gdf.price.min()}\")\nprint(f\"Median price is {gdf.price.median()}\")\nprint(f\"Standard dev is {gdf.price.std():.2f}\")\nprint(f\"25th quantile is {gdf.price.quantile(q=0.25)}\")\nCount of non-nulls\nMean\nHighest value\nLowest value\nMedian\nStandard deviation\n25th quantile"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-can-we-do-data-frame",
    "href": "lectures/7.3-EDA.html#what-can-we-do-data-frame",
    "title": "Exploratory Data Analysis",
    "section": "What Can We Do? (Data Frame)",
    "text": "What Can We Do? (Data Frame)\n\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(df.mean())\nprint(df.count())\nprint(df.max())\n# ...\nprint(df.corr())\nprint(df.describe())\nMean of each column\nNumber of non-null values in each column\nHighest value in each column\n$\\vdots$\nCorrelation between columns\nSummarise\n\n\n\n\nNotice how we have the same functionality, but it operates at the level of the data set itself now. We gain a few new functions as well that relate to interactions between columns (a.k.a. data series)."
  },
  {
    "objectID": "lectures/7.3-EDA.html#measures",
    "href": "lectures/7.3-EDA.html#measures",
    "title": "Exploratory Data Analysis",
    "section": "Measures",
    "text": "Measures\nSo pandas provides functions for commonly-used measures:\nprint(f\"{df.price.mean():.2f}\")\nprint(f\"{df.price.median():.2f}\")\nprint(f\"{df.price.quantile(0.25):.2f}\")\nOutput:\n118.4542\n80.50\n40.75"
  },
  {
    "objectID": "lectures/7.3-EDA.html#more-complex-measures",
    "href": "lectures/7.3-EDA.html#more-complex-measures",
    "title": "Exploratory Data Analysis",
    "section": "More Complex Measures",
    "text": "More Complex Measures\nBut Pandas also makes it easy to derive new variables… Here’s the z-score:\n\\[ z = \\frac{x - \\bar{x}}{s}\\]\ndf['zscore'] = (df.price - df.price.mean())/df.price.std()\ndf.plot.box(column='zscore')"
  },
  {
    "objectID": "lectures/7.3-EDA.html#and-even-more-complex",
    "href": "lectures/7.3-EDA.html#and-even-more-complex",
    "title": "Exploratory Data Analysis",
    "section": "And Even More Complex",
    "text": "And Even More Complex\nAnd here’s the Interquartile Range Standardised score:\n\\[ x_{iqrs} = \\frac{x - \\widetilde{x}}{Q_{75} - Q_{25}} \\]\ndf['iqr_std'] = (df.price - df.price.median())/ \\\n      (df.price.quantile(q=0.75)-df.price.quantile(q=0.25))\ndf.plot.box(column='iqr_std')"
  },
  {
    "objectID": "lectures/7.3-EDA.html#the-plot-thickens",
    "href": "lectures/7.3-EDA.html#the-plot-thickens",
    "title": "Exploratory Data Analysis",
    "section": "The Plot Thickens",
    "text": "The Plot Thickens\nWe’ll get to more complex plotting over the course of the term, but here’s a good start for exploring the data! All plotting depends on matplotlib which is the ogre in the attic to R’s ggplot.\nimport matplotlib.pyplot as plt\nGet used to this import as it will allow you to save and manipulate the figures created in Python. It is not the most intuitive approach (unless you’ve used MATLAB before) but it does work."
  },
  {
    "objectID": "lectures/7.3-EDA.html#boxplot",
    "href": "lectures/7.3-EDA.html#boxplot",
    "title": "Exploratory Data Analysis",
    "section": "Boxplot",
    "text": "Boxplot\ndf.price.plot.box()\nplt.savefig('pboxplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#frequency",
    "href": "lectures/7.3-EDA.html#frequency",
    "title": "Exploratory Data Analysis",
    "section": "Frequency",
    "text": "Frequency\ndf.room_type.value_counts().plot.bar()\nplt.savefig('phistplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-correlation-heatmap",
    "href": "lectures/7.3-EDA.html#a-correlation-heatmap",
    "title": "Exploratory Data Analysis",
    "section": "A Correlation Heatmap",
    "text": "A Correlation Heatmap\nWe’ll get to these in more detail in a couple of weeks, but here’s some output…"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-map",
    "href": "lectures/7.3-EDA.html#a-map",
    "title": "Exploratory Data Analysis",
    "section": "A ‘Map’",
    "text": "A ‘Map’\ndf.plot.scatter(x='longitude',y='latitude')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-fancy-map",
    "href": "lectures/7.3-EDA.html#a-fancy-map",
    "title": "Exploratory Data Analysis",
    "section": "A Fancy ‘Map’",
    "text": "A Fancy ‘Map’\ndf.plot.scatter(x='longitude',y='latitude',\n                c='price',colormap='viridis',\n                figsize=(10,5),title='London',\n                grid=True,s=24,marker='x')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#an-actual-map",
    "href": "lectures/7.3-EDA.html#an-actual-map",
    "title": "Exploratory Data Analysis",
    "section": "An Actual ‘Map’",
    "text": "An Actual ‘Map’\ngdf.plot(column='price', cmap='viridis', \n         scheme='quantiles', markersize=8, legend=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#resources",
    "href": "lectures/7.3-EDA.html#resources",
    "title": "Exploratory Data Analysis",
    "section": "Resources",
    "text": "Resources\nThere’s so much more to find, but:\n\n\n\nPandas Reference\nA Guide to EDA in Python (Looks very promising)\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nEDA: DataPrep.eda vs Pandas-Profiling [Requires module installation]\n\n\n\nA Data Science Project for Beginners (EDA)\nEDA: A Pracitcal Guide and Template for Structured Data\nEDA—Don’t ask how, ask what (Part 1)\nPreparing your Dataset for Modeling – Quickly and Easily (Part 2)\nHandling Missing Data\nIntroduction to Exploratory Data Analysis (EDA)\n\n\n\n\n\n\nExploratory Data Analysis • Jon Reades"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "href": "lectures/8.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "title": "Note(book)s to Documents",
    "section": "Recall: Tangled Workflows",
    "text": "Recall: Tangled Workflows\nIt’s not just about mixing code and comment, we also want:\n\nTo separate content from presentation\nTo define mappings between presentation styles\nTo produce the best-quality output for the format chosen\n\n\n\nExamples of this include CSS for web sites, LaTeX templates, and Markdown styles.\nMVC approach to software design."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#pandoc",
    "href": "lectures/8.1-Notebooks_as_Documents.html#pandoc",
    "title": "Note(book)s to Documents",
    "section": "Pandoc",
    "text": "Pandoc\nTool for converting documents between formats, including:\n\nPlain Text/YAML\nMarkdown\nLaTeX/PDF\nHTML/Reveal.js\nJupyter Notebook\nXML/ODT/DOCX\nEPUB/DocBook\nBibTeX/JSON"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#latex",
    "href": "lectures/8.1-Notebooks_as_Documents.html#latex",
    "title": "Note(book)s to Documents",
    "section": "LaTeX",
    "text": "LaTeX\n\n\n\n\n\n\n\nIntended for type-setting of scientific documents, but has been used for slides, posters, CVs, etc. It is not a word processor, it’s more like a compiler.\nThis format is based on Edward Tufte’s VSQD and can be found on GitHub."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#latex-in-practice",
    "href": "lectures/8.1-Notebooks_as_Documents.html#latex-in-practice",
    "title": "Note(book)s to Documents",
    "section": "LaTeX in Practice",
    "text": "LaTeX in Practice\nYou write LaTeX in any text editor, but specialist apps like Texpad or Overleaf make it easier.\n\\documentclass[11pt,article,oneside]{memoir}\n\\newcommand{\\bl}{\\textsc{bl}~\\/}\n\\usepackage{tabularx}\n\n\\begin{document}\n\\maketitle \n\nThis report provides an overview of activities ...\n\n\\section{Applications}\nA primary objective was the submission...\n\nUCL has an institutional license for Overleaf.\nThis document is then compiled (or ‘typeset’) with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.\nCompilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#bibtex",
    "href": "lectures/8.1-Notebooks_as_Documents.html#bibtex",
    "title": "Note(book)s to Documents",
    "section": "BibTeX",
    "text": "BibTeX\nProvides bilbiographic support for LaTeX but widely used by other utilities as is also plain-text.\n@article{Lavin:2019,\n        Author = {Lavin, Matthew J.},\n        Doi = {10.46430/phen0082},\n        Journal = {The Programming Historian},\n        Number = {8},\n        Title = {Analyzing Documents with TF-IDF},\n        Year = {2019},\n        Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}\n\n@incollection{Kitchin:2016,\n        Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},\n        Booktitle = {Smart Urbanism},\n        Chapter = {2},\n        Editor = {Marvin, Luque-Ayala, McFarlane},\n        Title = {Smart Cities and the Politics of Urban Data},\n        Year = {2016}}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "href": "lectures/8.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "title": "Note(book)s to Documents",
    "section": "BibTeX in Practice",
    "text": "BibTeX in Practice\nTo reference a document we then need to tell LaTeX or Pandoc where to look:\n\\bibliographystyle{apacite} # LaTeX\n\\bibliography{Spatial_Data_Chapter.bib} # LaTeX\nWith citations following formats like:\n\\citep[p.22]{Reades2018} # LaTeX\nOr:\n[@dignazio:2020, chap. 4] # Markdown"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#reveal.js",
    "href": "lectures/8.1-Notebooks_as_Documents.html#reveal.js",
    "title": "Note(book)s to Documents",
    "section": "Reveal.js",
    "text": "Reveal.js\nJavaScript-based presentation framework. Can use Markdown to generate portable interactive slides including references/bibliographies.\nHow this presentation was created.\nCompare:\n\nMarkdown\nHTML\nReveal"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#headings",
    "href": "lectures/8.1-Notebooks_as_Documents.html#headings",
    "title": "Note(book)s to Documents",
    "section": "Headings",
    "text": "Headings\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n# Heading Level 1\n\\section{Heading Level 1}\n\n\n## Heading Level 2\n\\subsection{Heading Level 2}\n\n\n### Heading Level 3\n\\subsubsection{Heading Level 3}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#inline-elements",
    "href": "lectures/8.1-Notebooks_as_Documents.html#inline-elements",
    "title": "Note(book)s to Documents",
    "section": "Inline Elements",
    "text": "Inline Elements\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n1. Numbered item 1\n\\begin{enumerate} \\n \\item ... \\end{enumerate}\n\n\n- Bulleted list item 1\n\\begin{itemize} \\n \\item ... \\n \\end{itemize}\n\n\n_italics_ or *italics*\n\\emph{italics} or \\textit{italics}\n\n\n**bold**\n\\textbf{bold}\n\n\n&gt; blockquote\n\\begin{quote} \\n blockquote \\end{quote}\n\n\nSome `code` is here\nSome \\texttt{code} is here\n\n\n[Link Text](URL)\n\\href{Link Text}{URL}\n\n\n![Alt Text](Image URL)\n\\begin{figure}\\n \\includegraphics[opts]{...}\\n \\end{figure}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#mathematics",
    "href": "lectures/8.1-Notebooks_as_Documents.html#mathematics",
    "title": "Note(book)s to Documents",
    "section": "Mathematics",
    "text": "Mathematics\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\nSame, but either 1 or 2 $’s\n$x=5$\n\n\nSame, but either 1 or 2 $’s\n$\\pi$\n\n\nSame, but either 1 or 2 $’s\n$e = mc^{2}$\n\n\n\nWe can show all this directly in the Notebook! \\(\\pi\\); \\(e = mc^{2}\\); \\(\\int_{0}^{\\inf} x^2 \\,dx\\); \\(\\sum_{n=1}^{\\infty} 2^{-n} = 1\\)\n\n\nOverleaf has good documentation for most (basic) applications."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#resources",
    "href": "lectures/8.1-Notebooks_as_Documents.html#resources",
    "title": "Note(book)s to Documents",
    "section": "Resources",
    "text": "Resources\n\nJupyter Tips and Tricks\nPandoc Demos\nBeginner’s Guide to Jupyter Notebooks\n7 Essential Tips to Writing With Jupyter Notebooks\nVersion Control with Jupyter\nSustainable Publishing using Pandoc and Markdown\nMaking Pretty PDFs with Quarto\nHOw to use Quarto for Parameterised Reporting\n\n\n\n\n\nNote(book)s to Documents • Jon Reades"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "href": "lectures/8.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "title": "Cleaning Text",
    "section": "When a Loop Is Not Best",
    "text": "When a Loop Is Not Best\nIf you need to apply the same operation to lots of data why do it sequentially?\n\nYour computer has many cores and can run many threads in parallel.\nThe computer divides the work across the threads it sees fit.\nThe computer reassemble the answer at the end from the threads.\n\nIf you have 4 cores then parallelisation cuts analysis time by 75%."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "href": "lectures/8.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "title": "Cleaning Text",
    "section": "So Do More with Each Clock Cycle",
    "text": "So Do More with Each Clock Cycle\n\nMany libraries/packages implement weak forms of vectorisation or parallelisation, but some libraries do more.\nYou must request it because it requires hardware or other support and it is highly optimsed.\nMultiple separate machines acting as one.\nMultiple GPUs acting as one.\n\n\nConceptually, these get challenging if you can’t clearly separate/parallelise tasks."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "href": "lectures/8.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "title": "Cleaning Text",
    "section": "Pandas.apply() vs. Numpy",
    "text": "Pandas.apply() vs. Numpy\nNumpy is fully vectorised and will almost always out-perform Pandas apply, but both are massive improvements on for loops:\n\nExecute row-wise and column-wise operations.\nApply any arbitrary function to individual elements or whole axes (i.e. row or col).\nCan make use of lambda functions too for ‘one off’ operations (ad-ohoc functions)."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#lambda-functions",
    "href": "lectures/8.3-Cleaning_Text.html#lambda-functions",
    "title": "Cleaning Text",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nFunctional equivalent of list comprehensions: 1-line, anonymous functions.\nFor example:\nx = lambda a : a + 10\nprint(x(5)) # 15\nOr:\nfull_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\nprint(full_name('guido', 'van rossum')) # 'Guido Van Rossum'\nThese are very useful with pandas."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#lets-compare",
    "href": "lectures/8.3-Cleaning_Text.html#lets-compare",
    "title": "Cleaning Text",
    "section": "Let’s Compare",
    "text": "Let’s Compare\nimport time\nimport numpy as np\ndef func(a,b):\n  c = 0\n  for i in range(len(a)): c += a[i]*b[i]\n  return c\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nt1 = time.time()\nprint(func(a,b))\nt2 = time.time()\nprint(np.dot(a,b))\nt3 = time.time()\n\nprint(f\"For loop took {(t2-t1)*1000:.0f} milliseconds\")\nprint(f\"Numpy took {(t3-t2)*1000:.0f} milliseconds\")\nGenerally, I get numpy taking 86ms, while the for loop takes 331ms!\n\n\n/ht to The Last Byte for inspiration."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#beautiful-soup-selenium",
    "href": "lectures/8.3-Cleaning_Text.html#beautiful-soup-selenium",
    "title": "Cleaning Text",
    "section": "Beautiful Soup & Selenium",
    "text": "Beautiful Soup & Selenium\nTwo stages to acquiring web-based documents:\n\nAccessing the document: urllib can deal with many issues (even authentication), but not with dynamic web pages (which are increasingly common); for that, you need Selenium (library + driver).\nProcessing the document: simple data can be extracted from web pages with RegularExpressions, but not with complex (esp. dynamic) content; for that, you need BeautifulSoup4.\n\nThese interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#regular-expressions-breaks",
    "href": "lectures/8.3-Cleaning_Text.html#regular-expressions-breaks",
    "title": "Cleaning Text",
    "section": "Regular Expressions / Breaks",
    "text": "Regular Expressions / Breaks\nNeed to look at how the data is organised:\n\nFor very large corpora, you might want one document at a time (batch).\nFor very large files, you might want one line at a time (streaming).\nFor large files in large corpora, you might want more than one ‘machine’.\n\n\nSee the OpenVirus Project."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#starting-points",
    "href": "lectures/8.3-Cleaning_Text.html#starting-points",
    "title": "Cleaning Text",
    "section": "Starting Points",
    "text": "Starting Points\nThese strategies can be used singly or all-together:\n\nStopwords\nCase\nAccent-stripping\nPunctuation\nNumbers\n\nBut these are just a starting point!\n\nWhat’s the semantic difference between 1,000,000 and 999,999?"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#distributional-pruning",
    "href": "lectures/8.3-Cleaning_Text.html#distributional-pruning",
    "title": "Cleaning Text",
    "section": "Distributional Pruning",
    "text": "Distributional Pruning\nWe can prune from both ends of the distribution:\n\nOverly rare words: what does a word used in one document help us to understand about a corpus?\nOverly common ones: what does a word used in every document help us to understand about a corpus?\n\n\nAgain, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#different-approaches",
    "href": "lectures/8.3-Cleaning_Text.html#different-approaches",
    "title": "Cleaning Text",
    "section": "Different Approaches",
    "text": "Different Approaches\nHumans use a lot of words/concepts1:\n\nStemming: rules-based truncation to a stem (can be augmented by language awareness).\nLemmatisation: usually dictionary-based ‘deduplication’ to a lemma (can be augmented by POS-tagging).\n\nA recent digitsation effort by Harvard and Google estimated 1,022,000 unique word-forms in English alone (Source)."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#different-outcomes",
    "href": "lectures/8.3-Cleaning_Text.html#different-outcomes",
    "title": "Cleaning Text",
    "section": "Different Outcomes",
    "text": "Different Outcomes\n\n\n\nSource\nPorter\nSnowball\nLemmatisation\n\n\n\n\nmonkeys\nmonkey\nmonkey\nmonkey\n\n\ncities\nciti\nciti\ncity\n\n\ncomplexity\ncomplex\ncomplex\ncomplexity\n\n\nReades\nread\nread\nReades\n\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nfor w in ['monkeys','cities','complexity','Reades']:\n    print(f\"Porter: {PorterStemmer().stem(w)}\")\n    print(f\"Snowball: {SnowballStemmer('english').stem(w)}\")\n    print(f\"Lemmatisation: {wnl.lemmatize(w)}\")"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#resources",
    "href": "lectures/8.3-Cleaning_Text.html#resources",
    "title": "Cleaning Text",
    "section": "Resources",
    "text": "Resources\n\nVectorisation in Python\nLambda Functions\nReal Python Lambda Functions\nStemming words with NLTK\nStemming and Lemmatisation in Python\nKD Nuggets: A Practitioner’s Guide to NLP\nKD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics\nRoadmap to Natural Language Processing (NLP)\n\n\n\n\n\nCleaning Text • Jon Reades"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\npd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join",
    "href": "lectures/9.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart\n\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join-result",
    "href": "lectures/9.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\npd.merge(df1, df2, \n         how = 'inner',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#but-what-if",
    "href": "lectures/9.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then:\npd.merge(df1, df2, \n         how      = 'inner',\n         left_on  = 'SensorID',\n         right_on = 'SensorKey')\n \nWe will get an ‘extra’ field:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#outer-join",
    "href": "lectures/9.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\npd.merge(df1, df2,\n         how = 'outer',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#left-join",
    "href": "lectures/9.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\npd.merge(df1, df2, \n        how = 'left',\n        on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append-concat",
    "href": "lectures/9.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#concat",
    "href": "lectures/9.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append",
    "href": "lectures/9.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 0, 'Parameter': 5,  'Humidity', 'Value': 0.45},\n    {'SensorID': 1, 'Parameter': 5,  'Humidity', 'Value': 0.31},\n    {'SensorID': 2, 'Parameter': 4, 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 3, 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#resources",
    "href": "lectures/9.1-Linking_Data.html#resources",
    "title": "Linking Data",
    "section": "Resources",
    "text": "Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append\n\n\n\n\n\nLinking Data • Jon Reades"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html",
    "href": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html",
    "title": "Preamble",
    "section": "",
    "text": "Complete\nPart 1: Foundations\nPart 2: Data\nPart 3: Analysis\n\n\n\n\n\n90%\n▓▓▓▓▓▓▓▓\n▓▓▓▓▓▓\n▓▓▓▓▓░\n10/10\nA common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: this group of observations are similar to one another, that group is dissimilar to this group. But what defines similarity and difference? There is no one answer to that question and so there are many different ways to cluster data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#load-geodata-for-display",
    "href": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#load-geodata-for-display",
    "title": "Preamble",
    "section": "Load GeoData for Display",
    "text": "Load GeoData for Display\n# Load Water GeoPackage\nw_path = os.path.join('data','geo','Water.gpkg')\nif not os.path.exists(w_path):\n    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n    water.to_file(w_path)\n    print(\"Downloaded Water.gpkg file.\")\nelse:\n    water = gpd.read_file(w_path)\n\n# Boroughs GeoPackage\nb_path = os.path.join('data','geo','Boroughs.gpkg')\nif not os.path.exists(b_path):\n    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n    boroughs.to_file(b_path)\n    print(\"Downloaded Boroughs.gpkg file.\")\nelse:\n    boroughs = gpd.read_file(b_path)\n\nUseful Functions for Plotting\ndef plt_ldn(w=water, b=boroughs):\n    fig, ax = plt.subplots(1, figsize=(14, 12))\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n    ax.set_xlim([502000,563000])\n    ax.set_ylim([155000,201500])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return fig, ax\n\ndef default_cmap(n, outliers=False):\n    cmap = mpl.cm.get_cmap('viridis_r', n)\n    colors = cmap(np.linspace(0,1,n))\n    if outliers:\n        gray = np.array([225/256, 225/256, 225/256, 1])\n        colors = np.insert(colors, 0, gray, axis=0)\n    return ListedColormap(colors)\n\n# mappable = ax.collections[-1] if you add the geopandas\n# plot last.\ndef add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n                    boundaries=breaks,\n                    extend=('min' if outliers else 'neither'), \n                    spacing='uniform',\n                    orientation='horizontal',\n                    fraction=0.05, shrink=0.5, pad=0.05)\n    cb.set_label(\"Cluster Number\")"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#loading-data",
    "href": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#loading-data",
    "title": "Preamble",
    "section": "Loading Data",
    "text": "Loading Data\n\nLoad the Listings Data\nFeel free to download this manually and load it locally rather that loading via the URL:\nfdf = pd.read_csv(url, compression='gzip', low_memory=False)\nfdf.columns\nfdf[fdf.host_listings_count == 0][['host_total_listings_count','calculated_host_listings_count']]\nurl = 'https://github.com/jreades/i2p/blob/master/data/clean/2020-08-24-listings.csv.gz?raw=true'\ndf = pd.read_csv(url, compression='gzip', low_memory=False, \n                 usecols=['room_type','calculated_host_listings_count','availability_90','availability_365','longitude','latitude','price'])\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\nYou should have: Data frame is 74,120 x 7.\n\n\nAggregating Listings by MSOA\nNext, let’s link all this using the MSOA Geography that we created last week and a mix or merge and sjoin!\nmsoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\ngdf = gpd.GeoDataFrame(df, \n      geometry=gpd.points_from_xy(df['longitude'], df['latitude'], crs='epsg:4326'))\ngdf = gdf.to_crs('epsg:27700')\n# ml == MSOA Listings\nml = gpd.sjoin(gdf, msoas[['MSOA11CD','geometry']], op='within').drop(columns=\n    ['latitude','longitude','index_right','geometry']\n)\nml = ml[~(ml.room_type.isin(['Hotel room','Shared room']))]\nml.head()\nax = ml.calculated_host_listings_count.hist(bins=1500)\nax.set_xlim([0,50]);\nml['multihost'] = False\nml.loc[ml.calculated_host_listings_count&gt;=3,'multihost'] = True\nax = ml.price.hist(bins=5000)\nax.set_xlim([0,1000]);\nfor p in range(300, 1050, 50):\n    print(f\"Percent of listings above ${p:,}/night: {(ml[ml.price &gt; p].price.count() / ml.shape[0])*100:0.2f}\")\nmlg = ml[ml.price &lt;= 700].groupby(\n    ['MSOA11CD','room_type','multihost'] \n)['price'].agg(Mean='mean', Count='count').reset_index()\nmlg.head()\nrs = RobustScaler(quantile_range=[2.5,97.5])\nYou should see wide ranges of counts by roomt type in the first MSOA alone: the largest number of listings is for Entire home/apt but there are 242 multi-host listings compared to ‘just’ 156 non-multis. I believe this is the City of London though, which is quite unusual for a MSOA and should probably be treated as an outlier in most cases.\nmlgp = mlg.pivot(index='MSOA11CD', columns=['room_type','multihost'], values=['Mean','Count']).reset_index().fillna(0)\nmlgp.head()\nmlgp[['Mean']].head()\nfor c in mlgp[['Mean']].columns.values:\n    mlgp[c] = rs.fit_transform(mlgp[c].values.reshape(-1,1))\nmlgp[['Mean']].head()\nmlgp[['Count']].head()\ntotal = mlgp[['Count']].sum(axis=1)\nfor c in mlgp[['Count']].columns.values:\n    mlgp[c] = rs.fit_transform( (mlgp[c]/total).values.reshape(-1,1) )\n\nmlgp[['Count']].head()\ncols = []\nfor c in mlgp.columns.values:\n    l0 = c[0]\n    l1 = c[1]\n    l2 = \"Multi-host\" if c[2] else \"Single host\"\n    \n    colname = \"\"\n    \n    if l0=='MSOA11CD':\n        cols.append(l0)\n    elif l0=='Mean':\n        cols.append(f\"{l1} {l2} Mean\")\n    elif l0=='Count':\n        cols.append(f\"{l1} {l2} Listings\")\n    else:\n        raise Exception(\"Sorry, please specify how to handle this column\")\n\nprint(cols)\nmlgp.columns = cols\nmlgp.set_index('MSOA11CD', inplace=True)\nmlgp.head()"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#pca",
    "href": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#pca",
    "title": "Preamble",
    "section": "PCA",
    "text": "PCA\nfrom sklearn.decomposition import PCA \n\nmax_components = mlgp.shape[0] if mlgp.shape[0] &lt; mlgp.shape[1] else mlgp.shape[1]\n\npca = PCA(n_components=max_components, whiten=True) \n\npca.fit(mlgp)\n\nexplained_variance = pca.explained_variance_ratio_\nsingular_values = pca.singular_values_\nx = np.arange(1,len(explained_variance)+1)\nplt.plot(x, explained_variance)\nplt.ylabel('Share of Variance Explained')\nplt.show()\nfor i in range(0, len(explained_variance)):\n    print(f\"Component {i+1:&gt;2} accounts for {explained_variance[i]*100:&gt;2.2f}% of variance\")\nkeep_n_components = 6\n\n# If we weren't changing the number of components we\n# could re-use the pca object created above. \npca = PCA(n_components=keep_n_components, whiten=True)\n\nX_train = pca.fit_transform(mlgp)\nmlgpca = pd.DataFrame(index=mlgp.index)\n\nfor i in range(0,len(X_train.T)):\n    mlgpca[f\"Component {i+1}\"] = X_train.T[i]\nmlgpca.sample(3)"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#clustering-data-frame",
    "href": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#clustering-data-frame",
    "title": "Preamble",
    "section": "Clustering Data Frame",
    "text": "Clustering Data Frame\ncldf = mlgpca.copy()\ncols_to_plot = np.random.choice(cldf.columns.values, 3, replace=False)\nprint(\"Plotting cols: \" + \", \".join(cols_to_plot))\nrs = cldf.copy()\nc_nm   = 'KMeans' # Clustering name\nk_pref = 4 # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=20, random_state=42).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\n\n# Add it to the data frame\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index) \n\n# How are the clusters distributed?\nrs[c_nm].hist(bins=k_pref)\n\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(rs, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde')\ncgdf = pd.merge(msoas, rs, on='MSOA11CD')\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\ncgdf.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True)\n\ndel(cgdf)\nOne More Thing…\nThere’s just one little problem: what assumption did I make when I started this k-means cluster analysis? It’s a huge one, and it’s one of the reasons that k-means clustering can be problematic when used naively…\nSTOP. What critical assumption did we make when running this analysis? The ‘Right’ Number of Clusters Again, there’s more than one way to skin this cat. In Geocomputation they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try it for a range of values…\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\ntext = []\n\nfor k in range(2,16):\n    # Debugging\n    print(\"Cluster count: \" + str(k))\n    \n    #############\n    # Do the clustering using the main columns\n    clusterer = KMeans(n_clusters=k, n_init=20, random_state=42)\n    cluster_labels = clusterer.fit_predict(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, cluster_labels)\n    text = text + [f\"For k={k} the average silhouette_score is: {silhouette_avg:6.4f}\"]\n    \n    # Calculate the silhouette values\n    sample_silhouette_values = silhouette_samples(cldf, cluster_labels)\n    \n    #############\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(9, 5)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1\n    ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \n    y_lower = 10\n    \n    # For each of the clusters...\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        \n        # Set the color ramp\n        #cmap  = cm.get_cmap(\"Spectral\")\n        color = plt.cm.Spectral(i/k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(cluster_labels.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()\n\nprint(\"\\n\".join(text))\nInterpreting the Results\nSTOP. Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine.\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\nc_nm   = 'KMeans'\nk_pref = 7\nkmeans = KMeans(n_clusters=k_pref, n_init=75, random_state=42).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n\n# We do this for plotting\nrs[c_nm] = s\ncgdf = pd.merge(msoas, rs, on='MSOA11CD')\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\ncgdf.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True)\n\ndel(cgdf)\n‘Representative’ Centroids To get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clmsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clmsoas.columns.values)\n    centroids = centroids.append(clmsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\nDBScan\nOf course, as we’ve said above k-means is just one way of clustering, DBScan is another. Unlike k-means, we don’t need to specify the number of clusters in advance. Which sounds great, but we still need to specify other parameters (typically, these are known as hyperparameters because they are about specifying parameters that help the aglorithm to find the right solution… or final set of parameters!) and these can have a huge impact on our results!\nFind a Reasonable Value for Epsilon\nBefore we an use DBSCAN it’s useful to find a good value for Epsilon. We can look for the point of maximum ‘curvature’ in a nearest neigbhours plot. Which seems to be in the vicinity of 0.55. Tips on selecting min_pts can be found here.\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(cldf)\ndistances, indices = nbrs.kneighbors(cldf)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances);\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.5, 2.5, 0.01):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf.shape[1]+1).fit(cldf.values)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\nodf['Count'] = odf.Count.astype(float)\nxmin = odf[odf.Cluster=='Cluster 0'].Epsilon.min()\nxmax = odf[(odf.Cluster=='Cluster -1') & (odf.Count &lt; cldf.shape[0]/5)].Epsilon.min()\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.set_xlim([xmin,xmax])\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\ne = 0.835\ndbs = DBSCAN(eps=e, min_samples=cldf.shape[1]+1).fit(cldf.values)\ns = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\ncgdf = pd.merge(msoas, rs, on='MSOA11CD')\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True)\n\ndel(cgdf)\nfrom sompy.sompy import SOMFactory\ncldf.columns.values\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 5\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=rs.index, name=c_nm)\n\nrs[c_nm] = s\ncgdf = pd.merge(msoas, rs, on='MSOA11CD')\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True)\n\ndel(cgdf)\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\nWhich Clustering Approach is Right?\nThe reason that there is no ‘right’ approach to clustering is that it all depends on what you’re trying to accomplish and how you’re reasoning about your problem. The image below highlights the extent to which the different clustering approaches in sklearn can produce different results – and this is only for the non-geographic algorithms!\n\nNote: for geographically-aware clustering you need to look at PySAL.\nTo think about this in a little more detail:\n\nIf I run an online company and I want to classify my customers on the basis of their product purchases, then I probably don’t care much about where they are, only about what they buy, and so my clustering approach doesn’t need to take geography into account. I might well discover that many of my most valuable customers live in a few areas, but that is a finding, not a factor, in my research.\nConversely, if I am looking for cancer clusters then I might well care a lot about geography because I want to make sure that I don’t overlook an important cluster of cases because it’s ‘hidden’ inside an area with lots of people who don’t have cancer. In that case, I want my clusters to take geography into account. That approach might classify an area with a smaller proportion of cancer patients as part of a ‘cancer cluster’ but that’s because it is still significant because of the geography.\n\nSo you can undertake a spatial analysis using either approach, it just depends on the role that you think geography should play in producing the clusters in the first place. We’ll see this in action today!\n\nEnsure Plotting Output\nimport matplotlib as mpl\nmpl.use('TkAgg')\n%matplotlib inline\n\n\nImporting the Libraries\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport requests\nimport zipfile\nimport re\nimport os\nimport pickle as pk\n\nfrom io import BytesIO, StringIO\nfrom os.path import join as pj\nfrom pathlib import Path\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\n\nimport sklearn\nsklv = int(sklearn.__version__.replace(\".\",\"\"))\nif sklv &lt; 210:\n    print(\"SciKit-Learn verion is: \" + sklearn.__version__)\n    print(\"The OPTICS part of this notebook relies on a version &gt;= 0.21.0\")\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn import preprocessing\nfrom sklearn import cluster\n\nimport random\nrandom.seed(42)    # For reproducibility\nnp.random.seed(42) # For reproducibility\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\nsrc = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Census.zip?raw=true'\ndst = os.path.join('analysis','Census.zip')\n\nif not os.path.exists(dst):\n    if not os.path.exists(os.path.dirname(dst)):\n        os.makedirs(os.path.dirname(dst))\n    \n    print(\"Downloading...\")\n    r = requests.get(src, stream=True)\n    \n    with open(dst, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=128):\n            fd.write(chunk)\nelse:\n    print(\"File already downloaded.\")\n    \nprint(\"Done.\")\n\n\n\nLoading the NomisWeb Data\nYou may need to make a few adjustments to the path to get the data loaded on your own computer. But notice what we’re now able to do here: using the zipfile library we can extract a data file (or any other file) from the Zip archive without even having to open it. Saves even more time and disk space!\nz = zipfile.ZipFile(os.path.join('analysis','Census.zip'))\nz.namelist()\nWe’re going to save each data set to a separate data frame to make it easier to work with during cleaning. But note that this code is fairly flexible since we stick each new dataframe in a dictionary (d) where we can retrieve them via an iterator.\nraw   = {}\nclean = {}\ntotal_cols = 0\n\nfor r in range(0, len(z.namelist())):\n    \n    m  = re.search(\"(?:-)([^\\.]+)\", z.namelist()[r])\n    nm = m.group(1)\n    \n    print(\"Processing {0} file: \".format(nm))\n    \n    with z.open(z.namelist()[r]) as f:\n                \n        if z.namelist()[r] == '99521530-Activity.csv': \n            raw[nm] = pd.read_csv(BytesIO(f.read()), header=7, skip_blank_lines=True, skipfooter=7, engine='python')\n        else:\n            raw[nm] = pd.read_csv(BytesIO(f.read()), header=6, skip_blank_lines=True, skipfooter=7, engine='python')\n    \n    print(\"\\tShape of dataframe is {0} rows by {1} columns\".format(raw[nm].shape[0], raw[nm].shape[1]))\n    total_cols += raw[nm].shape[1]\n\nprint(\"There are {0} columns in all.\".format(total_cols))\nnm  = 'Occupation'\nurl = 'https://github.com/jreades/urb-studies-predicting-gentrification/raw/master/data/src/2011/ks610ew.csv.gz'\n\nprint(f\"Processing {nm}\")\ndf = pd.read_csv(url, header=7, skip_blank_lines=True, compression='gzip', low_memory=False)\nmapping = {\n    '1. Managers, directors and senior officials':'Managers',\n    '2. Professional occupations':'Professionals',\n    '3. Associate professional and technical occupations':'Associates',\n    '4. Administrative and secretarial occupations':'Administrative',\n    '5. Skilled trades occupations':'Skilled trades',\n    '6. Caring, leisure and other service occupations':'Caring and Leisure',\n    '7. Sales and customer service occupations':'Customer Service',\n    '8. Process plant and machine operatives':'Operatives',\n    '9. Elementary occupations':'Elementary'\n}\ndf.rename(columns=mapping, inplace=True)\ndf.drop(['2011 super output area - lower layer','All categories: Occupation'], axis=1, inplace=True)\n\nraw[nm] = df\ndf.sample(3, random_state=42)\nnm  = 'Income'\nurl  = 'https://data.london.gov.uk/download/household-income-estimates-small-areas/7c1099d9-327b-4f20-abb8-8c24a3c10c47/modelled-household-income-estimates-lsoa.csv'\n\nprint(f\"Processing {nm}\")\ndf = pd.read_csv(url, encoding='latin-1')[['Code','Median 2011/12']]\ndf['Median Income'] = df['Median 2011/12'].str.replace('£','').str.replace(',','').astype('float')\ndf.drop('Median 2011/12', axis=1, inplace=True)\n\nraw[nm] = df\ndf.sample(3, random_state=42)\nnm  = 'Housing'\nurl = 'https://data.london.gov.uk/download/average-house-prices/9a92fbaf-c04e-498a-9f8c-6c85f280817e/land-registry-house-prices-LSOA.csv'\n\nprint(f\"Processing {nm}\")\ndf = pd.read_csv(url, encoding='latin-1', low_memory=False)\n\ndf['Borough'] = df.Area.str.replace(' [0-9A-Z]{4}$','')\ndf.drop('Area', axis=1, inplace=True)\n\ndf = df[ (df.Year=='Year ending Dec 2011') & (df.Measure=='Median') ][['Code','Value','Borough']]\n\n# Note: not all have a value for this year!\ndf['Median House Price'] = df.Value.str.replace(':','-1').astype(float)\n\nla = df.groupby('Borough')\nla_prices = pd.DataFrame(la['Median House Price'].median())\n\ndf = df.join(la_prices, how='inner', on='Borough', rsuffix='_la')\n\ndf.loc[df['Median House Price'] &lt; 50000,'Median House Price'] =  df[df['Median House Price'] &lt; 50000]['Median House Price_la']\ndf.drop(['Value','Borough','Median House Price_la'], inplace=True, axis=1)\n\nraw[nm] = df\ndf.sample(3, random_state=42)\n\n\nONS Boundary Data\nWe also need to download the LSOA boundary data. A quick Google search on “2011 LSOA boundaries” will lead you to the Data.gov.uk portal. The rest is fairly straightforward: * We want ‘generalised’ because that means that they’ve removed some of the detail from the boundaries so the file will load (and render) more quickly. * We want ‘clipped’ because that means that the boundaries have been clipped to the edges of the land (e.g. the Thames; the ‘Full’ data set splits the Thames down the middle between adjacent LSOAs).\n\nSaving Time\nAgain, in order to get you started more quickly I’ve already created a ‘pack’ for you. However, note that the format of this is a GeoPackage, this is a fairly new file format designed to replace ESRI’s antique Shapefile format, and it allows us to include all kinds of useful information as part of the download as well as doing away with the need to unzip a download first! So here we load the data directly into a geopandas dataframe:\nsrc = 'https://github.com/kingsgeocomp/applied_gsa/raw/master/data/London%20LSOAs.gpkg'\n\ngdf = gpd.read_file(src)\nprint(\"Shape of LSOA file: {0} rows by {1} columns\".format(gdf.shape[0], gdf.shape[1]))\ngdf.columns = [x.lower() for x in gdf.columns.values]\ngdf.set_index('lsoa11cd', drop=True, inplace=True)\ngdf.sample(4)\n\n\nError!\nDepending on your version of GDAL/Fiona, you may not be able to read the GeoPackage file directly. In this case you will need to replace the code above with the code below for downloading and extracting a Shapefile from a Zip archive:\nsrc = 'https://github.com/kingsgeocomp/applied_gsa/blob/master/data/Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.zip?raw=true'\ndst = os.path.join('analysis','LSOAs.zip')\nzpd = 'analysis'\n\nif not os.path.exists(dst):\n    if not os.path.exists(os.path.dirname(dst)):\n        os.makedirs(os.path.dirname(dst))\n\n    r = requests.get(src, stream=True)\n\n    with open(dst, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=128):\n            fd.write(chunk)\n\nif not os.path.exists(zpd):\n    os.makedirs(os.path.dirname(zpd))\n    \nzp = zipfile.ZipFile(dst, 'r')\nzp.extractall(zpd)\nzp.close()\n\ngdf = gpd.read_file(os.path.join('analysis','lsoas','Lower_Layer_Super_Output_Areas_December_2011_Generalised_Clipped__Boundaries_in_England_and_Wales.shp'))\ngdf.crs = {'init' :'epsg:27700'}\nprint(\\\"Shape of LSOA file: {0} rows by {1} columns\\\".format(gdf.shape[0], gdf.shape[1]))\ngdf.set_index('lsoa11cd', drop=True, inplace=True)\ngdf.sample(4)\nYou can probably see why I’m a big fan of GeoPackages when they’re available!\n\n\n\nOther Sources of Data\nIf you’re more interested in US Census data then there’s a nice-looking (though I haven’t used it) wrapper to the Census API. And Spielman and Singleton have done some work on large-scale geodemographic clustering of U.S. Census geographies.\n\n\nDirect Downloads\nThese are already clean, so we can just copy them over.\nfor t in ['Occupation','Housing','Income']:\n    raw[t].rename(columns={'Code':'mnemonic'}, inplace=True)\n    print(raw[t].columns)\n    clean[t] = raw[t]\n\n\nDwellings\nFrom dwellings we’re mainly interested in the housing type since we would expect that housing typologies will be a determinant of the types of people who live in an area. We could look at places with no usual residents as well, or explore the distribution of shared dwellings, but this is a pretty good start.\nt = 'Dwellings'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Whole house or bungalow: Detached', \n    'Whole house or bungalow: Semi-detached',\n    'Whole house or bungalow: Terraced (including end-terrace)',\n    'Flat, maisonette or apartment: Purpose-built block of flats or tenement',\n    'Flat, maisonette or apartment: Part of a converted or shared house (including bed-sits)',\n    'Flat, maisonette or apartment: In a commercial building'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nmapping = {}\nfor c in selection[1:]:\n    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n    nm = m.group(1).strip()\n    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n    mapping[c] = nm\n\nclean[t].rename(columns=mapping, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nAge\nClearly, some areas have more young people, some have older people, and some will be composed of families. A lot of these are going to be tied to ‘lifestage’ and so will help us to understand something about the types of areas in which they live.\nt = 'Age'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Age 0 to 14',\n    'Age 15 to 24',\n    'Age 25 to 44',\n    'Age 45 to 64',\n    'Age 65+'\n]\n\n# Derived columns\nraw[t]['Age 0 to 14']  = raw[t]['Age 0 to 4'] + raw[t]['Age 5 to 7'] + raw[t]['Age 8 to 9'] + raw[t]['Age 10 to 14'] \nraw[t]['Age 15 to 24'] = raw[t]['Age 15'] + raw[t]['Age 16 to 17'] + raw[t]['Age 18 to 19'] + raw[t]['Age 20 to 24']\nraw[t]['Age 25 to 44'] = raw[t]['Age 25 to 29'] + raw[t]['Age 30 to 44']\nraw[t]['Age 45 to 64'] = raw[t]['Age 45 to 59'] + raw[t]['Age 60 to 64']\nraw[t]['Age 65+']      = raw[t]['Age 65 to 74'] + raw[t]['Age 75 to 84'] + raw[t]['Age 85 to 89'] + raw[t]['Age 90 and over']\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nEthnicity\nWe might also think that the balance of ethnic groups might impact a categorisation of LSOAs in London.\nt = 'Ethnicity'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'White', \n    'Mixed/multiple ethnic groups', \n    'Asian/Asian British', \n    'Black/African/Caribbean/Black British', \n    'Other ethnic group'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nRooms\nLet’s next incorporate the amount of space available to each household.\nt = 'Rooms'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Occupancy rating (bedrooms) of -1 or less',\n    'Occupancy rating (rooms) of -1 or less',\n    'Average household size', \n#    'Average number of bedrooms per household',\n#    'Average number of rooms per household',\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nVehicles\nCar ownership and use is also known to be a good predictor of social and economic ‘status’: Guy Lansley’s article on the DLVA’s registration database offers a useful perpective on the usefulness of this approach.\nt = 'Vehicles'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'No cars or vans in household', \n    '1 car or van in household',\n    '2 cars or vans in household', \n    '3 or more cars or vans in household'\n]\n\n# Calculate a new column\nraw[t]['3 or more cars or vans in household'] = raw[t]['3 cars or vans in household'] + raw[t]['4 or more cars or vans in household']\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].sample(5, random_state=42)\n\n\nTenure\nOwnership structure is another categorisation predictor.\nt = 'Tenure'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Owned', \n    'Social rented', \n    'Private rented',\n    'Shared ownership (part owned and part rented)'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nclean[t].rename(columns={'Shared ownership (part owned and part rented)':'Shared ownership'}, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nQualifications\nYou can find out a bit more about qualifications here.\nt = 'Qualifications'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Highest level of qualification: Below Level 3 qualifications',\n    'Highest level of qualification: Level 3 and above qualifications',\n    'Highest level of qualification: Other qualifications'\n]\n\n# Derive a new aggregate field for 'didn't complete HS'\nraw[t]['Highest level of qualification: Below Level 3 qualifications'] = \\\n    raw[t]['No qualifications'] + \\\n    raw[t]['Highest level of qualification: Level 1 qualifications'] + \\\n    raw[t]['Highest level of qualification: Level 2 qualifications'] + \\\n    raw[t]['Highest level of qualification: Apprenticeship'] \n\nraw[t]['Highest level of qualification: Level 3 and above qualifications'] = \\\n    raw[t]['Highest level of qualification: Level 3 qualifications'] + \\\n    raw[t]['Highest level of qualification: Level 4 qualifications and above']\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nmapping = {}\nfor c in selection[1:]:\n    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n    nm = m.group(1).strip()\n    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n    mapping[c] = nm\n\nclean[t].rename(columns=mapping, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nActivity\nt = 'Activity'\nraw[t].columns\n# Select the columns we're interested in analysing\nselection = ['mnemonic',\n    'Economically active: In employment',\n    'Economically active: Unemployed',\n    'Economically active: Full-time student',\n#    'Economically inactive: Retired',\n    'Economically inactive: Looking after home or family',\n    'Economically inactive: Long-term sick or disabled',\n#    'Economically inactive: Other'\n]\n\n# Drop everything *not* in the selection\nclean[t] = raw[t].drop(raw[t].columns[~np.isin(raw[t].columns.values,selection)].values, axis=1)\n\nmapping = {}\nfor c in selection[1:]:\n    m  = re.search(\"^(?:[^\\:]*)(?:\\:\\s)?([^\\(]+)\", c)\n    nm = m.group(1).strip()\n    #print(\"Renaming '{0}' to '{1}'\".format(c, nm))\n    mapping[c] = nm\n\nclean[t].rename(columns=mapping, inplace=True)\n\nclean[t].sample(5, random_state=42)\n\n\nStandardisation with SKLearn\nLet’s try standardising the data now:\n# Here's how we can rescale and transform data easily\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nrandom.seed(42)\nt   = random.sample(population=list(clean.keys()), k=1)[0]\ncol = random.sample(population=list(clean[t].columns.values[1:]), k=1)[0]\nprint(f\"Looking at {col} column from {t}.\")\nHere’s the ‘original’ distribution:\nplt.rcParams['figure.figsize']=(7,3)\nsns.distplot(clean[t][col], kde=False)\nHere’s the version that has been re-scaled (standardised) using Min/Max rescaling:\nplt.rcParams['figure.figsize']=(7,3)\nsns.distplot(preprocessing.minmax_scale(clean[t][col].values.reshape(-1,1)), kde=False)\nHere’s a version that has been robustly rescaled:\nplt.rcParams['figure.figsize']=(7,3)\nsns.distplot(preprocessing.robust_scale(clean[t][col].values.reshape(-1,1), quantile_range=[5.0, 95.0]), kde=False)\nAnd here’s a version that has been Power Transformed… spot the difference!\nsns.distplot(\n    preprocessing.power_transform(clean[t][col].values.reshape(-1,1), method='yeo-johnson'), kde=False)\nCombining transformation and rescaling:\nsns.distplot(\n    preprocessing.robust_scale(\n        preprocessing.power_transform(\n            clean[t][col].values.reshape(-1,1), method='yeo-johnson'\n        ), quantile_range=[5.0, 95.0] \n    ), \n    kde=False\n)\n# Set up a new dictionary for the transforms\ntransformed = {}\n\ntransformer = preprocessing.PowerTransformer()\nscaler      = preprocessing.RobustScaler(quantile_range=[5.0, 95.0])\n#scaler      = preprocessing.MinMaxScaler()\n\n# Simple way to drop groups of data we don't want...\nsuppress       = set(['Rooms','Vehicles'])\n\nfor k in set(clean.keys()).difference(suppress):\n    print(f\"Transforming {k}\")\n    df = clean[k].copy(deep=True)\n    df.set_index('mnemonic', inplace=True)\n    \n    # For rescale and transforming everything when the operations\n    # apply to each Series separately you can do it as a 1-liner like this:\n    #df[df.columns] = scaler.fit_transform(transformer.fit_transform(df[df.columns]))\n\n    # To calculate within-*column* proportions it's like this:\n    #for c in df.columns.values: \n    #    df[c] = scaler.fit_transform( (df[c]/df[c].max() ).values.reshape(-1, 1) )\n\n    # To calculate within-*group* proportions it's like this:\n    if k in ['Housing','Income','Rooms']:\n        df[df.columns] = scaler.fit_transform( df[df.columns] )\n    else: \n        df['sum'] = df[list(df.columns)].sum(axis=1)\n        for c in df.columns.values:\n            if c == 'sum':\n                df.drop(['sum'], axis=1, inplace=True)\n            else:\n                df[c] = scaler.fit_transform( (df[c]/df['sum']).values.reshape(-1, 1) )\n            \n    #print(df.sample(5, random_state=42))\n    transformed[k] = df\n\n\nCreating the Single Data Set\nNow that we’ve converted everything to percentages, it’s time to bring the data together! We’ll initialise the data frame using the first matching data set, and then iterate over the rest, merging the data frames as we go.\nmatching = list(transformed.keys())\nprint(\"Found the following data sets:\\n\\t\" + \", \".join(matching))\n\n# Initialise the data frame simply by grabbing the\n# very first existing data frame and copying it \n# directly (SCaled Data Frame == scdf)\nscdf  = transformed[matching[0]].copy() \nlsoac = clean[matching[0]].copy() \n\nfor m in range(1, len(matching)):\n    scdf  = scdf.merge(transformed[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')\n    lsoac = lsoac.merge(clean[matching[m]], how='inner', left_on='mnemonic', right_on='mnemonic')\n\nscdf.to_csv(os.path.join('data','Scaled_and_Transformed.csv.gz'), compression='gzip')\nlsoac.to_csv(os.path.join('data','Cleaned.csv.gz'), compression='gzip')\nprint(\"Shape of full data frame is {0} by {1}\".format(scdf.shape[0], scdf.shape[1]))\nWith luck you still have 4,835 rows, but now you have rather fewer than 88 columns.\nrandom.seed(42)\ncols_to_plot = random.sample(population=list(scdf.columns.values), k=3)\nprint(\"Columns to plot: \" + \", \".join(cols_to_plot))\n# The data as it is now...\nsns.set(style=\"whitegrid\")\nsns.pairplot(lsoac, \n             vars=cols_to_plot, \n             markers=\".\", height=3, diag_kind='kde')\n# The data as it is now...\nsns.set(style=\"whitegrid\")\nsns.pairplot(scdf, \n             vars=cols_to_plot, \n             markers=\".\", height=3, diag_kind='kde')\n\nSTOP. Making sure that you understand how and why this results differns from the same plot above.\n:::\nRight, so you can see that rescaling the dimension hasn’t actually changed the relationships within each dimension, or even between dimensions, but it has changed the overall range so that the the data is broadly re-centered on 0 but we still have the original outliers from the raw data. You could also do IQR standardisation (0.25 and 0.75) with the percentages, but in those cases you would have more outliers and then more extreme values skewing the results of the clustering algorithm.\n\nFreeing Up Memory\nWe now have quite a few variables/datasets in memory, so it’s a good idea to free up some RAM by getting rid of anything we no longer need…\nin_scope  = set([x for x in dir() if not x.startswith('_')])\nto_delete = set(['raw','clean','transformed','col','k','c','lsoac','scdf'])\nz = list(in_scope.intersection(to_delete))\ndel(z)\n\n\nClustering Your Data\nOK, we’re finally here! It’s time to cluster the cleaned, normalised, and standardised data set! We’re going to start with the best-known clustering technique (k-means) and work from there… Don’t take my word for it, here are the 5 Clustering Techniques Every Data Scientist Should Know. This is also a good point to refer back to some of what we’ve been doing (and it’s a good point to potentially disagree with me!) since clustering in high dimensions can be problematic (i.e. the more dimensions the worse the Euclidean distance gets as a cluster metric).\nThe effectiveness of clustering algorithms is usually demonstrated using the ‘iris data’ – it’s available by default with both Seaborn and SciKit-Learn. This data doesn’t usually need normalisation but it’s a good way to start looking at the data across four dimensions and seeing how it varies and why some dimensions are ‘good’ for clustering, while others are ‘not useful’…\n\nUnfortunately, our data is a lot messier and has many more dimensions (&gt;25) than this.\n:::\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions\n\n\nBrief Discussion\nIn the practical I’ve followed the Geocomputation approach of basically converting everything to a share (percentage) and then clustering on that. This is one way to approach this problem, but there are many others. For instance, many people might skip the percentages part and apply robust rescaling (sklearn.preprocessing.RobustScaler) using centering and quantile standardisation (the 5th and 95th, for example) instead. And possibly using a normalising transformation (such as a Power Transform) as well.\nI would also consider using PCA on groups of related variables (e.g. the housing features as a group, the ethnicity features as a group, etc.) and then take the first few eigenvalues from each group and cluster on all of those together. This would remove quite a bit of the correlation between variables and still allow us to perform hierarchical and other types of clustering on the result. It might also do a better job of preserving outliers.\n\n\nCreate an Output Directory and Load the Data\no_dir = os.path.join('outputs','clusters')\nif os.path.isdir(o_dir) is not True:\n    print(\"Creating '{0}' directory.\".format(o_dir))\n    os.makedirs(o_dir)\ndf = pd.read_csv(os.path.join('data','Scaled_and_Transformed.csv.gz'))\ndf.rename(columns={'mnemonic':'lsoacd'}, inplace=True)\ndf.set_index('lsoacd', inplace=True)\ndf.describe()\ndf.sample(3, random_state=42)\n\n\nGrab Borough Boundaries and Water Courses\nNote: if reading these GeoPackages gives you errors then you will need to comment out the following two lines from the plt_ldn function immediately below:\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n# Load Water GeoPackage\nw_path = os.path.join('data','Water.gpkg')\nif not os.path.exists(w_path):\n    water = gpd.read_file('https://github.com/kingsgeocomp/applied_gsa/raw/master/data/Water.gpkg')\n    water.to_file(w_path)\n    print(\"Downloaded Water.gpkg file.\")\nelse:\n    water = gpd.read_file(w_path)\n\n# Boroughs GeoPackage\nb_path = os.path.join('data','Boroughs.gpkg')\nif not os.path.exists(b_path):\n    boroughs = gpd.read_file('https://github.com/kingsgeocomp/applied_gsa/raw/master/data/Boroughs.gpkg')\n    boroughs.to_file(b_path)\n    print(\"Downloaded Boroughs.gpkg file.\")\nelse:\n    boroughs = gpd.read_file(b_path)\n\n\nUseful Functions for Plotting\ndef plt_ldn(w=water, b=boroughs):\n    fig, ax = plt.subplots(1, figsize=(14, 12))\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n    ax.set_xlim([502000,563000])\n    ax.set_ylim([155000,201500])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return fig, ax\n\ndef default_cmap(n, outliers=False):\n    cmap = mpl.cm.get_cmap('viridis_r', n)\n    colors = cmap(np.linspace(0,1,n))\n    if outliers:\n        gray = np.array([225/256, 225/256, 225/256, 1])\n        colors = np.insert(colors, 0, gray, axis=0)\n    return ListedColormap(colors)\n\n# mappable = ax.collections[-1] if you add the geopandas\n# plot last.\ndef add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n                    boundaries=breaks,\n                    extend=('min' if outliers else 'neither'), \n                    spacing='uniform',\n                    orientation='horizontal',\n                    fraction=0.05, shrink=0.5, pad=0.05)\n    cb.set_label(\"Cluster Number\")\n\n\nSelect 4 Columns to Plot\nrandom.seed(42)\ncols_to_plot = random.sample(population=list(df.columns.values), k=4)\nprint(\"Columns to plot: \" + \", \".join(cols_to_plot))\n\n\nStoring Results\nresult_set = None\n\ndef add_2_rs(s, rs=result_set):\n    if rs is None:\n        # Initialise\n        rs = pd.DataFrame()\n    rs[s.name] = s\n    return rs\n\n\nK-Means\n\nImporting the Library\nfrom sklearn.cluster import KMeans\n#help(KMeans)\nThe next few code blocks may take a while to complete, largely because of the pairplot at the end where we ask Seaborn to plot every dimension against every other dimension while colouring the points according to their cluster. I’ve reduced the plotting to just three dimensions, if you want to plot all of them, then just replace the array attached to vars with main_cols, but you have to bear in mind that that is plotting 4,300 points each time it draws a plot… and there are 81 of them! It’ll take a while, but it will do it, and try doing that in Excel or SPSS?\n\n\nA First Cluster Analysis\nc_nm   = 'KMeans' # Clustering name\nk_pref = 6 # Number of clusters\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nkmeans = KMeans(n_clusters=k_pref, n_init=20, random_state=42, n_jobs=-1).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\n\n# Add it to the data frame\ncldf[c_nm] = pd.Series(kmeans.labels_, index=df.index) \n\n# How are the clusters distributed?\ncldf[c_nm].hist(bins=k_pref)\n\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(cldf, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde')\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(0,cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks))\n\nnorm    = mpl.colors.BoundaryNorm(breaks, cmap.N)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n\ndel(cgdf)\n\n\nOne More Thing…\nThere’s just one little problem: what assumption did I make when I started this k-means cluster analysis? It’s a huge one, and it’s one of the reasons that k-means clustering can be problematic when used naively…\n\nSTOP. What critical assumption did we make when running this analysis?\n:::\n\nThe ‘Right’ Number of Clusters\nAgain, there’s more than one way to skin this cat. In Geocomputation they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2...n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try it for the range 3-9.\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\ntext = []\n\nfor k in range(3,10):\n    # Debugging\n    print(\"Cluster count: \" + str(k))\n    \n    #############\n    # Do the clustering using the main columns\n    clusterer = KMeans(n_clusters=k, n_init=15, random_state=42, n_jobs=-1)\n    cluster_labels = clusterer.fit_predict(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, cluster_labels)\n    text = text + [f\"For k={k} the average silhouette_score is: {silhouette_avg:6.4f}\"]\n    \n    # Calculate the silhouette values\n    sample_silhouette_values = silhouette_samples(cldf, cluster_labels)\n    \n    #############\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(9, 5)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1\n    ax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \n    y_lower = 10\n    \n    # For each of the clusters...\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        \n        # Set the color ramp\n        #cmap  = cm.get_cmap(\"Spectral\")\n        color = plt.cm.Spectral(i/k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(cluster_labels.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()\n\nprint(\"\\n\".join(text))    \n\ndel(cldf)\n\n\nInterpreting the Results\n\nSTOP. Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine.\n:::\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\n\nFinal Clustering\nLet’s repeat the clustering process one more time using the silhouette score as a guide and then map it.\n#| scrolled: true\nc_nm = 'KMeans'\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nk_pref = 4\nkmeans = KMeans(n_clusters=k_pref, n_init=75, random_state=42).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\n\n# We do this for plotting\ncldf[c_nm] = s\n\n# We do this to keep track of the results\nresult_set=add_2_rs(s)\n\n\nMapping Results\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(0,cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks))\n\nnorm    = mpl.colors.BoundaryNorm(breaks, cmap.N)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}-{k_pref}.png\"), dpi=200)\ndel(cgdf)\nTo make sense of whether this is a ‘good’ result, you might want to visit datashine or think back to last year when we examined the NS-SeC data.\nYou could also think of ways of plotting how these groups differ. For instance…\n\n\n‘Representative’ Centroids\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids, cldf)\n\n\nDBScan\nOf course, as we’ve said above k-means is just one way of clustering, DBScan is another. Unlike k-means, we don’t need to specify the number of clusters in advance. Which sounds great, but we still need to specify other parameters (typically, these are known as hyperparameters because they are about specifying parameters that help the aglorithm to find the right solution… or final set of parameters!) and these can have a huge impact on our results!\n\nImporting the Library\nfrom sklearn.cluster import DBSCAN\n#?DSCAN\n\n\nFind a Reasonable Value for Epsilon\nBefore we an use DBSCAN it’s useful to find a good value for Epsilon. We can look for the point of maximum ‘curvature’ in a nearest neigbhours plot. Which seems to be in the vicinity of 0.55. Tips on selecting min_pts can be found here.\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(cldf)\ndistances, indices = nbrs.kneighbors(cldf)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\n\n\nExploration\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\n# There's an argument for making min_samples = len(df.columns)+1\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.15, 1.55, 0.01):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=12, n_jobs=-1).fit(cldf.values)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    for c in range(-1, max_clusters+1):\n        try:\n            row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\nedf.sample(random_state=42)\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)       \nxmin = odf[odf.Cluster=='Cluster 0'].Epsilon.min()\nxmax = odf[(odf.Cluster=='Cluster -1') & (odf.Count &lt; cldf.shape[0]/5)].Epsilon.min()\n\nfig, ax = plt.subplots(figsize=(12,8))\nax.set_xlim([xmin,xmax])\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster')\n\nFinal Clustering\ne = 0.835\ndbs = DBSCAN(eps=e, min_samples=12, n_jobs=-1).fit(cldf.values)\ns = pd.Series(dbs.labels_, index=cldf.index, name=c_nm)\ncldf[c_nm] = s\nresult_set=add_2_rs(s)\nprint(s.value_counts())\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks), outliers=True)\nnorm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks, outliers=True)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\ndel(cgdf)\n\n\n‘Representative’ Centroids\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). For algorithms other than k-means it may be better to use medians than means.\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nOPTICS Clustering\nThis is a fairly new addition to sklearn and is similar to DBSCAN in that there are very few (if any) parameters to specify. This means that we’re making fewer assumptions about the nature of any clustering in the data. It also allows us to have outliers that don’t get assigned to any cluster. The focus is mainly on local density, so in some sense it’s more like a geographically aware clustering approach, but applied in the data space, not geographical space.\n\nImporting the Library\nfrom sklearn.cluster import OPTICS\n\n\nFinal Clustering\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.9850\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(df.columns)+1, max_eps=math.ceil(e * 100)/100, n_jobs=-1).fit(cldf.values)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\ncldf[c_nm] = s\nresult_set=add_2_rs(s)\n\n# Distribution\nprint(s.value_counts())\n\nMapping Clustering Results\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks), outliers=True)\nnorm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks, outliers=True)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\ndel(cgdf)\n\n‘Representative’ Centroids\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). For algorithms other than k-Means it may be better to use medians, not means.\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\nSTOP. Aside from the fact that we should probably reduce the number of dimensions on which we’re clustering, what about the process of selecting variables (a.k.a. feature selection) might have led to the result that our results are a bit crap? Hint: how did we decide what to keep and what to drop, and is this a robust approach?\n:::\n\nHDBSCAN\nNot implemented, but you could give it a try after installing the package:\nconda activate &lt;your environment name here&gt;\nconda install -c conda-forge sklearn-contrib-hdbscan\nThen it should be something like:\nimport hdbscan\nclusterer = hdbscan.HDBSCAN()\n# HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n#    gen_min_span_tree=False, leaf_size=40, memory=Memory(cachedir=None),\n#    metric='euclidean', min_cluster_size=5, min_samples=None, p=None)\nclusterer.fit(&lt;data&gt;)\nclusterer.labels_\n\n\nHierarchical Clustering\nProbably not appropriate as it tends to be confused by noise.\n\n\nSelf-Organising Maps\nSOMs offer a third type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n(Re)Installing SOMPY\n\nWARNING. The maintainers of the main SOMPY library are fairly inactive, so we’ve had to write our own version that fixes a few Python3 bugs, but this means that it can’t be installed the ‘usual’ way without also having Git installed. Consequently, I have left the output from SOMPY in place so that you can see what it will produce even if you cannot successfully install SOMPY during this practical\n\nTo work out if there is an issue, check to see if the import statement below gives you errors:\nfrom sompy.sompy import SOMFactory\nIf this import has failed with a warning about being unable to find SOM or something similar, then you will need to re-install SOMPY using a fork that I created on our Kings GSA GitHub account. For that to work, you will need to ensure that you have git installed.\nIf the following Terminal command (which should also work in the Windows Terminal) does not give you an error then git is already installed:\ngit --version\nTo install git on a Mac is fairly simple. Again, from the Terminal issue the following command:\nxcode-select --install\nThis installation may take some time over eduroam since there is a lot to download.\nOnce that’s complete, you can move on to installing SOMPY from our fork. On a Mac this is done on the Terminal with:\nconda activate &lt;your kernel name here&gt;\npip install -e git+git://github.com/kingsgeocomp/SOMPY.git#egg=SOMPY\nconda deactivate\nOn Windows you probably drop the conda part of the command.\n\nTraining the SOM\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\n# Quick sanity check in case something hasn't\n# run successfully -- these muck up k-means\ncldf = df.drop(list(df.columns[df.isnull().any().values].values), axis=1)\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(os.path.join(o_dir, f\"{c_nm}-Map.png\"), dpi=200)\n\n\nHere’s What I Got\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(os.path.join(o_dir,f\"{c_nm}-BMU Hit View.png\"), dpi=200)\n\nBMU Hit Map\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 5\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(os.path.join(o_dir,f\"{c_nm}-Hit Map View.png\"), dpi=200)\n\nClustering the BMUs\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\ntry:\n    cldf.drop(c_nm,inplace=True,axis=1)\nexcept KeyError:\n    pass\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\ncldf[c_nm] = s\nresult_set = add_2_rs(s)\ncgdf = gdf.join(cldf, how='inner')\n\nbreaks = np.arange(cldf[c_nm].min(),cldf[c_nm].max()+2,1)\ncmap   = default_cmap(len(breaks))\nnorm   = mpl.colors.BoundaryNorm(breaks, cmap.N, clip=False)\n\nfig, ax = plt_ldn()\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\n\ncgdf.plot(column=c_nm, ax=ax, cmap=cmap, norm=norm, linewidth=0, zorder=0, legend=False)\n\nadd_colorbar(ax.collections[-1], ax, cmap, norm, breaks)\n\nplt.savefig(os.path.join(o_dir,f\"{c_nm}.png\"), dpi=200)\ndel(cgdf)\n\nResult!\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\nRepresentative Centroids\ncentroids = None\nfor k in sorted(cldf[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = cldf[cldf[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nWrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#wrap-up",
    "href": "practicals/fragments/Practical-X-Grouping_Data-Listings_Only.html#wrap-up",
    "title": "Preamble",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html",
    "title": "Preamble",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Practical 10: Grouping Data&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Classification &amp; Clusters&lt;/h2&gt;\n:::\n\n\n:::\nA common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: this group of observations are similar to one another, that group is dissimilar to this group. But what defines similarity and difference? There is no one answer to that question and so there are many different ways to cluster data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: \n\n\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib as mpl\nimport re\nimport os\n\nfrom matplotlib.colors import ListedColormap\n\n# All of these are potentially useful, though\n# not all have been used in this practical --\n# I'd suggest exploring the use of different \n# Scalers/Transformers as well as clustering \n# algorithms...\nimport sklearn\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.cluster import KMeans, DBSCAN, OPTICS\nfrom esda.adbscan import ADBSCAN\n\nimport random\nrandom.seed(42)    # For reproducibility\nnp.random.seed(42) # For reproducibility\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\n\nrbs = RobustScaler(quantile_range=[0.025,0.975])\nmms = MinMaxScaler(feature_range=(-1,1))\npts = PowerTransformer()\n\n\n\ndef plt_ldn(w, b):\n    fig, ax = plt.subplots(1, figsize=(14, 12))\n    w.plot(ax=ax, color='#79aef5', zorder=2)\n    b.plot(ax=ax, edgecolor='#cc2d2d', facecolor='None', zorder=3)\n    ax.set_xlim([502000,563000])\n    ax.set_ylim([155000,201500])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return fig, ax\n\n########################\n# These may no longer be relevant because of changes to geopandas API\n\ndef default_cmap(n, outliers=False):\n    cmap = mpl.cm.get_cmap('viridis_r', n)\n    colors = cmap(np.linspace(0,1,n))\n    if outliers:\n        gray = np.array([225/256, 225/256, 225/256, 1])\n        colors = np.insert(colors, 0, gray, axis=0)\n    return ListedColormap(colors)\n\n# mappable = ax.collections[-1] if you add the geopandas\n# plot last.\ndef add_colorbar(mappable, ax, cmap, norm, breaks, outliers=False):\n    cb = fig.colorbar(mappable, ax=ax, cmap=cmap, norm=norm,\n                    boundaries=breaks,\n                    extend=('min' if outliers else 'neither'), \n                    spacing='uniform',\n                    orientation='horizontal',\n                    fraction=0.05, shrink=0.5, pad=0.05)\n    cb.set_label(\"Cluster Number\")\n\n\n\n\n\n# Load Water GeoPackage\nw_path = os.path.join('data','geo','Water.gpkg')\nif not os.path.exists(w_path):\n    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n    water.to_file(w_path)\n    print(\"Downloaded Water.gpkg file.\")\nelse:\n    water = gpd.read_file(w_path)\n\n# Boroughs GeoPackage\nb_path = os.path.join('data','geo','Boroughs.gpkg')\nif not os.path.exists(b_path):\n    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n    boroughs.to_file(b_path)\n    print(\"Downloaded Boroughs.gpkg file.\")\nelse:\n    boroughs = gpd.read_file(b_path)\n\n\n\nYou should have this locally…\nmsoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\nmsoas = msoas.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm'])\n\n\n\nFeel free to load your local copy rather than loading via the URL:\nurl = 'https://github.com/jreades/i2p/blob/master/data/clean/2020-08-24-listings.csv.gz?raw=true'\ndf = pd.read_csv(url, compression='gzip', low_memory=False, \n                 usecols=['room_type','longitude','latitude','price'])\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\nYou should have: Data frame is 74,120 x 4.\ngdf = gpd.GeoDataFrame(df, \n      geometry=gpd.points_from_xy(?, ?, crs=?))\ngdf = gdf.to_crs(?)\ngdf.plot(column='price', cmap='plasma', scheme='quantiles', k=10, markersize=1, figsize=(8,6));\n\n\n\nNext, use a spatial join to link the listings to MSOAs.\n# ml == MSOA Listings\nml = gpd.sjoin(??).drop(\n                    columns=['latitude','longitude','index_right']\n)\nNow we want a count of room types by MSOA:\nmlgc = ml.groupby([??])['price'].agg(Count='??').reset_index() # msoa listings grouped counts\nmlgc.head()\nYou should get:\n\n\n\n\nMSOA11CD\nroom_type\nCount\n\n\n\n\n0\nE02000001\nEntire home/apt\n398\n\n\n1\nE02000001\nHotel room\n4\n\n\n2\nE02000001\nPrivate room\n48\n\n\n3\nE02000001\nShared room\n2\n\n\n4\nE02000002\nPrivate room\n10\n\n\n\nAnd let’s also get the median price by MSOA…\nmlgp = ml.groupby('MSOA11CD')['price'].agg(??).reset_index() # msoa listings grouped price\nmlgp.head()\nFor comparison purposes:\n\n\n\n\nMSOA11CD\nprice\n\n\n\n\n0\nE02000001\n125.00\n\n\n1\nE02000002\n50.00\n\n\n2\nE02000003\n40.00\n\n\n3\nE02000004\n29.00\n\n\n4\nE02000005\n69.00\n\n\n\nThe highest median price is $1,880, which implies not just heavy skew but something a bit out-of-this-world.\n\nWarning: I have a strong suspicion that price is so skewed that using it in most clustering contexts is almost impossible. This is why I use the PowerTransformer below.\n:::\nNow we can make use of the pivot table function to generate counts by MSOA in ‘wide’ format. I’d suggest using the empty cell below to work out the pivot table function first before assigning it back to the mlgc variable and overwriting the existing data.\n#mlgc.pivot(index=??, columns=??, values=??).reset_index()\n\n\n\n\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\n\nCount\n\n\n\n\nroom_type\n\nEntire home/apt\nHotel room\nPrivate room\nShared room\n\n\n0\nE02000001\n398.00\n4.00\n48.00\n2.00\n\n\n1\nE02000002\nnan\nnan\n10.00\nnan\n\n\n2\nE02000003\n5.00\nnan\n19.00\nnan\n\n\n\nmlgc = mlgc.pivot().reset_index()\nmlgc.columns = ['MSOA11CD','Entire home/apt','Hotel room','Private room','Shared room']\nmlgc.drop(columns=['Hotel room','Shared room'], inplace=True)\nmlgc.head()\nMerge the grouped counts and price back to a mlg (MSOA Listings Grouped) data frame:\nmlg = pd.merge(mlgc, mlgp, on=??)\nmlg = mlg.fillna(0).set_index('MSOA11CD')\nmlg.head()\nApply a PowerTransform to every column using a loop and fit_transform:\nfor c in mlg.columns.values:\n    mlg[c] = ??\nmlg.head(3)\nFor comparison:\n\n\n\n\nEntire home/apt\nPrivate room\nprice\n\n\n\n\nMSOA11CD\n\n\n\n\n\nE02000001\n2.12\n0.81\n1.81\n\n\nE02000002\n-1.99\n-0.76\n-0.21\n\n\nE02000003\n-0.66\n-0.16\n-0.83\n\n\n\nsns.set(style=\"white\")\nsns.pairplot(mlg, markers=\".\", height=3, diag_kind='kde');\n\n\nrddf = gpd.read_file(os.path.join('data','clean','Reduced_Dimension_Data.gpkg'), driver='GPKG').set_index('MSOA11CD')\nrddf.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm','Subregion',\n                   'geometry','Dimension 1','Dimension 2'], inplace=True)\nprint(f\"Reduced Dimensionality Data Frame is {rddf.shape[0]:,} x {rddf.shape[1]:,}\")\nYou should get Reduced Dimensionality Data Frame is 983 x 10.\nMerge the rddf and mlg files using the MSOA11CD and rescale all columns using MinMaxScaler:\n# Merge the reducded dimensionality data frame with the msoa listings grouped data frame\ncldf = pd.merge(??)\n\n# Rescale the columns using the MinMaxScaler set up in the Preamble\nfor c in cldf.columns:\n    cldf[c] = ??\n\n# We also create a 'result set' (rs) data frame to hold the \n# results of the clustering operation\nrs   = pd.merge(msoas, cldf, left_on='MSOA11CD', right_index=True).set_index('MSOA11CD')\n\n# Check output\ncldf.head(2)[['Component 1','Component 2','price']]\nA partial sample:\n\n\n\nMSOA11CD\nComponent 1\nComponent 2\nprice\n\n\n\n\nE02000001\n-0.08\n0.12\n0.25\n\n\nE02000002\n-0.88\n-0.33\n-0.42\n\n\n\n\n\n\nPlotting all columns is computationally costly and for little utility… it’s better to just select a few at random.\ncols_to_plot = np.random.choice(cldf.columns.values, 4, replace=False)\nprint(cols_to_plot)\n\n\n\nc_nm   = 'KMeans' # Clustering name\nk_pref = 3 # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\nNow capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (rs):\n# Add it to the data frame\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n# How are the clusters distributed?\nax = sns.histplot(data=rs, x=c_nm, bins=k_pref);\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(rs, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde');\nAnd here’s a map!\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=??, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\nStop: What critical assumption did we make when running this analysis?\n:::\n\n\n\n\nThere’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try clustering across a wider range.\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nx = []\ny = []\n\nfor k in range(??,??):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(??).fit(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gcf().suptitle(\"Average Silhouette Scores\");\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=2 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\nk_pref=??\n    \n#############\n# Do the clustering using the main columns\nkmeans = KMeans(??).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(cldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n    \n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\nStop: Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine..\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    c = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=c.columns.values)\n    centroids = centroids.append(c.mean(), ignore_index=True)\n    \nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\nodf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\n\n\nFor what it’s worth, I’ve had enormous trouble with DBSCAN and this kind of data. I don’t think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation.\ncldf2 = cldf.loc[:,['Component 1','Component 2']]\ncldf2.head()\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.gcf().suptitle(\"Nearest Neighbour Distances\");\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.25, 0.005):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\n\nodf['Count'] = odf.Count.astype(float)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\nax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')\ne = 0.06\ndbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n\n\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\nodf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nSee if you can work out why we have this issue by plotting the first 2 components… We could actually try clusteirng on secondary components instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so dominant.\n\n\n\nThis one invites you to experiment wih different values of \\(\\epsilon\\) so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I’ve deliberately left this untested so I can’t be sure it will work and you’ll need to debug…\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.06\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n\n# Distribution\nprint(s.value_counts())\n\n\nSOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#load-data",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#load-data",
    "title": "Preamble",
    "section": "",
    "text": "# Load Water GeoPackage\nw_path = os.path.join('data','geo','Water.gpkg')\nif not os.path.exists(w_path):\n    water = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Water.gpkg?raw=true')\n    water.to_file(w_path)\n    print(\"Downloaded Water.gpkg file.\")\nelse:\n    water = gpd.read_file(w_path)\n\n# Boroughs GeoPackage\nb_path = os.path.join('data','geo','Boroughs.gpkg')\nif not os.path.exists(b_path):\n    boroughs = gpd.read_file('https://github.com/jreades/i2p/blob/master/data/src/Boroughs.gpkg?raw=true')\n    boroughs.to_file(b_path)\n    print(\"Downloaded Boroughs.gpkg file.\")\nelse:\n    boroughs = gpd.read_file(b_path)\n\n\n\nYou should have this locally…\nmsoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')\nmsoas = msoas.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm'])\n\n\n\nFeel free to load your local copy rather than loading via the URL:\nurl = 'https://github.com/jreades/i2p/blob/master/data/clean/2020-08-24-listings.csv.gz?raw=true'\ndf = pd.read_csv(url, compression='gzip', low_memory=False, \n                 usecols=['room_type','longitude','latitude','price'])\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\nYou should have: Data frame is 74,120 x 4.\ngdf = gpd.GeoDataFrame(df, \n      geometry=gpd.points_from_xy(?, ?, crs=?))\ngdf = gdf.to_crs(?)\ngdf.plot(column='price', cmap='plasma', scheme='quantiles', k=10, markersize=1, figsize=(8,6));\n\n\n\nNext, use a spatial join to link the listings to MSOAs.\n# ml == MSOA Listings\nml = gpd.sjoin(??).drop(\n                    columns=['latitude','longitude','index_right']\n)\nNow we want a count of room types by MSOA:\nmlgc = ml.groupby([??])['price'].agg(Count='??').reset_index() # msoa listings grouped counts\nmlgc.head()\nYou should get:\n\n\n\n\nMSOA11CD\nroom_type\nCount\n\n\n\n\n0\nE02000001\nEntire home/apt\n398\n\n\n1\nE02000001\nHotel room\n4\n\n\n2\nE02000001\nPrivate room\n48\n\n\n3\nE02000001\nShared room\n2\n\n\n4\nE02000002\nPrivate room\n10\n\n\n\nAnd let’s also get the median price by MSOA…\nmlgp = ml.groupby('MSOA11CD')['price'].agg(??).reset_index() # msoa listings grouped price\nmlgp.head()\nFor comparison purposes:\n\n\n\n\nMSOA11CD\nprice\n\n\n\n\n0\nE02000001\n125.00\n\n\n1\nE02000002\n50.00\n\n\n2\nE02000003\n40.00\n\n\n3\nE02000004\n29.00\n\n\n4\nE02000005\n69.00\n\n\n\nThe highest median price is $1,880, which implies not just heavy skew but something a bit out-of-this-world.\n\nWarning: I have a strong suspicion that price is so skewed that using it in most clustering contexts is almost impossible. This is why I use the PowerTransformer below.\n:::\nNow we can make use of the pivot table function to generate counts by MSOA in ‘wide’ format. I’d suggest using the empty cell below to work out the pivot table function first before assigning it back to the mlgc variable and overwriting the existing data.\n#mlgc.pivot(index=??, columns=??, values=??).reset_index()\n\n\n\n\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\n\nCount\n\n\n\n\nroom_type\n\nEntire home/apt\nHotel room\nPrivate room\nShared room\n\n\n0\nE02000001\n398.00\n4.00\n48.00\n2.00\n\n\n1\nE02000002\nnan\nnan\n10.00\nnan\n\n\n2\nE02000003\n5.00\nnan\n19.00\nnan\n\n\n\nmlgc = mlgc.pivot().reset_index()\nmlgc.columns = ['MSOA11CD','Entire home/apt','Hotel room','Private room','Shared room']\nmlgc.drop(columns=['Hotel room','Shared room'], inplace=True)\nmlgc.head()\nMerge the grouped counts and price back to a mlg (MSOA Listings Grouped) data frame:\nmlg = pd.merge(mlgc, mlgp, on=??)\nmlg = mlg.fillna(0).set_index('MSOA11CD')\nmlg.head()\nApply a PowerTransform to every column using a loop and fit_transform:\nfor c in mlg.columns.values:\n    mlg[c] = ??\nmlg.head(3)\nFor comparison:\n\n\n\n\nEntire home/apt\nPrivate room\nprice\n\n\n\n\nMSOA11CD\n\n\n\n\n\nE02000001\n2.12\n0.81\n1.81\n\n\nE02000002\n-1.99\n-0.76\n-0.21\n\n\nE02000003\n-0.66\n-0.16\n-0.83\n\n\n\nsns.set(style=\"white\")\nsns.pairplot(mlg, markers=\".\", height=3, diag_kind='kde');\n\n\nrddf = gpd.read_file(os.path.join('data','clean','Reduced_Dimension_Data.gpkg'), driver='GPKG').set_index('MSOA11CD')\nrddf.drop(columns=['OBJECTID','MSOA11NM','BNG_E','BNG_N','Borough','msoa11hclnm','Subregion',\n                   'geometry','Dimension 1','Dimension 2'], inplace=True)\nprint(f\"Reduced Dimensionality Data Frame is {rddf.shape[0]:,} x {rddf.shape[1]:,}\")\nYou should get Reduced Dimensionality Data Frame is 983 x 10.\nMerge the rddf and mlg files using the MSOA11CD and rescale all columns using MinMaxScaler:\n# Merge the reducded dimensionality data frame with the msoa listings grouped data frame\ncldf = pd.merge(??)\n\n# Rescale the columns using the MinMaxScaler set up in the Preamble\nfor c in cldf.columns:\n    cldf[c] = ??\n\n# We also create a 'result set' (rs) data frame to hold the \n# results of the clustering operation\nrs   = pd.merge(msoas, cldf, left_on='MSOA11CD', right_index=True).set_index('MSOA11CD')\n\n# Check output\ncldf.head(2)[['Component 1','Component 2','price']]\nA partial sample:\n\n\n\nMSOA11CD\nComponent 1\nComponent 2\nprice\n\n\n\n\nE02000001\n-0.08\n0.12\n0.25\n\n\nE02000002\n-0.88\n-0.33\n-0.42\n\n\n\n\n\n\nPlotting all columns is computationally costly and for little utility… it’s better to just select a few at random.\ncols_to_plot = np.random.choice(cldf.columns.values, 4, replace=False)\nprint(cols_to_plot)\n\n\n\nc_nm   = 'KMeans' # Clustering name\nk_pref = 3 # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\nNow capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (rs):\n# Add it to the data frame\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n# How are the clusters distributed?\nax = sns.histplot(data=rs, x=c_nm, bins=k_pref);\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(rs, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde');\nAnd here’s a map!\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=??, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\nStop: What critical assumption did we make when running this analysis?\n:::\n\n\n\n\nThere’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try clustering across a wider range.\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nx = []\ny = []\n\nfor k in range(??,??):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(??).fit(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gcf().suptitle(\"Average Silhouette Scores\");\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=2 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\nk_pref=??\n    \n#############\n# Do the clustering using the main columns\nkmeans = KMeans(??).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(cldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n    \n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\nStop: Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine..\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    c = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=c.columns.values)\n    centroids = centroids.append(c.mean(), ignore_index=True)\n    \nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\nodf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\n\n\nFor what it’s worth, I’ve had enormous trouble with DBSCAN and this kind of data. I don’t think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation.\ncldf2 = cldf.loc[:,['Component 1','Component 2']]\ncldf2.head()\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.gcf().suptitle(\"Nearest Neighbour Distances\");\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.25, 0.005):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\n\nodf['Count'] = odf.Count.astype(float)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\nax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')\ne = 0.06\ndbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n\n\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\nodf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nSee if you can work out why we have this issue by plotting the first 2 components… We could actually try clusteirng on secondary components instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so dominant.\n\n\n\nThis one invites you to experiment wih different values of \\(\\epsilon\\) so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I’ve deliberately left this untested so I can’t be sure it will work and you’ll need to debug…\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.06\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n\n# Distribution\nprint(s.value_counts())\n\n\nSOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#first-k-means-clustering",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#first-k-means-clustering",
    "title": "Preamble",
    "section": "",
    "text": "c_nm   = 'KMeans' # Clustering name\nk_pref = 3 # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf) # The process\n\nprint(kmeans.labels_) # The results\nNow capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (rs):\n# Add it to the data frame\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n# How are the clusters distributed?\nax = sns.histplot(data=rs, x=c_nm, bins=k_pref);\n# Going to be a bit hard to read if \n# we plot every variable against every\n# other variables, so we'll just pick a few\nsns.set(style=\"white\")\nsns.pairplot(rs, \n             vars=cols_to_plot, \n             hue=c_nm, markers=\".\", height=3, diag_kind='kde');\nAnd here’s a map!\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=??, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\nStop: What critical assumption did we make when running this analysis?\n:::\n\n\n\n\nThere’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try clustering across a wider range.\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nx = []\ny = []\n\nfor k in range(??,??):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(??).fit(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gcf().suptitle(\"Average Silhouette Scores\");\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=2 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\nk_pref=??\n    \n#############\n# Do the clustering using the main columns\nkmeans = KMeans(??).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(cldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n    \n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\nStop: Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine..\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    c = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=c.columns.values)\n    centroids = centroids.append(c.mean(), ignore_index=True)\n    \nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\nodf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\n\n\nFor what it’s worth, I’ve had enormous trouble with DBSCAN and this kind of data. I don’t think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation.\ncldf2 = cldf.loc[:,['Component 1','Component 2']]\ncldf2.head()\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.gcf().suptitle(\"Nearest Neighbour Distances\");\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.25, 0.005):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\n\nodf['Count'] = odf.Count.astype(float)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\nax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')\ne = 0.06\ndbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n\n\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\nodf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nSee if you can work out why we have this issue by plotting the first 2 components… We could actually try clusteirng on secondary components instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so dominant.\n\n\n\nThis one invites you to experiment wih different values of \\(\\epsilon\\) so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I’ve deliberately left this untested so I can’t be sure it will work and you’ll need to debug…\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.06\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n\n# Distribution\nprint(s.value_counts())\n\n\nSOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#second-k-means-clustering",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#second-k-means-clustering",
    "title": "Preamble",
    "section": "",
    "text": "There’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nLet’s try clustering across a wider range.\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nx = []\ny = []\n\nfor k in range(??,??):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(??).fit(cldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(cldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gcf().suptitle(\"Average Silhouette Scores\");\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing.\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=2 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\nk_pref=??\n    \n#############\n# Do the clustering using the main columns\nkmeans = KMeans(??).fit(cldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(cldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(cldf, kmeans.labels_)\n    \n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, cldf.shape[0] + (k + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k)\n    ax2.scatter(cldf[cldf.columns[0]], cldf[cldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\nStop: Make sure that you understand how the silhouette plot and value work, and why your results may diverge from mine..\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    c = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=c.columns.values)\n    centroids = centroids.append(c.mean(), ignore_index=True)\n    \nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\nodf = odf[~odf.Variable.isin(['Borough','msoa11hclnm','Subregion','geometry'])]\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\n\n\nFor what it’s worth, I’ve had enormous trouble with DBSCAN and this kind of data. I don’t think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation.\ncldf2 = cldf.loc[:,['Component 1','Component 2']]\ncldf2.head()\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.gcf().suptitle(\"Nearest Neighbour Distances\");\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.25, 0.005):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\n\nodf['Count'] = odf.Count.astype(float)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\nax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')\ne = 0.06\ndbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n\n\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\nodf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nSee if you can work out why we have this issue by plotting the first 2 components… We could actually try clusteirng on secondary components instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so dominant.\n\n\n\nThis one invites you to experiment wih different values of \\(\\epsilon\\) so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I’ve deliberately left this untested so I can’t be sure it will work and you’ll need to debug…\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.06\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n\n# Distribution\nprint(s.value_counts())\n\n\nSOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#dbscan",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#dbscan",
    "title": "Preamble",
    "section": "",
    "text": "For what it’s worth, I’ve had enormous trouble with DBSCAN and this kind of data. I don’t think it deals very well with more than three dimensions, so the flexbility to not have to specify the number of clusters is balanced with a density-based approach that is severely hampered by high-dimensional distance-inflation.\ncldf2 = cldf.loc[:,['Component 1','Component 2']]\ncldf2.head()\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=2).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.gcf().suptitle(\"Nearest Neighbour Distances\");\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nWARNING. This next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn’t) the graph above.\n:::\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.25, 0.005):\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\nodf = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = {'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}\n            odf = odf.append(d, ignore_index=True)\n\nodf['Count'] = odf.Count.astype(float)\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=odf, x='Epsilon', y='Count', hue='Cluster');\nax.vlines(0.06, 0, ax.get_ylim()[1], linestyle='dashed', color='r')\ne = 0.06\ndbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n\n\n\nWARNING. My sense is that these results are a bit rubbish: the majority of items are assigned to one cluster??? I’ve tried PCA on the standardised data and that made little difference. This should also have worked better but it seems that a small number of LSOAs are so utterly different that the more sophisticated clustering algorithm effectively ‘chokes’ on them.\n:::\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\ncentroids = None\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n\n    clsoas = rs[rs[c_nm]==k]\n    if centroids is None:\n        centroids = pd.DataFrame(columns=clsoas.columns.values)\n    centroids = centroids.append(clsoas.mean(), ignore_index=True)\n\nodf = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    c_index = list(centroids.columns.values).index(c_nm)\n    for c in range(0,c_index):\n        d = {'Variable':centroids.columns[c], 'Cluster':row[c_index], 'Std. Value':row[c]}\n        odf = odf.append(d, ignore_index=True)\n\n# Drop outliers\nodf = odf[odf.Cluster &gt;= 0]\nodf.drop(index=odf[odf.Variable.isin(['geometry','KMeans'])].index, inplace=True)\n\ng = sns.FacetGrid(odf, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.plot, \"Cluster\", \"Std. Value\", marker=\".\")\n\ndel(odf, centroids)\n\n\nSee if you can work out why we have this issue by plotting the first 2 components… We could actually try clusteirng on secondary components instead (e.g. 2 and 3, or 3 and 4) on the basis that the first one is so dominant.\n\n\n\nThis one invites you to experiment wih different values of \\(\\epsilon\\) so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I’ve deliberately left this untested so I can’t be sure it will work and you’ll need to debug…\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.06\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n\n# Distribution\nprint(s.value_counts())\n\n\nSOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#optics",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#optics",
    "title": "Preamble",
    "section": "",
    "text": "This one invites you to experiment wih different values of \\(\\epsilon\\) so it makes even fewer assumptions than DBSCAN but is even more computationally intensive. I’ve deliberately left this untested so I can’t be sure it will work and you’ll need to debug…\n\nWARNING. This next step may take quite a lot of time since the algorithm is making far fewer assumptions about the structure of the data. On a 2018 MacBook Pro with 16GB of RAM it took about 5 minutes.\n:::\nc_nm = 'Optics'\n\n# Can try to set this from DBSCAN results\ne = 0.06\n\nimport math\n\n# Run the clustering\nopt = OPTICS(min_samples=len(cldf2.columns)+1, max_eps=math.ceil(e * 100)/10, n_jobs=-1).fit(cldf2)\n\n# See how we did\ns = pd.Series(opt.labels_, index=cldf.index, name=c_nm)\nrs[c_nm] = s\n\n# Distribution\nprint(s.value_counts())\n\n\nSOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#self-organising-maps",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#self-organising-maps",
    "title": "Preamble",
    "section": "",
    "text": "SOMs offer a fourth type of clustering algorithm. They are a relatively ‘simple’ type of neural network in which the ‘map’ (of the SOM) adjusts to the data: we’re going to see how this works over the next few code blocks, but the main thing is that, unlike the above approaches, SOMs build a 2D map of a higher-dimensional space and use this as a mechanism for subsequently clustering the raw data. In this sense there is a conceptual link between SOMs and PCA or tSNE (another form of dimensionality reduction).\n\n\nWe are going to actually train the SOM using the input data. This is where you specify the input parameters that have the main effect on the clustering results.\nfrom sompy.sompy import SOMFactory\nc_nm = 'SOM'\n\nsm = SOMFactory().build(\n    cldf.values, mapsize=(10,15),\n    normalization='var', initialization='random', component_names=cldf.columns.values)\nsm.train(n_job=4, verbose=False, train_rough_len=2, train_finetune_len=5)\nHow good is the fit?\ntopographic_error  = sm.calculate_topographic_error()\nquantization_error = np.mean(sm._bmu[1])\nprint(\"Topographic error = {0:0.5f}; Quantization error = {1:0.5f}\".format(topographic_error, quantization_error))\nHow do the results look?\nfrom sompy.visualization.mapview import View2D\nview2D = View2D(10, 10, \"rand data\", text_size=10)\nview2D.show(sm, col_sz=4, which_dim=\"all\", denormalize=True)\nplt.savefig(f\"{c_nm}-Map.png\", dpi=200)\n\n\n\n\nNote: Your results may differ.\n:::\n\nHow many data points were assigned to each BMU?\nfrom sompy.visualization.bmuhits import BmuHitsView\nvhts = BmuHitsView(15, 15, \"Hits Map\", text_size=8)\nvhts.show(sm, anotate=True, onlyzeros=False, labelsize=9, cmap=\"plasma\", logaritmic=False)\nplt.savefig(f\"{c_nm}-BMU Hit View.png\", dpi=200)\nHow many clusters do we want and where are they on the map?\nfrom sompy.visualization.hitmap import HitMapView\n\nk_val = 6 # The way this library is set up it's hard to explore the k-means clustering of Hits\nsm.cluster(k_val)\nhits  = HitMapView(15, 15, \"Clustering\", text_size=14)\na     = hits.show(sm)\nplt.savefig(f\"{c_nm}-Hit Map View.png\", dpi=200)\nFinally, let’s get the cluster results and map them back on to the data points:\n# Get the labels for each BMU\n# in the SOM (15 * 10 neurons)\nclabs = sm.cluster_labels\n\n# Project the data on to the SOM\n# so that we get the BMU for each\n# of the original data points \n# (This is similar to the transform \n# function is sklearn)\nbmus  = sm.project_data(cldf.values)\n\n# Turn the BMUs into cluster labels\n# and append to the data frame\ns = pd.Series(clabs[bmus], index=cldf.index, name=c_nm)\n\nrs[c_nm] = s\nfig, ax = plt_ldn(water, boroughs)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n\n\nWARNING. These are the results from the approach that is closest to the one outlined in Geocomputation.\n:::\n\n\n\nhttps://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html\n\n\n\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#adbscan",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#adbscan",
    "title": "Preamble",
    "section": "",
    "text": "https://pysal.org/esda/generated/esda.adbscan.ADBSCAN.html"
  },
  {
    "objectID": "practicals/fragments/Practical-X-Grouping_Data.html#wrap-up",
    "href": "practicals/fragments/Practical-X-Grouping_Data.html#wrap-up",
    "title": "Preamble",
    "section": "",
    "text": "Find the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "readings/Burton-1963.html",
    "href": "readings/Burton-1963.html",
    "title": "The quantitative revolution and theoretical geography",
    "section": "",
    "text": "1. What kind of reading is it?\n“The Quantitative Revolution and Theoretical Geography” by Ian Burton is an academic paper published in The Canadian Geographer in 1963¹. It aims to make a theoretical and conceptual contribution by discussing the significant methodological shift in geography towards quantitative methods and theoretical frameworks during the 1950s and 1960s².\n\n\n2. Who is the intended audience?\nThe intended audience for this paper is primarily academics and field experts in geography and related disciplines². The scholarly style, complex theoretical discussions, and detailed methodological analysis indicate it is meant for researchers, educators, and advanced students who are familiar with the basics of geographical study and are looking to deepen their understanding of its theoretical and methodological evolution².\n\n\n3. How is the piece structured?\nThe paper is structured in a typical academic format, including:\n\nIntroduction: Introduces the concept of the quantitative revolution in geography.\nMain Body: Divided into sections with headings and sub-headings, covering:\n\nHistorical context and background.\nKey developments and figures.\nTheoretical implications and methodological changes.\nCase studies or examples.\n\nConclusion: Summarizes the main points and discusses future directions or implications for the field¹².\n\nThis structure helps the academic audience navigate through complex discussions, making it easier to follow and understand the arguments presented².\n\n\n4. What are the key ideas, concepts, or theories discussed?\nThe key ideas and concepts discussed include:\n\nQuantitative Revolution: The shift towards quantitative methods and statistical analysis in geography.\nTheoretical Geography: Development of new theoretical frameworks emphasizing quantitative data and mathematical models.\nMethodological Changes: Adoption of sophisticated mathematical methods, model testing, and probabilistic thought.\nImpact on the Discipline: Transformation of geography from a descriptive discipline to one seeking general law-like regularities in spatial phenomena¹².\n\nThese concepts are central to understanding the transformation of geography during this period and are discussed in detail to illustrate the significance of the quantitative revolution¹².\n\n\n5. What is the overall contribution?\nThe paper builds on the existing body of geographical knowledge by highlighting the shift towards quantitative methods and theoretical frameworks. It responds to the gap in understanding how these changes transformed the discipline. The key findings include the identification of significant methodological and theoretical shifts that redefined geography as a more scientific and rigorous discipline¹².\n\n\n6. What issues or gaps remain?\nSome potential issues or gaps include:\n\nAssumptions: The paper assumes that quantitative methods are universally applicable, which might not hold in all geographical contexts.\nContextual Variability: The impact of the quantitative revolution might vary across different regions and sub-disciplines of geography.\nFuture Work: Areas for future work could include exploring the applicability of quantitative methods in diverse geographical contexts and integrating qualitative approaches to provide a more holistic understanding¹².\n\n\n\n7. How does this connect to the module/programme?\nThis reading connects to the module by providing a historical and theoretical foundation for understanding the evolution of geographical methods. It prompts reflection on the methodological shifts in the discipline and encourages students to think critically about the applicability and limitations of quantitative methods. This reading likely complements other modules focused on research methods, theoretical frameworks, and the history of geographical thought, raising further questions about the integration of different methodological approaches in geography¹².\nIf you have any more questions or need further details, feel free to ask!\n¹: Academia.edu ²: SpringerLink\nConersation with Copilot 24/09/2024\n\nThe Quantitative Revolution and Theoretical Geography - Academia.edu. https://www.academia.edu/102460407/The_Quantitative_Revolution_and_Theoretical_Geography.\nQuantitative Revolution in Geography | SpringerLink. https://link.springer.com/referenceworkentry/10.1007/978-3-031-25900-5_206-1.\n25 QUANTITATIVE REVOLUTION IN GEOGRAPHY - INFLIBNET Centre. https://ebooks.inflibnet.ac.in/geop06/chapter/quantitative-revolution-in-geography/.\nThe ‘Quantitative Revolution’: Regional Geography at its Apogee. https://link.springer.com/chapter/10.1007/978-94-009-0483-5_3.\nRetheorizing Economic Geography: From the Quantitative Revolution to …. https://blogs.ubc.ca/trevorbarnes/files/2015/01/PAPER_Retheorising_Economic_Geography.pdf.\n\n\n\n\n\n\nReferences\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x."
  },
  {
    "objectID": "ref/fontawesome.html",
    "href": "ref/fontawesome.html",
    "title": "Font Awesome Quarto Extension",
    "section": "",
    "text": "This extension allows you to use font-awesome icons in your Quarto HTML and PDF documents. It provides an {{&lt; fa &gt;}} shortcode:\n\nMandatory &lt;icon-name&gt;:\n{{&lt; fa &lt;icon-name&gt; &gt;}}\nOptional &lt;group&gt;, &lt;size=...&gt;, and &lt;title=...&gt;:\n{{&lt; fa &lt;group&gt; &lt;icon-name&gt; &lt;size=...&gt; &lt;title=...&gt; &gt;}}\n\nFor example:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up &gt;}}\n\n\n\n{{&lt; fa folder &gt;}}\n\n\n\n{{&lt; fa chess-pawn &gt;}}\n\n\n\n{{&lt; fa brands bluetooth &gt;}}\n\n\n\n{{&lt; fa brands twitter size=2xl &gt;}} (HTML only)\n\n\n\n{{&lt; fa brands github size=5x &gt;}} (HTML only)\n\n\n\n{{&lt; fa battery-half size=Huge &gt;}}\n\n\n\n{{&lt; fa envelope title=\"An envelope\" &gt;}}\n\n\n\n\nColors:\n\n\n\n\n\n\n\nShortcode\nIcon\n\n\n\n\n{{&lt; fa thumbs-up color=#0000ff &gt;}}\n\n\n\n{{&lt; fa thumbs-up color=rgb(0,255,0) &gt;}}\n\n\n\n{{&lt; fa thumbs-up color=rgb(1,0,0) &gt;}}\n\n\n\n{{&lt; fa thumbs-up color=rgb(1,0.1,0.8) size=Huge &gt;}}\n\n\n\n\nNote that the icon sets are currently not perfectly interchangeable across formats:\n\nhtml uses FontAwesome 6.4.2\npdf uses the fontawesome5 package, based on FontAwesome 5.\nOther formats are currently not supported, but PRs are always welcome!"
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Overview",
    "section": "",
    "text": "We have ‘flipped’ the classroom for this module so we expect you to come to ‘lecture’ (except in Week 1!) having already watched the assigned videos and completed the assigned readings. From Week 3 you may be called upon to answer questions about the key points in, and relevance of, an assigned video or reading to the rest of the class.\nThis means that there is a mix of ‘asynchronous’ (work that you do in your own time) and ‘synchronous’ (work that we do during scheduled hours) interaction. Synchronous activities will normally be recorded for review afterwards, but you should bear in the mind the following: 1) we cannot be responsible for equipment failure; 2) we are unable to record practicals and other small-group activities; and 3) a 2-hour video of the in-person session will be rather less educational and informative than actually being there.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#preparation",
    "href": "sessions/index.html#preparation",
    "title": "Overview",
    "section": "Preparation",
    "text": "Preparation\nThe nature and amount of preparation will vary from week to week, but may include:\n\nReadings from both academic and non-academic sources.\nPre-recorded lectures from CASA staff.\nPre-ecorded videos from non-CASA staff.\nPreparing contributions to set tasks (e.g. summaries, Q&A, etc.)\n\nTo get the most value from the module you must do the readings. We have raised the stakes for not doing so since, in previous years, students who did not do the readings often struggled with the final assessment and went on to have even more significant struggles with their dissertation.\nIf you don’t do the readings you are leaving a lot of easy marks on the table. More importantly, we believe that the single most important skill that you can acquire from FSDS is not the ability to code, it’s the ability to critically interrogate data and recognise the strengths and limitations that are relevant to the problem at-hand. You will learn the technical aspects of data analysis in the practicals. You will learn the critical dimension from doing the readings.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#in-person-session",
    "href": "sessions/index.html#in-person-session",
    "title": "Overview",
    "section": "In-Person Session",
    "text": "In-Person Session\nNote that this is not your usual lecture, it’s going to be a very interactive in-person session involving responses to assigned questions, lecturer-led discussions, and some amount of ‘live coding’. We will assume you have completed the preparatory activities, which will include both ‘Preparatory Lectures’ (a series of short videos for which the slides are also available for note-taking) and ‘Other Preparation’ (primarily readings).\nFor the readings you will have a Template to complete. Download the raw QMD file and fill one in for each of the week’s assigned readings.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#practicals",
    "href": "sessions/index.html#practicals",
    "title": "Overview",
    "section": "Practicals",
    "text": "Practicals\nIn order to make use of these materials you will need to install the Spatial Data Science programming environment.\nPracticals are run in groups to maximise your ability to ask questions and interact with other students. You will be notified of your group by the Professional Services team; there may be limited opportunities to switch, and the best way would be to swap with another student and then notify us of the arrangment. You may wish to download the week’s Jupyter notebook before the start of class in order to familiarise yourself with the material.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#improvements",
    "href": "sessions/index.html#improvements",
    "title": "Overview",
    "section": "Improvements",
    "text": "Improvements\nWe are always making improvements to FSDS and try to keep track of student-feedback here.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "lectures/5.2-Classes.html#whats-an-object",
    "href": "lectures/5.2-Classes.html#whats-an-object",
    "title": "Classes",
    "section": "What’s an Object?",
    "text": "What’s an Object?\nObjects are instantiated versions of classes:\n\n\"hello world\" is an instance of a string, and\n['A','B',1,3] is an instance of a list.\n\nThe class is your recipe, the object is your 🍕…"
  },
  {
    "objectID": "lectures/5.2-Classes.html#really-like-a-pizza",
    "href": "lectures/5.2-Classes.html#really-like-a-pizza",
    "title": "Classes",
    "section": "Really… Like a Pizza!",
    "text": "Really… Like a Pizza!\nclass pizza(object):\n  base = 'sourdough'\n  \n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\n    \n  def add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\n  \n  def get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/5.2-Classes.html#class-definition",
    "href": "lectures/5.2-Classes.html#class-definition",
    "title": "Classes",
    "section": "Class Definition",
    "text": "Class Definition\nclass pizza(object):\n\n    base = 'sourdough'\n    ...\nFollows the pattern: class &lt;name&gt;(&lt;parent class&gt;).\nYou can find many examples in: /opt/conda/envs/sds2020/lib/python3.7/site-packages (Docker)."
  },
  {
    "objectID": "lectures/5.2-Classes.html#the-constructor",
    "href": "lectures/5.2-Classes.html#the-constructor",
    "title": "Classes",
    "section": "The Constructor",
    "text": "The Constructor\n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce    = sauce\n    self.cheese   = cheese\nFollows the pattern: def __init__(self, &lt;params&gt;)\n\n\nNotice also the namespace: the parameters sauce and cheese are the same as the instance variables self.sauce and self.cheese because they occupy different namespaces."
  },
  {
    "objectID": "lectures/5.2-Classes.html#adding-toppings",
    "href": "lectures/5.2-Classes.html#adding-toppings",
    "title": "Classes",
    "section": "Adding Toppings",
    "text": "Adding Toppings\ndef add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\nFollows the pattern: def &lt;function&gt;(self, &lt;params&gt;):"
  },
  {
    "objectID": "lectures/5.2-Classes.html#getting-the-pizza",
    "href": "lectures/5.2-Classes.html#getting-the-pizza",
    "title": "Classes",
    "section": "Getting the Pizza",
    "text": "Getting the Pizza\ndef get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/5.2-Classes.html#pizza-in-action",
    "href": "lectures/5.2-Classes.html#pizza-in-action",
    "title": "Classes",
    "section": "Pizza in Action",
    "text": "Pizza in Action\np = pizza(sauce='white')\np.add_topping('peppers')\np.add_topping('chillis')\np.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chillis']"
  },
  {
    "objectID": "lectures/5.2-Classes.html#check-it-out",
    "href": "lectures/5.2-Classes.html#check-it-out",
    "title": "Classes",
    "section": "Check it Out",
    "text": "Check it Out\np1 = pizza(sauce='white')\np1.add_topping('peppers')\np1.add_topping('chilis')\n\np2 = pizza()\np2.base = \"Plain old base\"\np2.add_topping('pineapple')\np2.add_topping('ham')\n\np1.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()\n&gt; ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']"
  },
  {
    "objectID": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza",
    "href": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza",
    "text": "Recap: How to Make a Pizza\nA class is defined by:\nclass &lt;name&gt;(&lt;parent class):\n  ...\nA class is initialised by:\n  def __init__(self, &lt;any_parameters&gt;):\n    ...\nAll methods have to have this:\n  def &lt;method&gt;(self, &lt;any_parameters&gt;):\n    ..."
  },
  {
    "objectID": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "href": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "title": "Classes",
    "section": "Recap: How to Make a Pizza (cont’d)",
    "text": "Recap: How to Make a Pizza (cont’d)\nThis is an instance variable:\n  self.&lt;var&gt; = &lt;something&gt;\nThis is a class variable (in the class definition):\n  &lt;var&gt; = &lt;something&gt;"
  },
  {
    "objectID": "lectures/5.2-Classes.html#resources",
    "href": "lectures/5.2-Classes.html#resources",
    "title": "Classes",
    "section": "Resources",
    "text": "Resources\n\n\n\nClasses\nObjects\nBasic class definition\nInstance methods and attributes\nChecking instance types\nClass methods and members\n\n\n\nCreating a class\nConstructing an object\nClass methods\nClass vs Instance Variables\nObject data\nInheritance\nAn Introduction to Object-Oriented Programming (by a CASA alum)\n\n\n\n\n\n\nClasses • Jon Reades"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#iteration",
    "href": "lectures/2.6-Iteration.html#iteration",
    "title": "Iteration",
    "section": "it·er·a·tion",
    "text": "it·er·a·tion\n/itə’rāSHən/\nNoun\nThe repetition of a process or utterance.\n\nrepetition of a mathematical or computational procedure applied to the result of a previous application, typically as a means of obtaining successively closer approximations to the solution of a problem.\na new version of a piece of computer hardware or software. plural noun: iterations\n\n\nMany programmers also call these loops."
  },
  {
    "objectID": "lectures/2.6-Iteration.html#two-types-of-iteration",
    "href": "lectures/2.6-Iteration.html#two-types-of-iteration",
    "title": "Iteration",
    "section": "Two Types of Iteration",
    "text": "Two Types of Iteration\n\n\n‘For’ loops\n\nUsed with finite lists of definite length\nFor each item in this list do something…\n\n\n‘While’ loops:\n\nUsed with unknown or non-finite lists\nWhile a condition is still True, do something to the list…"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#making-the-difference-memorable",
    "href": "lectures/2.6-Iteration.html#making-the-difference-memorable",
    "title": "Iteration",
    "section": "Making the Difference Memorable",
    "text": "Making the Difference Memorable"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#for-loops-1",
    "href": "lectures/2.6-Iteration.html#for-loops-1",
    "title": "Iteration",
    "section": "For Loops",
    "text": "For Loops\nThis ‘simple’ loop allows us to print out every element of the list in turn:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\nNotice the format:\nfor x in list:\n  # ...do something using the current value of x...\n\nNotice that this is the same in that we saw with if 'Batty' in geographers!"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#while-loops-1",
    "href": "lectures/2.6-Iteration.html#while-loops-1",
    "title": "Iteration",
    "section": "While Loops",
    "text": "While Loops\nThis loop does the same thing, but differently:\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )\n  g += 1\nNotice the format:\nwhile &lt;some condition is true&gt;:\n  # ...do something..."
  },
  {
    "objectID": "lectures/2.6-Iteration.html#nesting-loops",
    "href": "lectures/2.6-Iteration.html#nesting-loops",
    "title": "Iteration",
    "section": "Nesting Loops",
    "text": "Nesting Loops\nWe can use one loop ‘inside’ another loop! What do you think this might print?\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  for h in g:\n    print(h)\nHuh??? Let’s puzzle this out…"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#debugging",
    "href": "lectures/2.6-Iteration.html#debugging",
    "title": "Iteration",
    "section": "Debugging",
    "text": "Debugging\nWhen you see something completely new, it’s often good to:\n\nAdd print(...) statements to see how the values of a variable are changing.\nComment out the parts you don’t understand so that you can focus on the parts you do\nThen iteratively add complexity!"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#step-1-the-outer-loop",
    "href": "lectures/2.6-Iteration.html#step-1-the-outer-loop",
    "title": "Iteration",
    "section": "Step 1: The ‘Outer Loop’",
    "text": "Step 1: The ‘Outer Loop’\nSo I would start off with:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\n#   for h in g:\n#   print(h)\nThis prints:\n'Rose'\n'Massey'\n'Jefferson'"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#step-1-summing-up",
    "href": "lectures/2.6-Iteration.html#step-1-summing-up",
    "title": "Iteration",
    "section": "Step 1: Summing Up",
    "text": "Step 1: Summing Up\nOK, so now we know:\n\nThat g is the name of a geographer.\nThe ‘outer’ loop sets g to the name of a new geographer on each iteration.\nSo if g is set to 'Rose' what does for h in g: do?"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#step-2-the-inner-loop",
    "href": "lectures/2.6-Iteration.html#step-2-the-inner-loop",
    "title": "Iteration",
    "section": "Step 2: The ‘Inner’ Loop",
    "text": "Step 2: The ‘Inner’ Loop\nWe know change it like this:\nfor g in geographers:\n  print(g)\n  for h in g:\n    print(h)\n  break # &lt;-- Notice this!\nThis prints:\nRose\nR\no\ns\ne"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#step-2-summing-up",
    "href": "lectures/2.6-Iteration.html#step-2-summing-up",
    "title": "Iteration",
    "section": "Step 2: Summing Up",
    "text": "Step 2: Summing Up\nAnd now we know that:\n\nh is looping over the string, meaning that a string can be treated as a list!\nAnd break is a really useful way to control a loop while you’re testing your code!"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#recap",
    "href": "lectures/2.6-Iteration.html#recap",
    "title": "Iteration",
    "section": "Recap",
    "text": "Recap\n\nfor iterates once over a collection items (e.g. a list).\nwhile keeps going until a condition is False."
  },
  {
    "objectID": "lectures/2.6-Iteration.html#test-yourself",
    "href": "lectures/2.6-Iteration.html#test-yourself",
    "title": "Iteration",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat will this code print? I’d suggest that you don’t run it!\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#test-yourself-tricksy-version",
    "href": "lectures/2.6-Iteration.html#test-yourself-tricksy-version",
    "title": "Iteration",
    "section": "Test Yourself (Tricksy Version)",
    "text": "Test Yourself (Tricksy Version)\nHere’s a really tricky one! The following two blocks of code produce the same output, how are they different?\ngeographers = ['Rose','Massey','Jefferson']\ngeographers.reverse()\nfor g in geographers:\n  print(g)\nAnd:\ngeographers = ['Rose','Massey','Jefferson']\ng = len(geographers)-1\nwhile g &gt;= 0:\n  print( geographers[g] )\n  g -= 1"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#one-more-thing",
    "href": "lectures/2.6-Iteration.html#one-more-thing",
    "title": "Iteration",
    "section": "One More Thing…",
    "text": "One More Thing…\nLet’s go back to the Lists examples for a second:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nHave a think about how this code works:\nfor ag in all_geographers:\n  for g in ag:\n    print(g)"
  },
  {
    "objectID": "lectures/2.6-Iteration.html#resources",
    "href": "lectures/2.6-Iteration.html#resources",
    "title": "Iteration",
    "section": "Resources",
    "text": "Resources\n\nWhat is Iteration?\nLoops\nFor Loop\nWhile Loop\n\nWe don’t cover the concept of recursion, but it’s quite a powerful idea and links nicely with Iteration:\n\nWhat is a recursive function?\nDefine recursive functions\n\n\n\n\n\nIteration • Jon Reades"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html",
    "href": "practicals/Practical-12-Grouping_Data.html",
    "title": "Practical 12: Grouping Data",
    "section": "",
    "text": "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: this group of observations are similar to one another, that group is dissimilar to this group. Sometimes we have a label that we can use as part of the process (in which case we’re doing classification), and somtimes we don’t (in which case we’re doing clustering). But what defines similarity and difference? There is no one answer to that question and so there are many different ways to cluster or classify data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#london-data-layers",
    "href": "practicals/Practical-12-Grouping_Data.html#london-data-layers",
    "title": "Practical 12: Grouping Data",
    "section": "London Data Layers",
    "text": "London Data Layers\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nmsoas = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', ddir) )\nmsoas = msoas.to_crs(epsg=27700)\n\n# I don't use this in this practical, but it's a\n# really useful data set that gives you 'names'\n# for MSOAs that broadly correspond to what most\n# Londoners would think of as a 'neighbourhood'.\nmsoa_nms = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011-Names.gpkg', ddir) )\nmsoa_nms = msoa_nms.to_crs(epsg=27700)\nprint(\"Done.\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#reduced-dimensionality-msoa-data",
    "href": "practicals/Practical-12-Grouping_Data.html#reduced-dimensionality-msoa-data",
    "title": "Practical 12: Grouping Data",
    "section": "Reduced Dimensionality MSOA Data",
    "text": "Reduced Dimensionality MSOA Data\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nYou should have this locally from last week, but just in case…\nhost = 'http://orca.casa.ucl.ac.uk'\npath = '~jreades/data'\nrddf = gpd.read_parquet( cache_data(f'{host}/{path}/Reduced_Dimension_Data.geoparquet', ddir) )\nprint(f\"Data frame is {rddf.shape[0]:,} x {rddf.shape[1]}\")\nYou should have: Data frame is 983 x 93.\nAnd below you should see both the components and the dimensions from last week’s processing.\nrddf.iloc[0:3, -7:]\nI get the results below, but note that the Dimension values may be slightly different:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent 5\nComponent 6\nComponent 7\nBorough\nDimension 1\nDimension 2\nSubregion\n\n\n\n\nE02000001\n1.44\n3.95\n-1.52\nCity of London\n7.74\n3.36\nInner West\n\n\nE02000002\n-0.28\n0.89\n0.26\nBarking and Dagenham\n2.04\n7.59\nOuter East and North East\n\n\nE02000003\n-0.11\n1.12\n0.83\nBarking and Dagenham\n2.20\n6.87\nOuter East and North East"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#listings-data",
    "href": "practicals/Practical-12-Grouping_Data.html#listings-data",
    "title": "Practical 12: Grouping Data",
    "section": "Listings Data",
    "text": "Listings Data\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nLet’s also get the listings data from a few weeks back:\n\n# Set download URL\nymd  = '2024-06-14'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-listings.geoparquet'\n\nlistings = gpd.read_parquet( cache_data(url, ddir) )\nlistings = listings.to_crs(epsg=27700)\nprint(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")\nYou should have: Data frame is 85,134 x 31.\nAnd a quick plot of the price to check:\nlistings.plot(???, cmap='plasma', scheme='quantiles', k=10, \n              markersize=.5, alpha=0.15, figsize=(10,7));"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#join-listings-to-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#join-listings-to-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Join Listings to MSOA",
    "text": "Join Listings to MSOA\n\n\n\n\n\n\nDifficulty: Medium-to-hard.\n\n\n\n\n\n\n\n\n\nFirst, let’s link all this using the MSOA Geography that we created last week and a mix or merge and sjoin!\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: Notice a few things going on here! We are calling `gpd.sjoin` because pandas (`pd`) doesn't know about spatial joins, only geopandas (`gpd`) does. More on this [next week](https://jreades.github.io/fsds/sessions/week10.html#lectures). Also see how we drop some columns _at the point where we do the join_ by taking advantage of the fact that most pandas/geopandas operations return a _copy_ of the (Geo)DataFrame. That allows us to get back from the spatial join a neat, tidy data frame ready for further analysis. If you're struggling to make sense of this, try removing the `drop` operations and see what your data frame looks like afterwards. This should all be old hat, but in case you need a refresher there's always [Week 5](https://jreades.github.io/fsds/sessions/week5.html) on pandas.\n\n\n# Before the spatial join\nlistings.columns\nmsoa_listings = gpd.sjoin(???, msoas.drop(\n                        columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM',\n                                 'USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', \n                                 'AVHHOLDSZ']), predicate='???').drop(\n                        columns=['latitude','longitude','index_right']\n                )\n# All we've added is the MSOA11CD\nmsoa_listings.columns\nAll being well you should now have:\nIndex(['listing_url', 'last_scraped', 'name', 'description', 'host_id',\n       'host_name', 'host_since', 'host_location', 'host_is_superhost',\n       'host_listings_count', 'host_total_listings_count',\n       'host_verifications', 'property_type', 'room_type', 'accommodates',\n       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n       'minimum_nights', 'maximum_nights', 'availability_365',\n       'number_of_reviews', 'first_review', 'last_review',\n       'review_scores_rating', 'reviews_per_month', 'geometry', 'MSOA11CD'],\n      dtype='object')"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#price-by-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#price-by-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Price by MSOA",
    "text": "Price by MSOA\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nLet’s calculate the median price by MSOA… Notice that we have to specify the column we want after the groupby so the we don’t get the median of every column returned\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: I find `groupby` to be a complex operation and often need a couple of gos before I get back waht I want. The thing to take away is that: 1) anything in the `groupby` will become part of the `index` afterwards (so if you group on multiple things you get a multi-part index); 2) aggregating functions apply to _all_ columns unless you filter them some way. Here we filter by selecting only the `price` column to aggregate. You can also filter for `numeric only`.\n\n\n# *m*soa *l*istings *g*rouped by *p*rice\nmlgp = msoa_listings.groupby('???')['price'].agg('???') \nmlgp.head()\nYou should get something like:\nMSOA11CD\nE02000001   170.00\nE02000002    97.00\nE02000003    80.00\nE02000004    54.00\nE02000005   100.00\nName: price, dtype: float64"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#room-type-by-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#room-type-by-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Room Type by MSOA",
    "text": "Room Type by MSOA\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow let’s calculate the count of room types by MSOA and compare the effects of reset_index on the outputs below. And notice too that we can assign the aggregated value to a column name!\n# *m*soa *l*istings *g*rouped *c*ount\nmlgc = msoa_listings.groupby(['???','???'], observed=False).listing_url.agg(Count='???')\nmlgc.head()\nYou should get something resembling this:\n\n\n\nMSOA11CD\nroom_type\nCount  \n\n\n\n\nE02000001\nEntire home/apt\n466\n\n\n\nHotel room\n0\n\n\n\nPrivate room\n61\n\n\n\nShared room\n1\n\n\nE02000002\nEntire home/apt\n4\n\n\n\n# *m*soa *l*istings *g*rouped *c*ount *r*eset index\nmlgcr = msoa_listings.groupby(['???','???'], observed=False).listing_url.agg(Count='???').reset_index() # msoa listings grouped counts\nmlgcr.head()\nYou should get something like:\n\n\n\n\nMSOA11CD\nroom_type\nCount\n\n\n\n\n0\nE02000001\nEntire home/apt\n466\n\n\n1\nE02000001\nHotel room\n0\n\n\n2\nE02000001\nPrivate room\n61\n\n\n3\nE02000001\nShared room\n1\n\n\n4\nE02000002\nEntire home/apt\n4"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#price-by-room-type",
    "href": "practicals/Practical-12-Grouping_Data.html#price-by-room-type",
    "title": "Practical 12: Grouping Data",
    "section": "Price by Room Type",
    "text": "Price by Room Type\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nBut perhaps median price/room type would make more sense? And do we want to retain values where there are no listings? For example, there are no hotel rooms listed for E02000001, how do we ensure that these NAs are dropped?\n# *m*soa *l*istings *g*rouped *r*oom *p*rice\nmlgrp = msoa_listings.???(???, observed=True\n                      )['price'].agg('???').reset_index()\nmlgrp.head()\nYou should get something like:\n\n\n\n\nMSOA11CD\nroom type\nprice\n\n\n\n\n0\nE02000001\nEntire home/apt\n177.00\n\n\n2\nE02000001\nPrivate room\n100.00\n\n\n3\nE02000001\nShared room\n120.00\n\n\n4\nE02000002\nEntire home/apt\n117.00\n\n\n6\nE02000002\nPrivate room\n42.00"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#explore-outlier-per-msoa-prices",
    "href": "practicals/Practical-12-Grouping_Data.html#explore-outlier-per-msoa-prices",
    "title": "Practical 12: Grouping Data",
    "section": "Explore Outlier Per-MSOA Prices",
    "text": "Explore Outlier Per-MSOA Prices\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nAre there MSOAs what look like they might contain erroneous data?\n\nPlot MSOA Median Prices\nmlgp.hist(bins=200);\n\n\nExamine Listings from High-Priced MSOAs\nCareful, this is showing the listings from MSOAs whose median price is above $300/night:\nmsoa_listings[\n    msoa_listings.MSOA11CD.isin(mlgp[mlgp &gt; 300].index)\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','name','description']\n]\nSome of these look legi (4, 5, and… 8 bedroom ‘villas’?), though not every one…\nAnd how about these?\nmsoa_listings[\n    (msoa_listings.MSOA11CD.isin(mlgp[mlgp &gt; 300].index)) & (msoa_listings.room_type!='Entire home/apt')\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','property_type','name','description']\n]\nIf we wanted to be rigorous then we’d have to investigate further: properties in Mayfair and Westminster are going to be expensive, but are these plausible nightly prices? In some cases, yes. In others…\nmsoa_listings[\n    (msoa_listings.MSOA11CD.isin(mlgp[mlgp &lt; 100].index)) & (msoa_listings.room_type!='Entire home/apt')\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','name','description']\n]\nOn the whole, let’s take a guess that there are a small number of implausibly high prices for individual units that aren’t in very expensive neighbourhoods and that these are either erroneous/deliberately incorrect, or represent a price that is not per-night.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: What's the right answer here? There isn't one. You could probably spend _months_ figuring out what's a real, available-to-let listing and waht isn't. I would argue that assuming all listings are 'legit' without doing some additional EDA and ESDA is negligent. You could also look at how some of the methods of standardisation/normalisation work and use those to identify improbable listings (but remember that a £10,000 in Mayfair _might_ be legit, while a $5,000 listing in Barking _probably_ isn't!). Or you could look at the inter-decile range (or just define your own range: 1%-99%?).\n\n\n\n\nFilter Unlikely Listings\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nSee if you can filter out these less likely listings on the following criteria:\n\nListings are priced above $300/night AND\nRoom type is not 'Entire home/apt' AND\nListings do not contain the words: suite, luxury, loft, stunning, prime, historic, or deluxe.\n\nI found 901 rows to drop this way.\ntarget_regex = r'(?:suite|luxury|loft|stunning|prime|historic|deluxe|boutique)'\nto_drop = msoa_listings[\n            (???) & \n            (???) &\n            ~(???(target_regex, flags=re.IGNORECASE, regex=True, na=True))]\nprint(f\"Have found {to_drop.shape[0]:,} rows to drop on the basis of unlikely per night prices.\")\nto_drop.sort_values(by='price', ascending=False)[['price','room_type','name','description']]\n\n\nPlot Unlikely Listings\nHere we use the plt_ldn function – notice how it’s designed to return f,ax in the same way that plt.subplots (which we’re already familiar with) does!\nf,ax = plt_ldn(???, ???)\nto_drop.plot(column='price', markersize=10, alpha=0.7, cmap='viridis', ax=ax);\n\n\n… And Drop\nSome might be legitimate, but I’m feeling broadly ok with the remainder.\ncleaned = msoa_listings.drop(index=to_drop.???)\nprint(f\"Cleaned data has {cleaned.shape[0]:,} rows.\")\nAfter this I had 84,308 rows.\nI would normally, at this point, spend quite a bit of time validating this cleaning approach, but right now we’re going to take a rough-and-ready approach.\n\n\nQuestions\n\nWhat data type did Task 2.2 return?\n\n\n\n\n\nWhat is the function of reset_index() in Task 2.3 and when might you choose to reset (or not)?"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#create-pivot-table",
    "href": "practicals/Practical-12-Grouping_Data.html#create-pivot-table",
    "title": "Practical 12: Grouping Data",
    "section": "Create Pivot Table",
    "text": "Create Pivot Table\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe can make use of the pivot table function to generate counts by MSOA in a ‘wide’ format.\npivot = cleaned.groupby(\n                ['MSOA11CD','room_type'], observed=False\n        ).listing_url.agg(Count='count').reset_index().pivot(\n                index='???', columns=['???'], values=['???'])\npivot.head(3)\nThe formatting will look a tiny bit different, but you should get something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCount\n\n\n\n\nroom_type\nEntire home/apt\nHotel room\nPrivate room\nShared room\n\n\n\nMSOA11CD\n\n\n\n\n\n\n\nE02000001\n466\n0\n55\n1\n\n\n\nE02000002\n4\n0\n2\n0\n\n\n\nE02000003\n12\n0\n13\n0"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#check-counts",
    "href": "practicals/Practical-12-Grouping_Data.html#check-counts",
    "title": "Practical 12: Grouping Data",
    "section": "Check Counts",
    "text": "Check Counts\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\npivot.sum()\nJust to reassure you that the pivot results ‘make sense’:\nprint(cleaned[cleaned.room_type=='Entire home/apt'].listing_url.count())\nprint(cleaned[cleaned.room_type=='Private room'].listing_url.count())"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#tidy-normalise",
    "href": "practicals/Practical-12-Grouping_Data.html#tidy-normalise",
    "title": "Practical 12: Grouping Data",
    "section": "Tidy & Normalise",
    "text": "Tidy & Normalise\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nMy instinct at this point is that, looking at the pivot table, we see quite different levels of Airbnb penetration and it is hard to know how handle this difference: share would be unstable because of the low counts in some places and high counts in others; a derived variable that tells us something about density or mix could be interesting (e.g. HHI or LQ) but wouldn’t quite capture the pattern of mixing.\n\nTidy\nPersonally, based on the room type counts above I think we can drop Hotel Rooms and Shared Rooms from this since the other two categories are so dominant.\n# Flatten the column index\npivot.columns = ['Entire home/apt','Hotel room','Private room','Shared room']\n# Drop the columns\npivot.drop(???, inplace=True)\npivot.head()\nYou should have only the Entire home/apt and Private room columns now.\n\n\nNormalise\npivot_norm = pd.DataFrame(index=pivot.index)\nfor c in pivot.columns.to_list():\n    # Power Transform\n    pivot_norm[c] = pts.???(pivot[c].to_numpy().reshape(???,???))\n\npivot_norm.head()\nYou should have something like:\n\n\n\n\nEntire home/apt\nPrivate room\n\n\n\n\nMSOA11CD\n\n\n\n\nE02000001\n2.20\n1.06\n\n\nE02000002\n-1.29\n-1.85\n\n\n\n\n\nPlot\npnm = pd.merge(msoas.set_index('MSOA11CD'), pivot_norm, left_index=True, right_index=True)\npnm.plot(column='Entire home/apt', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#pca",
    "href": "practicals/Practical-12-Grouping_Data.html#pca",
    "title": "Practical 12: Grouping Data",
    "section": "PCA",
    "text": "PCA\n\n\n\n\n\n\nDifficulty: Moderate, though you might find the questions hard.\n\n\n\n\n\n\n\n\n\nYou can merge the output of this next step back on to the rddf data frame as part of a clustering process, though we’d really want to do some more thinking about what this data means and what transformations we’d need to do in order to make them meaningful.\nFor instance, if we went back to last week’s code, we could have appended this InsideAirbnb data before doing the dimensionality reduction, or we could apply it now to create a new measure that could be used as a separate part of the clustering process together with the reduced dimensionality of the demographic data.\n\nPerform Reduction\npcomp = PCA(n_components=???, random_state=42)\nrd    = pcomp.???(pivot_norm)\nprint(f\"The explained variance of each component is: {', '.join([f'{x*100:.2f}%' for x in pcomp.explained_variance_ratio_])}\")\nTake the first component and convert to a series to enable the merge:\nairbnb_pca = pd.DataFrame(\n                {'Airbnb Component 1': mms.fit_transform(rd[:,1].reshape(-1,1)).reshape(1,-1)[0]}, \n                index=pivot.index)\n\nairbnb_pca.head()\nYou should have something like: | | Airbnb Component 1 | | :—- | —-: | | MSOA11CD | | | E02000001 | 0.47 | | E02000002 | 0.19\npcanm = pd.merge(msoas.set_index('MSOA11CD'), airbnb_pca, left_index=True, right_index=True)\npcanm.plot(column='Airbnb Component 1', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));\n\n\nWrite to Data Frame\n# Result Set from merge\nrs = pd.merge(rddf, airbnb_pca, left_index=True, right_index=True)\nGrab the PCA, UMAP, and Airbnb outputs for clustering and append rescaled price:\n# Merge the reducded dimensionality data frame with the PCA-reduced Airbnb data\n# to create the *cl*uster *d*ata *f*rame\ncldf = pd.merge(rddf.loc[:,'Component 1':], airbnb_pca, \n                left_index=True, right_index=True)\n\n# Append median price from cleaned listings grouped by MSOA too!\ns1 = cleaned.groupby(by='MSOA11CD').price.agg('median')\ncldf['median_price'] = pd.Series(np.squeeze(mms.fit_transform(s1.values.reshape(-1,1))), index=s1.index)\n\n# Append mean price from cleaned listings grouped by MSOA too!\ns2 = cleaned.groupby(by='MSOA11CD').price.agg('mean')\ncldf['mean_price'] = pd.Series(np.squeeze(mms.fit_transform(s2.values.reshape(-1,1))), index=s2.index)\n\ncldf.drop(columns=['Subregion','Borough'], inplace=True)\n\ncldf.head()\n\n\nQuestions\n\nHave a think about why you might want to keep the Airbnb data separate from the MSOA data when doing PCA (or any other kind of dimensionality reduction)!\n\n\n\n\n\nWhy might it be interesting to add both mean and median MSOA prices to the clustering process? Here’s a hint (but it’s very subtle): sns.jointplot(x=s1, y=s2, s=15, alpha=0.6)"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#perform-clustering",
    "href": "practicals/Practical-12-Grouping_Data.html#perform-clustering",
    "title": "Practical 12: Grouping Data",
    "section": "Perform Clustering",
    "text": "Perform Clustering\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nc_nm   = 'KMeans' # Clustering name\nk_pref = ??? # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf.drop(columns=['Dimension 1','Dimension 2'])) # The process\nHere are the results:\nprint(kmeans.labels_) # The results"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#save-clusters-to-data-frame",
    "href": "practicals/Practical-12-Grouping_Data.html#save-clusters-to-data-frame",
    "title": "Practical 12: Grouping Data",
    "section": "Save Clusters to Data Frame",
    "text": "Save Clusters to Data Frame\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nWrite Series and Assign\nNow capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (rs):\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n\n\nHistogram of Cluster Members\nHow are the clusters distributed?\nsns.histplot(data=???, x=c_nm, bins=k_pref);\n\n\nMap Clusters\nAnd here’s a map!\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=???, ax=ax, linewidth=0, zorder=0, categorical=???, legend=True);\n\n\nQuestions\n\nWhat critical assumption did we make when running this analysis?\n\n\n\n\n\nWhy did I not use the UMAP dimensions here?\n\n\n\n\n\nWhy do we have the c_nm='kMeans' when we know what kind of clustering we’re doing?\n\n\n\n\n\nDoes this look like a good clustering?"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#whats-the-right-number-of-clusters",
    "href": "practicals/Practical-12-Grouping_Data.html#whats-the-right-number-of-clusters",
    "title": "Practical 12: Grouping Data",
    "section": "What’s the ‘Right’ Number of Clusters?",
    "text": "What’s the ‘Right’ Number of Clusters?\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nThere’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nkcldf = cldf.drop(columns=['Dimension 1','Dimension 2'])\n\nRepeated Clustering\nLet’s try clustering across a wider range. Because we repeatedly re-run the clustering code (unlike with Hierarchical Clustering) this can take a few minutes. I got nearly 5 minutes on a M2 Mac.\n%%time \n\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n\nx = []\ny = []\n\n# For resolutions of 'k' in the range 2..40\nfor k in range(2,41):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(kcldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\n\nPlot Silhouette Scores\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gca().xaxis.grid(True);\nplt.gcf().suptitle(\"Average Silhouette Scores\");\n\n\n\n\n\n\nWarning\n\n\n\n**&#9888; Note**: Had we used the UMAP dimensions here you'd likely see more instability in the silhouette plot because the distribution is not remotely Gaussian, though a lot depends on the magnitude of the columns and the number of UMAP vs. PCA components.\n\n\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#final-clustering",
    "href": "practicals/Practical-12-Grouping_Data.html#final-clustering",
    "title": "Practical 12: Grouping Data",
    "section": "Final Clustering",
    "text": "Final Clustering\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=3 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\n\nPerform Clustering\nk_pref=???\n\n#############\n# Do the clustering using the main columns\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(kcldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=kcldf.index, name=c_nm)\n\n# We do this for plotting\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(kcldf, kmeans.labels_)\n\n\nPlot Diagnostics\n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, kcldf.shape[0] + (k_pref + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k_pref)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k_pref)\n    ax2.scatter(kcldf[kcldf.columns[0]], kcldf[kcldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\n\n\n\n\n\nWarning\n\n\n\n**&#9888; Stop**: Make sure that you understand how the silhouette plot and value work, and why your results _may_ diverge from mine.\n\n\n\n\nMap Clusters\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#representative-centroids",
    "href": "practicals/Practical-12-Grouping_Data.html#representative-centroids",
    "title": "Practical 12: Grouping Data",
    "section": "‘Representative’ Centroids",
    "text": "‘Representative’ Centroids\n\n\n\n\n\n\nDifficulty: Moderate since, conceptually, there’s a lot going on.\n\n\n\n\n\n\n\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\nThese are columns that we want to suppress from our sample:\nto_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n             'Airbnb Component 1']\nTake a sample of the full range of numeric columns:\ncols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\nprint(cols)\nCalculate the mean of these columns for each cluster:\n# Empty data frame with the columns we'll need\ncentroids = pd.DataFrame(columns=cols)\n\n# For each cluster...\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n    \n    # Select rows where the cluster name matches the cluster number\n    clust = rs[rs[c_nm]==k]\n    \n    # Append the means to the centroids data frame\n    centroids.loc[k] = clust[cols].mean()\ncentroids\ncentroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    for r in row.index:\n        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n        centroids_long = pd.concat([centroids_long, d], ignore_index=True)\ng = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.bar, \"Cluster\", \"Std. Value\")\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: The above centroid outputs are a way to think about how each cluster is 'loaded' on to the data. We can't show all of the variables in the data, so we've randomly selected a subset and can then look at how different clusters are more (or less) associated with the standardised value of a particular column/variable."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#work-out-the-neighbour-distance",
    "href": "practicals/Practical-12-Grouping_Data.html#work-out-the-neighbour-distance",
    "title": "Practical 12: Grouping Data",
    "section": "Work out the Neighbour Distance",
    "text": "Work out the Neighbour Distance\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=6).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#derive-approximate-knee",
    "href": "practicals/Practical-12-Grouping_Data.html#derive-approximate-knee",
    "title": "Practical 12: Grouping Data",
    "section": "Derive Approximate Knee",
    "text": "Derive Approximate Knee\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nfrom kneed import knee_locator\n\nkn = knee_locator.KneeLocator(np.arange(distances.shape[0]), distances, S=12,\n                              curve='convex', direction='increasing')\nprint(f\"Knee detected at: {kn.knee}\")\nkn.plot_knee()\nkn.plot_knee_normalized()\nprint(f\"Best guess at epsilon for DBSCAN is {distances[kn.knee]:0.4f}\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#explore-epsilons",
    "href": "practicals/Practical-12-Grouping_Data.html#explore-epsilons",
    "title": "Practical 12: Grouping Data",
    "section": "Explore Epsilons",
    "text": "Explore Epsilons\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error, though we can use the above result as a target. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nIterate Over Range\n\n\n\n\n\n\nCaution\n\n\n\n**&#9888; Warning**: Depending on the data volume, this next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn't) the graph above.\n\n\n%%time \n\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.76, 0.01): # &lt;- You might want to adjust these!\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\n\n\nExamine Clusters\nedf.head() # Notice the -1 cluster for small epsilons\nepsilon_long = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = pd.DataFrame({'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}, index=[1])\n            epsilon_long = pd.concat([epsilon_long, d], ignore_index=True)\n\nepsilon_long['Count'] = epsilon_long.Count.astype(float)\n\n\nPlot Cluster Sizes\nOne of the really big problems with DBSCAN and this kind of data is that you have no practical way of specifying epsilon (whereas if you were doing walkability analysis then you could cluster on walking distance!). So you can look at the data (as above) to get a reasoanble value, but look what the output below shows about the stability of the clusters for different values of epsilon!\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=epsilon_long, x='Epsilon', y='Count', hue='Cluster');\nplt.vlines(x=distances[kn.knee], ymin=0, ymax=epsilon_long.Count.max(), color=(1, .7, .7, .8), linestyles='dashed')\nplt.gcf().suptitle(f\"Cluster sizes for various realisations of Epsilon\");\nplt.tight_layout()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#final-clustering-1",
    "href": "practicals/Practical-12-Grouping_Data.html#final-clustering-1",
    "title": "Practical 12: Grouping Data",
    "section": "Final Clustering",
    "text": "Final Clustering\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\n###: Perform Clustering\nUse the value from kneed…\ndbs = DBSCAN(eps=distances[kn.knee], min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n###: Map Clusters\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n‘Representative’ Centroids\nto_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n             'Airbnb Component 1']\nTake a sample of the full range of numeric columns:\ncols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\nprint(cols)\nCalculate the mean of these columns for each cluster:\n# Empty data frame with the columns we'll need\ncentroids = pd.DataFrame(columns=cols)\n\n# For each cluster...\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n    \n    # Select rows where the cluster name matches the cluster number\n    clust = rs[rs[c_nm]==k]\n    \n    # Append the means to the centroids data frame\n    centroids.loc[k] = clust[cols].mean()\n# Drop the unclustered records (-1)\ncentroids.drop(labels=[-1], axis=0, inplace=True)\ncentroids\ncentroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    for r in row.index:\n        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n        centroids_long = pd.concat([centroids_long, d], ignore_index=True)\ng = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.bar, \"Cluster\", \"Std. Value\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#additional-setup",
    "href": "practicals/Practical-12-Grouping_Data.html#additional-setup",
    "title": "Practical 12: Grouping Data",
    "section": "Additional Setup",
    "text": "Additional Setup\n\n\n\n\n\n\nDifficulty: Hard, as I’ve left out quite a bit of code.\n\n\n\n\n\n\n\n\n\n\nImport Libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.inspection import permutation_importance\n\n\nSet Up Data\nI’m taking a fairly brutal approach here: anything that is not inherently numeric is gone (bye, bye, text), and I’m not bothering to convert implicitly numeric values either: dates could be converted to ‘months since last review’, for instance, while amenities could be One-Hot Encoded after some pruning of rare amenities. This leaves us with a much smaller number of columns to feed in to the classifier.\nprint(f\"Cleaned columns: {', '.join(cleaned.columns.to_list())}.\")\nclassifier_in = cleaned.drop(columns=['listing_url','last_scraped','name','description',\n                                      'host_name', 'host_location', 'property_type', \n                                      'bathrooms_text', 'amenities', 'geometry', 'MSOA11CD',\n                                      'host_since', 'first_review', 'last_review',\n                                      'host_verifications', 'review_scores_rating',\n                                      'reviews_per_month'])\n\n\nRemove NAs\nNot all classifiers have this issue, but some will struggle to make predictions (or not be able to do so at all) if there are NAs in the data set. The classifier we’re using can’t deal with NAs, so we have to strip these out, but before we do let’s check the effect:\nclassifier_in.isna().sum()\nWe can safely drop these now, and you should end up with about 54,000 rows to work with.\nclassifier_in = classifier_in.dropna(axis=0, how='any')\nprint(f\"Now have {classifier_in.shape[0]:,} rows of data to work with (down from {cleaned.shape[0]:,}).\")\nprint()\nprint(f\"Classifier training columns: {', '.join(classifier_in.columns.to_list())}.\")\nclassifier_in.head()\n\n\nRemap Non-Numeric Columns\nWe do still have a couple of non-numeric columns to deal with: booleans and the thing we’re actually trying to predict (the room type)!\nclassifier_in['host_is_superhost'] = classifier_in.host_is_superhost.replace({True:1, False:0}).astype('int')\nle = LabelEncoder()\nclassifier_in['room_class'] = le.fit_transform(classifier_in.room_type)\nA quick check: we should only have one type per class and vice versa.\nclassifier_in.groupby(by=['room_type','room_class']).host_id.agg('count').reset_index()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#random-forest-classification",
    "href": "practicals/Practical-12-Grouping_Data.html#random-forest-classification",
    "title": "Practical 12: Grouping Data",
    "section": "Random Forest Classification",
    "text": "Random Forest Classification\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe’re going to use a Random Forest Classifier but the nice thing about sklearn is that you can quite easily swap in other classifiers if you’d like to explore further. This is one big advantage of Python over R in my book: whereas R tends to get new algorithms first, they are often implemented independently by many people and you can end up with incompatible data structures that require a lot of faff to reorganise for a different algorithm. Python is a bit more ‘managed’ and the dominance of numpy and sklearn and pandas means that people have an incentive to contribute to this library or, if it’s genuinely novel, to create an implementation that works like it would if it were part of sklearn!\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: So here's an _actual_ Machine Learning implementation, but you'll have seen a lot of parts of the code before! \n\n\n\nTrain/Test Split\ntrain, test = train_test_split(classifier_in, test_size=0.2, random_state=42)\nprint(f\"Train contains {train.shape[0]:,} records.\")\nprint(f\"Test contains {test.shape[0]:,} records.\")\ny_train = train.room_class\nX_train = train.drop(columns=['room_class','room_type'])\n\ny_test  = test.room_class\nX_test  = test.drop(columns=['room_class','room_type'])\n\n\nClassifier Setup\nrfc = RandomForestClassifier(\n    max_depth=8,\n    min_samples_split=7,\n    n_jobs=4,\n    random_state=42\n)\n\n\nFit and Predict\nrfc.fit(X_train, y_train)\ny_hat = rfc.predict(X_test)"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#validate",
    "href": "practicals/Practical-12-Grouping_Data.html#validate",
    "title": "Practical 12: Grouping Data",
    "section": "Validate",
    "text": "Validate\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nc_matrix = pd.DataFrame(confusion_matrix(y_test, y_hat))\nc_matrix.index = le.inverse_transform(c_matrix.index)\nc_matrix.columns = le.inverse_transform(c_matrix.columns)\nc_matrix\n\n\nFeature Importance\nCompare the Random Forest’s built-in ‘feature importance’ with Permutation Feature Importance as documented here.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: This next section is the reason you shouldn't blindly run ML algorithms on your data. It turns out that the Random Forest is seriously affected by the scale of different variables, and of binary variables in particular. You will tend to get erroneous feature importance values back from `sklearn`'s RF implementation and should _normally_ look at Permutation Feature Importance values instead. But this little demonstration also shows (above) a more subtle issue: imbalanced data. There are far fewer hotels than there are private rooms, and far fewer of _those_ than there are entire home/apt listings in the sample. So you'll see that the RF has trouble predicting the classes correctly: that's because with a data set like this it's hard to to _better_ than just predicting entire home/apt _Every Single Time_.\n\n\nmdi_importances = pd.Series(\n    rfc.feature_importances_, index=rfc.feature_names_in_\n).sort_values(ascending=True)\n\nax = mdi_importances.plot.barh()\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nax.figure.tight_layout()\n\n\nPermutation Feature Importance\nresult = permutation_importance(\n    rfc, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X_test.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (Test Set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#shapely-values",
    "href": "practicals/Practical-12-Grouping_Data.html#shapely-values",
    "title": "Practical 12: Grouping Data",
    "section": "Shapely Values",
    "text": "Shapely Values\nShapely values are a big part of explainable AI and they work (very broadly) by permuting the data to explore how sensitive the predictions made by the model are to the results that you see. For these we need to install two libraries: shap (to do the heavy lifting) and slicer to deal with the data.\n\nInstall Libraries\nWe should now have this already available in Docker, but just in case…\ntry:\n    import shap\nexcept ModuleNotFoundError:\n    ! pip install slicer shap\n    import shap\n\n\nCheck for Data Types\nYou are looking for anything other than int64 or float64 for the most part. Boolean should be fine, but pandas’ internal, nullable integer type will give you a ufunc error.\nX_test.info() \nX_test['beds'] = X_test.beds.astype('int')\n\n\nPlot Partial Dependence\nshap.partial_dependence_plot(\n    \"price\", rfc.predict, X_test, ice=False,\n    model_expected_value=True, feature_expected_value=True\n)\n\n\nCalculate Shapely Values\nThis can take a long time: 4-5 hours (!!!) without developing a strategy for tackling it. See the long discussion here. I’ve taken the approach of subsetting the data substantially (the model is already trained so it won’t impact the model’s predictions) with a 20% fraction of the test data and an explainer sample of 5%. On my laptop the ‘Permutation explainer’ stage took about 14 minutes, but your results may obviously be rather different.\nXsample = shap.utils.sample(X_test.sample(frac=0.2, random_state=41), 10)\nexplainer = shap.Explainer(rfc.predict, Xsample)\nNow we calculate the shap values for the 5% sample from X_test.\n\n\n\n\n\n\nCaution\n\n\n\n**&#9888; Warning**: This next block is the one that takes a long time to run. I got between 3mn and 4mn.\n\n\n%%time\nshap_values = explainer(X_test.sample(frac=.05, random_state=42))\n\n\nSingle Observation\nNow you can take any random record (sample_ind) and produce a shap plot to show the role that each attribute played in its classification. Note that getting these plots to save required some searching on GitHub.\nsample_ind=250\nshap.plots.waterfall(shap_values[sample_ind], max_display=14, show=False);\nplt.title(f\"Shapely values for observation #{sample_ind} ({X_test.sample(frac=.05, random_state=42).iloc[sample_ind].name})\")\nplt.tight_layout()\n#plt.savefig('practical-09-waterfall.png', dpi=150)\n\n\n\nShapely Feature Plot for Feature 250\n\n\n\n\nAll Observations\nshap.plots.beeswarm(shap_values, show=False)\nplt.title(f\"Shapely Swarm Plot for Sample\")\nplt.tight_layout()\nplt.savefig('practical-09-swarm.png', dpi=150)\n\n\n\nShapely Swarm Plot"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#wrap-up",
    "href": "practicals/Practical-12-Grouping_Data.html#wrap-up",
    "title": "Practical 12: Grouping Data",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Setting Up",
    "section": "",
    "text": "In the first week we will be focussing on the supporting infrastructure for ‘doing data science’. That is to say, we’ll be dealing with the installation and configuration of tools such as GitHub and Docker which support replicable, shareable, and document-able data science. As a (free) bonus, the use of these tools also protects you against catastrophic (or the merely irritating) data loss thanks to over-zealous editing of code or content. You should see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career.\n\n\n\n\n\n\nLearning objectives\n\n\n\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nYou should also see this session as connecting to Quantitative Methods Week 1 content on ‘setting quantitative research questions’ since the main assessment will require you to develop a data-led policy briefing. In other words, you’ll need to map current policy on to one or more research questions that can be quantitatively examined using the tools and techniques acquired over the course of the term! While you don’t need to start work on this yet, you should keep it in the back of your mind for when you come across readings/results that you’d like to explore in more detail.",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#overview",
    "href": "sessions/week1.html#overview",
    "title": "Setting Up",
    "section": "",
    "text": "In the first week we will be focussing on the supporting infrastructure for ‘doing data science’. That is to say, we’ll be dealing with the installation and configuration of tools such as GitHub and Docker which support replicable, shareable, and document-able data science. As a (free) bonus, the use of these tools also protects you against catastrophic (or the merely irritating) data loss thanks to over-zealous editing of code or content. You should see this as preparing the foundation not only for your remaining CASA modules (especially those in Term 2) but also for your post-MSc career.\n\n\n\n\n\n\nLearning objectives\n\n\n\n\nA basic understanding of the data science ‘pipeline’.\nAn understanding of how data scientists use a wide range of ‘tools’ to do data science.\nA completed installation/configuration of these tools.\n\n\n\nYou should also see this session as connecting to Quantitative Methods Week 1 content on ‘setting quantitative research questions’ since the main assessment will require you to develop a data-led policy briefing. In other words, you’ll need to map current policy on to one or more research questions that can be quantitatively examined using the tools and techniques acquired over the course of the term! While you don’t need to start work on this yet, you should keep it in the back of your mind for when you come across readings/results that you’d like to explore in more detail.",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#other-preparation",
    "href": "sessions/week1.html#other-preparation",
    "title": "Setting Up",
    "section": "Other Preparation",
    "text": "Other Preparation\nAlthough none of these activities are compulsory in advance of the first session, getting your computer set up to code does take time and most of these preparatory activites are fairly straightforward… with a few exceptions noted below. If you are able to get these tools installed in advance then you can focus on the taught content in the first two practicals rather than also wrestling with an installation process. This will also give us more time to help you if you discover that you’re one of the unlucky few for whom getting set up is a lot more work!\n\n\n\n\n\n\nTip\n\n\n\nComplete as many of these activities as you can:\n\nGo through the computer health check.\nFinish installing the programming environment.\n\n\n\nThe last of these is the stage where you’re most likely to encounter problems that will need our assistance, so knowing that you need our help in Week 1 means that you can ask for it much sooner in the practical!",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#in-person-lectures",
    "href": "sessions/week1.html#in-person-lectures",
    "title": "Setting Up",
    "section": "In-Person Lectures",
    "text": "In-Person Lectures\nIn this week’s workshop we will review the module aims, learning outcomes, and expectations with a general introduction to the course.\n\n\n\nSession\nPresentation\n\n\n\n\nGetting Oriented\nSlides\n\n\nTools of the Trade\nSlides\n\n\nWriting (Code)\nSlides\n\n\nWriting\nSlides\n\n\nGroup Working\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week1.html#practical",
    "href": "sessions/week1.html#practical",
    "title": "Setting Up",
    "section": "Practical",
    "text": "Practical\nThis week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2, and familiarising you with ‘how people do data science’. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: get out of the way of doing ‘stuff’; support teams in negotating conflicts in code; make it easy to share results; and make it easy to ensure that everyone is ‘on the same page’.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nGetting you up and running with the coding and collaboration tools.\nProviding you with hands-on experience of using these tools.\nConfiguring your programming environment for the rest of the programme.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "1. Setting Up"
    ]
  },
  {
    "objectID": "sessions/week11.html",
    "href": "sessions/week11.html",
    "title": "Dimensions in Data",
    "section": "",
    "text": "This session is ‘supplemental’, meaning that it is here to help you integrate ideas seen across Term 1 (and which will be encountered again in Term 2) in a way that sup ports long-term learning. It is not essential to passing the course and there are no ‘bonus points’ for using methods found in this session.",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#overview",
    "href": "sessions/week11.html#overview",
    "title": "Dimensions in Data",
    "section": "Overview",
    "text": "Overview\nThis is the most profoundly abstract aspect of data analysis: how to conceive of your data as a multi-dimensional space that can be reshaped and transformed to support your analytical objectives. This foregrounds the importance of judgement since, as the economist Ronald Coase is reputed to have said:\n\n“If you torture the data long enough, it will confess.”\n\nBy which you should understand that transformation is a form of ‘torture’1: it can force the data to reveal relationships that were previously hidden from the data scientist. However, taken too far the data will confess to whatever you want, which isn’t the purpose of critical, reproducible, sound data science!\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nA deeper understanding of the issues surrounding clustering that were covered in Week 6 of CASA0005 (GIS) and CASA0007 (QM).\nAn understanding of how data transformation works and the reasons for choosing one transform over another.\nAn appreciation of the pros and cons of at least two dimensionality reduction techniques.",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#preparatory-lectures",
    "href": "sessions/week11.html#preparatory-lectures",
    "title": "Dimensions in Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nThe Data Space\nVideo\nSlides\n\n\nTransformation\nVideo\nSlides\n\n\nDimensionality\nVideo\nSlides",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#other-preparation",
    "href": "sessions/week11.html#other-preparation",
    "title": "Dimensions in Data",
    "section": "Other Preparation",
    "text": "Other Preparation\n\nThe following readings may be useful for reflecting on the topics covered in this session:\n\nBunday (n.d.) &lt;URL&gt;\nHarris (n.d.) &lt;URL&gt;\nCima (n.d.) &lt;URL, PDF with Figures&gt;\n\n\n\n\n\n\n\n\nConnections\n\n\n\nThese readings provide very practical insights into the ways that data transformation can ‘torture the data until it confesses’ as well as how we can use data transformation to generate new ways of seeing our data and, consequently, new ways of understanding it. You should be coming out of these readings with a clearer understanding of why there’s rarely a ‘right’ or ‘wrong’ approach to a real-world data set, but there are ‘better’ and ‘worse’ approaches. These readings are predominantly non-academic so they should (I hope) be fairly accessible and quick to read despite the potential dryness of the topics.",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#practical",
    "href": "sessions/week11.html#practical",
    "title": "Dimensions in Data",
    "section": "Practical",
    "text": "Practical\nThis practical will show you how data transformation is an essential, but often overlooked, aspect of data analysis: depending on the choices we make here, we can reduce (or increase) the dimensionality of the data and make it more (or less) tractable for subsequent analysis. This approach to the pipeline relies on you being able to see your data as existing in an abstract ‘space’ that can be manipulated in order to foreground, compress, or even mask attributes.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nWorking with a more complex data structure to create new ‘grouped’ variables (as the simplest form of transformation)\nUsing sklearn to fit and transform data in a flexible manner.\nDoing two types of dimensionality reduction to demonstrate how different linear and non-linear dimensionality reduction are.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "sessions/week11.html#footnotes",
    "href": "sessions/week11.html#footnotes",
    "title": "Dimensions in Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this is a metaphor only!↩︎",
    "crumbs": [
      "Part 3: Bonus",
      "11. Dimensions in Data"
    ]
  },
  {
    "objectID": "assessments/exam.html",
    "href": "assessments/exam.html",
    "title": "Timed Open Book Exam",
    "section": "",
    "text": "This ‘exam’ (to be completed on Friday, 22 November 2024 between 9-11am or 11-1pm and worth 30%) will be administered as a quiz through Moodle: you will be asked to download a small data set and perform a series of specified analytical tasks using that data. For each question you will be asked both to type in an answer as a comment (which may be numerical or textual) and to paste in the code that you have used to obtain that answer.\nFor the exam itself you have 2 hours (3 hours with a relevant SORA) to do the coding and submit your answers. The ‘clock’ starts on the first question and finishes either after 2 hours (3 hours with a relevant SORA) or at the end of the window period (9-11am or 11-1pm) whichever comes first. So all incomplete exams are closed at the end of the window: if you have a SORA you should plan to start the exam one hour earlier so as to have the maximum amount of time.",
    "crumbs": [
      "Elements",
      "Timed Open Book Exam"
    ]
  },
  {
    "objectID": "assessments/exam.html#expectations",
    "href": "assessments/exam.html#expectations",
    "title": "Timed Open Book Exam",
    "section": "Expectations",
    "text": "Expectations\nThis is the first year that we have attempted to run a timed, open-book examination as part of this module so we may make adjustments to the process based on the capabilities of the class (taken in the round) and the challenges presented by the data set selected.\nHowever, as a general set of expectations:\n\nAny content covered prior to Reading Week may be encountered in the exam (so we will only test you as far as the use of pandas with numeric data)\nThe principal library upon which the exam will rely is pandas.\nThe exam is ‘open book’, but there are three important limitations:\n\nYou may not obtain direct assisstance from another person (either in-person or online);\nYou may not ask questions relating to the exam on StackOverflow or other ‘online help boards’.\nYou may use ChatGPT or another LLM.\n\nHowever, if you make use of an external resource (e.g. you search for and find relevant code on StackOverflow or use ChatGPT) you must acknowledge this using the form set out below.\nAnyone found to have cheated on this submission will receive a mark of 0 and the resit assessment will involve producing an analysis in a timed context under direct supervision of module staff: you will be required to be present in-person for the resit (this is likely to be during the summer period) and will have to complete a set of coding tasks specified by the module leader.\n\nIn other words: don’t do it.",
    "crumbs": [
      "Elements",
      "Timed Open Book Exam"
    ]
  },
  {
    "objectID": "assessments/exam.html#format",
    "href": "assessments/exam.html#format",
    "title": "Timed Open Book Exam",
    "section": "Format",
    "text": "Format\nThe format of the answer to each question should be of the form:\n\n# Answer: \n# Used LLM: False/True\n\n&lt;valid python code here...&gt;\nSo, for example, your answer to the question ‘To two decimal places, what is the modal price and number of nights in the data set?’ might look something like this:\n\n# Answer: 42.13, 2.00\n# Used LLM: True\n\nfor x in ['Price','Nights']:\n    print(f\"{df.x.mode(dropna=True):0.2f}\")\nGood luck!",
    "crumbs": [
      "Elements",
      "Timed Open Book Exam"
    ]
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Foundations (Pt. 2)",
    "section": "",
    "text": "This week we will dig into data (lists and dictionaries) in greater detail so that you understand how we design structures to store and organise data to simplify our analysis. We will also be looking to the Unix Shell/Terminal as a ‘power user feature’ that is often overlooked by novice data scientists.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTo see how ‘simple’ concepts can be (re)combined to tackle complex problems.\nAn introduction to making use of Git+GitHub.\nAn introdution to making use of the Shell/Terminal.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#overview",
    "href": "sessions/week3.html#overview",
    "title": "Foundations (Pt. 2)",
    "section": "",
    "text": "This week we will dig into data (lists and dictionaries) in greater detail so that you understand how we design structures to store and organise data to simplify our analysis. We will also be looking to the Unix Shell/Terminal as a ‘power user feature’ that is often overlooked by novice data scientists.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nTo see how ‘simple’ concepts can be (re)combined to tackle complex problems.\nAn introduction to making use of Git+GitHub.\nAn introdution to making use of the Shell/Terminal.\n\n\n\nThis week we also start to move beyond Code Camp, so although you should recognise many of the parts that we discuss, you’ll see that we begin to put them together in a new way. The next two weeks are a critical transition between content that you might have seen before in Code Camp (see Practical) or other introductory materials, and the ‘data science’ approach.",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#preparatory-lectures",
    "href": "sessions/week3.html#preparatory-lectures",
    "title": "Foundations (Pt. 2)",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nDictionaries\nVideo\nSlides\n\n\nLOLs\nVideo\nNotes\n\n\nDOLs to Data\nVideo\nSlides\n\n\nThe Command Line\nVideo\nSlides\n\n\nGetting Stuck into Git\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#other-preparation",
    "href": "sessions/week3.html#other-preparation",
    "title": "Foundations (Pt. 2)",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nDonoho (2017)\nURL\nSummary\n\n\nFranklin (2024)\nURL\nSummary\n\n\nTravers, Sims, and Bosetti (2016)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nFranklin (2024) offers another perspective on ‘the discipline’ of quantitative human geography and its heterogeneity, while Donoho (2017) will give you context on how data science might differ from what’s covered in Quantitative Methods. You might also find Unwin (1980) useful for understanding why the practicals are set up the way they are and why we don’t post ‘answers’ until a few days after the last practical group has completed its session.",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week3.html#practical",
    "href": "sessions/week3.html#practical",
    "title": "Foundations (Pt. 2)",
    "section": "Practical",
    "text": "Practical\nThis week’s practical will take you through the use of dictionaries and introduce the concept of ‘nested’ data structures. We’ll also be looking at how functions (and variables) can be collected into resuable packages that we can either make ourselves or draw on a worldwide bank of experts – I know who I’d rather depend on when the opportunity arises! However, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling the following sessions:\n\nDictionaries\nLoops\nRecap 2\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nComparing the use of Python lists and dictionaries to store tabular data.\nExtending lists and dictionaries into nested data structures.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "3. Foundations (Pt.2)"
    ]
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Objects",
    "section": "",
    "text": "This week we will see how Python actually works by looking beyond simple functions and into methods, classes, and the outlines of Object Oriented Design and Programming (OOD/OOP). We’ll also look at what to do when ‘things go wrong’, because they will, but sometimes we want that to blow up the application while other times we want Python to handle the ‘exception’ gracefully. Learning to read Exceptions is essential to debugging code: the one thing that almost never works when you get an exception is ignoring it.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nDevelop enough of an understanding of classes and inheritance that you can make effective use of Python.\nDevelop an understanding of how to read and write Exceptions so as to be able to create robust code.\nUnder the process of moving code from in-line scripting, to functions, to packages of functions.\nBegin developing an appreciation of the substantive concerns of the module (data, ethics, bias, and the risks of ‘experts’ who ‘know it all’).\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#overview",
    "href": "sessions/week5.html#overview",
    "title": "Objects",
    "section": "",
    "text": "This week we will see how Python actually works by looking beyond simple functions and into methods, classes, and the outlines of Object Oriented Design and Programming (OOD/OOP). We’ll also look at what to do when ‘things go wrong’, because they will, but sometimes we want that to blow up the application while other times we want Python to handle the ‘exception’ gracefully. Learning to read Exceptions is essential to debugging code: the one thing that almost never works when you get an exception is ignoring it.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nDevelop enough of an understanding of classes and inheritance that you can make effective use of Python.\nDevelop an understanding of how to read and write Exceptions so as to be able to create robust code.\nUnder the process of moving code from in-line scripting, to functions, to packages of functions.\nBegin developing an appreciation of the substantive concerns of the module (data, ethics, bias, and the risks of ‘experts’ who ‘know it all’).\n\n\n\nThis week deals with ‘objects’ and ‘classes’, which is fundamental to mastering the Python programming language: in Python, everything is an object, you just didn’t need to know it until now. Understanding how classes and objects work is essential to using Python effectively, but it will also make you a better programmer in any language because it will help you to think about how data and code work together to achieve your goals.",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#preparatory-lectures",
    "href": "sessions/week5.html#preparatory-lectures",
    "title": "Objects",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMethods\nVideo\nSlides\n\n\nClasses\nVideo\nSlides\n\n\nDesign\nVideo\nSlides\n\n\nExceptions\nVideo\nSlides\n\n\nMore on the Assessments\nIn class\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#other-preparation",
    "href": "sessions/week5.html#other-preparation",
    "title": "Objects",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nD’Ignazio and Klein (2020a) Ch.5\nURL\nSummary\n\n\nEtherington (2016)\nURL\nSummary\n\n\nLansley (2016)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nD’Ignazio and Klein (2020b, Ch.5) will hopefully help you to reorient your thoughts on what it is that data scientists do (are they rock stars or janitors or something else?), while Etherington (2016) will help you to see how spatial and programming concepts can be connected to one another to improve our understanding of both. For a particularly good example of this see the GIS Algorithms text (Xiao 2016).\n\n\nWe’re now going to shift our focus slightly towards more critical takes on data and data science. These are not by people who can’t ‘do’ data science, rather they point to ways in which neither the data, nor the people who process them, are neutral or objective in the way that we often like to think they are.",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week5.html#practical",
    "href": "sessions/week5.html#practical",
    "title": "Objects",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nWarning\n\n\n\nThis practical will lead you through the process of converting inline scripts into functions and, ultimately, into a simple package. The last parts of the practical are optional – creating classes in hierarchies is integral to how Python works, but many data scientists will rarely need to write their own classes… they just make use of classes written by others (which is why understanding what they are is important, but being able to write your own is a little less so).\n\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nBedding in the ‘data thinking’ from last week’s practical.\nCreating functions to perform repetitive tasks.\nPackaging these functions up and accessing via the appropriate namespace.\nImplementing a few simple classes so that you understand the basics of how they work.\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "5. Objects"
    ]
  },
  {
    "objectID": "sessions/week7.html",
    "href": "sessions/week7.html",
    "title": "Spatial Data",
    "section": "",
    "text": "This week we will be focussing on the use of the geopandas library for spatial data analysis and management through a focus on spatial data and its distribution(s). Geopandas will help to clarify how Object-Oriented design and inheritance processes work, while also allowing to interrogate and map the assigned data set(s). A critical concept that should be emerging here is that spatial and numerical data analyses are, fundamentally, just two different views of the same data.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou develop better judgement about interpreting and representing data.\nYou understand how GeoPandas extends Pandas with spatial functionality.\nYou build on material covered in Week 1-3, and 5 of CASA0005 to extend your understanding of mapping and spatial data.\nYou develop better practices for (spatial) data exploration.\n\n\n\nSo we’re going to be looking at both how to work geo-data in Python and how to explore a real-world data set using Exploratory Data Analysis (EDA) and Exploratory Spatial Data Analysis (ESDA) approaches to mapping distributions, testing for NaNs, and so on.",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#overview",
    "href": "sessions/week7.html#overview",
    "title": "Spatial Data",
    "section": "",
    "text": "This week we will be focussing on the use of the geopandas library for spatial data analysis and management through a focus on spatial data and its distribution(s). Geopandas will help to clarify how Object-Oriented design and inheritance processes work, while also allowing to interrogate and map the assigned data set(s). A critical concept that should be emerging here is that spatial and numerical data analyses are, fundamentally, just two different views of the same data.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\nYou develop better judgement about interpreting and representing data.\nYou understand how GeoPandas extends Pandas with spatial functionality.\nYou build on material covered in Week 1-3, and 5 of CASA0005 to extend your understanding of mapping and spatial data.\nYou develop better practices for (spatial) data exploration.\n\n\n\nSo we’re going to be looking at both how to work geo-data in Python and how to explore a real-world data set using Exploratory Data Analysis (EDA) and Exploratory Spatial Data Analysis (ESDA) approaches to mapping distributions, testing for NaNs, and so on.",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#preparatory-lectures",
    "href": "sessions/week7.html#preparatory-lectures",
    "title": "Spatial Data",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nMapping\nVideo\nSlides\n\n\nGeoPandas\nVideo\nSlides\n\n\nEDA\nVideo\nSlides\n\n\nESDA\nVideo\nSlides",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#other-preparation",
    "href": "sessions/week7.html#other-preparation",
    "title": "Spatial Data",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class prepared to discuss the following readings:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nD’Ignazio and Klein (2020) Ch.6\nURL\nN/A\n\n\nLu and Henning (2013)\nURL\nN/A\n\n\nBunday (n.d.)\nURL\nN/A\n\n\nVanderPlas (2014)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nWe’re focussing this week on the links between the data you’re working with and the process you’re trying to study! You might (quite reasonably) assume that these line up nicely, but in the era of big data that isn’t the case. ‘Accidental’ data (Arribas-Bel 2014) such as smartcard, mobile, web traffic, etc. are only ever partial accounts of messy human reality, so we want you to think about the gap between what you have and what you want to study.",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "sessions/week7.html#practical",
    "href": "sessions/week7.html#practical",
    "title": "Spatial Data",
    "section": "Practical",
    "text": "Practical\nIn the practical we will continue to work with the InsideAirbnb data, here focussing on the second ‘class’ of data in the data set: geography. We will see how to use GoePandas and PySAL for (geo)visualisation and analysis.\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nCreating/working with geo-data in Python.\nMaking maps with Python.\nExploring the data visually.\n\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Process",
      "7. Spatial Data"
    ]
  },
  {
    "objectID": "assessments/Group_Work.html",
    "href": "assessments/Group_Work.html",
    "title": "Group Name’s Group Project",
    "section": "",
    "text": "We, [insert your group’s names], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\nDate:\nStudent Numbers:"
  },
  {
    "objectID": "assessments/Group_Work.html#declaration-of-authorship",
    "href": "assessments/Group_Work.html#declaration-of-authorship",
    "title": "Group Name’s Group Project",
    "section": "",
    "text": "We, [insert your group’s names], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\nDate:\nStudent Numbers:"
  },
  {
    "objectID": "assessments/Group_Work.html#brief-group-reflection",
    "href": "assessments/Group_Work.html#brief-group-reflection",
    "title": "Group Name’s Group Project",
    "section": "Brief Group Reflection",
    "text": "Brief Group Reflection\n\n\n\nWhat Went Well\nWhat Was Challenging\n\n\n\n\nA\nB\n\n\nC\nD"
  },
  {
    "objectID": "assessments/Group_Work.html#priorities-for-feedback",
    "href": "assessments/Group_Work.html#priorities-for-feedback",
    "title": "Group Name’s Group Project",
    "section": "Priorities for Feedback",
    "text": "Priorities for Feedback\nAre there any areas on which you would appreciate more detailed feedback if we’re able to offer it?"
  },
  {
    "objectID": "assessments/Group_Work.html#who-collected-the-insideairbnb-data",
    "href": "assessments/Group_Work.html#who-collected-the-insideairbnb-data",
    "title": "Group Name’s Group Project",
    "section": "1. Who collected the InsideAirbnb data?",
    "text": "1. Who collected the InsideAirbnb data?\n\n( 2 points; Answer due Week 7 )\n\nAn inline citation: As discussed on “Inside airbnb” (n.d.), there are many…\nA parenthetical citation: There are many ways to research Airbnb (see, for example, “Inside airbnb,” n.d.)…"
  },
  {
    "objectID": "assessments/Group_Work.html#why-did-they-collect-the-insideairbnb-data",
    "href": "assessments/Group_Work.html#why-did-they-collect-the-insideairbnb-data",
    "title": "Group Name’s Group Project",
    "section": "2. Why did they collect the InsideAirbnb data?",
    "text": "2. Why did they collect the InsideAirbnb data?\n\n( 4 points; Answer due Week 7 )\n\n\n\nData frame is 69,358 x 76"
  },
  {
    "objectID": "assessments/Group_Work.html#how-was-the-insideairbnb-data-collected",
    "href": "assessments/Group_Work.html#how-was-the-insideairbnb-data-collected",
    "title": "Group Name’s Group Project",
    "section": "3. How was the InsideAirbnb data collected?",
    "text": "3. How was the InsideAirbnb data collected?\n\n( 5 points; Answer due Week 8 )"
  },
  {
    "objectID": "assessments/Group_Work.html#how-does-the-method-of-collection-impact-the-completeness-andor-accuracy-of-the-insideairbnb-data-sets-representation-of-the-process-it-seeks-to-study-and-what-wider-issues-does-this-raise",
    "href": "assessments/Group_Work.html#how-does-the-method-of-collection-impact-the-completeness-andor-accuracy-of-the-insideairbnb-data-sets-representation-of-the-process-it-seeks-to-study-and-what-wider-issues-does-this-raise",
    "title": "Group Name’s Group Project",
    "section": "4. How does the method of collection impact the completeness and/or accuracy of the InsideAirbnb data set’s representation of the process it seeks to study, and what wider issues does this raise?",
    "text": "4. How does the method of collection impact the completeness and/or accuracy of the InsideAirbnb data set’s representation of the process it seeks to study, and what wider issues does this raise?\n\n( 11 points; Answer due Week 9 )"
  },
  {
    "objectID": "assessments/Group_Work.html#what-ethical-considerations-does-the-use-of-the-insideairbnb-data-raise",
    "href": "assessments/Group_Work.html#what-ethical-considerations-does-the-use-of-the-insideairbnb-data-raise",
    "title": "Group Name’s Group Project",
    "section": "5. What ethical considerations does the use of the InsideAirbnb data raise?",
    "text": "5. What ethical considerations does the use of the InsideAirbnb data raise?\n\n( 18 points; Answer due Tuesday, 17 December 2024 )"
  },
  {
    "objectID": "assessments/Group_Work.html#with-reference-to-the-insideairbnb-data-i.e.-using-numbers-figures-maps-and-descriptive-statistics-what-does-an-analysis-of-hosts-and-listing-types-suggest-about-the-nature-of-airbnb-lets-in-london",
    "href": "assessments/Group_Work.html#with-reference-to-the-insideairbnb-data-i.e.-using-numbers-figures-maps-and-descriptive-statistics-what-does-an-analysis-of-hosts-and-listing-types-suggest-about-the-nature-of-airbnb-lets-in-london",
    "title": "Group Name’s Group Project",
    "section": "6. With reference to the InsideAirbnb data (i.e. using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London?",
    "text": "6. With reference to the InsideAirbnb data (i.e. using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London?\n\n( 15 points; Answer due Tuesday, 17 December 2024 )"
  },
  {
    "objectID": "assessments/Group_Work.html#drawing-on-your-previous-answers-and-supporting-your-response-with-evidence-e.g.-figures-maps-and-statistical-analysismodels-how-could-the-insideairbnb-data-set-be-used-to-inform-the-regulation-of-short-term-lets-stl-in-london",
    "href": "assessments/Group_Work.html#drawing-on-your-previous-answers-and-supporting-your-response-with-evidence-e.g.-figures-maps-and-statistical-analysismodels-how-could-the-insideairbnb-data-set-be-used-to-inform-the-regulation-of-short-term-lets-stl-in-london",
    "title": "Group Name’s Group Project",
    "section": "7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how could the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?",
    "text": "7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, and statistical analysis/models), how could the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?\n\n( 45 points; Answer due Tuesday, 17 December 2024 )"
  },
  {
    "objectID": "assessments/Group_Work.html#sustainable-authorship-tools",
    "href": "assessments/Group_Work.html#sustainable-authorship-tools",
    "title": "Group Name’s Group Project",
    "section": "Sustainable Authorship Tools",
    "text": "Sustainable Authorship Tools\nYour QMD file should automatically download your BibTeX file. We will then re-run the QMD file to generate the output successfully.\nWritten in Markdown and generated from Quarto. Fonts used: Spectral (mainfont), Roboto (sansfont) and JetBrains Mono (monofont)."
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Foundations (Pt. 1)",
    "section": "",
    "text": "This week we will be quickly covering the fundamentals of Python programming, while developing a critical appreciation of data science as an ongoing ‘process’ that calls for iterative improvement and deeper reflection. We will be contextualising computers within a wider landscape of geographical/spatial research. And we will be (briefly) reviewing the basics of Python with a focus on simple data structures. We’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nA review of basic Python syntax and operators.\nAn understanding of how none of this all that new.\n\n\n\nSo we will be contextualising all of this within the longer history of the study of geography (or planning!) through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#overview",
    "href": "sessions/week2.html#overview",
    "title": "Foundations (Pt. 1)",
    "section": "",
    "text": "This week we will be quickly covering the fundamentals of Python programming, while developing a critical appreciation of data science as an ongoing ‘process’ that calls for iterative improvement and deeper reflection. We will be contextualising computers within a wider landscape of geographical/spatial research. And we will be (briefly) reviewing the basics of Python with a focus on simple data structures. We’re focussing here on how computers ‘think’ and how that differs from what you might be expecting as an intelligen human being!\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nA review of basic Python syntax and operators.\nAn understanding of how none of this all that new.\n\n\n\nSo we will be contextualising all of this within the longer history of the study of geography (or planning!) through computation. I hope to convince you that many of the problems we face today are not new and why that should encourage you to continue to do the readings!",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#preparatory-lectures",
    "href": "sessions/week2.html#preparatory-lectures",
    "title": "Foundations (Pt. 1)",
    "section": "Preparatory Lectures",
    "text": "Preparatory Lectures\nThis week is very busy because we need to cover off the basics for those of you who were unable to engage with Code Camp, while recapping only the crucial bits for those of you who were able to do so.\nCome to class prepared to present/discuss:\n\n\n\nSession\nVideo\nPresentation\n\n\n\n\nComputers in Urban Studies\nIn Class\nSlides\n\n\nPrinciples of Programming\nIn Class\nSlides\n\n\nPython: the Basics (Part 1)\nVideo\nSlides\n\n\nPython: the Basics (Part 2)\nVideo\nSlides\n\n\nLists\nVideo\nSlides\n\n\nIteration\nVideo\nSlides",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#other-preparation",
    "href": "sessions/week2.html#other-preparation",
    "title": "Foundations (Pt. 1)",
    "section": "Other Preparation",
    "text": "Other Preparation\nCome to class having read:\n\n\n\nCitation\nArticle\nChatGPT Summary\n\n\n\n\nBurton (1963)\nURL\nSummary\n\n\nArribas-Bel and Reades (2018)\nURL\nSummary\n\n\nXie (2024)\nURL\nN/A\n\n\nMattern (2017)\nURL\nN/A\n\n\n\n\n\n\n\n\n\nConnections\n\n\n\nYou should read Burton (1963) and Arribas-Bel and Reades (2018) with a view to seeing that ‘there is nothing new under the sun’: we tend to think that the challenges we face now in terms of data volumes and complexity are novel, but they are not. Indeed, here’s John Graham-Cumming keynoting a 2012 conference talking about the Lyons Tea Company and how its programmers invented Dykstra’s shortest path algorithm more than 20 years before Dykstra did!\n\n\n\n\nInstalling the Programming Environment\n\n\n\n\n\n\nWarning\n\n\n\nThis week’s practical requires you to have completed installation of the programming environment. Make sure you have completed setup of the environment.\n\n\nIn principle, we fully support students who want to do things their own way; however, we are also not able to sit down with each person and develop a custom learning environment. With Docker, we can give you full access to the cutting-edge Python libraries and other tools needed to ‘do’ spatial data science, while only needing to install 1 application, download 1 (big) file, and run 1 command. When it works… There are alternatives, but there are more things that can go wrong and they can go wrong in more complex ways. Solving the Anaconda environment can take several hours before it even starts installing.\nSo here’s what we ask: if you already know what to do with an Anaconda YAML file, or can work out how to edit the Dockerfile and build a new image, then by all means knock yourself out! We are not going to tell you that cannot do something, and eventually you will need to learn to stand on your own two feet. But please do not expect us to support you individually if you’ve gone off and done your own thing and ‘it doesn’t work’. OK? We’ll offer advice (if we can) but only if no one else is waiting for help.",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#practical",
    "href": "sessions/week2.html#practical",
    "title": "Foundations (Pt. 1)",
    "section": "Practical",
    "text": "Practical\nThis week’s practical will take you through the fundamentals of Python, including the use of simple1 Boolean logic and lists. However, if you have not yet completed Code Camp (or were not aware of it!), then you will benefit enormously from tackling the following sessions:\n\nThe Basics\nBoolean Logic\nLists\n\nTo run the code for these sessions you can:\n\nFollow the instructions for running these in Google’s Collaboratory; or\nCreate a new Notebook in Docker (File &gt; New &gt; Notebook) and copy+paste the code into new Code cells.\n\n\n\n\n\n\n\nConnections\n\n\n\nThe practical focusses on:\n\nEnsuring that you are set up with Git/GitHub\nReviewing Python basics\nReviewing Python lists and logic\n\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "sessions/week2.html#footnotes",
    "href": "sessions/week2.html#footnotes",
    "title": "Foundations (Pt. 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: simple does not mean ‘easy’! Just because we say something is ‘basic’ or ‘simple’ does not mean that we think it is straightforward for someone learning to code for the first time!↩︎",
    "crumbs": [
      "Part 1: Foundations",
      "2. Foundations (Pt.1)"
    ]
  },
  {
    "objectID": "extra/Live-04-Objects-1.html",
    "href": "extra/Live-04-Objects-1.html",
    "title": "Intro",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 4: Object-Oriented Programming 1&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with Classes &amp; Objects&lt;/h2&gt;\nYou will be encountering classes and dealing with inheritance from next week and this is fundamental to how Python works. In Python, everything is an object and that means that everything – absolutely everything – is part of the Python class hierarchy, at the bottom of which sits the base object class.\nFor our purposes, the place you’re going to encounter this every day is that Pandas is our main data analysis tool, but when we deal with geography we’ll be using GeoPandas, which inherits from Pandas and adds (we say “extends”) functionality for processing geo-data.\nThere’s even a Moving Pandas for trajectory analysis. These all work on the basis of inheritance."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-1-creating-a-class-hierarchy",
    "href": "extra/Live-04-Objects-1.html#task-1-creating-a-class-hierarchy",
    "title": "Intro",
    "section": "Task 1: Creating a Class Hierarchy",
    "text": "Task 1: Creating a Class Hierarchy\nWe want to create a set of ideal shape classes with methods allowing us to derive various properties of that shape:\n\nDiameter: which we’ll define as the longest line that can be drawn across the inside of the shape.\nVolume: the total volume of the shape.\nSurface Area: the total outside area of the shape.\n\nWe will create all of these shape classes in the notebook so that we know they work and then will move them to an external package file so that they can be imported and re-used easily in other notebooks.\nWe’re also going to make use of a few features of Python:\n\nYou can access the class name of an instance using: self.__class__.__name__. And here’s one key point: self refers to the instance, not to the class… we’ll see why this matters.\nYou can raise your own exceptions easily if you don’t want to implement a particular method yet.\nYou can have an ‘abstract’ base class that does nothing except provide a template for the ‘real’ classes so that they can be used interchangeably.\n\n\nprint(\"Hello world\".__class__.__name__)\n\nstr\n\n\n\nTask 1.1: Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\nfrom math import pi\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        print(\"I'm a shape!\")\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\n\n\nTask 1.2: Demonstrate\nBriefly discuss what’s going on, paying particular attention to:\n\nclass shape(object) – this means that shape extends the object base class.\ndef __init__ – what is self (reference to the instance) and what does dimension:float=None mean?\nCreate a new shape object:\n\n\nWhat happens when you run s = shape()?\nWhat happens when you try to call s.diameter()?\nWhat happens when you try to call s.type()?"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-2-cube",
    "href": "extra/Live-04-Objects-1.html#task-2-cube",
    "title": "Intro",
    "section": "Task 2: Cube",
    "text": "Task 2: Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nTask 2.1: Cube Class\n\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        print(\"I'm a cube!\")\n        return\n    \n    def diameter(self):\n        return (3 * self.dim**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return 6*(self.dim**2)\n\n\n\nTask 2.2: Demonstrate\n\nCreate a cube and do the same things we did with shape (show that it’s been overridden).\nLook at the print statements created from the __init__ and discuss.\nShow how the type method works for Cubes even though we never specified it."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-3-sphere",
    "href": "extra/Live-04-Objects-1.html#task-3-sphere",
    "title": "Intro",
    "section": "Task 3: Sphere",
    "text": "Task 3: Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2d\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nTask 3.1: Sphere Class\n\n# Sphere class\nclass sphere(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return self.dim*2\n    \n    def volume(self):\n        return (4/3) * pi * self.dim**3\n    \n    def surface(self):\n        return 4 * pi * (self.dim**2)\n\n\n\nTask 3.2: Demonstrate how this works\n\nCreate one Cube and one Sphere with the same parameter (dimension)\nShow how they return different values for diameter, volume, surface\nShow that both have a type function that works without being defined."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-4-pyramids",
    "href": "extra/Live-04-Objects-1.html#task-4-pyramids",
    "title": "Intro",
    "section": "Task 4: Pyramids",
    "text": "Task 4: Pyramids\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them!\n\nTask 4.1: Regular Pyramids\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n    \n    has_mummies = True # This is for *all* regular pyramids\n    \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n    \n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n    \n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**2 * self.height() / 3\n    \n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)\n\n\n\nTask 4.2: Demonstrate\nShow how we added a new method for regular pyramids:\n\nDoesn’t work for any other class\nBut all the rest of the methods do\n\nDiscuss: what are the pros and cons of this new method?\nAnswer: in general, we try to extend and never to break functionality of other classes in the hierarchy. So you wouldn’t create a shape sub-class that broke the diameter or surface methods by returning something like “I don’t have this dimension” as a text string, but you could throw an error or return a Null value. It depends on what makes sense.\n\n\nTask 4.3: Triangular Pyramid\nWe want triangular pyramid to inherit from regular pyramid, and all sides are equal so it’s an equilateral triangular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nAnyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n    \n    has_mummies = False # This is for all triangular pyramids\n    \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n    \n    def diameter(self):\n        return self.dim\n    \n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n    \n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n    \n    def volume(self):\n        return (1/3) * self.base() * self.height()\n    \n    def surface(self):\n        return 4 * self.base()\n\n\n\nTask 4.4: Demonstrate\n\nCompare regular and triangular pyramids: notice that the has_mummies class variable has changed its default value.\nNotice too that you can override/change a class variable or an instance variable at run-time. There is a very important difference between doing something like self.has_mummies = has_mummies and t_pyramid.has_mummies = True.\n\nHere’s an example of everything run together:\n\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaagh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaagh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\n\nI'm a shape!\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nI'm a shape!\nI'm a cube!\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nI'm a shape!\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaagh!\n\nI'm a shape!\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'\n\n\n\n\nprint(p2.__class__)\nprint(p2.__class__.__mro__)\n\n&lt;class '__main__.t_pyramid'&gt;\n(&lt;class '__main__.t_pyramid'&gt;, &lt;class '__main__.pyramid'&gt;, &lt;class '__main__.shape'&gt;, &lt;class 'object'&gt;)\n\n\nThis shows the class name and then the full hierarchy, so you know in which order Python will search for a method before throwing an error. Some, such as print, exist at the very basic level of the object."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-5-packaging",
    "href": "extra/Live-04-Objects-1.html#task-5-packaging",
    "title": "Intro",
    "section": "Task 5: Packaging",
    "text": "Task 5: Packaging\nNow that we’ve created our classes, we want to move them to a separate file that can be imported and re-used by code elsewhere. We need to turn this into a package. Formally packaging things up for distribution requires a lot more work(see this and this). But for something that we’re only going to use ourselves it’s basically as simple as:\n\nCreate a directory that will become the package name (e.g. shapes).\nWrite the code in a file called __init.py__ inside that new directory (i.e. ./shapes/__init__.py).\n\nWe could do this by hand by copy-pasting each cell above into a new file, but because we prefer to be lazy let’s try automating it…\n\nTask 5.1: Shell Commands\nFirst, we need to create the shapes directory. To do this, although you can use Python code to create and delete files and folders, it’s helpful to know that you can actually execute shell commands directly from a notebook. Oviously, the degree to which this works depends on which Operating System you’re using!\nFor instance, here’s our trusty list command:\n\n!ls\n\nLive-02-Foundations_1.qmd             Live-08-Dimensions_in_Shakespeare.qmd\nLive-03-Foundations_2.qmd             Live-09-Grouping_Data.qmd\nLive-03-Foundations_2_Intro.qmd       Live-10-Visualising_Data.qmd\nLive-04-Objects-1.qmd                 Note_Hint_Danger.qmd\nLive-04-Objects-1.quarto_ipynb        XX-Function_Development.ipynb\nLive-04-Objects-2.qmd                 css\nLive-05-Numeric_Data.qmd              img\nLive-06-Spatial_Data.qmd              shapes\nLive-07-Textual_Data.qmd              textual\nLive-08-Dimensions_in_Data.qmd        world.geojson\n\n\nSo we can simply ask mkdir (make directory) to do this for us. The -p means that it creates directory hierarhices if they’re missing and doesn’t complain if a directory already exists.\n\n!mkdir -p shapes\n\n\n\nTask 5.2: Creating an init file\nThe easiest and most intelligible way to create the shapes package is to copy and paste each of the class definitions above into a new file called __init__.py manually.\nI would demonstrate this approach; however…\n\nAside: Notebook Convert\nIt turns out that jupyter also offers a nbconvert (Notebook Convert) utility that helps you to convert notebooks to other formats including HTML, LaTeX, PDF, Markdown, Executable Python and so on! We’re going to take advantage of the fact that __init__.py is just plain old python following a particular naming and placement scheme…\n\n!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \\\n    --to python --output=shapes/__init__.py \\\n    Live-04-Objects-1.ipynb\n\n[NbConvertApp] WARNING | pattern 'Live-04-Objects-1.ipynb' matched no files\nThis application is used to convert notebook files (*.ipynb)\n        to various other formats.\n\n        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n\nOptions\n=======\nThe options below are convenience aliases to configurable class-options,\nas listed in the \"Equivalent to\" description-line of the aliases.\nTo see all configurable class-options for some &lt;cmd&gt;, use:\n    &lt;cmd&gt; --help-all\n\n--debug\n    set log level to logging.DEBUG (maximize logging output)\n    Equivalent to: [--Application.log_level=10]\n--show-config\n    Show the application's configuration (human-readable format)\n    Equivalent to: [--Application.show_config=True]\n--show-config-json\n    Show the application's configuration (json format)\n    Equivalent to: [--Application.show_config_json=True]\n--generate-config\n    generate default config file\n    Equivalent to: [--JupyterApp.generate_config=True]\n-y\n    Answer yes to any questions instead of prompting.\n    Equivalent to: [--JupyterApp.answer_yes=True]\n--execute\n    Execute the notebook prior to export.\n    Equivalent to: [--ExecutePreprocessor.enabled=True]\n--allow-errors\n    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n--stdin\n    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n    Equivalent to: [--NbConvertApp.from_stdin=True]\n--stdout\n    Write notebook output to stdout instead of files.\n    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n--inplace\n    Run nbconvert in place, overwriting the existing notebook (only\n            relevant when converting to notebook format)\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n--clear-output\n    Clear output of current file and save in place,\n            overwriting the existing notebook.\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n--coalesce-streams\n    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n--no-prompt\n    Exclude input and output prompts from converted document.\n    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n--no-input\n    Exclude input cells and output prompts from converted document.\n            This mode is ideal for generating code-free reports.\n    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n--allow-chromium-download\n    Whether to allow downloading chromium if no suitable version is found on the system.\n    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n--disable-chromium-sandbox\n    Disable chromium security sandbox when converting to PDF..\n    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n--show-input\n    Shows code input. This flag is only useful for dejavu users.\n    Equivalent to: [--TemplateExporter.exclude_input=False]\n--embed-images\n    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n    Equivalent to: [--HTMLExporter.embed_images=True]\n--sanitize-html\n    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n    Equivalent to: [--HTMLExporter.sanitize_html=True]\n--log-level=&lt;Enum&gt;\n    Set the log level by value or name.\n    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n    Default: 30\n    Equivalent to: [--Application.log_level]\n--config=&lt;Unicode&gt;\n    Full path of a config file.\n    Default: ''\n    Equivalent to: [--JupyterApp.config_file]\n--to=&lt;Unicode&gt;\n    The export format to be used, either one of the built-in formats\n            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n            or a dotted object name that represents the import path for an\n            ``Exporter`` class\n    Default: ''\n    Equivalent to: [--NbConvertApp.export_format]\n--template=&lt;Unicode&gt;\n    Name of the template to use\n    Default: ''\n    Equivalent to: [--TemplateExporter.template_name]\n--template-file=&lt;Unicode&gt;\n    Name of the template file to use\n    Default: None\n    Equivalent to: [--TemplateExporter.template_file]\n--theme=&lt;Unicode&gt;\n    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n    as prebuilt extension for the lab template)\n    Default: 'light'\n    Equivalent to: [--HTMLExporter.theme]\n--sanitize_html=&lt;Bool&gt;\n    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n    should be set to True by nbviewer or similar tools.\n    Default: False\n    Equivalent to: [--HTMLExporter.sanitize_html]\n--writer=&lt;DottedObjectName&gt;\n    Writer class used to write the\n                                        results of the conversion\n    Default: 'FilesWriter'\n    Equivalent to: [--NbConvertApp.writer_class]\n--post=&lt;DottedOrNone&gt;\n    PostProcessor class used to write the\n                                        results of the conversion\n    Default: ''\n    Equivalent to: [--NbConvertApp.postprocessor_class]\n--output=&lt;Unicode&gt;\n    Overwrite base name use for output files.\n                Supports pattern replacements '{notebook_name}'.\n    Default: '{notebook_name}'\n    Equivalent to: [--NbConvertApp.output_base]\n--output-dir=&lt;Unicode&gt;\n    Directory to write output(s) to. Defaults\n                                  to output to the directory of each notebook. To recover\n                                  previous default behaviour (outputting to the current\n                                  working directory) use . as the flag value.\n    Default: ''\n    Equivalent to: [--FilesWriter.build_directory]\n--reveal-prefix=&lt;Unicode&gt;\n    The URL prefix for reveal.js (version 3.x).\n            This defaults to the reveal CDN, but can be any url pointing to a copy\n            of reveal.js.\n            For speaker notes to work, this must be a relative path to a local\n            copy of reveal.js: e.g., \"reveal.js\".\n            If a relative path is given, it must be a subdirectory of the\n            current directory (from which the server is run).\n            See the usage documentation\n            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n            for more details.\n    Default: ''\n    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n--nbformat=&lt;Enum&gt;\n    The nbformat version to write.\n            Use this to downgrade notebooks.\n    Choices: any of [1, 2, 3, 4]\n    Default: 4\n    Equivalent to: [--NotebookExporter.nbformat_version]\n\nExamples\n--------\n\n    The simplest way to use nbconvert is\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to html\n\n            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n\n            &gt; jupyter nbconvert --to latex mynotebook.ipynb\n\n            Both HTML and LaTeX support multiple output templates. LaTeX includes\n            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n            'classic'. You can specify the flavor of the format used.\n\n            &gt; jupyter nbconvert --to html --template lab mynotebook.ipynb\n\n            You can also pipe the output to stdout, rather than a file\n\n            &gt; jupyter nbconvert mynotebook.ipynb --stdout\n\n            PDF is generated via latex\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to pdf\n\n            You can get (and serve) a Reveal.js-powered slideshow\n\n            &gt; jupyter nbconvert myslides.ipynb --to slides --post serve\n\n            Multiple notebooks can be given at the command line in a couple of\n            different ways:\n\n            &gt; jupyter nbconvert notebook*.ipynb\n            &gt; jupyter nbconvert notebook1.ipynb notebook2.ipynb\n\n            or you can specify the notebooks list in a config file, containing::\n\n                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n\n            &gt; jupyter nbconvert --config mycfg.py\n\nTo see all available configurables, use `--help-all`.\n\n\n\n\n\nTidying Up the Output\nUsing the file browser provided by Jupyter Lab (on the left), open up the new __init__.py file created in the shapes directory. You will want (at the very least) to search for ‘Task 2’ and delete everything in the file after that point.\n\nReally Important: if you do not delete everything from Task 2: Packaging onwards then every time you try to (re)load the shapes you will also output another copy of this notebook because this code is also outputted by nbconvert. In fact, you might find it easier to search for the line “How would you test these changes?” and delete everything from there onwards in the file.\n\n\n\n\nTask 5.3: Importing the Classes\n\nTask 5.3.1: Autoreload Cell Magic\nWhen writing a new package from a notebook, you need to ensure that Python knows to reload the package every time you run the code. Otherwise, Python will keep running the version of the package that you loaded when you first ran import &lt;package name&gt;!\nThis the first time you’ll have seen this special type of code block: % at the start of a line in a codeblock indicates that a ‘magic command’ is being issued. autoreload is one such magic command. There is also the ! at the start of a line which indicates a shell command to be executed (e.g. !ls).\nThere are other magics we will use later like %%time which track the running time of an operation.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nTask 5.3.2: Import\n\nimport shapes\n\nstr\nI'm a shape!\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nI'm a shape!\nI'm a cube!\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nI'm a shape!\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaagh!\n\nI'm a shape!\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'\n\n&lt;class 'shapes.t_pyramid'&gt;\n(&lt;class 'shapes.t_pyramid'&gt;, &lt;class 'shapes.pyramid'&gt;, &lt;class 'shapes.shape'&gt;, &lt;class 'object'&gt;)\nLive-02-Foundations_1.qmd             Live-08-Dimensions_in_Shakespeare.qmd\nLive-03-Foundations_2.qmd             Live-09-Grouping_Data.qmd\nLive-03-Foundations_2_Intro.qmd       Live-10-Visualising_Data.qmd\nLive-04-Objects-1.qmd                 Note_Hint_Danger.qmd\nLive-04-Objects-1.quarto_ipynb        XX-Function_Development.ipynb\nLive-04-Objects-2.qmd                 css\nLive-05-Numeric_Data.qmd              img\nLive-06-Spatial_Data.qmd              shapes\nLive-07-Textual_Data.qmd              textual\nLive-08-Dimensions_in_Data.qmd        world.geojson\n[NbConvertApp] WARNING | pattern 'Live-04-Objects-1.ipynb' matched no files\nThis application is used to convert notebook files (*.ipynb)\n        to various other formats.\n\n        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n\nOptions\n=======\nThe options below are convenience aliases to configurable class-options,\nas listed in the \"Equivalent to\" description-line of the aliases.\nTo see all configurable class-options for some &lt;cmd&gt;, use:\n    &lt;cmd&gt; --help-all\n\n--debug\n    set log level to logging.DEBUG (maximize logging output)\n    Equivalent to: [--Application.log_level=10]\n--show-config\n    Show the application's configuration (human-readable format)\n    Equivalent to: [--Application.show_config=True]\n--show-config-json\n    Show the application's configuration (json format)\n    Equivalent to: [--Application.show_config_json=True]\n--generate-config\n    generate default config file\n    Equivalent to: [--JupyterApp.generate_config=True]\n-y\n    Answer yes to any questions instead of prompting.\n    Equivalent to: [--JupyterApp.answer_yes=True]\n--execute\n    Execute the notebook prior to export.\n    Equivalent to: [--ExecutePreprocessor.enabled=True]\n--allow-errors\n    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n--stdin\n    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n    Equivalent to: [--NbConvertApp.from_stdin=True]\n--stdout\n    Write notebook output to stdout instead of files.\n    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n--inplace\n    Run nbconvert in place, overwriting the existing notebook (only\n            relevant when converting to notebook format)\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n--clear-output\n    Clear output of current file and save in place,\n            overwriting the existing notebook.\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n--coalesce-streams\n    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n--no-prompt\n    Exclude input and output prompts from converted document.\n    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n--no-input\n    Exclude input cells and output prompts from converted document.\n            This mode is ideal for generating code-free reports.\n    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n--allow-chromium-download\n    Whether to allow downloading chromium if no suitable version is found on the system.\n    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n--disable-chromium-sandbox\n    Disable chromium security sandbox when converting to PDF..\n    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n--show-input\n    Shows code input. This flag is only useful for dejavu users.\n    Equivalent to: [--TemplateExporter.exclude_input=False]\n--embed-images\n    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n    Equivalent to: [--HTMLExporter.embed_images=True]\n--sanitize-html\n    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n    Equivalent to: [--HTMLExporter.sanitize_html=True]\n--log-level=&lt;Enum&gt;\n    Set the log level by value or name.\n    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n    Default: 30\n    Equivalent to: [--Application.log_level]\n--config=&lt;Unicode&gt;\n    Full path of a config file.\n    Default: ''\n    Equivalent to: [--JupyterApp.config_file]\n--to=&lt;Unicode&gt;\n    The export format to be used, either one of the built-in formats\n            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n            or a dotted object name that represents the import path for an\n            ``Exporter`` class\n    Default: ''\n    Equivalent to: [--NbConvertApp.export_format]\n--template=&lt;Unicode&gt;\n    Name of the template to use\n    Default: ''\n    Equivalent to: [--TemplateExporter.template_name]\n--template-file=&lt;Unicode&gt;\n    Name of the template file to use\n    Default: None\n    Equivalent to: [--TemplateExporter.template_file]\n--theme=&lt;Unicode&gt;\n    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n    as prebuilt extension for the lab template)\n    Default: 'light'\n    Equivalent to: [--HTMLExporter.theme]\n--sanitize_html=&lt;Bool&gt;\n    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n    should be set to True by nbviewer or similar tools.\n    Default: False\n    Equivalent to: [--HTMLExporter.sanitize_html]\n--writer=&lt;DottedObjectName&gt;\n    Writer class used to write the\n                                        results of the conversion\n    Default: 'FilesWriter'\n    Equivalent to: [--NbConvertApp.writer_class]\n--post=&lt;DottedOrNone&gt;\n    PostProcessor class used to write the\n                                        results of the conversion\n    Default: ''\n    Equivalent to: [--NbConvertApp.postprocessor_class]\n--output=&lt;Unicode&gt;\n    Overwrite base name use for output files.\n                Supports pattern replacements '{notebook_name}'.\n    Default: '{notebook_name}'\n    Equivalent to: [--NbConvertApp.output_base]\n--output-dir=&lt;Unicode&gt;\n    Directory to write output(s) to. Defaults\n                                  to output to the directory of each notebook. To recover\n                                  previous default behaviour (outputting to the current\n                                  working directory) use . as the flag value.\n    Default: ''\n    Equivalent to: [--FilesWriter.build_directory]\n--reveal-prefix=&lt;Unicode&gt;\n    The URL prefix for reveal.js (version 3.x).\n            This defaults to the reveal CDN, but can be any url pointing to a copy\n            of reveal.js.\n            For speaker notes to work, this must be a relative path to a local\n            copy of reveal.js: e.g., \"reveal.js\".\n            If a relative path is given, it must be a subdirectory of the\n            current directory (from which the server is run).\n            See the usage documentation\n            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n            for more details.\n    Default: ''\n    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n--nbformat=&lt;Enum&gt;\n    The nbformat version to write.\n            Use this to downgrade notebooks.\n    Choices: any of [1, 2, 3, 4]\n    Default: 4\n    Equivalent to: [--NotebookExporter.nbformat_version]\n\nExamples\n--------\n\n    The simplest way to use nbconvert is\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to html\n\n            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n\n            &gt; jupyter nbconvert --to latex mynotebook.ipynb\n\n            Both HTML and LaTeX support multiple output templates. LaTeX includes\n            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n            'classic'. You can specify the flavor of the format used.\n\n            &gt; jupyter nbconvert --to html --template lab mynotebook.ipynb\n\n            You can also pipe the output to stdout, rather than a file\n\n            &gt; jupyter nbconvert mynotebook.ipynb --stdout\n\n            PDF is generated via latex\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to pdf\n\n            You can get (and serve) a Reveal.js-powered slideshow\n\n            &gt; jupyter nbconvert myslides.ipynb --to slides --post serve\n\n            Multiple notebooks can be given at the command line in a couple of\n            different ways:\n\n            &gt; jupyter nbconvert notebook*.ipynb\n            &gt; jupyter nbconvert notebook1.ipynb notebook2.ipynb\n\n            or you can specify the notebooks list in a config file, containing::\n\n                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n\n            &gt; jupyter nbconvert --config mycfg.py\n\nTo see all available configurables, use `--help-all`.\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nI'm a shape!\nI'm a cube!\n&lt;shapes.cube object at 0x1225c0470&gt;\n1000\n17.320508075688775\n600\nI'm a shape!\n150\n122.47448713915888\n397747.5644174329\n38971.143170299736\nHelp on class shape in module shapes:\n\nclass shape(builtins.object)\n |  shape(dimension: float = None)\n |\n |  # Base class shape\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension: float = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |\n |  diameter(self)\n |\n |  surface(self)\n |\n |  type(self)\n |\n |  volume(self)\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\nHelp on function type in module shapes:\n\ntype(self)\n\n\n\n\n\nTask 5.3.3: Demonstrate\nNotice that because we just imported shapes we need to call shapes.cube() below. We could also have done this as from shapes import cube which would allow us to call cube() directly.\nPros and Cons: This is about namespaces. If I dump everything into the main namespace and I have several classes/packages that implement different aspects of cubes the issue is that only the last cube class I imported will be accessible (from shapes import cube), but if I import just shapes then I can use the namespace (shapes) to specify that I mean exactly that class. This is a choice based on what you want to do.\n\nb = shapes.cube(10)\n\nI'm a shape!\nI'm a cube!\n\n\nWhat’s particularly interesting below is the output of dir: notice all of the methods we didn’t create but which are available! And notice that many of them start __ (like __init__ up above). That means the method is ‘private’ and shouldn’t be called directly by the programmer. But what might __gt__ do and why might the person creating the shape classes need to do something here?\n\nprint(b)\nprint(b.volume())\nprint(b.diameter())\nprint(b.surface())\ndir(b)\n\n&lt;shapes.cube object at 0x147b88c80&gt;\n1000\n17.320508075688775\n600\n\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'diameter',\n 'dim',\n 'shape_type',\n 'surface',\n 'type',\n 'volume']\n\n\n\np = shapes.t_pyramid(150)\nprint(p.diameter())\nprint(p.height())\nprint(p.volume())\nprint(p.surface())\n\nI'm a shape!\n150\n122.47448713915888\n397747.5644174329\n38971.143170299736"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-6.-adding-documentation",
    "href": "extra/Live-04-Objects-1.html#task-6.-adding-documentation",
    "title": "Intro",
    "section": "Task 6.: Adding Documentation",
    "text": "Task 6.: Adding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\n\nimport shapes\nhelp(shapes.shape)\nhelp(shapes.shape.type)\n\nHelp on class shape in module shapes:\n\nclass shape(builtins.object)\n |  shape(dimension: float = None)\n |\n |  # Base class shape\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension: float = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |\n |  diameter(self)\n |\n |  surface(self)\n |\n |  type(self)\n |\n |  volume(self)\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\nHelp on function type in module shapes:\n\ntype(self)"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html",
    "href": "practicals/Practical-02-Foundations_1.html",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "",
    "text": "This notebook is focussed on ensuring that you’re able to run Jupyter notebooks locally (i.e. on youu own computer) in Docker and are comfortable with the basics of Python: if you’ve already done Code Camp then this will be a refresher and you’ll have plenty of time to get to grips with Git and GitHub, which often presents significant practical challenges. You should find this notebook quite straightforward, but take any challenges as a sign that you need to keep practicing since subsequent weeks will build on these foundational concepts."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#is-the-container-running",
    "href": "practicals/Practical-02-Foundations_1.html#is-the-container-running",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Is the Container Running?",
    "text": "Is the Container Running?\nDocker will happily keep a container running in the background even if you close every open window. So how do you know if the sds2023 container is already running? There are two ways:\n\nOpen the Docker Dashboard/Desktop from the menu and make sure that you select the Containers tab on the left (it may be hidden by a message from Docker about upcoming conferences). You should see something like this if the container is running and available:\n\n\n\n\nsds2023 container running\n\n\n\nFrom the Terminal/Power Shell you should be able to run: docker ps. This will give you output something like this:\n\n\n\n\nsds2023 container running from Terminal\n\n\nIf the sds2023 container is not running then you’ll need to run the startup command (docker run...) covered in the last practical session"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#connect-to-jupyter-lab",
    "href": "practicals/Practical-02-Foundations_1.html#connect-to-jupyter-lab",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Connect to Jupyter Lab",
    "text": "Connect to Jupyter Lab\nOnce you know the container is running you can connected to Jupyter Lab on localhost:8888 and should see something like this:\n\n\n\nScreenshot of Jupyter Lab\n\n\nYou’re connected and can go to Section 1.3."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#sec-notebook",
    "href": "practicals/Practical-02-Foundations_1.html#sec-notebook",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Getting the Notebook",
    "text": "Getting the Notebook\n\nDownloading from GitHub\nOn the Week 2 of FSDS page you’ll see that there is a ‘preview’ link and a a ‘download’ link in the Practical section. If you click the preview link you will be taken to the GitHub page for the notebook where it has been ‘rendered’ as a web page by GitHub. But to make the notebook useable on your computer, you need to download the RAW notebook content.\nSo now:\n\nClick on the Download the Notebook link.\nWhen you see a page of raw code, select File then Save Page As....\nSave the file somewhere easy to find (e.g. Downloads or Desktop).\nNotice that most browsers will save the file as Practical-02-Foundations_1.ipynb.txt; they have added .txt to the file name without asking. You will need to correct this by either: 1) changing the name in the command line using mv &lt;name_of_practical&gt;.ipynb.txt &lt;name_of_practical&gt;.ipynb (you can even do this in Jupyter Lab after completing Section 1.3.2); or 2) renaming the file using the Finder (Mac) or Windows Explorer.\n\n\n\nMaking the Notebook Available to Jupyter Lab\nYou now need to place the notebook where Jupyter Lab can access it. There is only one place that you can do this, and that is where you ran the docker run ... command which mounted the current working directory to the work directory. Remember that the startup command is something like:\ndocker run ... -v \"$(pwd):/home/jovyan/work\" ...\nThe -v (short for volume) tells Docker what part of your computer ($(pwd)) to connect to container (/home/jovyan/work). pwd is short-hand for ‘working directory’ and is the location where you ran the Docker startup command! So we’re talking about the location on your computer when you access the work folder from within Docker/Jupyter Lab:\n\nOn a Mac it will most likely be your $HOME directory (e.g. /Users/your_username/).\nOn a Windows machine it may be your $HOME directory but we can’t promise. See the steps above for trying to ensure this is where you end up.\n\nPerhaps a video will help clarify?\n\n\n\nStaring the Notebook\nFrom within Jupyter Lab you should now be able to start the notebook:\n\nMake sure Jupyter Lab is showing (📁/work/) in the left-hand menu.\nNavigate to the location that you saved Practical-02-Foundations_1.ipynb.\nDouble-click the file and you should see the notebook appear on the right-hand side.\n\nNow you can run code directly in your browser, so let’s try it!"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#variables-names",
    "href": "practicals/Practical-02-Foundations_1.html#variables-names",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Variables Names",
    "text": "Variables Names\n\n\n\n\n\n\nLook closer!\n\n\n\nPay attention to the colour of the code, while it might be subtle (a single character in a different colour), it is giving you clues as to where problems might be found because it means the Python ‘interpreter’ is reading what you wrote differently from how you probably meant it…\n\n\nSome of the lines of code below are valid Python, but others may have a problem that will cause Python to generate an error. Each error can be fixed by changing one character. See if you can work out which ones you need to fix before running the code:\n\nQuestion\n\nPi = 3.14159      # Valid Python\npi = 3.14159      # ??\n3pi = 3.14159*3   # ??\npi_2 = 3.14159**2 # ??\npi^2 = 3.14159**2 # ??\nmy radius = 5     # ??\nMy_Radius = 5     # ??\nclass = 5         # ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#variable-types",
    "href": "practicals/Practical-02-Foundations_1.html#variable-types",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Variable Types",
    "text": "Variable Types\nBefore running the code below, work out what the output will be for each of the print commands below when the code is executed. Write them as comments on the same line as the code (after the #, see example).\n\nQuestion\n\nx = '2'\ny = z = 2\n\nprint(y * z)    # 4\nprint(x * y)    # ??\nprint(x + x)    # ??\nprint((y+z)**z) # ??\nprint(y/(y+z))  # ??\n\nprint( type(x * y) )    # ??\nprint( type(y * z) )    # ??\nprint( type((y+z)**z) ) # ??\nprint( type(y/(y+z)) )  # ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#assignment",
    "href": "practicals/Practical-02-Foundations_1.html#assignment",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Assignment",
    "text": "Assignment\nBefore running the code, work out what the values of x, y and z will be after every line of code in the block has been executed.\n\nQuestion\n\nx = 12\ny = 10\n\nz = x + y # ??\nx = x + y # ??\ny = z + y # ??\n\n# print(x)\n# ??\n# ??\n\nOnce you have worked out what you think x, y and z are, add print(...) statements to the code above to check your answers!\nMake sure you understand the results you find. Ask someone if you need help to understand."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#operators-precedence",
    "href": "practicals/Practical-02-Foundations_1.html#operators-precedence",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Operators & Precedence",
    "text": "Operators & Precedence\nBefore running the code, work out what the values of x, y and z will be after every line of code in the block has been executed. Feel free to use a calculator.\n\n\n\n\n\n\nTip\n\n\n\nThis question is about what operations (i.e. multiplication, division, powers, etc.) are done first based on the type of operation and the presence of parentheses… it’s the same as it would be for a maths problem!\n\n\n\nQuestion\n\nx = 1 + (2 * 3) / 4\ny = (1 + 2) * (3 / 4)\nz = 1 + 2 * (3 / 4)\n\nprint(x) # x is \nprint(y) # y is \nprint(z) # z is \n\nOnce you have calculated what you think x, y and z are, run the code to check."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#test-your-operator-knowledge",
    "href": "practicals/Practical-02-Foundations_1.html#test-your-operator-knowledge",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Test Your Operator Knowledge",
    "text": "Test Your Operator Knowledge\nNow let’s look at some of the stranger operators. Many of these can be very useful in more complex code but can seem a little pointless now.\nWork out what operator should replace the ?? in each of the lines of code below to produce the output I’ve shown in the comments. I’ve mixed in ones you have seen above with ones that we’ve not seen before.\n\nQuestion\n\nx = 10\ny = 3\n\nprint( x ?? y ) # 1\nprint( x ?? y ) # 13\nprint( x ?? y ) # False\nprint( x ?? y ) # 1000\nprint( x ?? y ) # 7\nprint( x ?? y ) # 3"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#applying-what-weve-learned",
    "href": "practicals/Practical-02-Foundations_1.html#applying-what-weve-learned",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Applying What We’ve Learned",
    "text": "Applying What We’ve Learned\nNow we are going to take what we’ve learned and apply it in a more ‘abstract’ way: how do we translate some well-known mathematical formulae into code? In particular, I’m interested in the formula for the volume of a sphere (and this gives me a chance to show that Notebooks can show formulae as well!):\n\\[\nV = \\frac{4}{3}\\pi r^{3}\n\\]\n\nCalculate the Volume of a Sphere\nSo, given a sphere with a diameter of 12cm, calculate its volume:\n\n\n\n\n\n\nTip\n\n\n\nI would strongly advise you to Google: python constant pi and look for code that will save you having to write down the value of \\(\\pi\\).\n\n\n\nQuestion\n\nfrom math import ??\nv = ??\nprint(f\"{v:0.3f} cm3\")\n\nI get an answer of 904.779cm\\(^3\\).\n\n\n\nCalculate the Radius of a Sphere\nNow, given a sphere of volume 14,137cm\\(^3\\) calculate its radius as a whole number. The formula for this can be worked out as:\n\\[\\begin{align*}\n\\frac{3}{4}V &= \\pi r^{3} \\\\\n\\frac{3}{4}\\frac{V}{\\pi} &= r^{3} \\\\\n(\\frac{3}{4}\\frac{V}{\\pi})^{1/3} &= r\n\\end{align*}\\]\nIf you can’t remember how to rearrange formulae this would be a good skill to refresh!\n\n\n\n\n\n\nTip\n\n\n\nThere are three ways to get a “whole number” from a float:\n\nWhen you’re starting out, the easiest is to change the variable’s type\nThe next step up is to make use of Google to find out if there are ways of rounding to the nearest integer\nThe third step is to change what’s visible to the user without altering the actual number\n\n\n\nI get an answer of either 14 or 15… can you work out why?\n\nQuestion\n\nfrom math import pi\nv = 14137\nr = ??\nprint(??)"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#working-with-conditions",
    "href": "practicals/Practical-02-Foundations_1.html#working-with-conditions",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Working with Conditions",
    "text": "Working with Conditions\nUse if, elif, and else so that you get the following output:\n\nWhen hours is 10 or more, then the code prints At least 10 hours worked!\nWhen hours is exactly 2, then the code prints Exactly 2 hours worked.\n\nWhen hours is 9 or less but not 2, then the code prints Less than 10 hours worked!\n\n\n\n\n\n\n\nHint\n\n\n\nYou will also need to think about the order in which these conditions are tested.\n\n\n\nQuestion\n\nhours = 2\n\nif hours ??:\n    print(\" \")\nelif hours ??:\n    print(\" \")\nelse:\n    print(\" \")"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#flow-control",
    "href": "practicals/Practical-02-Foundations_1.html#flow-control",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Flow Control",
    "text": "Flow Control\nUsing the flow chart shown in the image below as a model, write the code to make this condition work. You will need to complete the code such that it produces the following: 1. When a = 2 and b = 2 four lines of output will be written 2. When a = 1 and b = 2 one line of output will be written\n\n\nQuestion\n\na = 1\nb = 1\n\n# &lt;your code here&gt;\n??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#its-all-quite-logical",
    "href": "practicals/Practical-02-Foundations_1.html#its-all-quite-logical",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "It’s All Quite Logical…",
    "text": "It’s All Quite Logical…\nBefore adding a value for x and running the code below, try to answer the following questions:\n\nQuestion\n\nWhat names are name(s) are printed when x = 5?\nWhat value(s) can x be when the names Joe and Aled are printed?\nWhat name(s) are printed when x = -1?\n\nIs there any value for which all three names will be printed?\n\n\nx = ??\n\nif x &gt; 0 and x &lt; 5:\n    print(\"Joe\")\n\nif x &gt; 0 or x &lt; 5:\n    print(\"Aled\")\n\nif not(x &gt; 0):\n    print(\"Sarah\")"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#logic-contd",
    "href": "practicals/Practical-02-Foundations_1.html#logic-contd",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Logic (Cont’d)",
    "text": "Logic (Cont’d)\nStudy the flow chart below.\n\n\n\n\n\n\n\nTip\n\n\n\nThis will require you to combine logic with one of the operators that we saw earlier. Also note the new iterator that we’ve got here: range(&lt;start&gt;, &lt;stop&gt;) to create a range of numbers between two other numbers.\n\n\nIn the cell below, use the for loop already set up to as a starting point for implementing the flow chart shown above for values of x between 0 and 9.\n\nQuestion\n\nfor x in range(0,9):\n    #... do something...\n    ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#nested-conditionals",
    "href": "practicals/Practical-02-Foundations_1.html#nested-conditionals",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "‘Nested’ Conditionals",
    "text": "‘Nested’ Conditionals\nConditional statements can be nested within one another. That is, Python evaluates the first, or ‘outer’, condition and can then evaluate secondary, or ‘inner’, conditions. The code below shows an example of this.\n\nQuestion\n\nx = 5\ny = 4\n\nif x != y:                                 #line 1\n    print(\"x is not equal to y\")\n    \n    if(x &gt; y):                             #line 4\n        print(\"x is greater than y\")\n    \n    else:                                  #line 7\n        print(\"x is less than y\")\n\nelse:\n    print(\"&lt;insert conclusion here&gt;\")\n\nNote how the indentation makes it easier to work out which ‘level’ (outer or inner condition) the code is operating on. In the code above, lines 4 and 7 are at the same indentation meaning that both will be skipped if the initial condition (on line 1) is False.\nTo check you understand how the code above works:\n\nChange &lt;insert conclusion here&gt; to a string that explains the condition of x and y\nFor x = 2 and y = 3, type what line(s) will be output here: …\n\nGreat! You should now have a pretty good understanding of how conditional and logical operators work. This understanding will be handy in future as we work through other computational concepts."
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#whos-in-the-list",
    "href": "practicals/Practical-02-Foundations_1.html#whos-in-the-list",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Who’s in the List?",
    "text": "Who’s in the List?\nHere we are looking to interact with lists in a straightforward way that will help you to understand accessing them using indexes and slices, and searching.\n\nQuestion\n\ncities = ['New York', 'London', 'Beijing', 'Tokyo', 'Delhi']\n\n# Print out London from cities:\nprint( ?? )\n\n# Print out Tokyo using *negative* indexing:\nprint( ?? )\n\n# Print out Beijing *and* Tokyo using a list slice\nprint( ?? )\n\n# Print out London to Delhi using a slice\nprint( ?? ) # You could also do cities[1:5] but this way is neater\n\n# Combine positive and negative indexing to print out London, Beijing and Tokyo using a list slice\nprint( ?? )\n\n# Print out the position of New York in the list by searching for it (i.e. you can't just type 0)\nprint( ?? )"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#manipulating-lists",
    "href": "practicals/Practical-02-Foundations_1.html#manipulating-lists",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Manipulating Lists",
    "text": "Manipulating Lists\nLet’s break a few things…\n\nCreate an IndexError\n\nQuestion\n\n# Cause an 'IndexError: list index out of range' errr\n??\n\n\n\n\nCreate a ValueError\n\nQuestion\n\n# Cause a ValueError using the city of Toronto\n??\n\n\n\n\nSort the List\nSort the list in place in reverse alphabetical order (i.e. z…a) and then print the sorted list\n\nQuestion\n\n??\nprint(cities)\n\nThe output from this should be: [‘Tokyo’, ‘New York’, ‘London’, ‘Delhi’, ‘Beijing’]"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#addingremoving-values",
    "href": "practicals/Practical-02-Foundations_1.html#addingremoving-values",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "Adding/Removing Values",
    "text": "Adding/Removing Values\n\nInserting into a List\nAdd the city of Toronto to the list after New York in the sorted list.\n\nQuestion\n\n# Just in case you make a mistake...\ncities = ['Tokyo', 'New York', 'London', 'Delhi', 'Beijing']\n\n??\nprint(cities)\n\nThe output should be: [‘Tokyo’, ‘New York’, ‘Toronto’, ‘London’, ‘Delhi’, ‘Beijing’]\n\n\n\nRemoving from a List\nNow pop New York from the list without specifying its index (i.e. the number 1 should not appear in your code). Print out the value that you popped and the print out the cities list to check you’ve done the right thing…\n\nQuestion\n\n??\nprint(p)\nprint(cities)\n\nThe output should be:\n\nNew York\n[‘Tokyo’, ‘Toronto’, ‘London’, ‘Delhi’, ‘Beijing’]\n\n\n\n\nChecking Lists\nFinally, how can you check if the city of Moscow is in the list and let the user know if it is or is not?\n\nQuestion\n\nif ??\n    ??\nelse:\n    ??"
  },
  {
    "objectID": "practicals/Practical-02-Foundations_1.html#youre-done",
    "href": "practicals/Practical-02-Foundations_1.html#youre-done",
    "title": "Practical 2: Foundations (Part 1)",
    "section": "You’re Done!",
    "text": "You’re Done!\nThis is quite a lot to get through. If you’ve managed it in under 2 hours then congratulations! Either you must have paid a lot of attention when doing Code Camp, or you might want to check in with us as to whether you should really be doing this module…\n\nNo Wait, One More Thing…\nYou might want to refer to both The Command Line and Getting Stuck into Git lectures as well as to The Terminal Code Camp session.\nYou now want to push your completed notebook to your GitHub reposistory. Using the Terminal or Git Bash you need to:\n\nNavigate (if you’ve not got a shell already open there) to $HOME/Documents/CASA/modules/&lt;your repository&gt;.\nCheck the status of your notebooks using git status (you should see that Practical-02-Foundations_1.ipynb has been modified).\nAdd this changed notebook to git using git add  Practical-02-Foundations_1.ipynb\nCommit this changed notebook with a message using git commit -m \"&lt;your message here... but don't use a !&gt;\"\nPush this change to GitHub using: git push\n\nYou should now be able to visit your repository on GitHub and see that your changes are now stored there as well!\n\n\n\n\n\n\nNote\n\n\n\nIf you are using Docker then you can also save your work as a PDF using: File &gt; Export Notebook As... (this does not work for notebooks with lots of complex formatting).\n\n\n\nLicense\nThese teaching materials are licensed under a mix of the MIT and CC-BY licenses…"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html",
    "href": "practicals/Practical-10-Presenting_Data.html",
    "title": "Practical 10: Presenting Data",
    "section": "",
    "text": "Important\n\n\n\nThis practical focusses on the final topic we want to cover in Foundations: visualisation! You will have seen quite a bit of this across the preceding three to four weeks, but it was done in an ad-hoc way, here we try to systematise things a bit."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#anatomy-of-a-figure",
    "href": "practicals/Practical-10-Presenting_Data.html#anatomy-of-a-figure",
    "title": "Practical 10: Presenting Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\n\n\n\n\n\nTip\n\n\n\nYou might want to bookmark the ‘Anatomy of a Figure’ image so that you can easily find and refer to it in the future. This structure is why matplotlib is so much nastier than ggplot, but it does also give you greater control over the output if you really dig into the guts of things.\n\n\nOne of the reasons that Matplotlib is so much more complex than ggplot is that it can actually do a plot more than ggplot, including image manipulation, axis translation, and even 3D. You can get a sense of this by looking at the tutorials since the Users guide can be a bit overwhelming.\nNevertheless, the core components of all matplotlib figures can be seen here:"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#finding-fonts",
    "href": "practicals/Practical-10-Presenting_Data.html#finding-fonts",
    "title": "Practical 10: Presenting Data",
    "section": "Finding Fonts",
    "text": "Finding Fonts\nI find matplotlib’s use of fonts to be profoundly weird. If you use conda and install directly on to the computer then you might have access to all of your computer’s fonts (though there are different types of fonts as well, not all of which will show up), but for most users it will be those that were installed into Docker.\n\nUsing Fontconfig\nfontconfig is the base Linux utility for managing fonts. We can list font using fc-list and then a set of ‘switches’ determining the kind of information we want back. Since fontconfig doesn’t exist on OSX or Windows, you’ll need to do some more investigating and poking around to get these details on a conda install (I’ll show an option further down)…\nHere we ask fontconfig to format the output so that we only get the first part of the family name, and then we pipe (recall | sends output from one utility to another!) the output of that to sort, which sorts the output, and uniq which removes duplicates (which there will be because there are bold, italic, small-caps, etc. versions of each font). To make better sense of this you can always try playing around with all three steps in the output below!\n\nfonts = ! fc-list --format=\"%{family[0]}\\n\" | sort | uniq\nprint(fonts)\n\n['.Al Bayan PUA', '.Al Nile PUA', '.Al Tarikh PUA', '.Apple Color Emoji UI', '.Apple SD Gothic NeoI', '.Aqua Kana', '.Arial Hebrew Desk Interface', '.Baghdad PUA', '.Beirut PUA', '.Damascus PUA', '.DecoType Naskh PUA', '.Diwan Kufi PUA', '.Farah PUA', '.Geeza Pro Interface', '.Geeza Pro PUA', '.Helvetica LT MM', '.Hiragino Kaku Gothic Interface', '.Hiragino Sans GB Interface', '.Keyboard', '.KufiStandardGK PUA', '.LastResort', '.Lucida Grande UI', '.Muna PUA', '.Nadeem PUA', '.New York', '.Noto Nastaliq Urdu UI', '.PingFang HK', '.PingFang SC', '.PingFang TC', '.SF Arabic', '.SF Arabic Rounded', '.SF Armenian', '.SF Armenian Rounded', '.SF Camera', '.SF Compact', '.SF Compact Rounded', '.SF Georgian', '.SF Georgian Rounded', '.SF Hebrew', '.SF Hebrew Rounded', '.SF NS Mono', '.SF NS Rounded', '.SF Soft Numeric', '.Sana PUA', '.Savoye LET CC.', '.ThonburiUI', '.ThonburiUIWatch', '.Times LT MM', 'Academy Engraved LET', 'Adobe Garamond', 'Adobe Jenson MM', 'Adobe Wood Type', 'AdobeGaramond', 'Al Bayan', 'American Typewriter', 'Amethyst', 'Andale Mono', 'Antonio', 'Apple Braille', 'Apple Chancery', 'Apple Color Emoji', 'Apple SD Gothic Neo', 'Apple Symbols', 'AppleGothic', 'AppleMyungjo', 'Arial', 'Arial Black', 'Arial Hebrew', 'Arial Hebrew Scholar', 'Arial Narrow', 'Arial Rounded MT Bold', 'Arial Unicode MS', 'Athelas', 'Avenir', 'Avenir Next', 'Avenir Next Condensed', 'Ayuthaya', 'Bangla MN', 'Bangla Sangam MN', 'Barlow', 'Baskerville', 'Bergamo Caps', 'Bergamo Pro', 'Bergamo Std', 'Big Caslon', 'Big Shoulders Display', 'BigCheeseLight', 'Blockhead', 'Bodoni 72', 'Bodoni 72 Oldstyle', 'Bodoni 72 Smallcaps', 'Bodoni Ornaments', 'Bradley Hand', 'Brush Script MT', 'Chalkboard', 'Chalkboard SE', 'Chalkduster', 'Charter', 'CitizenLight', 'Cochin', 'Comic Sans MS', 'Copperplate', 'Corsiva Hebrew', 'Courier', 'Courier New', 'DIN Alternate', 'DIN Condensed', 'Damascus', 'DemocraticaRegular', 'Devanagari MT', 'Devanagari Sangam MN', 'Didot', 'EB Garamond', 'ESRI Default Marker', 'ESRI Public1', 'ESRI Transportation & Civic', 'ESRI US MUTCD 1', 'ESRI US MUTCD 2', 'ESRI US MUTCD 3', 'Euphemia UCAS', 'Fira Code', 'Fira Mono', 'Fira Sans', 'Fira Sans Condensed', 'Fira Sans Extra Condensed', 'Frutiger', 'Futura', 'GB18030 Bitmap', 'Gallagher', 'Galvji', 'Gasolyn Free', 'Geeza Pro', 'Geneva', 'Georgia', 'Gill Sans', 'Grantha Sangam MN', 'Gujarati MT', 'Gujarati Sangam MN', 'Gurmukhi MN', 'Gurmukhi MT', 'Gurmukhi Sangam MN', 'Heiti SC', 'Heiti TC', 'Helvetica', 'Helvetica Neue', 'Herculanum', 'Hiragino Kaku Gothic Pro', 'Hiragino Kaku Gothic ProN', 'Hiragino Kaku Gothic Std', 'Hiragino Kaku Gothic StdN', 'Hiragino Maru Gothic Pro', 'Hiragino Maru Gothic ProN', 'Hiragino Mincho Pro', 'Hiragino Mincho ProN', 'Hiragino Sans', 'Hiragino Sans GB', 'Hoefler Text', 'ITF Devanagari', 'ITF Devanagari Marathi', 'Impact', 'InaiMathi', 'Inconsolata', 'Inconsolata Condensed', 'Inconsolata Expanded', 'Inconsolata ExtraCondensed', 'Inconsolata ExtraExpanded', 'Inconsolata SemiCondensed', 'Inconsolata SemiExpanded', 'Inconsolata UltraCondensed', 'Inconsolata UltraExpanded', 'Input Mono', 'Input Mono Compressed', 'Input Mono Condensed', 'Input Mono Narrow', 'Input Sans', 'Input Sans Compressed', 'Input Sans Condensed', 'Input Sans Narrow', 'Input Serif', 'Input Serif Compressed', 'Input Serif Condensed', 'Input Serif Narrow', 'Inter Tight', 'Iowan Old Style', 'JetBrains Mono', 'Johnston ITC Std', 'Kailasa', 'Kannada MN', 'Kannada Sangam MN', 'Kefa', 'Khmer MN', 'Khmer Sangam MN', 'Khula', 'Kings Caslon Display', 'Kings Caslon Text', 'KingsBureauGrot', 'Klavika', 'Kohinoor Bangla', 'Kohinoor Devanagari', 'Kohinoor Gujarati', 'Kohinoor Telugu', 'Kokonor', 'Krungthep', 'Kubos', 'KufiStandardGK', 'Lao MN', 'Lao Sangam MN', 'Liberation Mono', 'Liberation Sans', 'Liberation Sans Narrow', 'Liberation Serif', 'Linotype Holiday Pi', 'Lobster', 'Lucida Grande', 'Luminari', 'Malayalam MN', 'Malayalam Sangam MN', 'Marion', 'Marker Felt', 'Menlo', 'Micro 5', 'Microsoft Sans Serif', 'Miguta', 'Monaco', 'Mshtakan', 'Mukta Mahee', 'Myanmar MN', 'Myanmar Sangam MN', 'Nadeem', 'New Peninim MT', 'Noble Company', 'Norse', 'Noteworthy', 'Noto Nastaliq Urdu', 'Noto Sans Adlam', 'Noto Sans Armenian', 'Noto Sans Avestan', 'Noto Sans Bamum', 'Noto Sans Bassa Vah', 'Noto Sans Batak', 'Noto Sans Bhaiksuki', 'Noto Sans Brahmi', 'Noto Sans Buginese', 'Noto Sans Buhid', 'Noto Sans Canadian Aboriginal', 'Noto Sans Carian', 'Noto Sans Caucasian Albanian', 'Noto Sans Chakma', 'Noto Sans Cham', 'Noto Sans Coptic', 'Noto Sans Cuneiform', 'Noto Sans Cypriot', 'Noto Sans Duployan', 'Noto Sans Egyptian Hieroglyphs', 'Noto Sans Elbasan', 'Noto Sans Glagolitic', 'Noto Sans Gothic', 'Noto Sans Gunjala Gondi', 'Noto Sans Hanifi Rohingya', 'Noto Sans Hanunoo', 'Noto Sans Hatran', 'Noto Sans Imperial Aramaic', 'Noto Sans Inscriptional Pahlavi', 'Noto Sans Inscriptional Parthian', 'Noto Sans Javanese', 'Noto Sans Kaithi', 'Noto Sans Kannada', 'Noto Sans Kayah Li', 'Noto Sans Kharoshthi', 'Noto Sans Khojki', 'Noto Sans Khudawadi', 'Noto Sans Lepcha', 'Noto Sans Limbu', 'Noto Sans Linear A', 'Noto Sans Linear B', 'Noto Sans Lisu', 'Noto Sans Lycian', 'Noto Sans Lydian', 'Noto Sans Mahajani', 'Noto Sans Mandaic', 'Noto Sans Manichaean', 'Noto Sans Marchen', 'Noto Sans Masaram Gondi', 'Noto Sans Meetei Mayek', 'Noto Sans Mende Kikakui', 'Noto Sans Meroitic', 'Noto Sans Miao', 'Noto Sans Modi', 'Noto Sans Mongolian', 'Noto Sans Mro', 'Noto Sans Multani', 'Noto Sans Myanmar', 'Noto Sans NKo', 'Noto Sans Nabataean', 'Noto Sans New Tai Lue', 'Noto Sans Newa', 'Noto Sans Ol Chiki', 'Noto Sans Old Hungarian', 'Noto Sans Old Italic', 'Noto Sans Old North Arabian', 'Noto Sans Old Permic', 'Noto Sans Old Persian', 'Noto Sans Old South Arabian', 'Noto Sans Old Turkic', 'Noto Sans Oriya', 'Noto Sans Osage', 'Noto Sans Osmanya', 'Noto Sans Pahawh Hmong', 'Noto Sans Palmyrene', 'Noto Sans Pau Cin Hau', 'Noto Sans PhagsPa', 'Noto Sans Phoenician', 'Noto Sans Psalter Pahlavi', 'Noto Sans Rejang', 'Noto Sans Samaritan', 'Noto Sans Saurashtra', 'Noto Sans Sharada', 'Noto Sans Siddham', 'Noto Sans Sora Sompeng', 'Noto Sans Sundanese', 'Noto Sans Syloti Nagri', 'Noto Sans Syriac', 'Noto Sans Tagalog', 'Noto Sans Tagbanwa', 'Noto Sans Tai Le', 'Noto Sans Tai Tham', 'Noto Sans Tai Viet', 'Noto Sans Takri', 'Noto Sans Thaana', 'Noto Sans Tifinagh', 'Noto Sans Tirhuta', 'Noto Sans Ugaritic', 'Noto Sans Vai', 'Noto Sans Wancho', 'Noto Sans Warang Citi', 'Noto Sans Yi', 'Noto Sans Zawgyi', 'Noto Serif Ahom', 'Noto Serif Balinese', 'Noto Serif Hmong Nyiakeng', 'Noto Serif Myanmar', 'Noto Serif Yezidi', 'OS OpenMap Local', 'Olympicons', 'Olympicons 2', 'Optima', 'Oriya MN', 'Oriya Sangam MN', 'PT Mono', 'PT Sans', 'PT Sans Caption', 'PT Sans Narrow', 'PT Serif', 'PT Serif Caption', 'Palatino', 'Papyrus', 'Party LET', 'Phosphate', 'PingFang HK', 'PingFang SC', 'PingFang TC', 'Plantagenet Cherokee', 'RODE Noto Sans CJK SC B', 'RODE Noto Sans CJK SC R', 'RODE Noto Sans Hindi B', 'RODE Noto Sans Hindi R', 'Raanana', 'Roboto Flex', 'Roboto Mono', 'Roboto Serif', 'Roboto Slab', 'Rockwell', 'Rolves Free', 'STIX Two Math', 'STIX Two Text', 'STIXGeneral', 'STIXIntegralsD', 'STIXIntegralsSm', 'STIXIntegralsUp', 'STIXIntegralsUpD', 'STIXIntegralsUpSm', 'STIXNonUnicode', 'STIXSizeFiveSym', 'STIXSizeFourSym', 'STIXSizeOneSym', 'STIXSizeThreeSym', 'STIXSizeTwoSym', 'STIXVariants', 'STSong', 'Sathu', 'Satisfy', 'Savoye LET', 'Sedan', 'Sedan SC', 'Seravek', 'Shree Devanagari 714', 'SignPainter', 'Silkscreen', 'Silom', 'Sinhala MN', 'Sinhala Sangam MN', 'Skia', 'Snell Roundhand', 'Songti SC', 'Songti TC', 'Source Code Pro', 'Source Sans 3', 'Source Serif 4', 'Source Serif Pro', 'SourceCodePro+Powerline+Awesome Regular', 'Spectral', 'Star Jedi', 'Star Jedi Hollow', 'Sukhumvit Set', 'Superclarendon', 'Symbol', 'System Font', 'Systémové písmo', 'Systemskrift', 'Tahoma', 'Tamil MN', 'Tamil Sangam MN', 'Telugu MN', 'Telugu Sangam MN', 'Thonburi', 'Times', 'Times New Roman', 'Tipus de lletra del sistema', 'Trattatello', 'Trebuchet MS', 'Unica One', 'Verdana', 'Webdings', 'Wingdings', 'Wingdings 2', 'Wingdings 3', 'Zapf Dingbats', 'Zapfino', 'Zebrawood', 'ZeitGuys', 'bridamount', 'فرح', 'منى', 'نسخ', 'وسيم', '系統字體', 'النيل', 'بغداد', 'بيروت', 'صنعاء', 'فارسي', 'ديوان ثلث', 'ديوان كوفي', 'مِصحفي', 'مِصحفي ذهبي', 'التاريخ']\n\n\n\n\n\n\n\n\nCapturing output\n\n\n\nNotice that we’re able to capture the output of an external application (called via the Terminal) with fonts = ! .... This can be useful when something is easy to do on the command line but hard to do in Python.\n\n\nThe below option also pipes output from fonctconfig, but to the grep utility which checks each line for the character sequence Liberation. Now we’re asking fontconfig to include style details which will relate to both weight (regular, bold, extra bold, light, etc.) and italic, bold, small caps, etc.\n\nfonts = ! fc-list : family style | grep \"Liberation\"\nprint(sorted(fonts))\n\n['Liberation Mono:style=Bold', 'Liberation Mono:style=Bold Italic', 'Liberation Mono:style=Italic', 'Liberation Mono:style=Regular', 'Liberation Sans Narrow:style=Bold', 'Liberation Sans Narrow:style=Bold Italic', 'Liberation Sans Narrow:style=Italic', 'Liberation Sans Narrow:style=Regular', 'Liberation Sans:style=Bold', 'Liberation Sans:style=Bold Italic', 'Liberation Sans:style=Italic', 'Liberation Sans:style=Regular', 'Liberation Serif:style=Bold', 'Liberation Serif:style=Bold Italic', 'Liberation Serif:style=Italic', 'Liberation Serif:style=Regular']\n\n\nYou can find more examples here, a more detailed set of instructions here, and even information about (for example) supported languages based on RFC 3066.\nHere are the languages supported by the Ubuntu Light font:\n\n! fc-list \"Liberation Mono\" : lang\n\n:lang=aa|af|av|ay|be|bg|bi|br|bs|ca|ce|ch|co|cs|cy|da|de|el|en|eo|es|et|eu|fi|fj|fo|fr|fur|fy|gd|gl|gv|ho|hr|hu|ia|id|ie|ik|io|is|it|ki|kl|kum|la|lb|lez|lt|lv|mg|mh|mo|mt|nb|nds|nl|nn|no|nr|nso|ny|oc|om|os|pl|pt|rm|ro|ru|se|sel|sk|sl|sma|smj|smn|so|sq|sr|ss|st|sv|sw|tk|tl|tn|tr|ts|uk|uz|vo|vot|wa|wen|wo|xh|yap|zu|an|crh|csb|fil|hsb|ht|jv|kj|ku-tr|kwm|lg|li|ms|na|ng|pap-an|pap-aw|rn|rw|sc|sg|sn|su|za\n\n\nHere are the monospace fonts installed:\n\n! fc-list :spacing=mono : family | sort | uniq\n\n.Apple Color Emoji UI\n.LastResort\n.SF NS Mono\n.Times LT MM\nAdobe Garamond\nAdobe Jenson MM\nAdobe Wood Type\nAmethyst\nAndale Mono\nApple Color Emoji\nCourier New\nFira Mono\nFira Mono,Fira Mono Medium\nGB18030 Bitmap\nInconsolata\nInconsolata Condensed\nInconsolata Condensed,Inconsolata Condensed Black\nInconsolata Condensed,Inconsolata Condensed ExtraBold\nInconsolata Condensed,Inconsolata Condensed ExtraLight\nInconsolata Condensed,Inconsolata Condensed Light\nInconsolata Condensed,Inconsolata Condensed Medium\nInconsolata Condensed,Inconsolata Condensed SemiBold\nInconsolata Expanded\nInconsolata Expanded,Inconsolata Expanded Black\nInconsolata Expanded,Inconsolata Expanded ExtraBold\nInconsolata Expanded,Inconsolata Expanded ExtraLight\nInconsolata Expanded,Inconsolata Expanded Light\nInconsolata Expanded,Inconsolata Expanded Medium\nInconsolata Expanded,Inconsolata Expanded SemiBold\nInconsolata ExtraCondensed\nInconsolata ExtraCondensed,Inconsolata ExtraCondensed Black\nInconsolata ExtraCondensed,Inconsolata ExtraCondensed ExtraBold\nInconsolata ExtraCondensed,Inconsolata ExtraCondensed ExtraLight\nInconsolata ExtraCondensed,Inconsolata ExtraCondensed Light\nInconsolata ExtraCondensed,Inconsolata ExtraCondensed Medium\nInconsolata ExtraCondensed,Inconsolata ExtraCondensed SemiBold\nInconsolata ExtraExpanded\nInconsolata ExtraExpanded,Inconsolata ExtraExpanded Black\nInconsolata ExtraExpanded,Inconsolata ExtraExpanded ExtraBold\nInconsolata ExtraExpanded,Inconsolata ExtraExpanded ExtraLight\nInconsolata ExtraExpanded,Inconsolata ExtraExpanded Light\nInconsolata ExtraExpanded,Inconsolata ExtraExpanded Medium\nInconsolata ExtraExpanded,Inconsolata ExtraExpanded SemiBold\nInconsolata SemiCondensed\nInconsolata SemiCondensed,Inconsolata SemiCondensed Black\nInconsolata SemiCondensed,Inconsolata SemiCondensed ExtraBold\nInconsolata SemiCondensed,Inconsolata SemiCondensed ExtraLight\nInconsolata SemiCondensed,Inconsolata SemiCondensed Light\nInconsolata SemiCondensed,Inconsolata SemiCondensed Medium\nInconsolata SemiCondensed,Inconsolata SemiCondensed SemiBold\nInconsolata SemiExpanded\nInconsolata SemiExpanded,Inconsolata SemiExpanded Black\nInconsolata SemiExpanded,Inconsolata SemiExpanded ExtraBold\nInconsolata SemiExpanded,Inconsolata SemiExpanded ExtraLight\nInconsolata SemiExpanded,Inconsolata SemiExpanded Light\nInconsolata SemiExpanded,Inconsolata SemiExpanded Medium\nInconsolata SemiExpanded,Inconsolata SemiExpanded SemiBold\nInconsolata UltraCondensed\nInconsolata UltraCondensed,Inconsolata UltraCondensed Black\nInconsolata UltraCondensed,Inconsolata UltraCondensed ExtraBold\nInconsolata UltraCondensed,Inconsolata UltraCondensed ExtraLight\nInconsolata UltraCondensed,Inconsolata UltraCondensed Light\nInconsolata UltraCondensed,Inconsolata UltraCondensed Medium\nInconsolata UltraCondensed,Inconsolata UltraCondensed SemiBold\nInconsolata UltraExpanded\nInconsolata UltraExpanded,Inconsolata UltraExpanded Black\nInconsolata UltraExpanded,Inconsolata UltraExpanded ExtraBold\nInconsolata UltraExpanded,Inconsolata UltraExpanded ExtraLight\nInconsolata UltraExpanded,Inconsolata UltraExpanded Light\nInconsolata UltraExpanded,Inconsolata UltraExpanded Medium\nInconsolata UltraExpanded,Inconsolata UltraExpanded SemiBold\nInconsolata,Inconsolata Black\nInconsolata,Inconsolata ExtraBold\nInconsolata,Inconsolata ExtraLight\nInconsolata,Inconsolata Light\nInconsolata,Inconsolata Medium\nInconsolata,Inconsolata SemiBold\nInput Mono\nInput Mono Compressed\nInput Mono Compressed,InputMonoCompressed Black\nInput Mono Compressed,InputMonoCompressed ExLight\nInput Mono Compressed,InputMonoCompressed Light\nInput Mono Compressed,InputMonoCompressed Medium\nInput Mono Compressed,InputMonoCompressed Thin\nInput Mono Condensed\nInput Mono Condensed,InputMonoCondensed Black\nInput Mono Condensed,InputMonoCondensed ExLight\nInput Mono Condensed,InputMonoCondensed Light\nInput Mono Condensed,InputMonoCondensed Medium\nInput Mono Condensed,InputMonoCondensed Thin\nInput Mono Narrow\nInput Mono Narrow,InputMonoNarrow Black\nInput Mono Narrow,InputMonoNarrow ExLight\nInput Mono Narrow,InputMonoNarrow Light\nInput Mono Narrow,InputMonoNarrow Medium\nInput Mono Narrow,InputMonoNarrow Thin\nInput Mono,InputMono Black\nInput Mono,InputMono ExLight\nInput Mono,InputMono Light\nInput Mono,InputMono Medium\nInput Mono,InputMono Thin\nLiberation Mono\nMenlo\nMiguta\nPT Mono\nRoboto Mono\nSource Code Pro,Source Code Pro ExtraLight\n\n\n\n\nUsing Python+Terminal\nAnother way to get at this information is to try asking matplotlib what fonts it already knows about in its cache:\n\nimport matplotlib\nloc = matplotlib.get_cachedir()\n!ls {loc}\n\nfontlist-v330.json fontlist-v390.json\n\n\nHopefully you will see a list of installed fonts when you run this next block of code. See if you can make sense of what this code does!\n\nfonts = !cat {loc + '/fontlist-v330.json'}\nfonts = set(list(filter(lambda x:'\"name\"' in x, fonts)))\nfonts = [x.replace('      \"name\": \"','').replace('\",','') for x in fonts]\nprint(fonts)\n\n['Roboto Serif 36pt Condensed', 'Monaco', 'Telugu Sangam MN', 'Avenir Next', 'Gallagher', 'Songti SC', 'Norse', 'Georgia', 'Avenir Next Condensed', 'Lao Sangam MN', 'Roboto Serif 72pt', 'Gujarati MT', 'Noto Sans Brahmi', 'Diwan Kufi', 'Noto Sans Coptic', 'Miguta', 'DIN Alternate', 'Noto Sans Myanmar', 'Menlo', 'New Century Schoolbook', 'Noto Sans Inscriptional Parthian', 'Rolves Free', 'Roboto Serif UltraCondensed', 'Thonburi', 'Apple Symbols', 'Party LET', 'Input Mono Compressed', 'Oriya MN', 'ESRI US MUTCD 3', 'Didot', 'Apple Braille', 'Damascus', 'American Typewriter', 'Silom', 'Hiragino Mincho ProN', 'Noto Sans Tai Tham', 'Noto Sans Tirhuta', 'STIXIntegralsSm', 'STIXIntegralsD', 'Olympicons 2', 'Noto Sans Osmanya', 'Noto Sans Saurashtra', 'Noto Sans Armenian', 'ITF Devanagari', 'Noto Sans Nabataean', 'Noto Sans Linear B', 'Waseem', 'Arial Narrow', 'Khula', 'Noto Sans Yi', 'STIX Two Text', 'Mishafi', 'Fira Sans', 'Noto Sans Kharoshthi', 'Wingdings 2', 'Noto Sans Lycian', 'Noto Sans Javanese', 'RODE Noto Sans CJK SC R', '.SF Soft Numeric', 'Roboto Serif 28pt SemiCondensed', 'AppleGothic', 'DIN Condensed', 'Roboto Serif SemiExpanded', 'Noto Sans Mahajani', 'Noto Sans Samaritan', 'Noto Sans Inscriptional Pahlavi', 'Snell Roundhand', 'Roboto', 'Roboto Serif 72pt Expanded', '.ThonburiUI', 'Helvetica Neue', 'Symbol', 'Noto Sans Siddham', 'Input Serif Compressed', 'Input Serif Narrow', 'Noto Sans Wancho', 'Barlow', 'Noto Sans Tagalog', 'Roboto Serif 28pt', 'ESRI Default Marker', 'Noto Sans Cuneiform', 'Myanmar MN', 'Noto Sans Warang Citi', 'PT Serif Caption', 'Khmer Sangam MN', 'Noteworthy', 'STIX Two Math', 'Roboto Serif 36pt Expanded', '.SF Compact Rounded', 'STIXIntegralsUpSm', 'Comic Sans MS', 'Futura', 'DecoType Naskh', 'PT Mono', 'Noto Sans Bassa Vah', 'Roboto Serif 72pt Condensed', 'Roboto Serif 28pt SemiExpanded', 'Avenir', 'New Peninim MT', 'Noto Sans Old South Arabian', 'Athelas', 'Kings Caslon Text', 'Malayalam MN', 'Liberation Mono', 'InaiMathi', 'Courier New', 'Noto Serif Balinese', 'Noto Sans Old Persian', 'Baghdad', 'KufiStandardGK', 'Inconsolata Expanded', 'Devanagari Sangam MN', 'Hiragino Sans GB', 'Noto Sans Batak', 'PingFang HK', 'Roboto Serif 28pt UltraCondensed', 'Devanagari MT', 'PT Serif', 'Bangla MN', '.SF Georgian', 'Noto Sans Newa', 'STIXNonUnicode', 'Noto Sans Osage', 'Farah', 'Noto Sans Sharada', 'cmss10', 'Roboto Serif 72pt SemiExpanded', 'DejaVu Serif Display', 'Noto Sans Tifinagh', 'Spectral', 'Phosphate', 'Inconsolata ExtraCondensed', 'Noto Serif Ahom', '.SF Arabic', 'ITC Bookman', 'Noto Sans Mandaic', 'Andale Mono', 'Noto Sans Old Hungarian', 'Noto Sans Bamum', 'ESRI US MUTCD 2', 'Roboto Serif 120pt UltraCondensed', 'AppleMyungjo', 'Bodoni Ornaments', 'Nadeem', 'Noto Sans Sora Sompeng', 'Input Sans', 'Helvetica', 'Roboto Serif 28pt ExtraExpanded', 'Noto Sans Chakma', 'ITC Avant Garde Gothic', 'Oriya Sangam MN', 'Kohinoor Bangla', 'Savoye LET', 'Bodoni 72 Smallcaps', 'cmr10', '.SF Camera', 'Big Caslon', 'Seravek', 'Klavika', 'Noto Sans Multani', 'Bergamo Std', 'Hiragino Maru Gothic Pro', 'Sinhala Sangam MN', 'STIXSizeThreeSym', 'Krungthep', 'Noto Sans Takri', 'Ayuthaya', 'Skia', 'Noto Sans Cham', 'Noto Sans Lepcha', 'Lao MN', 'Roboto Serif 72pt UltraCondensed', 'Trebuchet MS', 'Noto Sans Lydian', 'Tamil Sangam MN', 'Noto Sans New Tai Lue', 'Star Jedi', 'Mukta Mahee', 'Kings Caslon Display', 'Roboto Serif SemiCondensed', 'Roboto Serif 36pt ExtraCondensed', 'System Font', 'Myanmar Sangam MN', 'Kokonor', 'Noble Company', 'Noto Sans Buginese', 'Source Serif Pro', 'Geeza Pro', 'cmex10', 'Verdana', 'Roboto Serif Expanded', 'Noto Sans Miao', 'Inconsolata Condensed', 'Input Mono Narrow', 'Olympicons', 'ESRI Public1', 'cmsy10', 'STIXSizeTwoSym', 'Inconsolata ExtraExpanded', 'RODE Noto Sans Hindi B', 'Beirut', 'Al Bayan', 'Noto Sans Elbasan', 'Liberation Sans', 'Chalkboard SE', 'Noto Sans Palmyrene', 'Chalkduster', 'Noto Sans Manichaean', 'Cochin', 'Webdings', 'Noto Sans Kayah Li', 'OS OpenMap Local', '.SF Compact', 'Star Jedi Hollow', 'Input Serif Condensed', 'Roboto Serif 36pt SemiCondensed', 'Herculanum', 'cmtt10', 'Noto Sans Syriac', 'Marker Felt', 'Roboto Serif 36pt SemiExpanded', 'Input Sans Compressed', 'Superclarendon', 'Sana', '.Keyboard', 'Apple Chancery', 'Roboto Serif 120pt', 'Input Serif', 'Bodoni 72', 'Roboto Serif 72pt ExtraExpanded', 'Euphemia UCAS', 'Chalkboard', 'Farisi', 'Noto Sans Old Permic', 'STIXSizeFourSym', 'Heiti TC', 'Noto Sans Rejang', 'Roboto Serif 72pt SemiCondensed', 'Bodoni 72 Oldstyle', 'Noto Sans Hanunoo', 'STIXGeneral', 'Zapfino', 'Noto Sans PhagsPa', 'Johnston ITC Std', '.SF Hebrew Rounded', 'Noto Sans Tai Viet', 'Arial Hebrew', 'Baskerville', 'Kohinoor Gujarati', 'Arial Unicode MS', 'Inconsolata UltraCondensed', 'Academy Engraved LET', 'Al Tarikh', 'Lucida Grande', 'Hoefler Text', 'Raanana', 'Noto Sans Meroitic', 'Kohinoor Devanagari', 'Hiragino Sans', 'Noto Serif Myanmar', 'Roboto Serif 28pt Condensed', 'Noto Sans Caucasian Albanian', 'Input Sans Condensed', 'Noto Sans Egyptian Hieroglyphs', 'Liberation Sans Narrow', '.Aqua Kana', 'Copperplate', 'Geneva', 'Noto Sans Linear A', 'Fira Sans Extra Condensed', 'Noto Serif Hmong Nyiakeng', 'Tamil MN', 'KingsBureauGrot', 'STIXSizeFiveSym', 'DejaVu Sans Mono', '.New York', 'DejaVu Sans Display', 'Gurmukhi Sangam MN', 'Bergamo Pro', 'Noto Sans Avestan', 'Sathu', 'Roboto Serif 72pt ExtraCondensed', 'ZapfDingbats', 'Noto Sans Syloti Nagri', 'Noto Sans Gothic', 'Computer Modern', 'Noto Sans Adlam', 'Noto Sans Glagolitic', 'ITC Zapf Chancery', 'Amethyst', 'Noto Sans Phoenician', 'STIXIntegralsUpD', 'Roboto Serif 36pt', 'Noto Sans Canadian Aboriginal', 'Apple SD Gothic Neo', 'Kefa', 'Noto Sans Marchen', 'Arial', 'Arial Black', 'Gurmukhi MT', 'Noto Sans Bhaiksuki', 'Roboto Serif 120pt SemiCondensed', 'Fira Mono', 'Mishafi Gold', 'cmb10', 'Times New Roman', 'Noto Sans NKo', 'Fira Code', 'Shree Devanagari 714', 'Palatino', 'STIXVariants', 'Input Sans Narrow', 'Noto Sans Pahawh Hmong', 'Galvji', 'Noto Sans Oriya', 'Kannada Sangam MN', 'ESRI US MUTCD 1', 'Plantagenet Cherokee', 'Roboto Serif 28pt Expanded', 'Noto Sans Buhid', 'Sinhala MN', 'Wingdings', 'Utopia', 'Bradley Hand', 'Corsiva Hebrew', 'ESRI Transportation & Civic', 'Malayalam Sangam MN', 'Gurmukhi MN', 'Microsoft Sans Serif', 'Telugu MN', 'Noto Sans Cypriot', 'Rockwell', 'Kubos', 'Noto Sans Tai Le', 'Noto Sans Mongolian', 'Roboto Serif 120pt ExtraExpanded', 'Kohinoor Telugu', 'Charter', 'Noto Sans Kannada', '.SF Armenian Rounded', 'Noto Sans Lisu', 'Noto Sans Mro', 'Noto Sans Tagbanwa', 'Times', 'Bangla Sangam MN', '.SF Arabic Rounded', 'Noto Sans Thaana', 'Roboto Serif 120pt Condensed', 'Input Mono', 'Bergamo Caps', '.SF Georgian Rounded', 'Roboto Serif 120pt SemiExpanded', 'Noto Sans Ol Chiki', 'Gasolyn Free', 'SourceCodePro+Powerline+Awesome Regular', 'Noto Sans Sundanese', 'Noto Sans Vai', 'Noto Sans Kaithi', 'Diwan Thuluth', 'Noto Sans Old Turkic', 'SignPainter', 'Roboto Serif ExtraCondensed', 'Roboto Serif 120pt ExtraCondensed', 'Roboto Serif Condensed', 'Gill Sans', 'Optima', 'Noto Sans Hanifi Rohingya', 'ITC Zapf Dingbats', 'Inconsolata SemiCondensed', 'Roboto Serif', 'Inconsolata', 'bridamount', '.SF NS Mono', 'Noto Sans Psalter Pahlavi', 'Noto Sans Khudawadi', 'Kailasa', 'Trattatello', 'Noto Serif Yezidi', 'Noto Sans Carian', 'Mshtakan', 'Marion', 'Noto Sans Duployan', 'Tahoma', 'Noto Nastaliq Urdu', 'Noto Sans Ugaritic', 'Courier', 'Noto Sans Modi', 'Noto Sans Khojki', 'Roboto Serif 36pt ExtraExpanded', 'Noto Sans Meetei Mayek', 'Sukhumvit Set', '.SF NS Rounded', 'STIXSizeOneSym', 'Noto Sans Hatran', 'cmmi10', 'Arial Rounded MT Bold', 'Wingdings 3', 'Zapf Dingbats', 'Input Mono Condensed', 'Inconsolata UltraExpanded', 'Noto Sans Imperial Aramaic', 'Papyrus', '.SF Armenian', 'Fira Sans Condensed', 'Impact', 'JetBrains Mono', 'Liberation Serif', 'Noto Sans Limbu', 'Noto Sans Masaram Gondi', 'Iowan Old Style', 'Roboto Serif 36pt UltraCondensed', 'Noto Sans Old North Arabian', 'Brush Script MT', 'Roboto Serif 120pt Expanded', 'Inconsolata SemiExpanded', 'STIXIntegralsUp', 'Noto Sans Gunjala Gondi', '.SF Hebrew', 'Roboto Serif ExtraExpanded', 'PT Sans', 'Noto Sans Pau Cin Hau', 'RODE Noto Sans CJK SC B', 'Noto Sans Old Italic', 'Gujarati Sangam MN', 'Luminari', 'Muna', 'Noto Sans Mende Kikakui', 'Khmer MN', 'Al Nile', 'DejaVu Serif', 'RODE Noto Sans Hindi R', 'Kannada MN', 'Roboto Serif 28pt ExtraCondensed', 'DejaVu Sans']\n\n\n\nfonts = ! fc-list : family style | grep \"Ubuntu\"\nprint(sorted(fonts))\n\n[]\n\n\n\n\nFontdicts\nNow that we know what’s available, the next step is to set up some useful defaults that we can re-use across multiple plots to ensure consistency of output. The format for specifying fonts on a per-figure basis is a dictionary, so where you see fontdict in the matplotlib documentation the following should work:\nHere’s the example:\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n\ntfont = {'fontname':'Liberation Sans Narrow', 'weight':'bold'}\nbfont = {'fontname':'Liberation Sans Narrow', 'weight':'normal', 'horizontalalignment':'left'}\nafont = {'fontname':'Liberation Sans Narrow', 'weight':'light'}\n\nI am setting the ‘title font’ (tfont) and ‘body copy font’ (bfont) and ‘axis font’ (afont) here to use in the output below. You can pick another font and see what happens.\n\n\n2.3: Using Fonts\nAt this point we’re going to work towards a kind of ‘atlas’ that would make it easy to compare some features for different London boroughs. I basically implemented a the basic matplotlib version of QGIS’ Atlas functionality.\n\n# This will be whatever LA you processed last week\nLA = 'Waltham Forest'\n\n\nmsoa_gdf = gpd.read_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n\n\nmedian_gdf  = msoa_gdf[['MSOA11CD','median_price','geometry']]\nlisting_gdf = msoa_gdf[['MSOA11CD','listing_count','geometry']]\n\n\nimport matplotlib.pyplot as plt\n\n\n\nThe Defaults\nHere is a demonstration of some of the ways you can adjust features in a Python matplotlib plot. I’m not suggesting either of these is a good output, but that’s not the point! The idea is to see the various ways you can tweak a plot… And notice that we’ve not yet changed any fonts. And it shows.\n\n# Set up a 1 x 2 plot (you can also leave off the nrows= and ncols=)\nf,axes = plt.subplots(nrows=1, ncols=2, figsize=(8,6))\n# ax1 will be the first plot on the left, ax2 will be on the right;\n# a 2 (or more) *row* plot will return a list of lists... 1 list/row.\nax1 = axes[0]\nax2 = axes[1]\n\n# Left plot is the median price\nmedian_gdf.plot(column='median_price', ax=ax1, legend=True, cmap='viridis')\nax1.set_title(\"Median Price per MSOA\");\n# Turn off the frame, one side of the plat at a time\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n# Set the labels\nax1.set_xlabel(\"Easting\");\nax1.set_ylabel(\"Northing\");\n\n# Right plot is the number of listings; note\n# here the use of both zorder (which is the \n# 'stacking order' of elements on the plot, and\n# the legend_kwds (keywords) to change the \n# orientation of the plot to horizontal\nlisting_gdf.plot(column='listing_count', ax=ax2, legend=True, cmap='plasma', zorder=1, \n                 legend_kwds={\"orientation\": \"horizontal\"})\nax2.set_title(\"Count of Listings per MSOA\");\n# Set a background colour for the plot\nax2.set_facecolor((.4, .4, .4, .2))\n# Add grid lines and set their zorder to\n# below that of the data on the plot\nplt.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\nax2.set_axisbelow(True)\n\n# This is equivalent to the ax1.spines... \n# above, but if you use it here you lose\n# the background to the plot as well!\n#plt.gca().set(frame_on=False)\n\n# Remove the labels on the ticks of the \n# axes (meaning: remove the numbers on \n# x- and y-axes).\nax2.set_xticklabels([])\nax2.set_yticklabels([])\n\n# Set the labels\nax2.set_xlabel(\"Easting\");\nax2.set_ylabel(\"Northing\");\n\n\n\n\n\n\n\n\n\n\nImproving on Defaults\n\nf,axes = plt.subplots(1,2,figsize=(8,6))\n\n# Set up the plots\nmedian_gdf.plot(column='median_price', ax=axes[0], legend=True, cmap='viridis')\nlisting_gdf.plot(column='listing_count', ax=axes[1], legend=True, cmap='plasma')\nfor ax in axes:\n    ax.axis('off')\n    # Note that here, set_facebolor doesn't work,\n    # presumably because the axis is 'off'\n    ax.set_facecolor((.4, .4, .4, .2))\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expading the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\naxes[0].set_title('Median Price', size=20, **afont)\naxes[1].set_title('Count', size=20, **afont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.92, linespacing=1.4, va='top', size=12, \n            s=f\"Total listings: {listing_gdf['listing_count'].sum():,.0f}\\nMedian price: ${median_gdf['median_price'].median():,.2f}\", **bfont);"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#adding-picture-in-picture",
    "href": "practicals/Practical-10-Presenting_Data.html#adding-picture-in-picture",
    "title": "Practical 10: Presenting Data",
    "section": "Adding Picture-in-Picture",
    "text": "Adding Picture-in-Picture\nWe’re now going to emulate a bit of QGIS’ Atlas function by creating two subplots and then adding a third plot afterwards that shows where the borough is.\n\nf,axes = plt.subplots(1,3,gridspec_kw={'width_ratios':[1,4,4]}, figsize=(8,6))\n\n# Plot 0 is basically being used as a 'spacer' \n# as you'll see below\naxes[0].axis('off')\n\n# Plot 1 is the median price\nmedian_gdf.plot(column='median_price', ax=axes[1], legend=True, cmap='viridis')\naxes[1].set_title('Median Price', size=20, **afont)\n\n# Plot 2 is the count of listings\nlisting_gdf.plot(column='listing_count', ax=axes[2], legend=True, cmap='plasma')\naxes[2].set_title('Count', size=20, **afont)\n\n# For plots 1 and 2... if you were doing this a lot it could be a function!\nfor ax in axes[1:]:\n    ax.set_facecolor((.9, .9, .9, .5))\n    ax.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\n    ax.set_axisbelow(True)\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.tick_params(axis='both', which='both', length=0)\n\n# Add a *third* chart that we use as a kind of 'PiP'\n# to show which borough we're talking about. The \n# add_axes call is here taking information about the\n# positioning and size of the additional figure.\n# Disable ax2.axis('off') if you want to see the\n# figure in full.\nax3 = f.add_axes([0.015, 0.7, 0.2, 0.2])\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nboros.plot(facecolor='lightgrey', edgecolor='w', linewidth=1, ax=ax3)\nboros[boros.NAME==LA].plot(facecolor='r', edgecolor='none', hatch='///', ax=ax3)\nax3.axis('off')\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expanding the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.65, s=f\"Total listings: {listing_gdf['listing_count'].sum():,.0f}\", size=12, **bfont);\n\n+ data/geo/Boroughs.gpkg found locally!"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#bonus-achievement-unlocked",
    "href": "practicals/Practical-10-Presenting_Data.html#bonus-achievement-unlocked",
    "title": "Practical 10: Presenting Data",
    "section": "Bonus Achievement Unlocked!",
    "text": "Bonus Achievement Unlocked!\nIf you have the time and inclination, see if you can convert the above to an actual atlas output:\n\nYou’ll want to turn this plot into a function so as to be able to produce (and save) the map for every borough.\nYou’ll even need to parameterise the filename so that you save to different PNG files as well as going back to see how we generated the listing and pricing data frames for the Local Authority…\nAnd you’ll also need to make sure that you ensure a consistent colour bar (for all of London, because the median price and number of listings will vary rather a lot by LA)\nThen there’s the placement of the PiP for some boroughs with long names\nAnd finally, you might consider adding some more text to atlas–maybe pull some content from Wikipedia using Beautiful Soup (bs4)?"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#for-a-chart",
    "href": "practicals/Practical-10-Presenting_Data.html#for-a-chart",
    "title": "Practical 10: Presenting Data",
    "section": "For a Chart",
    "text": "For a Chart\nGroup the listings by Borough and Room Type, and aggregate by median price, also producing a count variable for the number of listings of each type in each Borough.\n\nla_tots = gdf_la[gdf_la.NAME==LA].groupby(by='room_type', observed=False).agg(\n                            {'price':'median', 'listing_url':'count'}\n                        ).reset_index().rename(columns={'listing_url':'count'})\nla_tots\n\n\n\n\n\n\n\n\nroom_type\nprice\ncount\n\n\n\n\n0\nEntire home/apt\n117.0\n946\n\n\n1\nHotel room\nNaN\n0\n\n\n2\nPrivate room\n47.0\n687\n\n\n3\nShared room\n24.5\n6\n\n\n\n\n\n\n\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.palettes import Spectral4\nfrom bokeh.models import CustomJS, Dropdown\n\noutput_notebook()\n\nroom_types = la_tots.room_type.to_list()\nprices     = la_tots.price.to_list()\ncounts     = la_tots['count'].to_list()\n\n# Add hover tooltip\nsource = ColumnDataSource(data=dict(\n    rt=room_types,\n    count=counts,\n    price=prices,\n))\n\nTOOLTIPS = [\n    (\"Room Type\", \"@rt\"),\n    (\"Number of Listings\", \"@count{,}\"),\n    (\"Median Price\", \"$@price{,}/night\")\n]\n\np = figure(x_range=room_types, height=300, tooltips=TOOLTIPS,\n           title=f\"Median Price by Room Type in {LA}\",\n           toolbar_location=None, tools=\"\")\n\np.vbar(x='rt', top='count', width=0.9, source=source)\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nshow(p)\n\n    \n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#for-a-map",
    "href": "practicals/Practical-10-Presenting_Data.html#for-a-map",
    "title": "Practical 10: Presenting Data",
    "section": "For a Map",
    "text": "For a Map\nThis is not the prettiest code, but it should work…\n\nfrom bokeh.plotting import figure\n\nfrom bokeh.io import output_file, show, output_notebook, push_notebook, export_png\nfrom bokeh.models import ColumnDataSource, GeoJSONDataSource, LinearColorMapper, ColorBar, HoverTool\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import brewer\n\n#output_notebook()\n\n\nmsoadf = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            msoas[msoas.Borough==LA].drop(columns=['index_right']), predicate='within')\n\n\nmsoagrdf = msoadf.groupby('MSOA11NM').agg({'price':['median','count']}).reset_index()\nmsoagrdf.columns=['msoa11nm','median','count']\n\nI cobbled the mapping functions below together from two tutorials I found online (this one and this one). As you can see, this is a very different approach to mapping data, but it has clear benefits for exploratory purposes and produces fast, interactive maps… and I’ve not even added selection and filtering tools!\n\nimport json\n\ndef get_geodatasource(gdf):    \n    \"\"\"Get getjsondatasource from geopandas object\"\"\"\n    json_data = json.dumps(json.loads(gdf.to_json()))\n    return GeoJSONDataSource(geojson = json_data)\n\ndef bokeh_plot_map(gdf, column=None, title=''):\n    \"\"\"Plot bokeh map from GeoJSONDataSource \"\"\"\n\n    geosource = get_geodatasource(gdf)\n    palette = brewer['OrRd'][8]\n    palette = palette[::-1]\n    vals = gdf[column]\n    \n    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n    color_mapper = LinearColorMapper(palette=palette, low=vals.min(), high=vals.max())\n    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8, width=500, height=10,\n                         location=(0,0), orientation='horizontal')\n\n    tools = 'wheel_zoom,pan,reset,hover'\n    \n    p = figure(title = title, height=700, width=850, toolbar_location='right', tools=tools)\n    p.add_tile(\"CartoDB Positron\", retina=True)\n    p.xgrid.grid_line_color = None\n    p.ygrid.grid_line_color = None\n    \n    # Add patch renderer to figure\n    p.patches('xs','ys', source=geosource, fill_alpha=0.5, line_width=0.5, line_color='white',  \n              fill_color={'field' :column , 'transform': color_mapper})\n    \n    # Specify figure layout.\n    p.add_layout(color_bar, 'below')\n    \n    # Add hover\n    hover = p.select_one(HoverTool)\n    hover.point_policy = \"follow_mouse\"\n    hover.tooltips = [(\"Borough\", \"@Borough\"),\n                      (\"Neighbourhood\", \"@msoa11hclnm\"),\n                      (\"Count of Listings\", \"@count\"),\n                      (\"Median Price\", \"$@median\")]\n    \n    return p\n\nReproject to Web Mercator:\n\nmsoa_gdf = pd.merge(msoagrdf, msoas, left_on='msoa11nm', right_on='MSOA11NM', how='inner')\nmsoa_gdf = msoa_gdf.set_geometry('geometry').set_crs('epsg:27700')\n\n\nmsoageo = msoa_gdf.to_crs('epsg:3785')\nmsoageo.total_bounds\n\narray([-6.74542047e+03,  6.71906611e+06,  3.04361304e+03,  6.73637453e+06])\n\n\nAnd map it!\n\np = bokeh_plot_map(msoageo, 'median', title=f'MSOA-Level Activity in {LA}')\n\nhandle = show(p, notebook_handle=True)\npush_notebook(handle=handle)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nAnd that’s it. That’s all she wrote! You’ve now covered in &lt;10 weeks what many people might take 10 months to cover. So do not feel like either: 1) you know it all; or 2) you know nothing. You have learned a lot, but it’s probably just enough to see how much you don’t know. And that is the start of wisdom. Good luck, young Python-master!"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html",
    "href": "practicals/Practical-03-Foundations_2.html",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "",
    "text": "In this notebook we are exploring basic (in the sense of fundamental) data structures so that you understand both how to manage more complex types of data and are prepared for what we will encounter when we start using pandas to perform data analysis. To achieve that, you will need to be ‘fluent’ in “nested” lists and dictionaries; we will focus primarily on lists-of-lists and dictionaries-of-lists, but note that file formats like JSON can be understood as dictionaries-of-dictionaries-of-lists-of-… so this is just a taster of real-world data structures."
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#list-refresher",
    "href": "practicals/Practical-03-Foundations_2.html#list-refresher",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "List Refresher",
    "text": "List Refresher\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nTo complete these tasks, all of the methods that you need are listed above, so this is about testing yourself on your understanding both of how to read the help and how to index elements in a list.\nThe next line creates a list of city names (each element is a string):\n\ncities = [\"Bristol\", \"London\", \"Manchester\", \"Edinburgh\", \"Belfast\", \"York\"]\n\n\nList Arithmetic\nReplace the ?? so that it prints Belfast.\n\nQuestion\n\nprint(cities[?? + 2])\n\n\n\n\nNegative List Arithmetic\nUse a negative index to print Belfast:\n\nQuestion\n\nprint(cities[??])\n\n\n\n\nFinding a Position in a List\nReplace the ?? so that it prints the index for Manchester in the list.\n\nQuestion\n\nprint(\"The position of Manchester in the list is: \" + str( ?? ))"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#looking-across-lists",
    "href": "practicals/Practical-03-Foundations_2.html#looking-across-lists",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Looking Across Lists",
    "text": "Looking Across Lists\n\n\n\n\n\n\nConnections\n\n\n\nThis section draws on the LOLs lecture and you will also find Code Camp’s Loops session useful here.\n\n\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNotice that the list of temperatures below is the same length as the list of cities, that’s because these are (roughly) the average temperatures for each city.\n\ncities = [\"Bristol\", \"London\", \"Manchester\", \"Edinburgh\", \"Belfast\", \"York\"]\ntemperatures = [15.6, 16.5, 13.4, 14.0, 15.2, 14.8]\n\n\nLateral Thinking\nGiven what you know about cities and temperatures, how do you print:\n\n“The average temperature in Manchester is 13.4 degrees”\n\nBut you have to do this without doing any of the following:\n\nUsing a list index directly (i.e. cities[2] and temperatures[2]) or\nHard-coding the name of the city?\n\nTo put it another way, neither of these solutions is the answer:\nprint(\"The average temperature in Manchester is \" + str(temperatures[2]) + \" degrees.\")\n# ...OR...\ncity=2\nprint(\"The average temperature in \" + cities[city] + \" is \" + str(temperatures[city]) + \" degrees.\")\n\n\n\n\n\n\nTip\n\n\n\nYou will need to combine some of the ideas above and also think about the fact that the list index is that we need is the same in both lists… Also, remember that you’ll need to wrap a str(...) around your temperature to make it into a string.\n\n\n\nQuestion\n\ncity=\"Manchester\" # Use this to get the solution...\n\n# This way is perfectly fine\nprint(\"The average temperature in \" + ?? + \" is \" + str(??))\n\n# This way is more Python 3 and a bit easier to read\nprint(f\"The average temperature in {??} is {??}\")\n\n\n\n\nDouble-Checking Your Solution\nYou’ll know that you got the ‘right’ answer to the question above if you can copy+paste your code and change only one thing in order to print out: “The average temperature in Belfast is 15.2 degrees”\n\nQuestion\n\ncity=\"Belfast\"\nprint(??)\n\n\n\n\nLoops\nNow use a for loop over the cities to print out the average temperature in each city:\n\nQuestion\n\nfor c in cities:\n    ??\n\nThe output should be:\n\nThe average temperature in Bristol is 15.6\nThe average temperature in London is 16.5\nThe average temperature in Manchester is 13.4\nThe average temperature in Edinburgh is 14.0\nThe average temperature in Belfast is 15.2\nThe average temperature in York is 14.8"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#creating-an-atlas",
    "href": "practicals/Practical-03-Foundations_2.html#creating-an-atlas",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Creating an Atlas",
    "text": "Creating an Atlas\nThe code below creates an Atlas using a dictionary. The dictionary key is a city name, and the value is the latitude, longitude, and main airport code.\n\ncities = {\n    'San Francisco': [37.77, -122.43, 'SFO'],\n    'London': [51.51, -0.08, 'LDN'],\n    'Paris': [48.86,2.29, 'PAR'],\n    'Beijing': [39.92,116.40 ,'BEI'],\n}\n\n\nAdding to a Dict\nAdd a record to the dictionary for Chennai (data here)\n\nQuestion\n\n??\n\n\n\n\nAccessing a Dict\nIn one line of code, print out the airport code for Chennai (MAA):\n\nQuestion\n\n??"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#dealing-with-errors",
    "href": "practicals/Practical-03-Foundations_2.html#dealing-with-errors",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Dealing With Errors",
    "text": "Dealing With Errors\nCheck you understand the difference between the following two blocks of code by running them.\n\ntry:\n    print(cities['Berlin'])\nexcept KeyError as e:\n    print(\"Error found\")\n    print(e)\n\nError found\n'Berlin'\n\n\n\ntry:\n    print(cities.get('Berlin','Not Found'))\nexcept KeyError as e:\n    print(\"Error found\")\n    print(e)\n\nNot Found\n\n\nNotice that trying to access a non-existent element of a dict triggers a KeyError, while asking the dict to get the same element does not, it simply returns None. Can you think why, depending on the situtation, either of these might be the ‘correct’ answer?"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#thinking-data",
    "href": "practicals/Practical-03-Foundations_2.html#thinking-data",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Thinking Data",
    "text": "Thinking Data\nThis section makes use of both the Dictionaries lecture and the DOLs to Data lecture.\n\n\n\n\n\n\nTip\n\n\n\nIn this section you’ll need to look up (i.e. Google) and make use of a few new functions that apply to dictionaries: &lt;dictionary&gt;.items(), &lt;dictionary&gt;.keys(). Remember: if in doubt, add print(...) statements to see what is going on!\n\n\n\nIterating over a Dict\nAdapting the code below, print out the city name and airport code for every city in our Atlas.\n\nQuestion\n\nfor c in cities.keys():\n    print(??)\n\nThe output should look something like this:\n\nSan Francisco -&gt; SFO\nLondon -&gt; LDN\nParis -&gt; PAR\nBeijing -&gt; BEI\nChennai -&gt; MAA\n\n\n\n\nMore Complex Dicts\nHow would your code need to change to produce the same output from this data structure:\n\ncities = {\n    'San Francisco': {\n        'lat': 37.77, \n        'lon': -122.43,\n        'airport': 'SFO'},\n    'London': {\n        'lat': 51.51, \n        'lon': -0.08, \n        'airport': 'LDN'},\n    'Paris': {\n        'lat': 48.86,\n        'lon': 2.29, \n        'airport': 'PAR'},\n    'Beijing': {\n        'lat': 39.92,\n        'lon': 116.40,\n        'airport': 'BEI'},\n    'Chennai': { \n        'lat': 13.08, \n        'lon': 80.28, \n        'airport': 'MAA'}\n}\n\n\nQuestion\n\nfor c in cities.keys():\n    print(??)\n\n\n\n\nMore Dictionary Action!\nAnd how would it need to change to print out the name and latitude of every city?\n\nQuestion\n\nfor c in cities.keys():\n    print(??)\n\nThe output should be something like this:\n\nSan Francisco is at latitude 37.77\nLondon is at latitude 51.51\nParis is at latitude 48.86\nBeijing is at latitude 39.92\nChennai is at latitude 13.08\n\n\n\n\nAnd Another Way to Use a Dict\nNow produce the same output using this new data structure:\n\ncities_alt = [\n    {'name':     'San Francisco',\n     'position': [37.77, -122.43],\n     'airport':  'SFO'},\n    {'name':     'London',\n     'position': [51.51, -0.08],\n     'airport':  'LDN'},\n    {'name':     'Paris',\n     'position': [48.86, 2.29],\n     'airport':  'PAR'},\n    {'name':     'Beijing',\n     'position': [39.92, 116.40],\n     'airport':  'BEI'},\n    {'name':     'Chennai', \n     'position': [13.08, 80.28],\n      'airport': 'MAA'}\n]\n\n\nQuestionAnswer\n\n\nfor c in cities_alt:\n    print(??)\nThe output should be something like this:\nSan Francisco is at latitude 37.77\nLondon is at latitude 51.51\nParis is at latitude 48.86\nBeijing is at latitude 39.92\nChennai is at latitude 13.08\n\n\n\nfor c in cities_alt:\n    print(f\"{c['name']} is at latitude {c['position'][0]}\")\n\nSan Francisco is at latitude 37.77\nLondon is at latitude 51.51\nParis is at latitude 48.86\nBeijing is at latitude 39.92\nChennai is at latitude 13.08\n\n\n\n\n\n\n\nThink Data!\nWhat are some of the main differences that you can think of between cities and cities_alt as data? There is no right answer.\n\n\n\n\n\n\nTip\n\n\n\nI just want you to think about these as data! If you were trying to use cities and cities_alt as data what differences would you find when accessing one or more ‘records’?\n\n\n\nPoint 1 here.\nPoint 2 here.\nPoint 3 here."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html",
    "href": "practicals/Practical-04-Reuse.html",
    "title": "Practical 4: Efficient Code",
    "section": "",
    "text": "In this notebook we are going to look in more detail at how we can reduce, reuse, and recycle our code to make our lives easier and our code more efficient."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#reading-a-remote-file",
    "href": "practicals/Practical-04-Reuse.html#reading-a-remote-file",
    "title": "Practical 4: Efficient Code",
    "section": "Reading a Remote File",
    "text": "Reading a Remote File\nSo, we are going to download a file from GitHub, but we aren’t going to to try to turn it into data or otherwise make ‘sense’ of it yet, we just want to read it. We are then going to build from this first step towards the rest of the steps!\nBecause we’re accessing data from a ‘URL’ we need to use the urlopen function from the urllib.request package. If you’re wondering how we know to use this function and package, you might google something like: read remote csv file python 3 which in turn might get you to a StackOverflow question and answer like this.\n\n\n\nfrom urllib.request import urlopen\nhelp(urlopen)\n\nHelp on function urlopen in module urllib.request:\n\nurlopen(url, data=None, timeout=&lt;object object at 0x102830b70&gt;, *, cafile=None, capath=None, cadefault=False, context=None)\n    Open the URL url, which can be either a string or a Request object.\n\n    *data* must be an object specifying additional data to be sent to\n    the server, or None if no such data is needed.  See Request for\n    details.\n\n    urllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\n    header in its HTTP requests.\n\n    The optional *timeout* parameter specifies a timeout in seconds for\n    blocking operations like the connection attempt (if not specified, the\n    global default timeout setting will be used). This only works for HTTP,\n    HTTPS and FTP connections.\n\n    If *context* is specified, it must be a ssl.SSLContext instance describing\n    the various SSL options. See HTTPSConnection for more details.\n\n    The optional *cafile* and *capath* parameters specify a set of trusted CA\n    certificates for HTTPS requests. cafile should point to a single file\n    containing a bundle of CA certificates, whereas capath should point to a\n    directory of hashed certificate files. More information can be found in\n    ssl.SSLContext.load_verify_locations().\n\n    The *cadefault* parameter is ignored.\n\n\n    This function always returns an object which can work as a\n    context manager and has the properties url, headers, and status.\n    See urllib.response.addinfourl for more detail on these properties.\n\n    For HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\n    object slightly modified. In addition to the three new methods above, the\n    msg attribute contains the same information as the reason attribute ---\n    the reason phrase returned by the server --- instead of the response\n    headers as it is specified in the documentation for HTTPResponse.\n\n    For FTP, file, and data URLs and requests explicitly handled by legacy\n    URLopener and FancyURLopener classes, this function returns a\n    urllib.response.addinfourl object.\n\n    Note that None may be returned if no handler handles the request (though\n    the default installed global OpenerDirector uses UnknownHandler to ensure\n    this never happens).\n\n    In addition, if proxy settings are detected (for example, when a *_proxy\n    environment variable like http_proxy is set), ProxyHandler is default\n    installed and makes sure the requests are handled through the proxy.\n\n\n\n\nAs you can see, there is lot of information here about how things work. A lot of it won’t make much sense at the moment. That’s ok. Some of this doesn’t make much sense to me, but that’s because this is the full documentation from Python so it’s trying to cover all the bases. You don’t need to read every line of this, what you are looking is information about things like the ‘signature’ (what parameters the function accepts) and its output. Of course, you can also just Google it!\n\n\n\n\n\n\nTip\n\n\n\nRemember that you can use dir(...) and help(...) to investigate what a package offers. You can also get help in Jupyter by typing ? before the function that you want to call.\n\n\n\n\n\n\n\n\nURLError?\n\n\n\n\n\nIf you are working behind a firewall (esp. if working on this practical in, say, China) then there is a chance you will get a URLError (&lt;urlopen error [Errno 110044] getaddrinfo failed&gt;). This is a ‘proxy error’ and in this case you may need to configure your environment as follows:\nimport os\nos.environ['HTTP_PROXY'] = 'http://127.0.0.1:10809'\nos.environ['HTTPS_PROXY'] = 'http://127.0.0.1:10809'\n\n\n\nBefore you start working on the code, why not open the data file directly in your browser? It’s pretty small, and it will give you a sense of what is going on.\nfrom urllib.request import URLError\nfrom urllib.request import urlopen\n\n# Given the info you were given above, what do you \n# think the value of 'url' should be? What\n# type of variable is it? int or string? \nurl = 'https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv'\n\n# Read the URL stream into variable called 'response'\n# using the function that we imported above\ntry:\n    response = urlopen(url)\nexcept URLError as e:\n    print(\"Unable to connect to URL!\")\n    print(e)\n\n# You might want to explore what `__class__` and `__name__`\n# are doing, but basically the give us a way of finding out what\n# is 'behind' more complex variables\n\n# Now read from the stream, decoding so that we get actual text\nraw = response.read()\n\nprint(f\"'raw' variable is of type: '{raw.__class__.__name__}'.\")\nprint(f\"Raw content is:\\n{raw[:75]}...\\n\")\n\ndata = raw.decode('utf-8')\n\nprint(f\"'data' variable is of type: '{data.__class__.__name__}'.\")\nprint(f\"Decoded content is:\\n{data[:75]}...\")\n\n\n\n\n\n\nNote\n\n\n\nNotice that the raw data has the format b'...' with all of the data seemingly on one line, while the decoded version in data is ‘correctly’ structured with lines! The ‘raw’ data is in bytecode format which is not, strictly, a string. It only becomes a string when we ‘decode it’ to utf-8 (which is the ‘encoding’ of text that supports most human languages). While the computer doesn’t particularly care, we do!\n\n\nRemember that you can treat strings as lists, so when we print below we cut off the output using the list[:&lt;Some Number&gt;] syntax.\n\nprint(f\"There are {len(data)} characters in the data variable.\")\nprint(f\"The first 125 characters are: '{data[:125]}'\") # Notice that '\\n' count here!\n\nThere are 352 characters in the data variable.\nThe first 125 characters are: 'City,Population,Latitude,Longitude\nPerth,45770,56.39583,-3.43333\nArmagh,14777,54.3499,-6.6546\nDundee,147268,56.462,-2.9707'\n\n\nSo this is definitely text, but it doesn’t (yet) look entirely like the data we see because it’s still just one long string, and not data which has individual records on each line. To split the text into individual lines, we can use the handily named .splitlines() method (more on methods below):\n\nQuestion\n\nrows = ??.splitlines()\nprint(f\"'rows' variable is of type: {rows.__class__.__name__}'.\")\n\nNote now, how the data variable has type list. So to view the data as we see them in the original online file, we can now use a for loop to print out each element of the list (each element being a row of the original online file):\n\n\nQuestion\n\nprint(f\"There are {??} rows of data.\")\nprint(\"\\n\".join(??[0:2])) # New syntax alert! notice we can *join* list elements\n\nThat’s a little hard to read, though something has clearly changed. Let’s try printing the last row:\n\nprint(rows[-1])\n\nBangor,18808,53.228,-4.128\n\n\nCongratulations! You’ve now read a text file sitting on a server in, I think, Canada and Python didn’t care. You’ve also converted a plain-text file to a row-formatted list."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#text-into-data",
    "href": "practicals/Practical-04-Reuse.html#text-into-data",
    "title": "Practical 4: Efficient Code",
    "section": "Text into Data",
    "text": "Text into Data\nWe now need to work on turning the response into useful data. We got partway there by splitting on line-breaks (splitlines()), but now we need to get columns for each line. You’ll notice that we are dealing with a CSV (Comma-Separated Value) file and that the format looks quite simple… So, in theory, to turn this into data we ‘just’ need to split each row into separate fields using the commas.\nThere’s a handy function associated with strings called split:\n\nprint('abcdefgh'.split('d'))\n\n['abc', 'efgh']\n\n\nYou can also investigate further how the split function works using:\n\nhelp('abcdefgh'.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the substrings in the string, using sep as the separator string.\n\n      sep\n        The separator used to split the string.\n\n        When set to None (the default value), will split on any whitespace\n        character (including \\n \\r \\t \\f and spaces) and will discard\n        empty strings from the result.\n      maxsplit\n        Maximum number of splits.\n        -1 (the default value) means no limit.\n\n    Splitting starts at the front of the string and works to the end.\n\n    Note, str.split() is mainly useful for data that has been intentionally\n    delimited.  With natural text that includes punctuation, consider using\n    the regular expression module.\n\n\n\nSo this seems like a good solution to turn our text into data:\n\ntest = rows[-1].split(',')\nprint(test)\nprint(f\"The population of {test[0]} is {int(test[1]):,}\")\n\n['Bangor', '18808', '53.228', '-4.128']\nThe population of Bangor is 18,808\n\n\nI’d say that we’re now getting quite close to something that looks like ‘real data’: I know how to convert a raw response from a web server into a string, to split that string into rows, and can even access individual elements from a row!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#the-advantages-of-a-package",
    "href": "practicals/Practical-04-Reuse.html#the-advantages-of-a-package",
    "title": "Practical 4: Efficient Code",
    "section": "The Advantages of a Package",
    "text": "The Advantages of a Package\n\n\n\n\n\n\nCaution\n\n\n\nThere are two problems to the data.splitlines() and row.split(',') approach! One of them is visible (though not obvious) in the examples above, the other is not.\n\n\n\nRemember that 10 and '10' are not the same thing. To comma-format the population of Sheffield you’ll see that I had to do int(...) in order to turn '685368' into a number. So our approach so far doesn’t know anything about the type of data we’re working with.\nWe are also implicitly assuming that commas can only appear at field boundaries (i.e. that they can only appear to separate one column of data from the next). In other words, just using split(',') doesn’t work if any of the fields can themselves contain a comma!\nThere’s actually a third potential issue, but it’s so rare that we would need to take a completely different approach to deal with it: we are also assuming that newlines (\\n) can only appear at record boundaries (i.e. that the can only appear to separate one row of data from the next). In those cases, using splitlines() also doesn’t work, but this situation is (thankfully) very rare indeed.\n\nThis is where using code that someone else who is much more interested (and knowledgeable) has written and contributed is helpful: we don’t need to think through how to deal with this sort of thing ourselves, we can just find a library that does what we need and make use of its functionality. I’ve given you the skeleton of the answer below, but you’ll need to do a little Googling to find out how to \"read csv python\".\nNote: For now just focus on problem #2.\nfrom urllib.request import urlopen\nimport csv\n\nurl = 'https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv'\nresponse = urlopen(url)\nraw = response.read()\n\n# Now take the raw data, decode it, and then\n# pass it over to the CSV reader function\ncsvfile  = csv.reader(raw.decode('utf-8').splitlines()) \n\nurlData = [] # Somewhere to store the data\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\nIf it worked, then you should have this output:\n\n\nurlData has 11 rows and 4 columns.\n['Bangor', '18808', '53.228', '-4.128']\n\n\nTo you that might look a lot worse that the data that you originally had, but to a computer that list-of-lists is something it can work with; check it out:\n\nfor u in urlData[1:6]: # For each row in the first 5 items in list\n    print(f\"The city of '{u[0]}' has a population of {int(u[1]):,}\") # Print out the name and pop\n\nThe city of 'Perth' has a population of 45,770\nThe city of 'Armagh' has a population of 14,777\nThe city of 'Dundee' has a population of 147,268\nThe city of 'Colchester' has a population of 194,706\nThe city of 'Salisbury' has a population of 40,302\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy did I use urlData[1:] instead of urlData?\nIf you print urlData[0] you’ll see that this is the ‘header’ row that tells us what each column contains! So if we try to convert the column name to an integer (int(u[1])) we will get an error!\nThe advantage of using the csv library over plain old string.split is that the csv library knows how to deal with fields that contain commas (e.g. \"Cardfiff, Caerdydd\" or \"An Amazing 4 Bedroom Home, Central London, Sleeps 12\") and so is much more flexible and consistent that our naive split approach. The vast majority of common tasks (reading certain types of files, getting remote files, etc.) have libraries that do exactly what you want without you needing to write much code yourself to take advantage of it. You should always have a look around online to see if a library exists before thinking that you need to write everything/anything from scratch. The tricky part is knowing what words to use for your search and how to read the answers that you find…\n\n\nLet’s try this with a ‘bigger’ data set… In an ideal world, the ‘power’ of code is that once we’ve solved the problem once, we’ve solved it more generally as well. So let’s try with the ‘scaled-up’ data set and see waht happens!\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities.csv\"\nresponse = urlopen(url)\nraw = response.read()\n\ncsvfile = csv.reader(raw.decode('utf-8').splitlines())\n\nurlData = [] # Somewhere to store the data\n\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(f\"urlData has {len(urlData)} rows and {len(urlData[0])} columns.\")\n\nfor u in urlData[70:]:  # For each row in the list\n    print(f\"The city of '{u[0]}' has a population of {u[1]}\") # Print out the name and pop\n\n\n\n\n\n\nWhat mistake have I made here?\n\n\n\n\n\nI have assumed that, just because the files have similar names, they must also have similar layouts!\n\nprint(f\"The URL's data labels are: {', '.join(urlData[0])}\")\n\nThe URL's data labels are: City, Population, Latitude, Longitude"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#insight",
    "href": "practicals/Practical-04-Reuse.html#insight",
    "title": "Practical 4: Efficient Code",
    "section": "Insight!",
    "text": "Insight!\nSo, although the code was basically the same for both of these files (good), we would need to change quite a bit in order to print out the same information from different versions of the same data. So our code is rather brittle.\nOne of the issues is that our instincts about how to manage data doesn’t align with how the computer can most efficiently manage it. We make the mistake of thinking that the computer needs to do things that same way that we do when reading text and so assume that we need to:\n\nRepresent the rows as a list.\nRepresent the columns as a list for each row.\n\nThis thinking suggests that the ‘right’ data structure would clearly be a list-of-lists (LoLs!), but if you understand what happened here then the next section will make a lot more sense!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#understanding-whats-an-appropriate-data-structure",
    "href": "practicals/Practical-04-Reuse.html#understanding-whats-an-appropriate-data-structure",
    "title": "Practical 4: Efficient Code",
    "section": "Understanding What’s an ‘Appropriate’ Data Structure",
    "text": "Understanding What’s an ‘Appropriate’ Data Structure\nIf you stop to think about it, then our list-of-lists approach to the data isn’t very easy to navigate. Notice that if the position or name of a column changes then we need to change our program every time we re-run it! It’s not very easy to read either since we don’t really know what u[5] is supposed to be. That way lies all kinds of potential errors!\nAlso consider that, in order to calculate out even a simple aggregate such as the sum of a field for all rows we need to step through a lot of irrelevant data as well: we have to write a for loop and then step through each row with an ‘accumulator’ (somewhere to store the total). That’s slow.\nThat doesn’t make much sense since this should all be easier and faster in Python than in Excel, but right now it’s harder, and quite possibly slower as well! So how does the experienced programmer get around this? ‘Simple’ (i.e. neither simple, nor obvious, until you know the answer): she realises that the data is organised the wrong way! We humans tend to think in rows of data: this apartment has the following attributes (price, location, etc.), or that city has the following attributes (population, location). We read across the row because that’s the easiest way for us to think about it. But, in short, a list-of-lists does not seem to be the right way to store this data!\nCrucially, a computer doesn’t have to work that way. For a computer, it’s as easy to read down a column as it is to read across a row. In fact, it’s easier, because each column has the same type of data: one column contains names (strings), another column contains prices (integers), and other columns contain other types of data (floats, etc.). Better still, the order of the columns often doesn’t matter as long as we know what the columns are called: it’s easier to ask for the ‘description column’ than it is to ask for the 6th column since, for all we know, the description column might be in a different place for different files but they are all (relatively) likely to use the ‘description’ label for the column itself.\n\nA Dictionary of Lists to the Rescue\nSo, if we don’t care about column order, only row order, then a dictionary of lists would be a nice way to handle things. And why should we care about column order? With our CSV files above we already saw what a pain it was to fix things when the layout of the columns changed from one data set to the next. If, instead, we can just reference the ‘description’ column then it doesn’t matter where that column actually is. Why is that?\nWell, here are the first four rows of data from a list-of-lists for city sizes:\nmyData = [\n  ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n  ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n  ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n  ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\nNow, here’s how it would look as a dictionary of lists organised by column, and not by row:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['London', 'Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nprint(myData['Name'])\nprint(myData['Population'])\n\n['London', 'Manchester', 'Birmingham', 'Edinburgh', 'Inverness', 'Lerwick']\n[9787426, 2705000, 1141816, 901455, 70000, 6958]\n\n\nWhat does this do better? Well, for starters, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already, but the real benefit is coming up…\n\n\nBehold the Power of the DoL\nNow let’s look at what you can do with this… but first we need to import one more package that you’re going to see a lot over the rest of term: numpy (Numerical Python), which is used so much that most people simply refer to it as np. This is a huge package in terms of features, but right now we’re interested only in the basic arithmatic functions: mean, max, and min.\n\n\n\n\n\n\nWe’ll step through most of these in detail below.\n\n\n\n\n\n\n\n\n\nFind the latitude of Manchester:\n\ncity = \"Manchester\"\nlat = myData['Latitude'][ myData['Name'].index(city) ]\nprint(f\"{city}'s latitude is {lat}\")\n\nManchester's latitude is 53.479\n\n\nPrint the location of Lerwick:\n\nQuestion\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData[??][ ?? ])}ºW, {myData['Latitude'][ ?? ]}ºN\")\n\nFind the easternmost city:\n\n\nQuestion\n\ncity = myData['Name'][ myData[??].index( max(myData[??]) ) ]\nprint(f\"The easternmost city is: {city}\")\n\nFind the mean population of the cities using a handy package called numpy:\n\n\nQuestion\n\nimport numpy as np\nmean = np.??(myData['Population'])\nprint(f\"The mean population is: {mean}\")\n\n\n\n\n\n\n\nWarning\n\n\n\nStop! Look closely at what is going on. There’s a lot of content to process in the code above, so do not rush blindly on if this is confusing. Try pulling it apart into pieces and then reassemble it. Start with the bits that you understand and then add complexity.\n\n\nWe’ll go through each one in turn, but they nearly all work in the same way and the really key thing is that you’ll notice that we no longer have any loops (which are slow) just index or np.&lt;function&gt; (which is very fast).\n\n\n\nThe Population of Manchester\nThe code can look pretty daunting, so let’s break it down into two parts. What would you get if you ran just this code?\n\nmyData['Population'][1]\n\n2705000\n\n\nRemember that this is a dictionary-of-lists (DoL). So, Python first looks for a key named Population in the myData dictionary. It finds out that the value associated with this key is a list and in this example, it just pulls out the second value (index 1). Does that part make sense?\nNow, to the second part:\n\nmyData['Name'].index('Manchester')\n\n1\n\n\nHere we look in the dictionary for the key Name and find that that’s also a list. All we’re doing here is asking Python to find the index of ‘Manchester’ for us in that list. And myData['Name'].index('Manchester') gives us back a 1, so instead of just writing myData['Population'][1] we can replace the 1 with myData['Name'].index('Manchester')! Crucially, notice the complete absence of a for loop?\nDoes that make sense? If it does then you should be having a kind of an 🤯 moment because what we’ve done by taking a column view, rather than a row view, is to make Python’s index() command do the work for us. Instead of having to look through each row for a field that matches ‘Name’ and then check to see if it’s ‘Manchester’, we’ve pointed Python at the right column immediately and asked it to find the match (which it can do very quickly). Once we have a match then we also have the row number to go and do the lookup in the ‘Population’ column because the index is the row number!\n\n\nThe Easternmost City\nWhere this approach really comes into its own is on problems that involve maths. To figure out the easternmost city in this list we need to find the maximum Longitude and then use that value to look up the city name. So let’s do the same process of pulling this apart into two steps. Let start with the easier bit:\n\nmyData['Name'][0]\n\n'London'\n\n\nThat would give us the name of a city, but we don’t just want the first city in the list, we want the one with the maximum longitude. To achieve that we need to somehow replace the 0 with the index of the maximum longitude. Let’s break this down further:\n\nWe first need to find the maximum longitude.\nWe then need to find the index of that maximum longitude.\n\nSo Step 1 would be:\n\nmax_lon = max(myData['Longitude'])\n\nBecause the max(...) helps us to find the maximum longitude in the Longitude list. Now that we have that we can proceed to Step 2:\n\nmyData['Longitude'].index(max_lon)\n\n0\n\n\nSo now we ask Python to find the position of max_lon in the list. But rather than doing this in two steps we can combine into one if we write it down to make it easier to read:\n\nmyData['Longitude'].index(\n    max(myData['Longitude'])\n)\n\n0\n\n\nThere’s the same .index which tells us that Python is going to look for something in the list associated with the Longitude key. All we’ve done is change what’s inside that index function to max(myData['Longitude']). This is telling Python to find the maximum value in the myData['Longitude'] list. So to explain this in three steps, what we’re doing is:\n\nFinding the maximum value in the Longitude column (we know there must be one, but we don’t know what it is!),\nFinding the index (position) of that maximum value in the Longitude column (now that we know what the value is!),\nUsing that index to read a value out of the Name column.\n\nI am a geek, but that’s pretty cool, right? In one line of code we managed to quickly find out where the data we needed was even though it involved three discrete steps. Think about how much work you’d have to do if you were still thinking in rows, not columns!\n\n\nThe Location of Lerwick\nLerwick is a small town in the Shetlands, way up to the North of mainland U.K. and somewhere I’ve wanted to go ever since I got back from Orkney–but then I spent my honeymoon in the far North of Iceland, so perhaps I just don’t like being around lots of people… 🙃\nAnyway, this one might be a tiny bit easier conceptually than the other problems, except that I’ve deliberately used a slightly different way of showing the output that might be confusing:\nPrint the location of Lerwick:\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData['Longitude'][myData['Name'].index(city)])}ºW, {myData['Latitude'][myData['Name'].index(city)]}ºN\")\n\nThe town of Lerwick can be found at 1.145ºW, 60.155ºN\n\n\nThe first thing to do is to pull apart the print statement: you can see that this is actually just two ‘f-strings’ joined by a +–having that at the end of the line tells Python that it should carry on to the next line. That’s a handy way to make your code a little easier to read. If you’re creating a list and it’s getting a little long, then you can also continue a line using a , as well!\n\n1. The first f-string\nThe first string will help you to make sense of the second: f-strings allow you to ‘interpolate’ a variable into a string directly rather than having to have lots of str(x) + \" some text \" + str(y). You can write f\"{x} some text {y}\" and Python will automatically convert the variables x and y to strings and replace {x} with the value of x and {y} with the value of y.\nSo here f\"The town of {city} can be found at \" becomes f\"The town of Lerwick can be found at \" because {city} is replaced by the value of the variable city. This makes for code that is easier for humans to read and so I’d consider that a good thing.\n\n\n2. The second f-string\nThis one is hard because there’s just a lot of code there. But, again, if we start with what we recognise that it gets just a little bit more manageable… Also, it stands to reason that the only difference between the two outputs is that one asks for the ‘Longitude’ and the other for the ‘Latitude’. So if you can make sense of one you have automatically made sense of the other and don’t need to work it all out.\nLet’s start with a part that you might recognise:\n\nmyData['Name'].index(city)\n\n5\n\n\nYou’ve got this. This is just asking Python to work out the index of Lerwick (because city = 'Lerwick'). So it’s a number. 5 in this case. And we can then think, ’OK so what does this return:\n\nmyData['Longitude'][5]\n\n-1.145\n\n\nAnd the answer is -1.145. That’s the Longitude of Lerwick! There’s just one last thing: notice that we’re talking about degrees West here. So the answer isn’t a negative (because negative West degrees would be East!), it’s the absolute value. And that is the final piece of the puzzle: abs(...) gives us the absolute value of a number!\n\nhelp(abs)\n\nHelp on built-in function abs in module builtins:\n\nabs(x, /)\n    Return the absolute value of the argument.\n\n\n\n\n\n\nThe Average City Size\nHere we’re going to ‘cheat’ a little bit: rather than writing our own function, we’re going to import a package and use someone else’s function. The numpy package contains a lot of useful functions that we can call on (if you don’t believe me, add “dir(np)” on a new line after the import statement), and one of them calculates the average of a list or array of data.\n\nprint(f\"The mean population is {np.mean(myData['Population'])}\")\n\nThe mean population is 2435442.5\n\n\nThis is where our new approach really comes into its own: because all of the population data is in one place (a.k.a. a series or column), we can just throw the whole list into the np.mean function rather than having to use all of those convoluted loops and counters. Simples, right?\nNo, not simple at all, but we’ve come up with a way to make it simple.\n\n\nRecap!\nSo the really clever bit in all of this isn’t switching from a list-of-lists to a dictionary-of-lists, it’s recognising that the dictionary-of-lists is a better way to work with the data that we’re trying to analyse and that that there are useful functions that we can exploit to do the heavy lifting for us. Simply by changing the way that we stored the data in a ‘data structure’ (i.e. complex arrangement of lists, dictionaries, and variables) we were able to do away with lots of for loops and counters and conditions, and reduce many difficult operations to something that could be done on one line!"
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#brain-teaser",
    "href": "practicals/Practical-04-Reuse.html#brain-teaser",
    "title": "Practical 4: Efficient Code",
    "section": "Brain Teaser",
    "text": "Brain Teaser\n\n\n\n\n\n\nDifficulty: 🤯.\n\n\n\n\n\n\n\n\n\nWhy not have a stab at writing the code to print out the 4th most populous city? This can still be done on one line, though you might want to start by breaking the problem down:\n\nHow do I find the 4th largest value in a list?\nHow do I find the index of the 4th largest value in a list?\nHow do I use that to look up the name associated with that index?\n\nYou’ve already done #2 and #3 above so you’ve solved that problem. If you can solve #1 then the rest should fall into place.\n\n\n\n\n\n\nTip\n\n\n\nYou don’t want to use &lt;list&gt;.sort() because that will sort your data in place and break the link between the indexes across the ‘columns’; you want to research the function sorted(&lt;list&gt;) where &lt;list&gt; is the variable that holds your data and sorted(...) just returns whatever you pass it in a sorted order without changing the original list. You’ll see why this matters if you get the answer… otherwise, wait a few days for the answers to post.\n\n\n\nQuestion\n\n# Print out the name of the 4th most populous city-region\ncity = ??\n\nprint(\"The fourth most populous city is: \" + str(city))\n\nThe answer is Edinburgh."
  },
  {
    "objectID": "practicals/Practical-04-Reuse.html#bringing-it-all-together",
    "href": "practicals/Practical-04-Reuse.html#bringing-it-all-together",
    "title": "Practical 4: Efficient Code",
    "section": "Bringing it all together…",
    "text": "Bringing it all together…\nConceptually, this is one of the hardest practicals in the entire term because it joins up so many of the seemingly simple ideas that you covered in Code Camp into a very complex ‘stew’ – all our basic ingredients (lists, dictionaries, etc.) have simmered for a bit, been stirred up together, and become something entirely new and more complex.\nSo if this practical doesn’t make sense to you on the first runthrough, I’d suggest going back through the second half of the practical again in a couple of days’ time – that will give your brain a little time to wrap itself around the basics before you throw the hard stuff at it again. Don’t panic if it doesn’t all make sense on the second runthrough either – this is like a language, you need to practice! With luck, the second time you went through this code a little bit more made sense. If you need to do it a third time you’ll find that even more makes sense… and so on."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html",
    "href": "practicals/Practical-09-Selecting_Data.html",
    "title": "Practical 9: Selecting Data",
    "section": "",
    "text": "Important\n\n\n\nThis practical focusses on data linkage! You will have seen quite a bit of this of these across the preceding three to four weeks, but they were picked up in an ad-hoc way, here we try to systematise things a bit."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#in-pandas",
    "href": "practicals/Practical-09-Selecting_Data.html#in-pandas",
    "title": "Practical 9: Selecting Data",
    "section": "In Pandas",
    "text": "In Pandas\n\nRecap: A First Query\n\npd.read_parquet(f'{pqt}').head(3) \n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_is_superhost\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nreviews_per_month\ngeometry\n\n\n\n\n0\n92644\nhttps://www.airbnb.com/rooms/92644\n2023-09-06\nRental unit in Earlsfield · ★4.57 · 1 bedroom ...\n&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Hi everyone! I have 2 ro...\n498201\nDee Dee\n2011-04-10\nLondon, United Kingdom\nFalse\n...\n42.0\n2\n730\n217\n216.0\n2011-06-21\n2022-10-29\n4.57\n1.45\nb'\\x01\\x01\\x00\\x00\\x00\\xeax\\xcc@e\\xfc\\xc7\\xbf\\...\n\n\n1\n93015\nhttps://www.airbnb.com/rooms/93015\n2023-09-06\nRental unit in Hammersmith · ★4.82 · 2 bedroom...\nGorgeous 2 bed ground floor apartment with per...\n499704\nSarah\n2011-04-11\nLondon, United Kingdom\nFalse\n...\n175.0\n5\n240\n40\n38.0\n2012-02-01\n2022-09-30\n4.82\n0.27\nb'\\x01\\x01\\x00\\x00\\x00\\r\\xabx#\\xf3\\xc8\\xcb\\xbf...\n\n\n2\n13913\nhttps://www.airbnb.com/rooms/13913\n2023-09-06\nRental unit in Islington · ★4.80 · 1 bedroom ·...\nMy bright double bedroom with a large window h...\n54730\nAlina\n2009-11-16\nLondon, United Kingdom\nFalse\n...\n79.0\n1\n29\n360\n41.0\n2010-08-18\n2022-12-11\n4.80\n0.26\nb'\\x01\\x01\\x00\\x00\\x00\\xeeZB&gt;\\xe8\\xd9\\xbc\\xbf\\...\n\n\n\n\n3 rows × 32 columns\n\n\n\nThis should (I hope) be trivial to read now: we are loading a parquet file using pandas and taking advantage of Python’s ‘chaining’ functionality (&lt;object&gt;.&lt;method&gt;().&lt;method&gt;()...) to return the first three rows using head. It is worth noticing that we’re not even bothering to save the result of this command to a data frame (thus the lack of a df = in the code) and We’re doing this here solely so that you can compare pandas and SQL/DuckDB syntax across each of the following steps.\n\n\nRecap: Selecting some columns\nTo load a columnar subset of the data we have two options:\n\nLoad all the data and then subset (which always happens with CSV files but is optional with other formats)\nLoad only the columns we care about (which is possible with parquet files)\n\nAnd in code these are:\n\nLoad then filter\n\n%%time\npd.read_parquet(f'{pqt}')[['listing_url', 'price', 'number_of_reviews', 'property_type', 'host_name']].head(5)\n\nCPU times: user 139 ms, sys: 48 ms, total: 187 ms\nWall time: 142 ms\n\n\n\n\n\n\n\n\n\nlisting_url\nprice\nnumber_of_reviews\nproperty_type\nhost_name\n\n\n\n\n0\nhttps://www.airbnb.com/rooms/92644\n42.0\n216.0\nPrivate room in rental unit\nDee Dee\n\n\n1\nhttps://www.airbnb.com/rooms/93015\n175.0\n38.0\nEntire rental unit\nSarah\n\n\n2\nhttps://www.airbnb.com/rooms/13913\n79.0\n41.0\nPrivate room in rental unit\nAlina\n\n\n3\nhttps://www.airbnb.com/rooms/15400\n150.0\n94.0\nEntire rental unit\nPhilippa\n\n\n4\nhttps://www.airbnb.com/rooms/93734\n46.0\n180.0\nPrivate room in condo\nWilliam\n\n\n\n\n\n\n\n\n\nFilter then load\n\n%%time\npd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'property_type', 'host_name']).head(5)\n\nCPU times: user 15.2 ms, sys: 2.12 ms, total: 17.3 ms\nWall time: 14.5 ms\n\n\n\n\n\n\n\n\n\nlisting_url\nprice\nnumber_of_reviews\nproperty_type\nhost_name\n\n\n\n\n0\nhttps://www.airbnb.com/rooms/92644\n42.0\n216.0\nPrivate room in rental unit\nDee Dee\n\n\n1\nhttps://www.airbnb.com/rooms/93015\n175.0\n38.0\nEntire rental unit\nSarah\n\n\n2\nhttps://www.airbnb.com/rooms/13913\n79.0\n41.0\nPrivate room in rental unit\nAlina\n\n\n3\nhttps://www.airbnb.com/rooms/15400\n150.0\n94.0\nEntire rental unit\nPhilippa\n\n\n4\nhttps://www.airbnb.com/rooms/93734\n46.0\n180.0\nPrivate room in condo\nWilliam\n\n\n\n\n\n\n\nNotice the difference in time!!!\n\n\n\nRecap: Adding a constraint\n\ndf = pd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'property_type', 'host_name'])\ndf[(df.price &lt; 250) & (df.number_of_reviews &gt; 0) & (df.property_type=='Entire home/apt')].head(5)\n\n\n\n\n\n\n\n\nlisting_url\nprice\nnumber_of_reviews\nproperty_type\nhost_name\n\n\n\n\n18922\nhttps://www.airbnb.com/rooms/20296839\n96.0\n7.0\nEntire home/apt\nLira\n\n\n18975\nhttps://www.airbnb.com/rooms/20349067\n99.0\n1.0\nEntire home/apt\nM\n\n\n22319\nhttps://www.airbnb.com/rooms/22959348\n100.0\n3.0\nEntire home/apt\nRobert\n\n\n38944\nhttps://www.airbnb.com/rooms/42969992\n173.0\n1.0\nEntire home/apt\nDuda\n\n\n52418\nhttps://www.airbnb.com/rooms/649784743352942906\n91.0\n9.0\nEntire home/apt\nTravelnest\n\n\n\n\n\n\n\nFor improved legibility you can also write this as:\ndf = pd.read_parquet(f'{pqt}', columns=['listing_url', 'price', 'number_of_reviews', 'last_review', 'host_name'])\ndf[\n    (df.price &lt; 250) & \n    (df.number_of_reviews &gt; 0) & \n    (df.property_type=='Entire home/apt')\n].head(5)\nNotice here that we are using three conditions to filter the data as well as a column filter on loading to minimise the amount of data loaded into memory. Applying the filters simultaneously will also make it easy to see what you’ve done (you aren’t applying each one separately) and to adjust the overall cleaning process.\nThis filter is fairly straightforward, but things get more complicated when you want to aggregate the return…\n\n\nAggregating the return\nThere is a lot to unpack here, and notice that it takes three steps to achieve our goal of selecting, grouping, aggregating, sorting, and printing out the ten most frequent combinations of room and property type.\n\ndf = pd.read_parquet(f'{pqt}', columns=['property_type','room_type','number_of_reviews','price'])\ndf = df[\n    (df.price &lt; 1050) & \n    (df.number_of_reviews &gt; 0)\n]\ndf.groupby(\n        by=['room_type','property_type'],\n        observed=True\n    ).agg(\n        freq = (\"property_type\", \"count\"),\n        median_price = (\"price\", \"median\"),\n).reset_index().sort_values(\n        by=['freq','room_type','property_type'], ascending=[False,True,True]\n    ).head(10)\n\n\n\n\n\n\n\n\nroom_type\nproperty_type\nfreq\nmedian_price\n\n\n\n\n18\nEntire home/apt\nEntire rental unit\n24662\n136.0\n\n\n68\nPrivate room\nPrivate room in rental unit\n9763\n52.0\n\n\n59\nPrivate room\nPrivate room in home\n7800\n49.0\n\n\n10\nEntire home/apt\nEntire condo\n7542\n156.0\n\n\n14\nEntire home/apt\nEntire home\n5243\n200.0\n\n\n52\nPrivate room\nPrivate room in condo\n2883\n67.0\n\n\n19\nEntire home/apt\nEntire serviced apartment\n1565\n200.0\n\n\n72\nPrivate room\nPrivate room in townhouse\n1205\n55.0\n\n\n20\nEntire home/apt\nEntire townhouse\n967\n235.0\n\n\n45\nPrivate room\nPrivate room in bed and breakfast\n412\n78.0\n\n\n\n\n\n\n\nHopefully the first two steps are fairly clear, so let’s focus on the final one:\n\nGroup By\nThis is a reasonably intelligible step in which we group the data loaded by room and property:\ndfg = df.groupby(\n        by=['room_type','property_type'],\n        observed=True\n    )\ndfg\nThe order here matters: groupby(by=[&lt;A&gt;,&lt;B&gt;]) does not return the same result as groupby(by=[&lt;B&gt;,&lt;A&gt;]). Try it:\ndf.groupby(\n        by=['property_type','room_type'],\n        observed=True\n    )\nThe other thing to note here is the observed=True. This is a nice bit of additional functionality that, if you set it to False will return a number for all possible combinations, inserting a zero if that combintaion is not observed in the data.\n\n\nAgg\nThe agg step aggregates the data specified in the functions:\ndfg.agg(\n        freq = (\"property_type\", \"count\"),\n        median_price = (\"price\", \"median\"),\n)\nPandas offers a lot of different ways to do this, but the above approach is perhaps the most flexible since we are telling Pandas to apply the count function to the property_type field and assign it to a column called freq, and to apply the median function to the price field and assign that to a column called median_price.\n\n\n‘Degroup’\nIn order to work with the aggregated data you will almost always want to convert your GroupedDataFrame back to a regular DataFrame and that means resetting the index reset_index() – this is just one of those things to learn about grouped data in Pandas.\n\n\nSort\nFinally, to sort the data (which is usually what you want) you need to sort_values, where by specifies the fields you want to sort on and ascending is a matching (optional) list that specifies the sort order for each sort column. If you just want to sort everything in ascending order then you don’t need to specify the ascending values, and if you wanted to sort everything in descending order then it’s just ascending=False."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#in-sql",
    "href": "practicals/Practical-09-Selecting_Data.html#in-sql",
    "title": "Practical 9: Selecting Data",
    "section": "In SQL",
    "text": "In SQL\nThat last example may have left you despairing of every being able to select/filter/aggregate/derive your data, but there is another way that is often far simpler if you are: a) willing to learn a different language, and b) willing to work with data in different formats. And that’s all thanks to Parquet and DuckDB.\n\nParquet and DuckDB\nOne of the recent technical revolutions that has fundamentally reshaped my workflow is the combination of parquet files and in-memory databases. Parquet and Apache Arrow are closely related but, in short, when you want to save large data sets in an easy-to-access format then Parquet should be your default choice. DuckDB gives you a way to treat Parquet files as a database table and run queries against it using standard SQL. You can install DuckDB on the command-line, but you can also query it from within Python using the appropriate module.\n\n\nA First Query\nLet’s see a quick demonstration:\n\nimport duckdb as db\n\nquery = f'''\nSELECT *\nFROM read_parquet('{pqt}') \nLIMIT 3;\n'''\n\ndb.sql(query).to_df()\n\n\n\n\n\n\n\n\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\nhost_is_superhost\n...\nprice\nminimum_nights\nmaximum_nights\navailability_365\nnumber_of_reviews\nfirst_review\nlast_review\nreview_scores_rating\nreviews_per_month\ngeometry\n\n\n\n\n0\n92644\nhttps://www.airbnb.com/rooms/92644\n2023-09-06\nRental unit in Earlsfield · ★4.57 · 1 bedroom ...\n&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Hi everyone! I have 2 ro...\n498201\nDee Dee\n2011-04-10\nLondon, United Kingdom\nFalse\n...\n42.0\n2\n730\n217\n216.0\n2011-06-21\n2022-10-29\n4.57\n1.45\n[1, 1, 0, 0, 0, 234, 120, 204, 64, 101, 252, 1...\n\n\n1\n93015\nhttps://www.airbnb.com/rooms/93015\n2023-09-06\nRental unit in Hammersmith · ★4.82 · 2 bedroom...\nGorgeous 2 bed ground floor apartment with per...\n499704\nSarah\n2011-04-11\nLondon, United Kingdom\nFalse\n...\n175.0\n5\n240\n40\n38.0\n2012-02-01\n2022-09-30\n4.82\n0.27\n[1, 1, 0, 0, 0, 13, 171, 120, 35, 243, 200, 20...\n\n\n2\n13913\nhttps://www.airbnb.com/rooms/13913\n2023-09-06\nRental unit in Islington · ★4.80 · 1 bedroom ·...\nMy bright double bedroom with a large window h...\n54730\nAlina\n2009-11-16\nLondon, United Kingdom\nFalse\n...\n79.0\n1\n29\n360\n41.0\n2010-08-18\n2022-12-11\n4.80\n0.26\n[1, 1, 0, 0, 0, 238, 90, 66, 62, 232, 217, 188...\n\n\n\n\n3 rows × 32 columns\n\n\n\nAnd now let’s unpack this:\n\nWe import the duckdb library as db.\nWe set up a SQL query using a multi-line f-string\nWe use DuckDb to execute the query and return a pandas dataframe (df)\n\nWhat’s particularly elegant here (and quite different from trying to talk to a Postres or MySQL database) is that there’s no connect-execute-collect pattern; we just build the query and execute it!\n\n\nDeciphering SQL\n\n\n\n\n\n\nI do declare…\n\n\n\nNow let’s take a look at the SQL query… SQL is what’s called a declarative language, meaning that it is about the logic we want the program to follow rather than the ‘flow’ of execution. Python supports some declarative elements but is more commonly seen as an imperative language supporting procedural or functional approaches. This is a long way of saying: SQL won’t look like Python even though we’re executing SQL from within Python.\n\n\nSo our query (with added line numbers for clarity) looked liked this:\nSELECT *\nFROM read_parquet('{pqt}') \nLIMIT 3\nLine-by-line this means:\n\nSelect all columns (SELECT &lt;* == everything&gt;)\nFrom the parquet file (FROM &lt;table location&gt;)\nLimit the return to 3 rows (LIMIT &lt;row count&gt;)\n\nLet’s look at some variations…\n\n\nChoosing some columns\n\nquery = f'''\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nLIMIT 5;\n'''\n\ndb.sql(query).to_df()\n\n\n\n\n\n\n\n\nlisting_url\nprice\nnumber_of_reviews\nlast_review\nhost_name\n\n\n\n\n0\nhttps://www.airbnb.com/rooms/92644\n42.0\n216.0\n2022-10-29\nDee Dee\n\n\n1\nhttps://www.airbnb.com/rooms/93015\n175.0\n38.0\n2022-09-30\nSarah\n\n\n2\nhttps://www.airbnb.com/rooms/13913\n79.0\n41.0\n2022-12-11\nAlina\n\n\n3\nhttps://www.airbnb.com/rooms/15400\n150.0\n94.0\n2023-05-01\nPhilippa\n\n\n4\nhttps://www.airbnb.com/rooms/93734\n46.0\n180.0\n2023-09-02\nWilliam\n\n\n\n\n\n\n\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nLIMIT 5;\nIt should be fairly easy to see how the query has changed from last time, but line-by-line this means:\n\nSelect a set of columns from the table in the order specified (SELECT &lt;column 1&gt;, &lt;column 30&gt;, &lt;column 5&gt;...)\nFrom the parquet file (FROM &lt;table location&gt;)\nLimit the return to 5 rows (LIMIT &lt;row count&gt;)\n\n\n\nAdding a constraint\n\nquery = f'''\nSELECT listing_url, price, number_of_reviews, last_review, host_name \nFROM read_parquet('{pqt}') \nWHERE price &lt; 250 \nAND number_of_reviews &gt; 0\nAND property_type='Entire home/apt'\nLIMIT 5;\n'''\n\ndb.sql(query).to_df()\n\n\n\n\n\n\n\n\nlisting_url\nprice\nnumber_of_reviews\nlast_review\nhost_name\n\n\n\n\n0\nhttps://www.airbnb.com/rooms/20296839\n96.0\n7.0\n2017-10-01\nLira\n\n\n1\nhttps://www.airbnb.com/rooms/20349067\n99.0\n1.0\n2017-11-12\nM\n\n\n2\nhttps://www.airbnb.com/rooms/22959348\n100.0\n3.0\n2018-02-04\nRobert\n\n\n3\nhttps://www.airbnb.com/rooms/42969992\n173.0\n1.0\n2021-10-24\nDuda\n\n\n4\nhttps://www.airbnb.com/rooms/649784743352942906\n91.0\n9.0\n2023-03-22\nTravelnest\n\n\n\n\n\n\n\nIn this query we’ve added three constraints using a WHERE, which is asking DuckDB to find all of the rows where the following things are true:\n\nThe price must be less than ($)250/night\nThe number_of_reviews must be more than 0\nThe property_type must be Entire home/apt\n\n\n\nAggregating the return\nSo far, we’ve seen a few ways (and hopefully enough to get you started) to select data, but databases also ‘excel’ at aggregating data in various ways. We aren’t going to get into things like windowing functions or stored procedures here, but even simple aggregates done in DuckDB can vastly improve on the performance of pandas.\n\n\n\n\n\n\nTip\n\n\n\nWhen you aggregate data you need to retrieve every column in the SELECT portion that you GROUP BY in the WHERE portion of the query. This will make sense when you see the examples below… (and should also make sense based on the Pandas equivalent above)\n\n\n\nquery = f'''\nSELECT property_type, room_type, COUNT(*) AS frequency, MEDIAN(price) \nFROM read_parquet('{pqt}') \nWHERE price &lt; 1000 \nAND number_of_reviews &gt; 0\nGROUP BY room_type, property_type\nORDER BY frequency DESC, room_type, property_type\nLIMIT 10;\n'''\n\ndb.sql(query).to_df()\n\n\n\n\n\n\n\n\nproperty_type\nroom_type\nfrequency\nmedian(price)\n\n\n\n\n0\nEntire rental unit\nEntire home/apt\n24634\n136.0\n\n\n1\nPrivate room in rental unit\nPrivate room\n9754\n52.0\n\n\n2\nPrivate room in home\nPrivate room\n7797\n49.0\n\n\n3\nEntire condo\nEntire home/apt\n7532\n155.0\n\n\n4\nEntire home\nEntire home/apt\n5228\n200.0\n\n\n5\nPrivate room in condo\nPrivate room\n2880\n67.0\n\n\n6\nEntire serviced apartment\nEntire home/apt\n1565\n200.0\n\n\n7\nPrivate room in townhouse\nPrivate room\n1204\n55.0\n\n\n8\nEntire townhouse\nEntire home/apt\n964\n234.5\n\n\n9\nPrivate room in bed and breakfast\nPrivate room\n412\n78.0\n\n\n\n\n\n\n\nThere are quite a few changes to the query here so it’s worth reviewing them in more detail:\nSELECT property_type, room_type, COUNT(*) AS frequency, MEDIAN(price) \nFROM read_parquet('{pqt}') \nWHERE price &lt; 1000 \nAND number_of_reviews &gt; 0\nGROUP BY room_type, property_type\nORDER BY frequency DESC, room_type, property_type\nLIMIT 10;\nKey things to note:\n\nWe have two new aggregate functions:\n\nCOUNT(*) returns a count of the number of rows in each group specified in the GROUP BY clause.\nMEDIAN(price) returns, unsurprisingly, the median value of the price column for each group specified in the GROUP BY clause.\nNote also the AS frequency which ‘renames’ the column returned by the query; it’s the same concept as the import x as y in Python.\n\nGROUP BY is where the aggregation happens, and here we’re asking DuckDB to take all of the rows selected (WHERE price &lt; 1000 AND number_of_reviews &gt; 0) and group them using the room_type and property_type fields.\nORDER BY orders the returned records by the columns we specify, and they can be either ASCending (the default) or DESCending (descending).\n\nWhat you should also be noting here is that:\n\nThis query returns very quickly compared to the pandas equivalent.\nWe have been able to express our selection, grouping, and organising criteria very succinctly.\n\nIn terms of both speed and intelligibility, there can be quite substantial advantages to moving some of your workflow into a database or a database-like format such as Parquet and then querying that from Python. Databases are designed for the kind of application that Pandas struggles with, and if you get to windowing functions and stored procedures you’ll see how there are situations where something is far easier to express in Python/Pandas than in SQL.\nSo the trick here is to recognise when you are facing a problem that: a) will benefit from being expressed/tackled in a different language; and b) won’t create undue overhead on your technology ‘stack’. In working with environmental and built environment data I was able to cut the processing time by 80% when I moved the bulk of the data linkage work from Pandas into Parquet+DuckDB. But, by the same token, what’s the point of using Postgres and managing a spatial database to perform a single step in a much longer workflow unless the performance considerations are so massive they outweigh any other issue."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#in-pandas-1",
    "href": "practicals/Practical-09-Selecting_Data.html#in-pandas-1",
    "title": "Practical 9: Selecting Data",
    "section": "In Pandas",
    "text": "In Pandas\nPandas distinguishes between several types of what SQL would call a ‘join’: the process of linking two data sets. Depending on what you want to do, this will fall into one of the merge, join, concatenate, or compare functions:\n\nconcat simply appends one data frame to another and won’t be discussed further, but keep in mind that you can concatenate horizontally and vertically (across and down), and that having named indexes can cause consternation. You would find it most useful for appending columns to a data set (appending rows should be approached differently) or extending a data set for year \\(n\\) with data from year \\(n+1\\)…\nmerge is what we normally want when we want to do something similar to a SQL join. You should refer back to the lecture for the differences between ‘one-to-one’, ‘one-to-many’, and ‘many-to-many’. Note too that merging is a function of the pandas library and not a method of a data frame.\n\n\nJoining by attribute\nSo in our case, to join the two MSOA data sets we’re going to need to match the MSOA codes which have (slightly) different names in the two datasets:\n\n%%time\n\nrs = pd.merge(msoa_df, msoa_nms[['msoa11cd','msoa11hclnm','Laname']], left_on='MSOA 2021 Code', right_on='msoa11cd', how='left')\nprint(f\"Result set has {rs.shape[0]:,} rows and {rs.shape[1]:,} columns.\")\nrs.head(3)\n\nResult set has 7,264 rows and 46 columns.\nCPU times: user 2.47 ms, sys: 422 µs, total: 2.89 ms\nWall time: 2.73 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM60 to 64\nM65 to 69\nM70 to 74\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11hclnm\nLaname\n\n\n\n\n0\nE06000001\nHartlepool\nE02002483\nHartlepool 001\n10323\n265\n296\n356\n302\n238\n...\n281\n254\n210\n180\n93\n82\n28\nE02002483\nClavering\nHartlepool\n\n\n1\nE06000001\nHartlepool\nE02002484\nHartlepool 002\n10460\n325\n349\n295\n340\n283\n...\n363\n276\n248\n175\n86\n49\n28\nE02002484\nHeadland & West View\nHartlepool\n\n\n2\nE06000001\nHartlepool\nE02002485\nHartlepool 003\n8040\n238\n287\n295\n262\n225\n...\n272\n198\n159\n143\n61\n31\n12\nE02002485\nJesmond\nHartlepool\n\n\n\n\n3 rows × 46 columns\n\n\n\nBut wait! There’s an issue lurking in the data!\n\nprint(f\"There are {rs.msoa11hclnm.isna().sum()} missing MSOA Names!\")\n\nThere are 184 missing MSOA Names!\n\n\nCan you work out why this has happened? There is a clue in the column names!\nThere’s no way to solve this problem except by changing the code to use this URL instead for the MSOA Names.\nWe can also try to constrain the result set to one LA thanks to data in the MSOA Names database:\n\n%%time \n\nla_nm = 'Waltham Forest'\nsdf   = msoa_nms[msoa_nms.Laname==la_nm][['msoa11cd','msoa11hclnm','Laname']].copy()\n\nrs = pd.merge(msoa_df, sdf, left_on='MSOA 2021 Code', right_on='msoa11cd', how='inner')\nprint(f\"Result set has {rs.shape[0]:,} rows and {rs.shape[1]:,} columns.\")\nrs.head(3)\n\nResult set has 28 rows and 46 columns.\nCPU times: user 1.75 ms, sys: 213 µs, total: 1.96 ms\nWall time: 1.84 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM60 to 64\nM65 to 69\nM70 to 74\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11hclnm\nLaname\n\n\n\n\n0\nE09000031\nWaltham Forest\nE02000895\nWaltham Forest 001\n8363\n208\n233\n250\n228\n215\n...\n242\n209\n153\n194\n137\n93\n45\nE02000895\nChingford Green West\nWaltham Forest\n\n\n1\nE09000031\nWaltham Forest\nE02000896\nWaltham Forest 002\n9322\n256\n278\n264\n230\n241\n...\n257\n218\n216\n190\n111\n111\n54\nE02000896\nChingford Green East\nWaltham Forest\n\n\n2\nE09000031\nWaltham Forest\nE02000897\nWaltham Forest 003\n8438\n233\n262\n276\n212\n209\n...\n205\n162\n136\n98\n104\n87\n24\nE02000897\nFriday Hill\nWaltham Forest\n\n\n\n\n3 rows × 46 columns\n\n\n\nWithout the how=inner, the result set would still have all of the rows but some of the columns would be nearly completely empty."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#in-sql-1",
    "href": "practicals/Practical-09-Selecting_Data.html#in-sql-1",
    "title": "Practical 9: Selecting Data",
    "section": "In SQL",
    "text": "In SQL\nSQL-based joins use very similar keywords (since Pandas is copying SQL), but how we put together the query is quite different.\n\nJoining by attribute\n\n%%time\n\nquery = f'''\nSELECT * \nFROM \n    read_parquet('data/MSOA_population_estimates.parquet') as n \nLEFT JOIN \n    read_csv('{cache_data(msoa_names_url, 'data')}', header=true) as m\nON \n    n.\"MSOA 2021 Code\"=m.msoa11cd;\n'''\n\ndb.sql(query).to_df().head(3)\n\n+ data/MSOA-Names-1.20.csv found locally!\nCPU times: user 27.1 ms, sys: 3.35 ms, total: 30.5 ms\nWall time: 29.8 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11nm\nmsoa11nmw\nmsoa11hclnm\nmsoa11hclnmw\nLaname\n\n\n\n\n0\nE09000001\nCity of London\nE02000001\nCity of London 001\n10847\n105\n78\n69\n191\n699\n...\n143\n86\n36\n22\nE02000001\nCity of London 001\nCity of London 001\nCity of London\nNone\nCity of London\n\n\n1\nE09000002\nBarking and Dagenham\nE02000002\nBarking and Dagenham 001\n8384\n350\n346\n392\n322\n211\n...\n108\n71\n29\n29\nE02000002\nBarking and Dagenham 001\nBarking and Dagenham 001\nMarks Gate\nNone\nBarking and Dagenham\n\n\n2\nE09000002\nBarking and Dagenham\nE02000003\nBarking and Dagenham 002\n11803\n494\n431\n359\n330\n372\n...\n125\n91\n40\n19\nE02000003\nBarking and Dagenham 002\nBarking and Dagenham 002\nChadwell Heath East\nNone\nBarking and Dagenham\n\n\n\n\n3 rows × 49 columns\n\n\n\n\n\n\n\n\n\nSlower???\n\n\n\nWithout the data caching function, the query above may appear slower than the Pandas one but if you look at the timing information you’ll see that the actual time spent processing the data was less. How can that be? Notice that above we’re reading the CSV file from the House of Commons library as part of the join, so most of that delay is spent waiting for the CSV file to download!\nThat’s why I prefer to download a file once and save it locally rather than downloading the same file again and again. Plus it’s friendlier (and cheaper!) to the person or organisation providing the data to you.\n\n\nLet’s take a look at the SQL:\nSELECT * \nFROM \n    read_parquet('data/MSOA_population_estimates.parquet') as n \nLEFT JOIN \n    read_csv(msoa_names_url, header=true) as m\nON \n    n.\"MSOA 2021 Code\"=m.msoa11cd;\nLine-by-line:\n\nSELECT every column (this is the *, change this if you want to only pull a subset of columns)\nFROM the following tables (it doesn’t really matter if the tables are on this line or the next for legibility)\n&lt;table 1 from parquet&gt; as n (we now refer to the data from this table using the prefix n.; e.g. n.Total)\nLEFT JOIN is the SQL way of saying to keep all of the rows in the first table (n, which is the first, and therefore ‘left’ table)\n&lt;table 2 from csv&gt; as m (we now refer to the data from this table using the prefix m.; e.g. m.geometry)\nON &lt;left table matching column&gt; = &lt;right table matching column&gt; (here, the unusual thin is the double-quotes around the column name required to deal with the fact that the label contains spaces).\n\nNotice how there are parallels between even quite different languages here: if you have spaces or special characters or whatever in your column name then you’re going to need to handle that a little differently, and if you have two tables to join you have a left (aka first) one and a right (aka second) one and the order matters.\nNow, running the same query to get the Waltham Forest data can be done two ways:\n\n%%time\n\nboro = 'Waltham Forest'\nquery = f'''\nSELECT * \nFROM \n    read_parquet('data/MSOA_population_estimates.parquet') as n \nINNER JOIN \n    read_csv('{cache_data(msoa_names_url, 'data')}', header=true) as m\nON \n    n.\"MSOA 2021 Code\"=m.msoa11cd\nWHERE \n    m.Laname='{boro}';\n'''\n\ndb.sql(query).to_df().head(3)\n\n+ data/MSOA-Names-1.20.csv found locally!\nCPU times: user 22.3 ms, sys: 2.54 ms, total: 24.9 ms\nWall time: 24.6 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11nm\nmsoa11nmw\nmsoa11hclnm\nmsoa11hclnmw\nLaname\n\n\n\n\n0\nE09000031\nWaltham Forest\nE02000895\nWaltham Forest 001\n8363\n208\n233\n250\n228\n215\n...\n194\n137\n93\n45\nE02000895\nWaltham Forest 001\nWaltham Forest 001\nChingford Green West\nNone\nWaltham Forest\n\n\n1\nE09000031\nWaltham Forest\nE02000896\nWaltham Forest 002\n9322\n256\n278\n264\n230\n241\n...\n190\n111\n111\n54\nE02000896\nWaltham Forest 002\nWaltham Forest 002\nChingford Green East\nNone\nWaltham Forest\n\n\n2\nE09000031\nWaltham Forest\nE02000897\nWaltham Forest 003\n8438\n233\n262\n276\n212\n209\n...\n98\n104\n87\n24\nE02000897\nWaltham Forest 003\nWaltham Forest 003\nFriday Hill\nNone\nWaltham Forest\n\n\n\n\n3 rows × 49 columns\n\n\n\nEverything here is basically the same except for:\n\nWe changed the LEFT JOIN to an INNER JOIN – this should make sense to you if you’ve watched the lectures.\nWe added a WHERE m.Laname=&lt;borough name&gt; which restricts the match to only those rows where the Local Authority name is Waltham Forest.\n\nHowever, note that this query can also be written this way:\n\n%%time\n\nboro = 'Waltham Forest'\nquery = f'''\nSELECT * \nFROM \n    read_parquet('data/MSOA_population_estimates.parquet') as n, \n    read_csv('{cache_data(msoa_names_url, 'data')}', header=true) as m\nWHERE m.Laname='{boro}'\nAND n.\"MSOA 2021 Code\"=m.msoa11cd;\n'''\n\ndb.sql(query).to_df().head(3)\n\n+ data/MSOA-Names-1.20.csv found locally!\nCPU times: user 22.3 ms, sys: 2.07 ms, total: 24.4 ms\nWall time: 24 ms\n\n\n\n\n\n\n\n\n\nLAD 2021 Code\nLAD 2021 Name\nMSOA 2021 Code\nMSOA 2021 Name\nTotal\nF0 to 4\nF5 to 9\nF10 to 14\nF15 to 19\nF20 to 24\n...\nM75 to 79\nM80 to 84\nM85 to 89\nM90 and over\nmsoa11cd\nmsoa11nm\nmsoa11nmw\nmsoa11hclnm\nmsoa11hclnmw\nLaname\n\n\n\n\n0\nE09000031\nWaltham Forest\nE02000895\nWaltham Forest 001\n8363\n208\n233\n250\n228\n215\n...\n194\n137\n93\n45\nE02000895\nWaltham Forest 001\nWaltham Forest 001\nChingford Green West\nNone\nWaltham Forest\n\n\n1\nE09000031\nWaltham Forest\nE02000896\nWaltham Forest 002\n9322\n256\n278\n264\n230\n241\n...\n190\n111\n111\n54\nE02000896\nWaltham Forest 002\nWaltham Forest 002\nChingford Green East\nNone\nWaltham Forest\n\n\n2\nE09000031\nWaltham Forest\nE02000897\nWaltham Forest 003\n8438\n233\n262\n276\n212\n209\n...\n98\n104\n87\n24\nE02000897\nWaltham Forest 003\nWaltham Forest 003\nFriday Hill\nNone\nWaltham Forest\n\n\n\n\n3 rows × 49 columns\n\n\n\nThe second way is a little easier to read, but it only allows you to do inner joins where attributes need to match in both tables for a row to be kept. This situation is such a common ‘use case’ that it makes sense to have this simpler syntax, but the previous code will work for inner, left, right, and outer joins."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#why-obvious-is-not-always-right-part-432",
    "href": "practicals/Practical-09-Selecting_Data.html#why-obvious-is-not-always-right-part-432",
    "title": "Practical 9: Selecting Data",
    "section": "Why obvious is not always right (Part 432)",
    "text": "Why obvious is not always right (Part 432)\nBuilding on what I said above in Section 3, even where you do have a spatial challenge, it can be worth it to convert it to a non-spatial solution to improve the overall performance of your code. For instance, say you have data from LSOAs and want to be able to aggregate it up to MSOAs and Boroughs to perform various analyses.\n\n\n\nLSOA Table\n\n\n\nLSOA Code\nPolygon\n\n\n\n\nLSOA1\nWKT(…)\n\n\nLSOA2\nWKT(…)\n\n\nLSOA3\nWKT(…)\n\n\n\n\n\n\n\n\nMSOA Table\n\n\n\nMSOA Code\nPolygon\n\n\n\n\nMSOA1\nWKT(…)\n\n\nMSOA2\nWKT(…)\n\n\nMSOA3\nWKT(…)\n\n\n\n\n\n\n\n\nBorough Table\n\n\n\nBorough Code\nPolygon\n\n\n\n\nBORO1\nWKT(…)\n\n\nBORO2\nWKT(…)\n\n\nBORO3\nWKT(…)\n\n\n\n\n\n\nThe obvious way to do this is as a spatial join: select all LSOAs within an MSOA and aggregate them. And you would then run this same query for every dimension you want to aggregate. This is not the right way to tackle this problem even though you can write the query to give you the correct answer.\nThe right way when you are going to repeatedly run an expensive spatial query is to work out if you can ‘cache’ the result to save time in the future. In this case the answer is to create a ‘lookup table’ which uses the LSOA and MSOA and Borough codes to tell you if a LSOA falls inside a borough or MSOA. You perform the hard spatial query just once to create the lookup table, and thereafter you are using a fast non-spatial query.\nIn this case your lookup table will be this…\n\nLookup Table\n\n\n\nLSOA Code\nMSOA Code\nBorough Code\n\n\n\n\nLSOA1\nMSOA1\nBORO1\n\n\nLSOA2\nMSOA1\nBORO1\n\n\nLSOA3\nMSOA2\nBORO1\n\n\n\nNow you can do any kind of spatial aggregation you want without having to incur the costs of running a spatial query using something like:\nSELECT m.\"MSOA Code\", SUM(&lt;attribute&gt;) as feature_sum, COUNT(&lt;attribute 2&gt;) as feature_count  \nFROM &lt;lsoa data table&gt; as l, &lt;lookup table&gt; as lkp  \nWHERE l.\"LSOA Code\" = lkp.\"LSOA Code\" \nGROUP BY lkp.\"MSOA Code\";\nSee, no need for a spatial query and you can run the same query easily for many features. You can also use this as a foundation for creating a VIEW or a MATERIALIZED VIEW, but that’s an advanced topic for managing your data more efficiently in an operational environment rather than a research-oriented one.\nBut first, we need some actual geodata to work with:\n\nmsoa_gpkg = gpd.read_file( cache_data(f'{host}/~jreades/data//MSOA-2011.gpkg', ddir) ).to_crs('epsg:27700')\nlistings  = gpd.read_parquet( cache_data(f'{host}/~jreades/data/{ymd}-{city}-listings.geoparquet', ddir) ).to_crs('epsg:27700')\n\n+ data/geo/MSOA-2011.gpkg found locally!\n+ data/geo/20240614-London-listings.geoparquet found locally!"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#in-geopandas",
    "href": "practicals/Practical-09-Selecting_Data.html#in-geopandas",
    "title": "Practical 9: Selecting Data",
    "section": "In Geopandas",
    "text": "In Geopandas\nLet’s try to find all of the listings that fall within the borough of Waltham Forest, so that implies two steps:\n\nSubset the MSOA geo-data so that it only includes the Waltham Forest MSOAs.\nRun a spatial query to find the listings that are within those MSOAs (we could, optionally, union the MSOAs to get the outline of the borough)\n\n\nboro = 'Waltham Forest'\nboro_gdf = msoa_gpkg[msoa_gpkg.LAD11NM==boro].copy()\n\n# Do the spatial join\nboro_listings = gpd.sjoin(listings, boro_gdf, predicate='within', rsuffix='_r')\n\n# Layer the plots\nf, ax = plt.subplots(1,1,figsize=(8,5))\nboro_gdf.plot(color=\"white\", edgecolor=\"black\", linewidth=0.5, ax=ax)\nboro_listings.plot(column='price', cmap='viridis', legend=True, s=1.5, aspect=1, ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you get ValueError: aspect must be finite and positive when you try to make a plot (this seems fairly common with GeoPackages (.gpkg files) then you will need to specify aspect=1 in the plot(...) command."
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#in-sql-2",
    "href": "practicals/Practical-09-Selecting_Data.html#in-sql-2",
    "title": "Practical 9: Selecting Data",
    "section": "In SQL",
    "text": "In SQL\nAfter quite a bit of faff my conclusion is that, while you can do spatial queries in DuckDB it is a lot of work and probably not worth the effort at this time. The ‘issue’ is that spatial support (as well as Excel supprt) is provided via the GDAL framework and this takes quite a different approach. After working it out, spatial queries do work fairly well if you do them entirely within DuckDB (reading, merging, and writing the data) and then load the results in a separate step using GeoPandas; however, you cannot get a GeoDataFrame back via db.query(&lt;query&gt;).to_df() since that only returns a Pandas data frame and the geometry column is unreadable. In addition, geoparquet support seems limited while GeoPackage performance is poor, so you’re basically losing all the advantages of a parquet-based workflow.\nSo the examples below are provided for reference only and, on the whole, right now I’d recommend using GeoPandas and geoparquet files directly.\n\n%%time\n\nboro = 'Waltham Forest'\n\nquery = f'''\nLOAD SPATIAL;\nCOPY(\n  SELECT m.MSOA11CD, n.msoa11nm, n.Laname, m.geom \n  FROM \n      (SELECT MSOA11CD, geom FROM ST_Read(\"{cache_data(f'{host}/~jreades/data/MSOA-2011.gpkg', ddir)}\")) AS m,\n      read_csv(\"{cache_data(msoa_names_url, 'data')}\") AS n\n  WHERE m.MSOA11CD=n.msoa11cd\n  AND n.Laname='{boro}'\n) TO 'data/geo/merged.gpkg' WITH (FORMAT GDAL, DRIVER 'GPKG', LAYER_CREATION_OPTIONS 'WRITE_BBOX=YES');\n'''\n\ndb.sql(query)\nrs = gpd.read_file('data/geo/merged.gpkg')\nprint(f\"Result set has {rs.shape[0]:,} rows and {rs.shape[1]:,} columns.\")\nrs.head(5)\nrs.plot(aspect=1)\n\n+ data/geo/MSOA-2011.gpkg found locally!\n+ data/MSOA-Names-1.20.csv found locally!\nResult set has 28 rows and 4 columns.\nCPU times: user 306 ms, sys: 40.2 ms, total: 346 ms\nWall time: 100 ms"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#load-geodata",
    "href": "practicals/Practical-09-Selecting_Data.html#load-geodata",
    "title": "Practical 9: Selecting Data",
    "section": "Load Geodata",
    "text": "Load Geodata\nA lot of useful geo-data can be accessed from the GeoPortal. And see also my discussion on lookup tables.\n\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\nmsoas = gpd.read_file( cache_data(f'{host}/~jreades/data/MSOA-2011.gpkg', ddir) ).to_crs('epsg:27700')\n\n+ data/geo/Water.gpkg found locally!\n+ data/geo/Boroughs.gpkg found locally!\n+ data/geo/Greenspace.gpkg found locally!\n+ data/geo/MSOA-2011.gpkg found locally!"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#select-london-msoas",
    "href": "practicals/Practical-09-Selecting_Data.html#select-london-msoas",
    "title": "Practical 9: Selecting Data",
    "section": "Select London MSOAs",
    "text": "Select London MSOAs\n\n\n\n\n\n\n🔗 Connections\n\n\n\nOne thing to remember here is that computers are exact. So if you say that the selection should only be of MSOAs within London then you actually need to think about whether a shared border qualifies as ‘within’. Watch the lectures again if you’re unsure, but that’s why here we take this slightly clunk approach of buffering the London boundary before doing the selection.\n\n\n\nUnion\nAs we don’t have a boundary file for London, we can generate use using the unary_union operator (as we do here) or using the dissolve() approach. Consider the pros and cons of each approach in terms of performance, output format, and leigibility.\nSo here’s approach 1, which is a method call returning a GeoDataFrame (which is why we can call plot):\n\nboros.dissolve().plot();\n\n\n\n\n\n\n\n\nAnd here’s approach 2, which is an attribute and returns a raw polygon (so no reason to call plot, but it’s come back without the rest of the data frame!):\n\nboros.unary_union\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nNotice how we’re also demonstrating some additional ways of plotting ‘on the fly’ (without generating a data frame) as well as (below) showing you how to zoom in/out.\n\n\n\nldn = gpd.GeoDataFrame(gpd.GeoSeries(data=boros.unary_union)).rename(columns={0:'geometry'}).set_geometry(\"geometry\")\nldn = ldn.set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n\n\n\n\n\n\n\n\n\n\nA (Bad) First Join\n\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='within', how='inner')\nax = ldn.plot(facecolor=(.5, .5, .9, .5))\nldn_msoas.plot(ax=ax, facecolor='none', edgecolor=(.8, .4, .4), linewidth=0.75)\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat has gone wrong???\n\n\n\nBefore you move on to the solution, stop and actually think about what this hasn’t done what you would have expected? THis is another reason that you need to pay attention to the differences between spatial and non-spatial joins.\n\n\n\n\nBuffer and Join\nIn order to ensure that we get all the MSOAs within London we need to buffer the boundary by some amount to ensure that within returns what we want. If cover were easier to use then that option might be preferable.\n\nQuestion\n\nldn['buffered'] = ldn.geometry.???(???)\nldn = ldn.set_geometry('buffered').set_crs(epsg=27700)\nax  = ldn.plot(facecolor=(.5, .5, .9, .5))\nmsoas.plot(ax=ax, facecolor='none', edgecolor=(.6, .6, .6, .6))\nax.set_xlim(500000, 515000)\nax.set_ylim(180000, 195000);\n\nBy default we want do an inner join because we want to drop everything that doesn’t line up between the two data sets (i.e. don’t keep the thousands of other non-London MSOAs).\n\n\nQuestion\n\nldn_msoas = gpd.sjoin(msoas, ldn, predicate='???', how='inner')\nldn_msoas.plot()\n\n\n\nQuestion\nHmmmm, not quite what you were expecting? See if you can figure out from the list of columns and the documentation for set_geometry what is going wrong? This might also help:\n\nprint(\", \".join(ldn_msoas.columns.to_list()))\n\nMSOA11CD, MSOA11NM, LAD11CD, LAD11NM, RGN11CD, RGN11NM, USUALRES, HHOLDRES, COMESTRES, POPDEN, HHOLDS, AVHHOLDSZ, geometry_left, index_right, geometry_right\n\n\nThe issue arises because we’ve joined two geo-data frames but the join function comes from pandas, which doesn’t know anything about spatial data and we have therefore ‘lost track’ of the column in which the geometry is stored. Worse, there are actually two geometry columns now, so we need to tell Geopandas which one to use!\nThe easiest way to do this is to simply rename the geometry we want and then set is as the active geometry:\n\nldn_msoas = ldn_msoas.rename(columns={'geometry_left':'geometry'}).set_geometry('geometry')\nldn_msoas.drop(columns='geometry_right', inplace=True)\n\nWe also no longer really need to keep the full MSOA data set hanging about.\n\ntry:\n    del(msoas)\nexcept NameError:\n    print(\"msoas already deleted.\")\n\n\n\nQuestion\n\nCan you explain why the outputs of the dissolve and unary_union look differnet? And use that as the basis for explaining why they are different?\n\n\nAnswer 1\n\n\nHow do you know that the units for the buffering operation are metres? 250 could be anything right?\n\n\nAnswer 2\n\n\nWhy do we need to buffer the London geometry before performing the within spatial join?\n\n\nAnswer 3"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#append-or-derive-names",
    "href": "practicals/Practical-09-Selecting_Data.html#append-or-derive-names",
    "title": "Practical 9: Selecting Data",
    "section": "Append or Derive Names",
    "text": "Append or Derive Names\nWe don’t actually make use of these in this session, but both operations could be relevant to your final reports:\n\nThe Borough-to-Subregion mapping could help you to group your data into larger sets so that your resulst become more reobust. it also connects us to long-run patterns of socio-economic development in London.\nThe MSOA Names data set (which you used above) gives you something that you could use to label one or more ‘neighbourhoods’ on a map with names that are relevant. So rather than talking about “As you can see, Sutton 003, is…”, you can write “The Wrythe neighbourhood [or area] of Sutton is significantly different from the surrounding areas…”\n\nThey also usefully test your understanding of regular expressions and a few other aspects covered in previous weeks.\n\nReplace\nYou’ve done this before: notice that the MSOA Name contains the Borough name with a space and some digits at the end. Use a regex (in str.replace()) to extract the LA name from the MSOA name. See if you do this without having to find your previous answer!\n\nQuestion\n\nldn_msoas['Borough'] = ldn_msoas.MSOA11NM.str.replace(r'???','',regex=True)\n\n# Just check results look plausible; you should have:\n# - 33 boroughs\n# - A df shape of 983 x 13\nprint(ldn_msoas.Borough.unique())\nprint(f\"There are {len(ldn_msoas.Borough.unique())} boroughs.\")\nprint(f\"Overall shape of data frame is {' x '.join([str(x) for x in ldn_msoas.shape])}\")\n\n\n\n\nMap\nNow that we’ve got the borough names we can set up a mapping dict here so that we can apply it as part of the groupby operation below (you should have 33 keys when done):\n\nmapping = {}\nfor b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n    mapping[b]='Outer East and North East'\nfor b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n    mapping[b]='Inner East'\nfor b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n    mapping[b]='Outer South'\nfor b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden']:\n    mapping[b]='Inner West'\nfor b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet','City of London']:\n    mapping[b]='Outer West and North West'\nprint(len(mapping.keys()))\n\n33\n\n\n\nQuestion\n\nldn_msoas['Subregion'] = ldn_msoas.Borough.map(???)\n\n\n\n\nAnd Save\n\nldn_msoas.to_parquet(os.path.join('data','geo','London_MSOA_Names.geoparquet'))"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#load-insideairbnb-data",
    "href": "practicals/Practical-09-Selecting_Data.html#load-insideairbnb-data",
    "title": "Practical 9: Selecting Data",
    "section": "Load InsideAirbnb Data",
    "text": "Load InsideAirbnb Data\n\nlistings = gpd.read_parquet( cache_data(f'{host}/~jreades/data/{ymd}-{city}-listings.geoparquet', ddir) ).to_crs(epsg=27700)\nprint(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")\n\n+ data/geo/20240614-London-listings.geoparquet found locally!\nData frame is 85,127 x 32\n\n\n\nSpatial Join\nAssociate LA (Local Authority) names to the listings using a spatial join, but notice the how here:\n\nQuestion\n\ngdf_la = gpd.sjoin(listings, ???, predicate='???', how='left')\nprint(gdf_la.columns.to_list())\n\n\n\n\nTidy Up\n\ngdf_la.drop(columns=['index_right','HECTARES','NONLD_AREA','ONS_INNER'], inplace=True)\n\nYou’ll need to look closely to check that the value_counts output squares with your expectations. If you don’t get 33 then there’s an issue and you’ll need to run the code in Section 5.4.3:\n\nif len(gdf_la.NAME.unique()) == 33:\n    print(\"All good...\")\nelse:\n    print(\"Need to run the next section of code...\")\n    print(f\"Now there are... {len(gdf_la.NAME.unique())} boroughs?\")\n    gdf_la.NAME.value_counts(dropna=False)\n\nAll good...\n\n\n\n\nFind Problematic Listings\nIf you were told that you need to run the next sectin of code then see if you can work out what happened…\ntry:\n    print(gdf_la[gdf_la.NAME.isna()].sample(2)[['name', 'NAME']])\n    ax = gdf_la[gdf_la.NAME.isna()].plot(figsize=(9,6), markersize=5, alpha=0.5)\n    boros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\nexcept ValueError as e:\n   pass\nIn short: in some cases there may be records that fall outside of London because of Airbnb’s shuffling approach:\ngdf_la.drop(index=gdf_la[gdf_la.NAME.isna()].index, axis=1, inplace=True)\nprint(f\"Data frame is {gdf_la.shape[0]:,} x {gdf_la.shape[1]}\")\n\n\nCheck and Save\nax = gdf_la.plot(column='NAME', markersize=0.5, alpha=0.5, figsize=(9,7))\nboros.plot(ax=ax, edgecolor='r', facecolor='None', alpha=0.5);\nYou should get the following:\n\n\n\n\n\n\n\n\n\ngdf_la.to_parquet(os.path.join('data','geo','Listings_with_LA.geoparquet'))\n\nQuestion\n\nDo you understand the difference between how='inner' and how='left'?"
  },
  {
    "objectID": "practicals/Practical-09-Selecting_Data.html#create-la-data",
    "href": "practicals/Practical-09-Selecting_Data.html#create-la-data",
    "title": "Practical 9: Selecting Data",
    "section": "Create LA Data",
    "text": "Create LA Data\nNow that we’ve assigned every listing to a borough, we can derive aggregate values for different groups of zones.\n\nSelect LA\nSelect a LA that is relevant to you to explore further…\n\nLA = 'Waltham Forest'\n\n\n\nSpatial Join\nThe first thing we want to do is join MSOA identifiers to each listing. In both cases we want to constrain the data to only be for ‘our’ LA of interest since that will speed up the process substantially:\n\ngdf_msoa = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            ldn_msoas[ldn_msoas.Borough==LA][['MSOA11CD','MSOA11NM','USUALRES','HHOLDS','Subregion','geometry']], predicate='within')\ngdf_msoa.head(2)\n\n\n\n\n\n\n\n\nindex\nid\nlisting_url\nlast_scraped\nname\ndescription\nhost_id\nhost_name\nhost_since\nhost_location\n...\nreviews_per_month\ngeometry\nNAME\nGSS_CODE\nindex_right\nMSOA11CD\nMSOA11NM\nUSUALRES\nHHOLDS\nSubregion\n\n\n\n\n0\n37\n41870\nhttps://www.airbnb.com/rooms/41870\n2023-09-07\nHome in Walthamstow · 2 bedrooms · 1 bed · 2.5...\nLovely friendly house, close to central line u...\n182993\nBimpe\n2010-07-27\nLondon, United Kingdom\n...\n0.01\nPOINT (538919.280 186290.652)\nWaltham Forest\nE09000031\n888\nE02000921\nWaltham Forest 027\n11001\n3966\nOuter East and North East\n\n\n1\n90\n78606\nhttps://www.airbnb.com/rooms/78606\n2023-09-07\nRental unit in Walthamstow · 1 bedroom · 1 pri...\nComfortable, modern home with a friendly host ...\n422362\nNicola\n2011-03-04\nLondon, United Kingdom\n...\n0.04\nPOINT (539419.512 187953.652)\nWaltham Forest\nE09000031\n882\nE02000915\nWaltham Forest 021\n8643\n3305\nOuter East and North East\n\n\n\n\n2 rows × 41 columns\n\n\n\n\n\nAggregate\nNow aggregate the data by MSOA, deriving median price and a count of the listings:\n\ngrdf_msoa = gdf_msoa.groupby('MSOA11NM').agg(\n    listing_count = ('price','count'),\n    median_price = ('price','median')\n).reset_index()\nprint(f\"Have {grdf_msoa.shape[0]:,} rows and {grdf_msoa.shape[1]:,} columns\")\ngrdf_msoa.head(2)\n\nHave 28 rows and 3 columns\n\n\n\n\n\n\n\n\n\nMSOA11NM\nlisting_count\nmedian_price\n\n\n\n\n0\nWaltham Forest 001\n17\n97.0\n\n\n1\nWaltham Forest 002\n14\n58.0\n\n\n\n\n\n\n\n\n\nJoin (Again)\nHere we see the difference between merge and join. You’ll notice that join operates by taking one data frame as the implicit ‘left’ table (the one which calls join) while the one that is passed to the join function is, implicitly, the ‘right’ table. Join operates only using indexes, so you’ll need to insert the code to specify the same index on both data frames, but this can be done on-the-fly as part of the joining operation:\n\nmsoa_gdf = grdf_msoa.set_index('MSOA11NM').join(\n                ldn_msoas[ldn_msoas.Borough==LA].set_index('MSOA11NM'), \n                rsuffix='_r').set_geometry('geometry')\nmsoa_gdf.head(3)\n\n\n\n\n\n\n\n\nlisting_count\nmedian_price\nMSOA11CD\nLAD11CD\nLAD11NM\nRGN11CD\nRGN11NM\nUSUALRES\nHHOLDRES\nCOMESTRES\nPOPDEN\nHHOLDS\nAVHHOLDSZ\ngeometry\nindex_right\nBorough\nSubregion\n\n\nMSOA11NM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWaltham Forest 001\n17\n97.0\nE02000895\nE09000031\nWaltham Forest\nE12000007\nLondon\n7979\n7962\n17\n36.4\n3271\n2.4\nMULTIPOLYGON (((537919.442 195742.428, 538051....\n0\nWaltham Forest\nOuter East and North East\n\n\nWaltham Forest 002\n14\n58.0\nE02000896\nE09000031\nWaltham Forest\nE12000007\nLondon\n8814\n8719\n95\n31.3\n3758\n2.3\nMULTIPOLYGON (((539172.688 195540.000, 539696....\n0\nWaltham Forest\nOuter East and North East\n\n\nWaltham Forest 003\n7\n89.0\nE02000897\nE09000031\nWaltham Forest\nE12000007\nLondon\n8077\n7991\n86\n42.9\n3345\n2.4\nMULTIPOLYGON (((538862.624 194017.438, 539001....\n0\nWaltham Forest\nOuter East and North East\n\n\n\n\n\n\n\nmsoa_gdf.plot(column='median_price', legend=True, figsize=(8,8));\nYou should get something like:\n\n\n\n\n\n\n\n\n\n\n\nSave\nJust so that we can pick up here without having to re-run all the preceding cells.\n\nmsoa_gdf.to_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n\n\nQuestion\n\nDo you understand the differences between pd.merge and df.join? and gpd.sjoin?\n\n\n\n\n\nDo you understand why it may be necessary to set_geometry in some cases?"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html",
    "href": "practicals/Practical-07-Spatial_Data.html",
    "title": "Practical 7: Spatial Data",
    "section": "",
    "text": "Last week we did some initial processing on the Inside Airbnb listings data, focussing on its numeric properties. This week we are going to focus on the spatial properties of the data set."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#caching-remote-data",
    "href": "practicals/Practical-07-Spatial_Data.html#caching-remote-data",
    "title": "Practical 7: Spatial Data",
    "section": "Caching Remote Data",
    "text": "Caching Remote Data\nWe don’t want to continually download data over the Internet: 1) because it’s not nice to whoever is hosting the data; and 2) because it requires you be online in order to run your code. So if we can ‘cache’ the data locally so that it’s only downloaded once this makes life much, much easier.\nI’ve used the Numpy-style comments here, but the Google-style also look good in this context and all styles of answer are acceptable so long as they work. See overview of commenting styles on DataCamp.\n\n\n\n\n\n\nTip\n\n\n\nUse this as an opportunity to improve your ability to read code and to learn through documentation.\n\n\n\n\n\n\n\n\nDifficulty level: Moderate\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -&gt; str:\n    \"\"\"\n    \n    ??\n    \n    \n    \"\"\"    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[??] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    if not os.path.isfile(dfn) or os.path.getsize(dfn) &lt; 250:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        path = os.path.split(dest)\n        \n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(??.content)\n            \n        print(\"\\tDone downloading...\")\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n    return dfn\n\nhelp(cache_data) # &lt;- This should show the docstring you've written"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#downloading-the-data",
    "href": "practicals/Practical-07-Spatial_Data.html#downloading-the-data",
    "title": "Practical 7: Spatial Data",
    "section": "Downloading the Data",
    "text": "Downloading the Data\n\n\n\n\n\n\nDifficulty level: Low, if your function works!\n\n\n\n\n\n\n\n\n\n\nGeopackages\nUse the function above to download and cache the GeoPackage files found on GitHub for Boroughs, Water, and Greenspace, then pass the output of these to GeoPandas. If you have been having trouble downloading files from GitHub, then use the understanding of the function developed above to download the file manually and place it where this function expects to find it!\n\nQuestion\n\nddir  = os.path.join('data','geo') # destination directory\nspath = 'https://github.com/jreades/i2p/blob/master/data/src/' # source path\n\nboros = gpd.read_file( ??(spath+'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( ??(spath+'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( ??(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n\n\n\n\nParquet\nLet’s re-use our cache_data function to download and save the full Inside Airbnb data set. Again, if you have trouble with downloading via code, use your understanding of the function to work out where to save your own copy of this file so that the function works as expected.\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\n\nymd  = '20240614'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.parquet'\n\n\nQuestion\n\n# your code here\ndf = pd.read_parquet( ??(??, os.path.join('data','raw')) )\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nYou should see that the file was ‘not found’ so ‘downloading’ happened and then the size of the data frame was printed out."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#creating-a-geodataframe",
    "href": "practicals/Practical-07-Spatial_Data.html#creating-a-geodataframe",
    "title": "Practical 7: Spatial Data",
    "section": "Creating a GeoDataFrame",
    "text": "Creating a GeoDataFrame\n\n\n\n\n\n\nDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nRight, we’re finally there! We need to convert our coordinates into some kind of geo-data. GeoPandas offers two ways to do this: the original way using zip and a new utility method called points_from_xy. Here’s the old way:\nfrom shapely.geometry import Point\ngdf = gpd.GeoDataFrame(df, \n                geometry=[Point(x,y) for x, y in zip(df.Longitude,df.Latitude)])\nNote, however, that this did not automatically set a projection, unlike the new approach with the ‘helper function’:\n\ngdf = gpd.GeoDataFrame(df,\n      geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))\n\n\nprint(type(gdf))\nprint(type(gdf.geometry))\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#saving-geo-data-locally",
    "href": "practicals/Practical-07-Spatial_Data.html#saving-geo-data-locally",
    "title": "Practical 7: Spatial Data",
    "section": "Saving Geo-Data Locally",
    "text": "Saving Geo-Data Locally\n\nThink Storage Formats\nWe want to save the InsideAirbnb GeoDataFrame to the ‘geo’ directory, but first let’s see what file formats are supported.\n\n\n\n\n\n\n\n\nFormat\nAdvantages\nDisadvantages\n\n\n\n\nShapefile\nWide support\nEasy to break via loss of individual file element and not very efficient format\n\n\nGeoJSON\nHuman-readable\nHighly inefficient storage format\n\n\nGeoPackage\nLightweight spatial database with benefits of indexing\nLightweight spatial database with limits on data types supported\n\n\nParquet\nHighly compressed columnar database\nLimited GIS support (except QGIS)\n\n\n\nGeoPackages are really well-supported by QGIS: you can even embed multiple layers with different style information so that your final analysis is easy to distribute; however, they also carry quite a bit of overhead that makes them inefficient for distributing smaller data sets, while also not supporting the full spectrum of Pythonic data structures such as categorical data or lists.\nGeoParquet is an extension of the Parquet format. For working with large data sets this has revolutionised my workflow: getting excited about a columnar database might seem a bit… nerdy… but it’s been transformative for many data scientists. First, because the data set is columnar you only read in the data that you need, so reading Parquet file is blindingly fast. Second, you can stream data from a Parquet file over the Internet, so that means you even gain these advantages reading remote files. Third, you have full support for Python data types, including (up to a point) objects. And, fourth, you can treat multiple Parquet files with the same layout as a single data set or mutiple Parquet files with different layouts as tables in a single database!\n\n\nSpecifying a Driver\nSince there are many formats in which to save geo-data, rather than have multiple to_format_x methods, GeoPandas has one for local files (to_file). If you are reading/writing a filename than ends in a valid extension (e.g. .shp, .gpkg, or .geojson) then GeoPandas will ‘do the right thing’. Where you may run into trouble is if you are reading/writing a URL (e.g. https://github.com/jreades/fsds/blob/master/data/src/Boroughs.gpkg?raw=true). With a URL ending in ?raw=true there’s no extension that GeoPandas can see so you will need to specify a driver. If in doubt, specify the driver.\n\n\n\n\n\n\nClarification\n\n\n\nIn this practical we are reading geodata from GitHub and I’m saying that we need to specify the driver. So why didn’t we need to do with the cache_data function earlier as well? Well, this was a side-benefit of using the standard URL library: it automatically stripped off the query string (?raw=true) when I asked it for the file name, so we saved the file locally as a GeoPackage with .gpkg extension, which means that GeoPandas could read it without any problems.\n\n\nSo the following two bits of code are equivalent:\n\n# This *may* not always do what we want, but should be fine for local files\nboros.to_file('test.gpkg') \n\n\n# This is safer if working across computers/the Internet\nboros.to_file('test.gpkg', driver='GPKG')\n\nSo, starting with fn = ‘20240614-listings.gpkg’\nNotice the difference:\n\nprint(f\"Using '{fn}' as basis for saving data...\")\ntry:\n   gdf.to_file(os.path.join('data','geo',fn), driver='GPKG')\nexcept TypeError as e:\n    print(\"Caught type error...\")\n    print(str(e)[:1000] + \" ...\")\n\nUsing '20240614-listings.gpkg' as basis for saving data...\nCaught type error...\nCannot interpret 'CategoricalDtype(categories=['Barn', 'Boat', 'Camper/RV', 'Campsite', 'Casa particular',\n                  'Castle', 'Dome', 'Earthen home', 'Entire bungalow',\n                  'Entire cabin', 'Entire chalet', 'Entire condo',\n                  'Entire cottage', 'Entire guest suite', 'Entire guesthouse',\n                  'Entire home', 'Entire home/apt', 'Entire loft',\n                  'Entire place', 'Entire rental unit',\n                  'Entire serviced apartment', 'Entire townhouse',\n                  'Entire vacation home', 'Entire villa', 'Farm stay', 'Floor',\n                  'Houseboat', 'Hut', 'Island', 'Minsu', 'Private room',\n                  'Private room in bed and breakfast', 'Private room in boat',\n                  'Private room in bungalow', 'Private room in cabin',\n                  'Private room in camper/rv',\n                  'Private room in casa particular', 'Private room in chalet',\n                  'Private room in condo', 'Private room  ...\n\n\nIf you try to save as a GeoPackage file then the code above typically throws a TypeError because of the presence of Categorical data.\nBut the below, in which we specify as a ‘geoparquet’ because of the coordinate data, does not:\n\nfn = fn.replace('.gpkg','.geoparquet')\ngdf.to_parquet(os.path.join('data','geo',fn))\nprint(\"Saved.\")\n\nSaved."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#spatial-indexing",
    "href": "practicals/Practical-07-Spatial_Data.html#spatial-indexing",
    "title": "Practical 7: Spatial Data",
    "section": "Spatial Indexing",
    "text": "Spatial Indexing\nIt’s also worth comparing the output of a point with the output of a polygon or multi-polygon because you may well come across data in formats (e.g. WKT) resembling both of these in real data sets and they can be read as well. Notice too that we can use loc and iloc accessor methods to pull individual points and polygons out of a GeoDataFrame!\n\nprint(gdf.geometry.iloc[1]) # Print out the object's contents\ngdf.geometry.iloc[1] # The object knows how to print itself as a point\n\nPOINT (-0.21707 51.49993)\n\n\n\n\n\n\n\n\n\n\n# Object to string then print out first 399 characters\nprint(str(boros.geometry.iloc[1])[:399] + \"...\") \n# So this is a multi-polygon boundary\nboros.geometry.iloc[1] \n\nMULTIPOLYGON (((535009.2 159504.7, 535005.5 159502, 535002.9 159500.1, 535000.1 159503.2, 535000 159503.2, 534747.8 159517.8, 534588 159522.1, 534512.3 159780.5, 534474.8 159760.8, 534398.8 159726.7, 534391.9 159723.3, 534378.9 159712.4, 534345.1 159702.8, 534314.3 159691.8, 534292.7 159683.9, 534253.1 159666.6, 534229.4 159657.9, 534207.5 159654.6, 534160.5 159651.5, 534159.7 159637.1, 534109.5 ...\n\n\n\n\n\n\n\n\n\nSo each element of this Series has text indicating the type of shape the geometry applies to (e.g. POLYGON) followed by a bunch of numbers. These numbers are truncated here just to make things a little more legible."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#work-out-the-colour-scheme",
    "href": "practicals/Practical-07-Spatial_Data.html#work-out-the-colour-scheme",
    "title": "Practical 7: Spatial Data",
    "section": "Work Out the Colour Scheme",
    "text": "Work Out the Colour Scheme\n\n\n\n\n\n\nDifficulty level: Moderate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nR and Python take very different approaches to plotting. Do not think of Python’s output as being ‘maps’ in the GIS sense, they are composed of ‘patches’ of color on abstract ‘axes’ that can use any arbitrary coordinate space. So colours are ‘really’ triplet (or quadruplet if you have alpha-blending transparency) values in the range 0.0-1.0. Annotations are then added in similarly abstract fashion.\n\n\nI’d suggest the following colour scheme as a way to test out different ways of specifying colour (though anything you like is fine so long as you manipulate the colours):\n\nThe boroughs can have red edges and white fill with a thick edge.\nThe water should have no edges and XKCD Lightblue fill.\nThe greenspace should have edges and faces specified using different ‘alpha blending’ (i.e. transparency) levels.\n\n\nBoroughs\nBy way of a hint, matplotlib uses edgecolor and facecolor for controlling ‘patches’ (which is what polygons are considered), but the thicker-than-default line-width is specified differently (you’ll need to look this up). So the intention is:\n\nThick red borough borders, and\nWhite fill colour.\n\nJust to drive home how different this is from R, you can find the answer to question 1 on the page for bar plots.\n\nQuestion\n\nboros.plot(??)\n\nYour plot should look similar to this:\n\n\n\n\n\n\n\n\n\n\n\n\nWater\nThe process is the same as above, but I’d like you to work out how to specify: 1. No color for an edge, and 2. An XKCD color for the face.\n\nQuestion\n\nwater.plot(??)\n\nYour plot should look similar to this:\n\n\n\n\n\n\n\n\n\n\n\n\nGreenspace\nThe process is also the same as above, but I’d like you to work out how to specify colours and transparency using RGBA (red-green-blue-alpha transparency) tuples. So we’re looking for: 1. No edge color. 2. A partially transparent green specified as a ‘tuple’ (4 numbers in parentheses in the range 0.0-1.0).\n\nQuestion\n\ngreen.plot(??)\n\nYour plot should look similar to this:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#combining-layers",
    "href": "practicals/Practical-07-Spatial_Data.html#combining-layers",
    "title": "Practical 7: Spatial Data",
    "section": "Combining Layers",
    "text": "Combining Layers\n\n\n\n\n\n\nWarning\n\n\n\nR and Python take very different approaches to plotting. Do not think of Python’s output as being ‘maps’ in the GIS sense, they are composed of ‘patches’ of color on abstract ‘axes’ that can use any arbitrary coordinate space. So colours are ‘really’ numerical triplets (or quadruplets if you have transparency as well) in the range 0.0-1.0. Annotations are then added in similarly abstract fashion.\n\n\nNow that we’ve got our layers looking roughly how we want them, it’s time to combine them. This is also reliant on matplotlib and basically involves plotting items to shared axes which is done by passing in ax=&lt;axis object&gt; to each plot(...). By convention, if you only have a single figure (e.g. a single map) then you create an axis object and name it ax so you will see a lot of ax=ax code in graphing libraries, but =ax is just saying ‘assign to the axis object that I created’.\nSince the axes are how you control what is shown, see if you can find out by Googling how to set the x- and y-limits on the map so that it shows only London and trims out the much larger area of water that is outside of the Greater London Authority. As a rough guideline, this has the Easting range 501,000 to 563,000, and the Northing range 155,000 to 202,000.\nYou can set these limits before or after you start adding layers to the ‘map’, but it’s probably easier conceptually to add them after with the idea of ‘zooming in’ on the features of interest. It’s also easier to debug since you can start by seeing if you can plot the elements at all, and then add the limits to zoom.\nSo the steps are:\n\nWrite the code to plot every image on the same set of axes (I’ve given you something to get started).\nGoogle how to set the limits of the map and then use the ranges I’ve offered above.\nWork out how to change the width of the edges for the boroughs layer.\nSave it somewhere local so that you could, say, load it into a Markdown file!\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a first pass at a map, over the next few weeks we’ll see how to add things like axis labels and titles to make it more ‘map-like’. We don’t have quite the built-in functionality of ggplot alas, but Python is advancing very quickly in this area. There is even an implementation of ggplot in Python, but it’s functionality is more limited. In fact, there’s more than one…\n\n\n\n\n\n\n\n\nDifficulty level: Hard\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n# Creates a new figure with specified number of\n# subplots (we'll see more of this later) and \n# and the specified size (in inches by default).\nfig, ax = plt.subplots(1,1, figsize=(8,6))\n\n# Plot all three GeoPackages to the same axes\nwater.plot(??, ax=ax)\ngreen.??\nboros.??\n\n# Set the x and y limits\n\n\n# Save the image (dpi is 'dots per inch')\nos.??('img', exist_ok=True)\nplt.savefig(os.path.join('img','My_First_Map.png'), dpi=150)\n\nYou may wish for a different look, but here’s one version of the output:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#choropleth-plots",
    "href": "practicals/Practical-07-Spatial_Data.html#choropleth-plots",
    "title": "Practical 7: Spatial Data",
    "section": "Choropleth Plots",
    "text": "Choropleth Plots\n\n\n\n\n\n\nDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nNow that we’ve converted the InsideAirbnb data to a GeoDataFrame, we can plot it, reproject it, etc.\nSee if you can work out how to plot the points coloured by their price using the appropriate BNG projection.\n\nQuestion\n\ngdf.to_crs(??).plot(column=??, cmap=??, alpha=0.25, markersize=1, figsize=(8,6));\n\nUsing the Viridis colourmap I get the following:\n\n\n\n\n\n\n\n\n\n\n\nWork out the Data Range\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nAs we saw above with the point-plot, in its original form the pricing data will not reveal much of interest because of the range of the data. However, as you will have seen in QM already (and as we explore in greater detail in Weeks 7/8), using transformations we can manipulate the data to increase its tractability for analysis.\nLet’s start by getting a feel for the full data set in terms of the range of prices that it contains:\n\nQuestion\n\nprint(f\"The range of price is ${??:,.2f} to ${??:,.2f}\")\nprint(f\"The mean and median of the price are ${??:,.2f} and ${??:,.2f}\")\n\nNotice the neat little comma-separated thousands in there? That’s fairly easy to do in English, but to use a thousands separator common to another language you would need to do something a little more tricky.\n\n\n\nInheritance!\nWe already know that GeoPandas inherits functionality from Pandas, but let’s formalise this…\nFirst, let’s check what class of object gdf is using the isinstance function:\n\n# Is gdf a GeoDataFrame object?\nif isinstance(gdf, gpd.GeoDataFrame): \n    print(\"\\tI'm a geopandas data frame!\")\n\n# Is gdf *also* a DataFrame object?\nif isinstance(gdf, pd.DataFrame): \n    print(\"\\tI'm a pandas data frame!\")\n\n    I'm a geopandas data frame!\n    I'm a pandas data frame!\n\n\n\n\nBenefiting from Inheritance\nThat result means that we can also investigate the data using, for instance, a pandas histogram:\n\nQuestion\n\n# Oooooh, let's use a *pandas* method here\ngdf.price.plot.??(bins=??, figsize=(8,3)); \n\nNotice how we’ve used our GeoDataFrame as if it’s a plain old DataFrame here? That’s the miracle of Object-Oriented Design: we can do anything we would with a regular Pandas df as we do with a GeoPandas gdf because GeoPandas inherits all the methods of its parent super-class.\nWe can see that there’s very little data above (at a guess) about $2,000, but at this scale it’s hard to tell. We’ve already seen that you can use axes limits to adjust the display of a map, but the same technique applies to plain old plots because they’re fundamentally the same thing.\nTry adjusting the axis so that the x-range is 0..2500:\nax = gdf.price.plot.??(bins=??, figsize=(8,3));\nax.??\nYou can do the same thing with a boxplot:\n\n\nQuestion\n\nax = gdf.price.plot.??(vert=False, figsize=(8,3))\nax.??\n\nMore complex formatting is also possible if you really know your pandas and your matplotlib:\n\ngdf.price.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#truncate-and-transform",
    "href": "practicals/Practical-07-Spatial_Data.html#truncate-and-transform",
    "title": "Practical 7: Spatial Data",
    "section": "Truncate and Transform",
    "text": "Truncate and Transform\n\n\n\n\n\n\nDifficulty level: Hard\n\n\n\n\n\n\n\n\n\n\nWorking it Out\nAnyway, drawing on everything we’ve seen over the past couple of weeks (and in this practical) I’d like you to:\n\nTry to take the natural-log of the price (hint: use numpy) and assign to a new Series called lnprice.\nWork out what the error means.\nWork out how to fix the error and then repeate step 1.\nWork out how many rows were affected.\nReport on the new min/max values.\nWork out if other outliers need to be removed (use code from above).\nRemove outliers and then continue with your work…\n\n# Use this as a 'scratch space' to work out what's needed below...\n\nQuestion\n\nprint(f\"gdf has {gdf.shape[0]:,.0f} rows.\")\n\n# ---------- Do the processing -------------\n# You may need more than one of these 'drops'\n# to get the data the way you want...\ngdf.drop(gdf[??].index, axis=0, inplace=True)\ngdf['lnprice'] = np.log(gdf.price)\n\n# ---------- Check effects -----------\nprint(f\"gdf now has {gdf.shape[0]:,.0f} rows.\")\nprint(f\"The range of price is {gdf.price.min():,.2f} to {gdf.price.max():,.2f}\")\nprint(f\"The range of ln(price) is {gdf.lnprice.min():,.4f} to {gdf.lnprice.max():,.4f}\")\n\ngdf.price.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);\nplt.title(\"Price (Outliers not shown)\")\nplt.show()\n\ngdf.lnprice.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);\nplt.title(\"Ln(Price) (Outliers not shown)\")\nplt.show()"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#plot-options",
    "href": "practicals/Practical-07-Spatial_Data.html#plot-options",
    "title": "Practical 7: Spatial Data",
    "section": "Plot Options",
    "text": "Plot Options\nNow plot the ln(price) as a chloropleth using:\n\nA figure size of 9 x 6\nA marker size of 0.25\nThe Viridis colourmap\nA legend\nA legend label of ‘Natural Log of Price per Night ($)’\n\nI’d suggest referring to the documentation.\n\nQuestion\n\nax = gdf.plot(figsize=??, marker='*', markersize=0.25, \n         column=??, cmap=??, \n         legend=??, legend_kwds=??);\nax.set_title(\"Plot of Natural Log of Nightly Price for Airbnb Listings (Outliers Removed)\");\n\nYou should get something like:\n\n\n\n\n\nNatural Log of Price per Night ($) with Outliers REmoved"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#zooming-inout",
    "href": "practicals/Practical-07-Spatial_Data.html#zooming-inout",
    "title": "Practical 7: Spatial Data",
    "section": "Zooming In/Out",
    "text": "Zooming In/Out\n\n\n\n\n\n\nDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nThat’s a little hard to see, let’s try zooming in on Central London! Very roughly, let’s call that an Easting range of 525,000 to 535,000 and a Northing range of 178,000 to 185,000.\n\n\n\n\n\n\nPlotting\n\n\n\nWe show one way to do this below (f,ax = plt.subplots(...)) because it gives you greater control, but gdf.plot() can return an axis object (ax = gdf.plot(...)) that gives you the same kind of access… but with a bit more ‘faff’.\n\n\n\nQuestion\n\n# Note this new f,ax syntax and that we then\n# pass ax=ax to gdf.plot -- this has to do with\n# where and how things are plotted.\nf,ax = plt.subplots(1,1,figsize=(8,6))\ngdf.plot(ax=ax, marker='*', markersize=0.25, \n         column='lnprice', cmap='viridis', \n         legend=True, legend_kwds={'label':'Natural Log of Price per Night ($)'});\nax.set_title(\"Ln(Price/Night) for Airbnb Listings (Central London Detail)\")\nax.??\nax.??\nf\n\nYour result should look something like this:\n\n\n\n\n\nNatural Log of Price per Night ($)\n\n\n\n\nThat’s a little better, but ideally we’d do more thinking about outliers…"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#changing-the-classification-scheme",
    "href": "practicals/Practical-07-Spatial_Data.html#changing-the-classification-scheme",
    "title": "Practical 7: Spatial Data",
    "section": "Changing the Classification Scheme",
    "text": "Changing the Classification Scheme\n\n\n\n\n\n\nDifficulty Level: Moderate (mainly computation time)\n\n\n\n\n\n\n\n\n\nLet’s give this one last try using the quantiles classification scheme for Central London!\n\nQuestion\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax = gdf.plot(marker='*', markersize=0.25, \n         column='lnprice', cmap='viridis', ??, k=5, \n         legend=True, ax=ax); \n         # Note that the legend *label* had to go -- \n         # there are other ways to add it.\n\nax.set_xlim([525000,535000])\nax.set_ylim([178000,185000])\n\nYour answer should look like:\n\n\n\n\n\nNatural Log of Price per Night ($) in Central London"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#credits",
    "href": "practicals/Practical-07-Spatial_Data.html#credits",
    "title": "Practical 7: Spatial Data",
    "section": "Credits!",
    "text": "Credits!\n\nContributors:\nThe following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk), James Millington (james.millington@kcl.ac.uk)\n\n\nLicense\nThese teaching materials are licensed under a mix of The MIT License and the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license.\n\n\nAcknowledgements:\nSupported by the Royal Geographical Society (with the Institute of British Geographers) with a Ray Y Gildea Jr Award.\n\n\nPotential Dependencies:\nThis notebook may depend on the following libraries: geopandas, pandas, matplotlib, seaborn"
  },
  {
    "objectID": "readings/Donoho-2017.html",
    "href": "readings/Donoho-2017.html",
    "title": "50 Years of Data Science",
    "section": "",
    "text": "Here’s a detailed breakdown of the questions in relation to David Donoho’s paper “50 Years of Data Science”:\n\nWhat kind of reading is it?\n\nType of Document: This is an academic paper, delivered as a technical lecture at the Tukey Centennial Workshop in 2015.\nContribution Type: The paper is conceptual and theoretical. It reflects on the evolution of data science, tracing its history, comparing it with traditional statistics, and proposing ideas for the future direction of the field. It doesn’t present new empirical data or experiments but instead offers a critical reflection on the field and a framework for thinking about its development.\n\n\n\nWho is the intended audience?\n\nAudience: The primary audience for this paper includes:\n\nAcademics and Field Experts: The paper targets statisticians, data scientists, and those involved in academia or research. It discusses in-depth technical and philosophical issues in statistics and data science that would resonate with professionals in these fields.\nResearchers in Related Disciplines: It can also serve researchers in fields like computer science, machine learning, and applied mathematics.\n\nHow do we know?:\n\nThe technical language, historical overview, and references to statistical theory and scientific methods make it clear that it is not written for a general public or policy-makers.\nThe discussion assumes familiarity with key statistical figures (like Tukey), theories, and computational methods, indicating that the audience is expected to have a solid understanding of the subject matter.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: Sets up the motivation for the paper and presents the concept of data science as a distinct discipline.\nHistorical Perspective: Explores the origins and evolution of data science, tracing its roots back to John Tukey’s work in exploratory data analysis.\nComparison of Statistics and Data Science: Highlights key distinctions between traditional statistics and the emerging field of data science.\nChallenges for the Future: Discusses unresolved issues and suggests ways in which the field might develop over the next few decades.\n\nResponse to Audience and Reading Type:\n\nThe structure is academic and follows a clear logical progression, from laying out historical background to proposing future directions. This responds to the needs of an expert audience, providing a deep conceptual reflection rather than a “how-to” guide for practitioners.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nData Science as a Field: Donoho argues that data science is not just a rebranding of statistics but a distinct discipline that encompasses new tools, techniques, and paradigms driven by the rise of big data and computational power.\nCriticism of Narrow Views of Statistics: He critiques traditional statistics for being too focused on formal models and not embracing the broader data-driven approaches seen in machine learning and data science.\nThe Role of Algorithms and Computing: The paper emphasizes the importance of computation and algorithms as a core part of data science, diverging from classical statistical methods.\nExploratory Data Analysis (EDA): Building on Tukey’s idea of EDA, Donoho highlights how exploration of data, rather than fitting models, has become central to the work of data scientists.\n\nHow do we know?: These ideas are made explicit throughout the paper, with sections dedicated to the contrast between statistics and data science, and the historical context provided through references to foundational figures like John Tukey.\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nDonoho’s paper acts as a conceptual roadmap, defining data science as a field that is separate from traditional statistics and highlighting the key methodological and philosophical differences. It contributes to the ongoing debate over what data science should encompass and lays out a vision for the field’s future development.\n\nWhat gap does it respond to?:\n\nIt addresses the gap in understanding between traditional statisticians and those who work in the broader data science community, particularly with regard to the importance of algorithms and the role of exploratory methods.\nIt also touches on the need to recognize data science as a distinct discipline, not just a subfield of statistics or computer science.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nContext Dependence: While Donoho argues that data science is distinct, the boundaries between data science, machine learning, and statistics are still somewhat fluid. The paper doesn’t fully address how these fields should coexist or cross-pollinate.\nPotential Gaps in Methodology: There’s limited discussion on the integration of qualitative research methodologies, which could provide additional perspectives in fields like social sciences or healthcare.\n\nFuture Work:\n\nDonoho suggests the need for ongoing development in the education of data scientists, advocating for the inclusion of ethical considerations and interdisciplinary approaches.\nHe calls for a greater focus on reproducibility and data provenance, which remain critical challenges in the field.\n\nBroader Applicability:\n\nThe paper is relevant in other fields where data-driven decision-making is essential (e.g., social sciences, economics, and public policy), but it might not address specific challenges related to non-technical domains like interpretability or ethics in certain societal contexts. The broader societal implications of data science are an area for future development.\n\n\n\n\nConclusion\nDonoho’s paper “50 Years of Data Science” offers a reflective and conceptual contribution to the debate about what constitutes data science as a discipline. It builds on historical foundations and responds to the gaps in the understanding of its relationship to statistics and other computational fields. While it offers valuable insights, it also leaves room for future work on methodological challenges and interdisciplinary collaborations.\n\n\n\n\n\nReferences\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71."
  },
  {
    "objectID": "readings/Franklin-2024.html",
    "href": "readings/Franklin-2024.html",
    "title": "Quantitative methods III: Strength in numbers?",
    "section": "",
    "text": "Here’s a detailed breakdown of “Quantitative Methods III: Strength in Numbers?” by Rachel Franklin, addressing your questions:\n\nWhat kind of reading is it?\n\nType of Document: This is an academic journal article published in the journal Progress in Human Geography. It is the third in a series discussing quantitative methods in geography.\nContribution Type: The paper makes a conceptual and critical contribution. It reflects on the growing influence of quantitative methods within human geography, examining their role, limitations, and the broader implications of relying heavily on numerical data. It does not present original empirical findings but critically assesses the field and suggests areas for reflection.\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe paper is written primarily for academics and experts in the field of human geography, particularly those interested in the methodological debates within the discipline. It engages with scholars and researchers who are familiar with the ongoing discourse on quantitative vs. qualitative approaches in geography.\n\nHow do we know?:\n\nThe language and concepts used in the article are highly specialized and assume a background in geography and an understanding of quantitative methods. It references theoretical discussions and trends within human geography, which suggests that it is targeting a well-informed, scholarly audience.\nThe article doesn’t aim to teach new skills or offer practical advice for policymakers, but rather to engage with methodological debates within academia.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: Sets the context for the discussion and reviews recent trends in the use of quantitative methods in human geography.\nMain Sections:\n\nThe paper is divided into several sections, each addressing different aspects of quantitative methods. It covers topics such as:\n\nThe growing emphasis on big data and data analytics.\nThe balance between quantitative and qualitative methods in the discipline.\nThe critical perspective on the dominance of numerical methods.\n\n\nConclusion: Reflects on the potential risks of over-reliance on quantitative approaches and offers suggestions for the future.\n\nResponse to Audience and Reading Type:\n\nThe structure of the article is analytical and discursive, with each section building on the previous one to present a cohesive argument. This structure suits the intended academic audience, facilitating a reflective critique of quantitative methods and their role in human geography.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nBig Data and Quantification: The paper discusses the increasing role of big data and how the availability of vast amounts of information has led to a rise in the use of quantitative techniques in geography.\nCritique of Quantitative Dominance: Franklin critiques the dominance of quantitative methods, arguing that while numbers can provide strength, they can also mask important contextual and qualitative insights. She questions whether this reliance on data may overlook the nuances of human experiences and socio-spatial processes.\nBalancing Quantitative and Qualitative Approaches: The article emphasizes the need to balance quantitative methods with qualitative approaches to avoid a one-dimensional view of geographic phenomena.\n\nHow do we know?:\n\nThese ideas are explicitly discussed throughout the paper. Franklin references the growing use of computational tools and big data in geography and critiques the risks associated with relying too heavily on these methods. The argument for balance between qualitative and quantitative methods is a central theme.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nThe paper contributes to the ongoing methodological debate in human geography about the role of quantitative methods. It provides a critical reflection on the growing reliance on big data and advanced analytics in the field and calls for a more thoughtful integration of qualitative perspectives.\n\nWhat gap does it respond to?:\n\nIt addresses the gap between the rising dominance of quantitative methods and the need for a more nuanced understanding of social and spatial processes, which qualitative methods often provide. Franklin argues that the increased focus on big data may be narrowing the scope of geographic inquiry.\n\nKey Findings or Conclusions:\n\nThe main conclusion is that while quantitative methods offer valuable insights, human geography should be cautious about over-reliance on them. There is a risk of losing sight of important social and contextual factors that are not easily quantifiable.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nOver-reliance on Numbers: Franklin highlights the assumption that quantitative methods are inherently objective or superior. This assumption may not hold in contexts where human experiences and socio-spatial dynamics are complex and difficult to quantify.\nContexts Where It Applies: The reading is particularly relevant in fields where both qualitative and quantitative methods are used to understand human behavior and societal processes, such as urban studies, social geography, or policy research. In these contexts, a balanced approach is essential to capture both numerical trends and individual experiences.\n\nFuture Work Identified:\n\nFranklin suggests future work should explore how quantitative methods can be better integrated with qualitative insights. She calls for methodological pluralism, where both approaches are valued and used in tandem to provide a fuller understanding of geographical issues.\n\nWhy Is the Contribution Valid?\n\nThe contribution is valid because it highlights a critical concern in the discipline: the need to avoid methodological silos. By advocating for a balance between qualitative and quantitative methods, Franklin addresses a longstanding debate that is relevant to many fields beyond geography.\n\n\n\n\nConclusion\nRachel Franklin’s “Quantitative Methods III: Strength in Numbers?” provides a reflective and critical contribution to discussions about the role of quantitative methods in human geography. It responds to the increasing emphasis on big data and computational approaches, cautioning against over-reliance on these methods. The paper makes a compelling case for balancing quantitative and qualitative methods to ensure a more comprehensive understanding of geographical phenomena, leaving room for further exploration of how these approaches can be integrated more effectively in future research.\n\n\n\n\n\nReferences\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512."
  },
  {
    "objectID": "readings/Massey-1996.html",
    "href": "readings/Massey-1996.html",
    "title": "Politicising Space and Place.",
    "section": "",
    "text": "Here’s a detailed analysis of Doreen Massey’s work “Politicising Space and Place” based on your questions:\n\nWhat kind of reading is it?\n\nType of Document:\n\n“Politicising Space and Place” is an academic essay. It engages with critical geographical theory and offers conceptual and theoretical insights into how space and place are inherently political concepts.\n\nContribution Type:\n\nThe essay makes a conceptual and theoretical contribution. Massey seeks to reshape our understanding of space and place, arguing against static, fixed ideas of these concepts and instead proposing that they are dynamic, relational, and political. Her work challenges established geographical and social thought and promotes a more progressive, pluralistic view of spatiality.\n\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe piece is primarily intended for an academic and expert audience, particularly those in fields such as human geography, sociology, political theory, and cultural studies.\n\nHow do we know?:\n\nMassey uses sophisticated theoretical arguments and critiques established thinkers and ideas within geography. The discussion engages with abstract concepts like relational space, the politics of place, and social theory, which suggests it’s written for an audience familiar with these debates.\nThe essay is not aimed at practitioners or policymakers seeking immediate solutions but rather for scholars and thinkers engaging in critical debates about the conceptualization of space and place.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: Sets up the motivation for the essay, establishing space and place as under-theorized or misinterpreted in popular and academic discourse.\nKey Theoretical Sections:\n\nThe essay moves through several theoretical discussions about space and place. It critiques traditional, fixed understandings of these terms, offering instead a view of space as relational and political.\nMassey also introduces the notion that spaces are produced through social relations and power dynamics.\n\nConclusion: Reinforces her arguments that space and place should be politicized, providing a call for new ways to think about these terms as fluid, open, and influenced by global forces.\n\nResponse to Audience and Reading Type:\n\nThe structure is academic, focusing on a theoretical critique followed by an alternative proposition. It is intended to engage scholars in conceptual debates and is organized in a way that facilitates this kind of critical, reflective reading.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nRelational Space: Massey challenges the idea that space is a neutral, empty container where things happen. Instead, she proposes that space is always relational, produced through interactions between people, places, and events.\nPolitical Nature of Space and Place: Massey argues that space and place are inherently political. They are shaped by power relations, inequalities, and struggles, and are contested by different social groups.\nChallenging Fixed and Essentialist Views of Place: The essay critiques the idea of place as something fixed, bounded, or essential. Instead, Massey argues for a view of place as dynamic, constantly changing through interactions with broader global processes.\nGlobalization and Spatial Politics: She discusses how globalization changes our understanding of place, complicating the local and the global. Places are not isolated or self-contained but are interconnected through global flows of people, goods, and ideas.\n\nHow do we know?:\n\nThese ideas are central throughout the essay, as Massey explicitly critiques the conventional ways of thinking about space and place. Her argument that space is relational and political, rather than neutral or natural, is a core theme.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nMassey’s essay challenges traditional geographical and social thought by politicizing space and place. She provides a new framework for thinking about these concepts as relational, dynamic, and shaped by social and power relations.\n\nWhat gap does it respond to?:\n\nThe essay responds to the gap in geographical and social theory where space and place have often been treated as static, neutral, or apolitical. Massey pushes back against these conceptions and calls for a more politically informed understanding of spatiality.\n\nKey Findings or Conclusions:\n\nThe main conclusion is that space and place should be understood as political, relational, and constantly shaped by social processes. Massey’s work emphasizes that space is not merely a backdrop to social action but is actively involved in the production and reproduction of social relations and power structures.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nContext Dependence: One potential issue is how Massey’s relational understanding of space and place applies across different contexts. While her framework is compelling, it may need further adaptation for more specific or local case studies, such as those in non-Western settings or within marginalized communities.\nTheoretical Complexity: Massey’s work is highly theoretical and abstract, which could make it difficult to apply directly in policy-making or practical contexts. Bridging the gap between theory and practice remains a challenge for scholars who engage with her ideas.\n\nFuture Work Identified:\n\nMassey’s framework invites further exploration of how space and place are produced in various contexts, particularly in light of ongoing globalization and socio-political changes. Future work could investigate how her ideas about relational space and spatial politics manifest in specific empirical settings, such as urban spaces, border regions, or postcolonial contexts.\n\nBroader Applicability:\n\nMassey’s concepts are highly applicable to case studies involving issues like urban development, migration, and global capitalism, where space and place are clearly influenced by social and economic forces. For example, in rapidly globalizing cities, the relational and political nature of space can be examined in terms of gentrification or the displacement of communities.\n\n\n\n\nConclusion\nDoreen Massey’s “Politicising Space and Place” makes a significant conceptual and theoretical contribution to the understanding of space and place in human geography. By challenging traditional, static views of these terms and proposing a relational, political framework, Massey reshapes the way we think about spatiality and its role in social and power relations. While her ideas are theoretically complex and may be difficult to apply in policy or practice, they offer a rich foundation for future research and critique in various geographical and social contexts.\n\n\n\n\n\nReferences\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458."
  },
  {
    "objectID": "readings/DIgnazio-2020ch3.html",
    "href": "readings/DIgnazio-2020ch3.html",
    "title": "Data Feminism. Ch.3, On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints",
    "section": "",
    "text": "Here’s a detailed analysis of Chapter 3 of “Data Feminism” by Catherine D’Ignazio and Lauren F. Klein based on your questions:\n\nWhat kind of reading is it?\n\nType of Document:\n\n“Data Feminism” is a book aimed at both academic and non-academic readers. Chapter 3 in particular serves as a theoretical and conceptual contribution to the intersection of data science and feminist theory.\n\nContribution Type:\n\nThe chapter is largely conceptual and theoretical. It presents feminist principles as a framework for understanding and challenging power dynamics embedded in data science. It doesn’t present empirical research but focuses on reshaping how we think about data practices through a feminist lens.\n\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe intended audience includes a mix of academics, data practitioners, activists, and the general public interested in social justice, feminism, and data science.\n\nHow do we know?:\n\nThe language used is accessible, but the arguments are grounded in academic theory and social critique. While the authors refer to academic works and feminist theory, they also explain these ideas in a way that is understandable to a broader audience, including those working in data-related fields or people interested in activism.\nThe book is designed to reach a wide audience, including those in tech, data, or policy-making positions, and those involved in social activism, providing them with a critical framework for evaluating data practices.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: The chapter begins by framing the importance of looking at data through a feminist lens and discusses the power structures embedded in data collection, analysis, and interpretation.\nMain Sections:\n\nThe chapter is divided into several sections, each addressing a different feminist principle or approach to data science, such as power dynamics, intersectionality, and the need for participatory methods in data work.\nCase Studies or Examples: The authors often introduce real-world examples to illustrate how feminist principles can be applied to data practices, making abstract concepts more tangible.\n\nConclusion: The chapter ends by synthesizing the ideas presented, reinforcing the need for feminist approaches to challenge oppressive structures within data science.\n\nResponse to Audience and Reading Type:\n\nThe structure is both expository and illustrative, suited to readers who may be encountering these ideas for the first time as well as those looking for deeper theoretical insights. The clear sections and examples support readers in understanding and applying the ideas.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nFeminist Approaches to Data: The chapter argues that data science, like other fields, is shaped by existing power structures and often reflects systemic inequalities. By applying feminist principles, we can uncover and challenge these biases.\nIntersectionality: One of the core feminist principles discussed is intersectionality, which recognizes that individuals experience oppression in multiple, intersecting ways (e.g., race, gender, class). The authors argue that data science must take intersectionality into account to avoid reproducing harmful biases.\nChallenging Power Structures in Data: The authors emphasize the importance of understanding how power operates in data collection, analysis, and interpretation, noting that who controls the data often shapes whose voices are heard and whose experiences are marginalized.\nData as a Tool for Justice: The chapter presents the idea that data, when approached with feminist principles, can be used as a tool for social justice by highlighting inequities and advocating for marginalized groups.\n\nHow do we know?:\n\nThese ideas are directly stated and discussed throughout the chapter. The authors are explicit in connecting feminist theory to data science, offering a critique of traditional data practices and suggesting alternatives that are rooted in social justice.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nThe chapter contributes to the growing body of work that critiques mainstream data practices from a social justice perspective. It offers feminist theory as a valuable framework for reshaping data science, making it more inclusive, ethical, and just.\n\nWhat gap does it respond to?:\n\nIt responds to a gap in both data science and feminist theory: the lack of attention to how data practices can reinforce or challenge existing power dynamics. Traditional data science often lacks a critical lens that considers issues of bias, power, and inequality, which the chapter seeks to address.\n\nKey Findings or Conclusions:\n\nThe key conclusion is that data science, like any field, is not neutral. It is shaped by human decisions and societal structures. Therefore, applying feminist principles—like examining power dynamics and considering intersectionality—can lead to more equitable and ethical data practices.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nApplicability in Different Contexts: While the chapter makes a strong theoretical case for feminist data practices, its applicability in different cultural or institutional contexts might vary. For example, organizations with deeply entrenched power structures may resist implementing feminist approaches to data, even if they are proven to be more ethical or inclusive.\nChallenges of Implementation: One of the potential gaps is the difficulty of translating these principles into practice in highly technical environments, where the focus is often on efficiency and quantification rather than justice or inclusion.\nTheoretical Foundations vs. Practical Tools: While the chapter provides an excellent conceptual framework, readers interested in practical strategies for integrating feminist principles into data science may need additional resources or guidance.\n\nOther Case Studies or Contexts:\n\nThe ideas in this chapter could be applied to various case studies involving surveillance, predictive policing, healthcare data, or social media algorithms. For instance, in contexts where algorithms disproportionately target marginalized communities (e.g., facial recognition or risk assessments in criminal justice), a feminist analysis would reveal the systemic biases embedded in these tools.\n\nFuture Work Identified:\n\nThe book as a whole suggests there is more work to be done in creating participatory, inclusive, and intersectional approaches to data science. Future work could explore how to institutionalize these feminist principles in data-driven industries, particularly in AI, government policy, and big tech.\n\n\n\nConclusion\nChapter 3 of “Data Feminism” offers a powerful conceptual and theoretical contribution to data science by introducing feminist principles as a framework for critiquing and reshaping the field. It highlights the importance of understanding power dynamics, intersectionality, and the potential for data to act as a tool for justice. While the chapter is rich in theoretical insight, its practical application may vary depending on the context. Future work could focus on bridging the gap between theory and practice, particularly in highly technical environments or those resistant to social justice-driven change.\n\n\n\n\n\n\nReferences\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "readings/dignazio-2020ch3.html",
    "href": "readings/dignazio-2020ch3.html",
    "title": "Data Feminism. Ch.3, On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints",
    "section": "",
    "text": "Here’s a detailed analysis of Chapter 3 of “Data Feminism” by Catherine D’Ignazio and Lauren F. Klein based on your questions:\n\nWhat kind of reading is it?\n\nType of Document:\n\n“Data Feminism” is a book aimed at both academic and non-academic readers. Chapter 3 in particular serves as a theoretical and conceptual contribution to the intersection of data science and feminist theory.\n\nContribution Type:\n\nThe chapter is largely conceptual and theoretical. It presents feminist principles as a framework for understanding and challenging power dynamics embedded in data science. It doesn’t present empirical research but focuses on reshaping how we think about data practices through a feminist lens.\n\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe intended audience includes a mix of academics, data practitioners, activists, and the general public interested in social justice, feminism, and data science.\n\nHow do we know?:\n\nThe language used is accessible, but the arguments are grounded in academic theory and social critique. While the authors refer to academic works and feminist theory, they also explain these ideas in a way that is understandable to a broader audience, including those working in data-related fields or people interested in activism.\nThe book is designed to reach a wide audience, including those in tech, data, or policy-making positions, and those involved in social activism, providing them with a critical framework for evaluating data practices.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: The chapter begins by framing the importance of looking at data through a feminist lens and discusses the power structures embedded in data collection, analysis, and interpretation.\nMain Sections:\n\nThe chapter is divided into several sections, each addressing a different feminist principle or approach to data science, such as power dynamics, intersectionality, and the need for participatory methods in data work.\nCase Studies or Examples: The authors often introduce real-world examples to illustrate how feminist principles can be applied to data practices, making abstract concepts more tangible.\n\nConclusion: The chapter ends by synthesizing the ideas presented, reinforcing the need for feminist approaches to challenge oppressive structures within data science.\n\nResponse to Audience and Reading Type:\n\nThe structure is both expository and illustrative, suited to readers who may be encountering these ideas for the first time as well as those looking for deeper theoretical insights. The clear sections and examples support readers in understanding and applying the ideas.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nFeminist Approaches to Data: The chapter argues that data science, like other fields, is shaped by existing power structures and often reflects systemic inequalities. By applying feminist principles, we can uncover and challenge these biases.\nIntersectionality: One of the core feminist principles discussed is intersectionality, which recognizes that individuals experience oppression in multiple, intersecting ways (e.g., race, gender, class). The authors argue that data science must take intersectionality into account to avoid reproducing harmful biases.\nChallenging Power Structures in Data: The authors emphasize the importance of understanding how power operates in data collection, analysis, and interpretation, noting that who controls the data often shapes whose voices are heard and whose experiences are marginalized.\nData as a Tool for Justice: The chapter presents the idea that data, when approached with feminist principles, can be used as a tool for social justice by highlighting inequities and advocating for marginalized groups.\n\nHow do we know?:\n\nThese ideas are directly stated and discussed throughout the chapter. The authors are explicit in connecting feminist theory to data science, offering a critique of traditional data practices and suggesting alternatives that are rooted in social justice.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nThe chapter contributes to the growing body of work that critiques mainstream data practices from a social justice perspective. It offers feminist theory as a valuable framework for reshaping data science, making it more inclusive, ethical, and just.\n\nWhat gap does it respond to?:\n\nIt responds to a gap in both data science and feminist theory: the lack of attention to how data practices can reinforce or challenge existing power dynamics. Traditional data science often lacks a critical lens that considers issues of bias, power, and inequality, which the chapter seeks to address.\n\nKey Findings or Conclusions:\n\nThe key conclusion is that data science, like any field, is not neutral. It is shaped by human decisions and societal structures. Therefore, applying feminist principles—like examining power dynamics and considering intersectionality—can lead to more equitable and ethical data practices.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nApplicability in Different Contexts: While the chapter makes a strong theoretical case for feminist data practices, its applicability in different cultural or institutional contexts might vary. For example, organizations with deeply entrenched power structures may resist implementing feminist approaches to data, even if they are proven to be more ethical or inclusive.\nChallenges of Implementation: One of the potential gaps is the difficulty of translating these principles into practice in highly technical environments, where the focus is often on efficiency and quantification rather than justice or inclusion.\nTheoretical Foundations vs. Practical Tools: While the chapter provides an excellent conceptual framework, readers interested in practical strategies for integrating feminist principles into data science may need additional resources or guidance.\n\nOther Case Studies or Contexts:\n\nThe ideas in this chapter could be applied to various case studies involving surveillance, predictive policing, healthcare data, or social media algorithms. For instance, in contexts where algorithms disproportionately target marginalized communities (e.g., facial recognition or risk assessments in criminal justice), a feminist analysis would reveal the systemic biases embedded in these tools.\n\nFuture Work Identified:\n\nThe book as a whole suggests there is more work to be done in creating participatory, inclusive, and intersectional approaches to data science. Future work could explore how to institutionalize these feminist principles in data-driven industries, particularly in AI, government policy, and big tech.\n\n\n\nConclusion\nChapter 3 of “Data Feminism” offers a powerful conceptual and theoretical contribution to data science by introducing feminist principles as a framework for critiquing and reshaping the field. It highlights the importance of understanding power dynamics, intersectionality, and the potential for data to act as a tool for justice. While the chapter is rich in theoretical insight, its practical application may vary depending on the context. Future work could focus on bridging the gap between theory and practice, particularly in highly technical environments or those resistant to social justice-driven change.\n\n\n\n\n\n\nReferences\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "readings/dignazio-2020ch5.html",
    "href": "readings/dignazio-2020ch5.html",
    "title": "Data Feminism. Ch.5, Unicorns Janitors, Ninjas, Wizards, and Rock Stars",
    "section": "",
    "text": "Here’s a detailed analysis of Chapter 5 of “Data Feminism” by Catherine D’Ignazio and Lauren F. Klein based on your questions:\n\nWhat kind of reading is it?\n\nType of Document:\n\n“Data Feminism” is a book with both academic and general public appeal. Chapter 5 is a mix of conceptual, theoretical, and empirical contributions, as it presents feminist concepts but also uses real-world case studies and examples to apply and illustrate those concepts.\n\nContribution Type:\n\nChapter 5 is primarily conceptual and theoretical, offering insights into how feminist theory can be applied to the collection, analysis, and representation of data. However, it also makes empirical contributions by including examples of data projects that have applied feminist principles in practice.\n\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe chapter is intended for a broad audience, including academics, data scientists, field experts, activists, and the general public.\n\nHow do we know?:\n\nThe authors balance sophisticated theoretical arguments with accessible language and use of real-world case studies, making the content approachable for readers who may not have an academic background but are interested in social justice, feminism, or data science.\nThe book is designed to be informative for those in tech and data fields as well as individuals involved in activism or policy-making, providing them with feminist approaches to interrogate and reshape data practices.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: The chapter begins with a general framing of the key theme, setting up the feminist principles that will be applied to data.\nMain Sections:\n\nThe chapter is organized into distinct sections that break down feminist principles and apply them to issues of data collection, analysis, and representation.\nCase Studies and Examples: The authors use case studies to illustrate how feminist interventions in data practices can address issues of bias, exclusion, or injustice.\n\nConclusion: The chapter wraps up with a synthesis of the ideas discussed, reinforcing the necessity of applying feminist insights to create more equitable data practices.\n\nResponse to Audience and Reading Type:\n\nThe structure is designed to be both informative and illustrative, using a combination of theoretical exposition and real-world examples to engage a broad audience. This helps ensure that both academic and general readers can grasp the key concepts and see their practical applications.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nChallenging Objectivity in Data Science: One of the central ideas in this chapter is the critique of data science’s claim to objectivity. The authors argue that data is never neutral; it always reflects the biases of those who collect, analyze, and interpret it.\nPower in Data Representation: The chapter emphasizes that the way data is represented has the potential to reproduce or challenge existing power structures. The authors discuss how feminist approaches to data representation can illuminate marginalized voices and experiences that are often excluded or misrepresented in traditional data practices.\nData Justice and Equity: A key theme is the idea that data practices must actively work toward social justice and equity. Feminist data practices are not only about identifying bias but about rethinking the entire process of how data is collected, analyzed, and represented to ensure fairness and inclusivity.\nData for Collective Good: The authors promote the use of data for collective empowerment, particularly for marginalized communities, and advocate for data science to be more participatory, ensuring that those affected by data collection are included in the process.\n\nHow do we know?:\n\nThese ideas are articulated throughout the chapter, as the authors explicitly critique existing practices and provide feminist alternatives. They also use case studies to illustrate how these principles can be applied in practice.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nChapter 5 makes a significant contribution by presenting feminist principles as a framework for rethinking data science practices. It critiques the myth of data objectivity and advocates for more inclusive, participatory approaches that prioritize equity and justice.\n\nWhat gap does it respond to?:\n\nThe chapter responds to the gap in data science where issues of power, representation, and bias are often overlooked or under-theorized. By introducing feminist perspectives, the authors challenge traditional approaches and call for a rethinking of how data is collected, analyzed, and shared.\n\nKey Findings or Conclusions:\n\nThe chapter concludes that data is not neutral, and it is critical to apply a feminist framework to challenge existing power structures and ensure that data practices are inclusive and just. Feminist data practices are essential for creating a more equitable world where data serves the needs of all people, especially marginalized groups.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nApplicability Across Different Contexts: One challenge is the potential difficulty in applying feminist data principles in large institutions or corporate settings where efficiency and profit often take precedence over ethical concerns. Although the authors present compelling arguments, the translation of these principles into corporate or governmental contexts may be met with resistance.\nNeed for Practical Guidance: While the chapter provides theoretical insights and some examples, there might be a gap in terms of offering detailed, practical strategies for data scientists or organizations on how to implement feminist data practices in day-to-day work. More specific tools or frameworks for applying these principles in real-world data projects could be beneficial.\n\nOther Case Studies or Contexts:\n\nThese ideas can be applied to a wide range of contexts, particularly those involving marginalized communities or social justice issues, such as public health data, policing and criminal justice, or algorithmic bias in social media platforms. For example, in the context of healthcare, feminist data practices could ensure that the needs of underrepresented groups (e.g., women, minorities, and LGBTQ+ individuals) are not overlooked in research and policy-making.\n\nFuture Work Identified:\n\nFuture work could focus on scaling feminist data practices in institutional contexts, particularly in large-scale data initiatives that involve government agencies or tech companies. Additionally, there is room for more research into how feminist data principles can be applied to emerging technologies, such as artificial intelligence and machine learning, where biases often become deeply embedded.\n\n\n\n\nConclusion\nChapter 5 of “Data Feminism” provides a critical framework for rethinking data practices through a feminist lens. It challenges the myth of data objectivity, emphasizing the importance of inclusivity, equity, and social justice in data science. The chapter’s contributions are both conceptual and empirical, as it critiques traditional approaches and offers feminist principles as an alternative. While the ideas presented are valuable, there are challenges in applying these principles in certain institutional contexts, and future work could explore more practical strategies for implementing feminist data practices in a variety of settings.\n\n\n\n\n\nReferences\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. “Data Feminism.” In. MIT Press. https://bookbook.pubpub.org/data-feminism."
  },
  {
    "objectID": "readings/Etherington-2016.html",
    "href": "readings/Etherington-2016.html",
    "title": "Teaching introductory GIS programming to geographers using an open source Python approach",
    "section": "",
    "text": "Here’s a detailed analysis of Etherington’s “Teaching introductory GIS programming to geographers using an open source Python approach” based on your questions:\n\nWhat kind of reading is it?\n\nType of Document:\n\nEtherington’s work is an academic paper that can be classified as both a technical and instructional contribution. The paper is designed to share insights about teaching GIS (Geographic Information Systems) programming using an open-source Python approach to a geography-focused audience.\n\nContribution Type:\n\nThe contribution is primarily empirical and pedagogical. It draws from the author’s teaching experiences and proposes methods for effectively teaching GIS programming with Python to students who may not have a strong programming background. The paper also highlights the challenges and successes encountered when implementing this approach in the classroom.\n\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe paper is aimed at academics and educators in geography, GIS specialists, and field experts who are interested in incorporating programming into GIS education. It may also appeal to curriculum developers in higher education who are looking to enhance GIS teaching with open-source tools like Python.\nHow do we know?\n\nThe content is focused on the teaching of technical skills to geography students, particularly through the lens of an academic or instructional setting. The language, the detailed discussions about curriculum structure, and teaching methods signal that this is intended for educators and academics who need practical approaches for introducing programming concepts to geographers.\n\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: The paper begins by explaining the rationale for teaching GIS programming, particularly using Python as an open-source tool. It emphasizes the need for programming skills in modern geography and GIS careers.\nMethodology/Approach: A large portion of the paper is dedicated to the teaching approach, explaining how introductory GIS programming is structured in Etherington’s courses. This section discusses the pedagogical methods, including hands-on coding exercises, use of specific Python libraries, and challenges students face.\nChallenges and Solutions: The paper also provides a section addressing common challenges that students encounter, especially when they have little programming background, and offers solutions for how to overcome these obstacles.\nCase Studies/Examples: Some practical examples and case studies are used to demonstrate the application of Python in solving real GIS problems.\nConclusion: The paper ends with reflections on the success of the approach and suggestions for further improvements in teaching GIS programming.\n\nResponse to Audience and Reading Type:\n\nThe structure is organized to provide both a theoretical foundation and practical insights for educators. The detailed breakdown of the teaching process, combined with real-world examples, makes the content accessible and actionable for an academic audience interested in enhancing their teaching methods.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nImportance of Programming in GIS Education: The paper emphasizes that programming skills are increasingly necessary for geographers and GIS professionals. Python, as an open-source language, is highlighted as an accessible and widely-used tool for these purposes.\nOpen-Source Tools in Education: Etherington advocates for the use of open-source programming languages like Python to make GIS education more accessible and affordable.\nPedagogical Strategies for Teaching GIS Programming: The paper discusses effective teaching methods, including hands-on learning, collaborative problem-solving, and incremental learning approaches that help students build confidence in programming.\nChallenges in Teaching Programming to Geographers: The paper identifies specific challenges, such as students’ resistance to learning programming or their initial struggles with syntax and logic, and how educators can address these issues through structured teaching approaches.\n\nHow do we know?\n\nThese ideas are explicitly discussed in the paper. Etherington presents the case for why geographers need programming skills, explains the benefits of Python for GIS education, and gives detailed examples of how these concepts are applied in teaching.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nThe paper contributes by offering practical strategies for teaching GIS programming using Python, specifically tailored for geographers who may have little to no programming experience. It emphasizes a gradual, supportive learning process and demonstrates the benefits of using Python’s open-source libraries for GIS work.\n\nWhat gap does it respond to?\n\nIt addresses the gap in the GIS education field where many students, especially in geography programs, may lack programming skills but increasingly require them for GIS-related jobs. The paper responds to the need for approachable teaching methods that can bridge this gap between geography and programming.\n\nKey Findings or Conclusions:\n\nEtherington concludes that Python is a highly effective tool for introducing programming in GIS education and that, with proper pedagogical support, students can overcome their initial difficulties with programming. The open-source nature of Python also makes it a valuable resource for institutions with limited budgets for software.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nContext-Specific Challenges: The teaching methods presented may not easily translate to all institutional contexts, particularly those where students have varied levels of prior programming knowledge. For example, students in some programs may need more intensive foundational programming courses before they can grasp Python for GIS.\nLearning Curves and Resources: The paper acknowledges that despite Python being a relatively accessible programming language, students may still find the learning curve steep. Etherington addresses this by recommending incremental learning and hands-on practice but doesn’t fully resolve the issue of how to scale this approach in larger, more diverse classrooms.\n\nOther Case Studies or Contexts:\n\nThe ideas in this paper could be applied to other fields where spatial data analysis is critical, such as urban planning, environmental science, or disaster management. In these fields, the use of open-source programming tools could similarly provide accessible, practical GIS training that prepares students for data-heavy roles.\n\nFuture Work Identified:\n\nFuture work could explore how to scale this teaching method to larger classrooms or more diverse student populations, and how to integrate more advanced programming concepts in later stages of GIS education. There could also be research into the long-term impact of teaching Python in GIS programs—do students continue to use these skills in their careers, and how effective are they in applying them to real-world GIS challenges?\n\n\n\n\nConclusion\nEtherington’s paper offers a valuable pedagogical contribution to GIS education by advocating for the use of Python as an open-source programming tool. It provides practical strategies for teaching GIS programming to students with limited coding experience, emphasizing the importance of incremental learning and hands-on application. The paper fills a gap in GIS education by addressing the challenges students face when learning programming and offering solutions to make the learning process more approachable. While the approach is effective, there remain questions about its scalability and applicability in different educational contexts. Future work could focus on expanding this teaching method and exploring its long-term impact on students’ careers in GIS.\n\n\n\n\n\nReferences\n\nEtherington, Thomas R. 2016. “Teaching introductory GIS programming to geographers using an open source Python approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30. https://doi.org/10.1080/03098265.2015.1086981."
  }
]