[
  {
    "objectID": "lectures/demo_postgis_and_sds.html",
    "href": "lectures/demo_postgis_and_sds.html",
    "title": "Running Multiple Containers",
    "section": "",
    "text": "Search Quay.io – on a modern M-chip Mac you need to use arm64 images, on Windows it’s usually amd64; however, in most cases you don’t need to search for these explicitly because ‘builds’ for most images are completed for both."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#find-a-postgis-image",
    "href": "lectures/demo_postgis_and_sds.html#find-a-postgis-image",
    "title": "Running Multiple Containers",
    "section": "",
    "text": "Search Quay.io – on a modern M-chip Mac you need to use arm64 images, on Windows it’s usually amd64; however, in most cases you don’t need to search for these explicitly because ‘builds’ for most images are completed for both."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#create-a-pod",
    "href": "lectures/demo_postgis_and_sds.html#create-a-pod",
    "title": "Running Multiple Containers",
    "section": "Create a ‘Pod’",
    "text": "Create a ‘Pod’\nThis pod exposes two ‘ports’ (8888 and 5432) to the wider world.\npodman pod create -p 8888:8888 -p 5432:5432 myapp\nIn this case we ‘map’ 8888 inside the pod to 8888 outside the pod, but we could change this: -p 7777:8888 so that requests for 7777 from the outside world are ‘forwarded’ to 8888 inside the pod."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#attach-postgis-to-the-pod",
    "href": "lectures/demo_postgis_and_sds.html#attach-postgis-to-the-pod",
    "title": "Running Multiple Containers",
    "section": "Attach PostGIS to the Pod",
    "text": "Attach PostGIS to the Pod\nThere’s a lot going on here that took quite some to figure out1, but the key thing turned out to be the pg_hba.conf file which tells Postgres on which ports it can listen.\npodman run --rm -d --name postgres --pod myapp \\\n-e POSTGRES_USER=postgres \\\n-e POSTGRES_PASSWORD=test \\\n-e POSTGRES_DB=test \\\n-e PGDATA=/var/lib/postgresql/data/pgdata \\\n-v \"${PWD}\"/data/postgres:/var/lib/postgresql/data \\\n-v /tmp:/tmp \\\n-v \"${PWD}\"/data/postgres/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf \\\nquay.io/taolu/postgis:14-3.5-alpine\nThis command is telling Podman to start a postgis image (14-3.5-alpine) it downloads from Quay with a test of startup options set at launch. The first three are specifying the databasei, user, and password. The remainder are connecting various ‘mount points’ on the container to locations on the host computer. So the data that is added to the database will be stored under the current working directory (PWD == Print Working Directory). We allow Postgis to the use the computer’s /tmp folder for working data. And the final bit is taking a local copy that we’ve set up of pg_hba.conf and putting that in the place that a regular Postgres server would expect to find it.\nYou can now connect using psql on port 5432.\nFor example:\npsql -h localhost -p 5432 -U postgres -d test"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#attach-sds-to-the-pod",
    "href": "lectures/demo_postgis_and_sds.html#attach-sds-to-the-pod",
    "title": "Running Multiple Containers",
    "section": "Attach SDS to the Pod",
    "text": "Attach SDS to the Pod\nOnly containers inside the Pod can talk to other containers in the pod. So for the SDS container to talk to PostGIS, they both need to be attached to the pod using myapp.\npodman run --rm -d --name sds --pod=myapp \\\n-v \"$(pwd):/home/jovyan/work\" \\\ndocker.io/jreades/sds:2025-amd \\\nstart.sh \\\njupyter lab --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\nYou can now connect using your browser: http://localhost:8888/\n\n\n\n\n\n\nTip\n\n\n\nThe rest of this short tutorial is all run on the SDS container using your browser as the interface. This is true even for bits about the command line interface: in Jupyter you pick File &gt; New &gt; Terminal."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#install-psycopg2",
    "href": "lectures/demo_postgis_and_sds.html#install-psycopg2",
    "title": "Running Multiple Containers",
    "section": "Install psycopg2",
    "text": "Install psycopg2\nIf I haven’t had time to update the SDS container then you can do this on the SDS Terminal in your browser using the folllowing command:\npip install psycopg2`\nThis is because the sqlalchemy framework is already there but the psycopg2 driver for Postgres isn’t."
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#run-python",
    "href": "lectures/demo_postgis_and_sds.html#run-python",
    "title": "Running Multiple Containers",
    "section": "Run Python",
    "text": "Run Python\nNow you will start a new Notebook (File &gt; New &gt; Notebook) and create code cells for each of the following sections of code.\n\nLoad Data\nfrom sqlalchemy import create_engine\nengine = create_engine('postgresql://postgres:test@localhost:5432/test')\n\nimport geopandas as gpd\ngdf = gpd.read_file('work/data/src/TM_WORLD_BORDERS-0.3.gpkg')\ngdf.to_postgis('world', engine)\n\n\nQuery Data\ninsp = inspect(engine) \ninsp.get_table_names()\nimport geopandas as gpd\n\ngdf = gpd.read_postgis('SELECT * FROM msoa', geom_col='geometry', con=engine)\n\ngdf.head(2)\ngdf.plot()\ngdf = gpd.read_postgis(\"\"\"\n    SELECT * \n    FROM msoa \n    WHERE \"MSOA21NM\" LIKE 'Waltham%%'\n\"\"\", geom_col='geometry', con=con)\ngdf.plot()"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#query-the-data-without-python",
    "href": "lectures/demo_postgis_and_sds.html#query-the-data-without-python",
    "title": "Running Multiple Containers",
    "section": "Query the Data without Python",
    "text": "Query the Data without Python\nOne final thing: if you run a Terminal on your computer (so not in the SDS terminal any more) you can also query the data that was just loaded. Your computer is the only other machine that access the pod.\npsql -h localhost -p 5432 -U postgres -d test\nSELECT * FROM world LIMIT 0;\nSELECT \"NAME\", \"ISO3\", \"POP2005\", \"REGION\" FROM world LIMIT 5;\nSELECT \"NAME\", \"POP2005\" FROM world WHERE AREA &gt; 900000;"
  },
  {
    "objectID": "lectures/demo_postgis_and_sds.html#footnotes",
    "href": "lectures/demo_postgis_and_sds.html#footnotes",
    "title": "Running Multiple Containers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPart of an ESRC research project I was supporting.↩︎"
  },
  {
    "objectID": "ref/Template.html",
    "href": "ref/Template.html",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "ref/Template.html#what-kind-of-reading-is-it",
    "href": "ref/Template.html#what-kind-of-reading-is-it",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "ref/Template.html#who-is-the-intended-audience",
    "href": "ref/Template.html#who-is-the-intended-audience",
    "title": "Summary of Reading",
    "section": "2 Who is the intended audience?",
    "text": "2 Who is the intended audience?\n\nIs it intended for academics, field experts, the general public? etc.? For example, if you think this is intended for someone who needs to learn new skills or someone who is in a policy-making position how do you know this?"
  },
  {
    "objectID": "ref/Template.html#how-is-the-piece-structured",
    "href": "ref/Template.html#how-is-the-piece-structured",
    "title": "Summary of Reading",
    "section": "3 How is the piece structured?",
    "text": "3 How is the piece structured?\n\nBriefly, how is it organised in terms of headings, sub-headings, sections, etc.? Can you explain this structure responds to the kind of reading and type of audience?"
  },
  {
    "objectID": "ref/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "href": "ref/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "title": "Summary of Reading",
    "section": "4 What are the key ideas, concepts, or theories discussed?",
    "text": "4 What are the key ideas, concepts, or theories discussed?\n\nBriefly identify the specific areas that the contribution engages with. How do you know this?"
  },
  {
    "objectID": "ref/Template.html#what-is-the-overall-contribution",
    "href": "ref/Template.html#what-is-the-overall-contribution",
    "title": "Summary of Reading",
    "section": "5 What is the overall contribution?",
    "text": "5 What is the overall contribution?\n\nWhat does it build on or what gap does it respond to? What are the key findings or conclusions?"
  },
  {
    "objectID": "ref/Template.html#what-issues-or-gaps-remain",
    "href": "ref/Template.html#what-issues-or-gaps-remain",
    "title": "Summary of Reading",
    "section": "6 What issues or gaps remain?",
    "text": "6 What issues or gaps remain?\n\nIf relevant, are there assumptions that might not hold in other contexts? Can you think of other case studies or contexts where the reading would apply and explain why? If you think the contribution is generally valid can you explain why? Are there areas for future work identified?"
  },
  {
    "objectID": "readings/dignazio-2020ch3.html",
    "href": "readings/dignazio-2020ch3.html",
    "title": "Data Feminism. Ch.3, On Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints",
    "section": "",
    "text": "Here’s a detailed analysis of Chapter 3 of “Data Feminism” by Catherine D’Ignazio and Lauren F. Klein based on your questions:\n\nWhat kind of reading is it?\n\nType of Document:\n\n“Data Feminism” is a book aimed at both academic and non-academic readers. Chapter 3 in particular serves as a theoretical and conceptual contribution to the intersection of data science and feminist theory.\n\nContribution Type:\n\nThe chapter is largely conceptual and theoretical. It presents feminist principles as a framework for understanding and challenging power dynamics embedded in data science. It doesn’t present empirical research but focuses on reshaping how we think about data practices through a feminist lens.\n\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe intended audience includes a mix of academics, data practitioners, activists, and the general public interested in social justice, feminism, and data science.\n\nHow do we know?:\n\nThe language used is accessible, but the arguments are grounded in academic theory and social critique. While the authors refer to academic works and feminist theory, they also explain these ideas in a way that is understandable to a broader audience, including those working in data-related fields or people interested in activism.\nThe book is designed to reach a wide audience, including those in tech, data, or policy-making positions, and those involved in social activism, providing them with a critical framework for evaluating data practices.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: The chapter begins by framing the importance of looking at data through a feminist lens and discusses the power structures embedded in data collection, analysis, and interpretation.\nMain Sections:\n\nThe chapter is divided into several sections, each addressing a different feminist principle or approach to data science, such as power dynamics, intersectionality, and the need for participatory methods in data work.\nCase Studies or Examples: The authors often introduce real-world examples to illustrate how feminist principles can be applied to data practices, making abstract concepts more tangible.\n\nConclusion: The chapter ends by synthesizing the ideas presented, reinforcing the need for feminist approaches to challenge oppressive structures within data science.\n\nResponse to Audience and Reading Type:\n\nThe structure is both expository and illustrative, suited to readers who may be encountering these ideas for the first time as well as those looking for deeper theoretical insights. The clear sections and examples support readers in understanding and applying the ideas.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nFeminist Approaches to Data: The chapter argues that data science, like other fields, is shaped by existing power structures and often reflects systemic inequalities. By applying feminist principles, we can uncover and challenge these biases.\nIntersectionality: One of the core feminist principles discussed is intersectionality, which recognizes that individuals experience oppression in multiple, intersecting ways (e.g., race, gender, class). The authors argue that data science must take intersectionality into account to avoid reproducing harmful biases.\nChallenging Power Structures in Data: The authors emphasize the importance of understanding how power operates in data collection, analysis, and interpretation, noting that who controls the data often shapes whose voices are heard and whose experiences are marginalized.\nData as a Tool for Justice: The chapter presents the idea that data, when approached with feminist principles, can be used as a tool for social justice by highlighting inequities and advocating for marginalized groups.\n\nHow do we know?:\n\nThese ideas are directly stated and discussed throughout the chapter. The authors are explicit in connecting feminist theory to data science, offering a critique of traditional data practices and suggesting alternatives that are rooted in social justice.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nThe chapter contributes to the growing body of work that critiques mainstream data practices from a social justice perspective. It offers feminist theory as a valuable framework for reshaping data science, making it more inclusive, ethical, and just.\n\nWhat gap does it respond to?:\n\nIt responds to a gap in both data science and feminist theory: the lack of attention to how data practices can reinforce or challenge existing power dynamics. Traditional data science often lacks a critical lens that considers issues of bias, power, and inequality, which the chapter seeks to address.\n\nKey Findings or Conclusions:\n\nThe key conclusion is that data science, like any field, is not neutral. It is shaped by human decisions and societal structures. Therefore, applying feminist principles—like examining power dynamics and considering intersectionality—can lead to more equitable and ethical data practices.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nApplicability in Different Contexts: While the chapter makes a strong theoretical case for feminist data practices, its applicability in different cultural or institutional contexts might vary. For example, organizations with deeply entrenched power structures may resist implementing feminist approaches to data, even if they are proven to be more ethical or inclusive.\nChallenges of Implementation: One of the potential gaps is the difficulty of translating these principles into practice in highly technical environments, where the focus is often on efficiency and quantification rather than justice or inclusion.\nTheoretical Foundations vs. Practical Tools: While the chapter provides an excellent conceptual framework, readers interested in practical strategies for integrating feminist principles into data science may need additional resources or guidance.\n\nOther Case Studies or Contexts:\n\nThe ideas in this chapter could be applied to various case studies involving surveillance, predictive policing, healthcare data, or social media algorithms. For instance, in contexts where algorithms disproportionately target marginalized communities (e.g., facial recognition or risk assessments in criminal justice), a feminist analysis would reveal the systemic biases embedded in these tools.\n\nFuture Work Identified:\n\nThe book as a whole suggests there is more work to be done in creating participatory, inclusive, and intersectional approaches to data science. Future work could explore how to institutionalize these feminist principles in data-driven industries, particularly in AI, government policy, and big tech.\n\n\n\nConclusion\nChapter 3 of “Data Feminism” offers a powerful conceptual and theoretical contribution to data science by introducing feminist principles as a framework for critiquing and reshaping the field. It highlights the importance of understanding power dynamics, intersectionality, and the potential for data to act as a tool for justice. While the chapter is rich in theoretical insight, its practical application may vary depending on the context. Future work could focus on bridging the gap between theory and practice, particularly in highly technical environments or those resistant to social justice-driven change.\n\n\n\n\n\n\nReferences\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/."
  },
  {
    "objectID": "readings/Donoho-2017.html",
    "href": "readings/Donoho-2017.html",
    "title": "50 Years of Data Science",
    "section": "",
    "text": "Here’s a detailed breakdown of the questions in relation to David Donoho’s paper “50 Years of Data Science”:\n\nWhat kind of reading is it?\n\nType of Document: This is an academic paper, delivered as a technical lecture at the Tukey Centennial Workshop in 2015.\nContribution Type: The paper is conceptual and theoretical. It reflects on the evolution of data science, tracing its history, comparing it with traditional statistics, and proposing ideas for the future direction of the field. It doesn’t present new empirical data or experiments but instead offers a critical reflection on the field and a framework for thinking about its development.\n\n\n\nWho is the intended audience?\n\nAudience: The primary audience for this paper includes:\n\nAcademics and Field Experts: The paper targets statisticians, data scientists, and those involved in academia or research. It discusses in-depth technical and philosophical issues in statistics and data science that would resonate with professionals in these fields.\nResearchers in Related Disciplines: It can also serve researchers in fields like computer science, machine learning, and applied mathematics.\n\nHow do we know?:\n\nThe technical language, historical overview, and references to statistical theory and scientific methods make it clear that it is not written for a general public or policy-makers.\nThe discussion assumes familiarity with key statistical figures (like Tukey), theories, and computational methods, indicating that the audience is expected to have a solid understanding of the subject matter.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: Sets up the motivation for the paper and presents the concept of data science as a distinct discipline.\nHistorical Perspective: Explores the origins and evolution of data science, tracing its roots back to John Tukey’s work in exploratory data analysis.\nComparison of Statistics and Data Science: Highlights key distinctions between traditional statistics and the emerging field of data science.\nChallenges for the Future: Discusses unresolved issues and suggests ways in which the field might develop over the next few decades.\n\nResponse to Audience and Reading Type:\n\nThe structure is academic and follows a clear logical progression, from laying out historical background to proposing future directions. This responds to the needs of an expert audience, providing a deep conceptual reflection rather than a “how-to” guide for practitioners.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nData Science as a Field: Donoho argues that data science is not just a rebranding of statistics but a distinct discipline that encompasses new tools, techniques, and paradigms driven by the rise of big data and computational power.\nCriticism of Narrow Views of Statistics: He critiques traditional statistics for being too focused on formal models and not embracing the broader data-driven approaches seen in machine learning and data science.\nThe Role of Algorithms and Computing: The paper emphasizes the importance of computation and algorithms as a core part of data science, diverging from classical statistical methods.\nExploratory Data Analysis (EDA): Building on Tukey’s idea of EDA, Donoho highlights how exploration of data, rather than fitting models, has become central to the work of data scientists.\n\nHow do we know?: These ideas are made explicit throughout the paper, with sections dedicated to the contrast between statistics and data science, and the historical context provided through references to foundational figures like John Tukey.\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nDonoho’s paper acts as a conceptual roadmap, defining data science as a field that is separate from traditional statistics and highlighting the key methodological and philosophical differences. It contributes to the ongoing debate over what data science should encompass and lays out a vision for the field’s future development.\n\nWhat gap does it respond to?:\n\nIt addresses the gap in understanding between traditional statisticians and those who work in the broader data science community, particularly with regard to the importance of algorithms and the role of exploratory methods.\nIt also touches on the need to recognize data science as a distinct discipline, not just a subfield of statistics or computer science.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nContext Dependence: While Donoho argues that data science is distinct, the boundaries between data science, machine learning, and statistics are still somewhat fluid. The paper doesn’t fully address how these fields should coexist or cross-pollinate.\nPotential Gaps in Methodology: There’s limited discussion on the integration of qualitative research methodologies, which could provide additional perspectives in fields like social sciences or healthcare.\n\nFuture Work:\n\nDonoho suggests the need for ongoing development in the education of data scientists, advocating for the inclusion of ethical considerations and interdisciplinary approaches.\nHe calls for a greater focus on reproducibility and data provenance, which remain critical challenges in the field.\n\nBroader Applicability:\n\nThe paper is relevant in other fields where data-driven decision-making is essential (e.g., social sciences, economics, and public policy), but it might not address specific challenges related to non-technical domains like interpretability or ethics in certain societal contexts. The broader societal implications of data science are an area for future development.\n\n\n\n\nConclusion\nDonoho’s paper “50 Years of Data Science” offers a reflective and conceptual contribution to the debate about what constitutes data science as a discipline. It builds on historical foundations and responds to the gaps in the understanding of its relationship to statistics and other computational fields. While it offers valuable insights, it also leaves room for future work on methodological challenges and interdisciplinary collaborations.\n\n\n\n\n\nReferences\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html",
    "href": "practicals/Practical-12-Grouping_Data.html",
    "title": "Practical 12: Grouping Data",
    "section": "",
    "text": "A common challenge in data analysis is how to group observations in a data set together in a way that allows for generalisation: this group of observations are similar to one another, that group is dissimilar to this group. Sometimes we have a label that we can use as part of the process (in which case we’re doing classification), and somtimes we don’t (in which case we’re doing clustering). But what defines similarity and difference? There is no one answer to that question and so there are many different ways to cluster or classify data, each of which has strengths and weaknesses that make them more, or less, appropriate in different contexts."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#london-data-layers",
    "href": "practicals/Practical-12-Grouping_Data.html#london-data-layers",
    "title": "Practical 12: Grouping Data",
    "section": "London Data Layers",
    "text": "London Data Layers\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nmsoas = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011.gpkg', ddir) )\nmsoas = msoas.to_crs(epsg=27700)\n\n# I don't use this in this practical, but it's a\n# really useful data set that gives you 'names'\n# for MSOAs that broadly correspond to what most\n# Londoners would think of as a 'neighbourhood'.\nmsoa_nms = gpd.read_file( cache_data('http://orca.casa.ucl.ac.uk/~jreades/data/MSOA-2011-Names.gpkg', ddir) )\nmsoa_nms = msoa_nms.to_crs(epsg=27700)\nprint(\"Done.\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#reduced-dimensionality-msoa-data",
    "href": "practicals/Practical-12-Grouping_Data.html#reduced-dimensionality-msoa-data",
    "title": "Practical 12: Grouping Data",
    "section": "Reduced Dimensionality MSOA Data",
    "text": "Reduced Dimensionality MSOA Data\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nYou should have this locally from last week, but just in case…\nhost = 'http://orca.casa.ucl.ac.uk'\npath = '~jreades/data'\nrddf = gpd.read_parquet( cache_data(f'{host}/{path}/Reduced_Dimension_Data.geoparquet', ddir) )\nprint(f\"Data frame is {rddf.shape[0]:,} x {rddf.shape[1]}\")\nYou should have: Data frame is 983 x 93.\nAnd below you should see both the components and the dimensions from last week’s processing.\nrddf.iloc[0:3, -7:]\nI get the results below, but note that the Dimension values may be slightly different:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent 5\nComponent 6\nComponent 7\nBorough\nDimension 1\nDimension 2\nSubregion\n\n\n\n\nE02000001\n1.44\n3.95\n-1.52\nCity of London\n7.74\n3.36\nInner West\n\n\nE02000002\n-0.28\n0.89\n0.26\nBarking and Dagenham\n2.04\n7.59\nOuter East and North East\n\n\nE02000003\n-0.11\n1.12\n0.83\nBarking and Dagenham\n2.20\n6.87\nOuter East and North East"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#listings-data",
    "href": "practicals/Practical-12-Grouping_Data.html#listings-data",
    "title": "Practical 12: Grouping Data",
    "section": "Listings Data",
    "text": "Listings Data\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nLet’s also get the listings data from a few weeks back:\n\n# Set download URL\nymd  = '2024-06-14'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-listings.geoparquet'\n\nlistings = gpd.read_parquet( cache_data(url, ddir) )\nlistings = listings.to_crs(epsg=27700)\nprint(f\"Data frame is {listings.shape[0]:,} x {listings.shape[1]}\")\nYou should have: Data frame is 85,134 x 31.\nAnd a quick plot of the price to check:\nlistings.plot(???, cmap='plasma', scheme='quantiles', k=10, \n              markersize=.5, alpha=0.15, figsize=(10,7));"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#join-listings-to-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#join-listings-to-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Join Listings to MSOA",
    "text": "Join Listings to MSOA\n\n\n\n\n\n\nDifficulty: Medium-to-hard.\n\n\n\n\n\n\n\n\n\nFirst, let’s link all this using the MSOA Geography that we created last week and a mix or merge and sjoin!\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: Notice a few things going on here! We are calling `gpd.sjoin` because pandas (`pd`) doesn't know about spatial joins, only geopandas (`gpd`) does. More on this [next week](https://jreades.github.io/fsds/sessions/week10.html#lectures). Also see how we drop some columns _at the point where we do the join_ by taking advantage of the fact that most pandas/geopandas operations return a _copy_ of the (Geo)DataFrame. That allows us to get back from the spatial join a neat, tidy data frame ready for further analysis. If you're struggling to make sense of this, try removing the `drop` operations and see what your data frame looks like afterwards. This should all be old hat, but in case you need a refresher there's always [Week 5](https://jreades.github.io/fsds/sessions/week5.html) on pandas.\n\n\n# Before the spatial join\nlistings.columns\nmsoa_listings = gpd.sjoin(???, msoas.drop(\n                        columns=['MSOA11NM', 'LAD11CD', 'LAD11NM', 'RGN11CD', 'RGN11NM',\n                                 'USUALRES', 'HHOLDRES', 'COMESTRES', 'POPDEN', 'HHOLDS', \n                                 'AVHHOLDSZ']), predicate='???').drop(\n                        columns=['latitude','longitude','index_right']\n                )\n# All we've added is the MSOA11CD\nmsoa_listings.columns\nAll being well you should now have:\nIndex(['listing_url', 'last_scraped', 'name', 'description', 'host_id',\n       'host_name', 'host_since', 'host_location', 'host_is_superhost',\n       'host_listings_count', 'host_total_listings_count',\n       'host_verifications', 'property_type', 'room_type', 'accommodates',\n       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n       'minimum_nights', 'maximum_nights', 'availability_365',\n       'number_of_reviews', 'first_review', 'last_review',\n       'review_scores_rating', 'reviews_per_month', 'geometry', 'MSOA11CD'],\n      dtype='object')"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#price-by-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#price-by-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Price by MSOA",
    "text": "Price by MSOA\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nLet’s calculate the median price by MSOA… Notice that we have to specify the column we want after the groupby so the we don’t get the median of every column returned\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: I find `groupby` to be a complex operation and often need a couple of gos before I get back waht I want. The thing to take away is that: 1) anything in the `groupby` will become part of the `index` afterwards (so if you group on multiple things you get a multi-part index); 2) aggregating functions apply to _all_ columns unless you filter them some way. Here we filter by selecting only the `price` column to aggregate. You can also filter for `numeric only`.\n\n\n# *m*soa *l*istings *g*rouped by *p*rice\nmlgp = msoa_listings.groupby('???')['price'].agg('???') \nmlgp.head()\nYou should get something like:\nMSOA11CD\nE02000001   170.00\nE02000002    97.00\nE02000003    80.00\nE02000004    54.00\nE02000005   100.00\nName: price, dtype: float64"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#room-type-by-msoa",
    "href": "practicals/Practical-12-Grouping_Data.html#room-type-by-msoa",
    "title": "Practical 12: Grouping Data",
    "section": "Room Type by MSOA",
    "text": "Room Type by MSOA\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow let’s calculate the count of room types by MSOA and compare the effects of reset_index on the outputs below. And notice too that we can assign the aggregated value to a column name!\n# *m*soa *l*istings *g*rouped *c*ount\nmlgc = msoa_listings.groupby(['???','???'], observed=False).listing_url.agg(Count='???')\nmlgc.head()\nYou should get something resembling this:\n\n\n\nMSOA11CD\nroom_type\nCount  \n\n\n\n\nE02000001\nEntire home/apt\n466\n\n\n\nHotel room\n0\n\n\n\nPrivate room\n61\n\n\n\nShared room\n1\n\n\nE02000002\nEntire home/apt\n4\n\n\n\n# *m*soa *l*istings *g*rouped *c*ount *r*eset index\nmlgcr = msoa_listings.groupby(['???','???'], observed=False).listing_url.agg(Count='???').reset_index() # msoa listings grouped counts\nmlgcr.head()\nYou should get something like:\n\n\n\n\nMSOA11CD\nroom_type\nCount\n\n\n\n\n0\nE02000001\nEntire home/apt\n466\n\n\n1\nE02000001\nHotel room\n0\n\n\n2\nE02000001\nPrivate room\n61\n\n\n3\nE02000001\nShared room\n1\n\n\n4\nE02000002\nEntire home/apt\n4"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#price-by-room-type",
    "href": "practicals/Practical-12-Grouping_Data.html#price-by-room-type",
    "title": "Practical 12: Grouping Data",
    "section": "Price by Room Type",
    "text": "Price by Room Type\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nBut perhaps median price/room type would make more sense? And do we want to retain values where there are no listings? For example, there are no hotel rooms listed for E02000001, how do we ensure that these NAs are dropped?\n# *m*soa *l*istings *g*rouped *r*oom *p*rice\nmlgrp = msoa_listings.???(???, observed=True\n                      )['price'].agg('???').reset_index()\nmlgrp.head()\nYou should get something like:\n\n\n\n\nMSOA11CD\nroom type\nprice\n\n\n\n\n0\nE02000001\nEntire home/apt\n177.00\n\n\n2\nE02000001\nPrivate room\n100.00\n\n\n3\nE02000001\nShared room\n120.00\n\n\n4\nE02000002\nEntire home/apt\n117.00\n\n\n6\nE02000002\nPrivate room\n42.00"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#explore-outlier-per-msoa-prices",
    "href": "practicals/Practical-12-Grouping_Data.html#explore-outlier-per-msoa-prices",
    "title": "Practical 12: Grouping Data",
    "section": "Explore Outlier Per-MSOA Prices",
    "text": "Explore Outlier Per-MSOA Prices\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nAre there MSOAs what look like they might contain erroneous data?\n\nPlot MSOA Median Prices\nmlgp.hist(bins=200);\n\n\nExamine Listings from High-Priced MSOAs\nCareful, this is showing the listings from MSOAs whose median price is above $300/night:\nmsoa_listings[\n    msoa_listings.MSOA11CD.isin(mlgp[mlgp &gt; 300].index)\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','name','description']\n]\nSome of these look legi (4, 5, and… 8 bedroom ‘villas’?), though not every one…\nAnd how about these?\nmsoa_listings[\n    (msoa_listings.MSOA11CD.isin(mlgp[mlgp &gt; 300].index)) & (msoa_listings.room_type!='Entire home/apt')\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','property_type','name','description']\n]\nIf we wanted to be rigorous then we’d have to investigate further: properties in Mayfair and Westminster are going to be expensive, but are these plausible nightly prices? In some cases, yes. In others…\nmsoa_listings[\n    (msoa_listings.MSOA11CD.isin(mlgp[mlgp &lt; 100].index)) & (msoa_listings.room_type!='Entire home/apt')\n].sort_values(by='price', ascending=False).head(7)[\n    ['price','room_type','name','description']\n]\nOn the whole, let’s take a guess that there are a small number of implausibly high prices for individual units that aren’t in very expensive neighbourhoods and that these are either erroneous/deliberately incorrect, or represent a price that is not per-night.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: What's the right answer here? There isn't one. You could probably spend _months_ figuring out what's a real, available-to-let listing and waht isn't. I would argue that assuming all listings are 'legit' without doing some additional EDA and ESDA is negligent. You could also look at how some of the methods of standardisation/normalisation work and use those to identify improbable listings (but remember that a £10,000 in Mayfair _might_ be legit, while a $5,000 listing in Barking _probably_ isn't!). Or you could look at the inter-decile range (or just define your own range: 1%-99%?).\n\n\n\n\nFilter Unlikely Listings\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nSee if you can filter out these less likely listings on the following criteria:\n\nListings are priced above $300/night AND\nRoom type is not 'Entire home/apt' AND\nListings do not contain the words: suite, luxury, loft, stunning, prime, historic, or deluxe.\n\nI found 901 rows to drop this way.\ntarget_regex = r'(?:suite|luxury|loft|stunning|prime|historic|deluxe|boutique)'\nto_drop = msoa_listings[\n            (???) & \n            (???) &\n            ~(???(target_regex, flags=re.IGNORECASE, regex=True, na=True))]\nprint(f\"Have found {to_drop.shape[0]:,} rows to drop on the basis of unlikely per night prices.\")\nto_drop.sort_values(by='price', ascending=False)[['price','room_type','name','description']]\n\n\nPlot Unlikely Listings\nHere we use the plt_ldn function – notice how it’s designed to return f,ax in the same way that plt.subplots (which we’re already familiar with) does!\nf,ax = plt_ldn(???, ???)\nto_drop.plot(column='price', markersize=10, alpha=0.7, cmap='viridis', ax=ax);\n\n\n… And Drop\nSome might be legitimate, but I’m feeling broadly ok with the remainder.\ncleaned = msoa_listings.drop(index=to_drop.???)\nprint(f\"Cleaned data has {cleaned.shape[0]:,} rows.\")\nAfter this I had 84,308 rows.\nI would normally, at this point, spend quite a bit of time validating this cleaning approach, but right now we’re going to take a rough-and-ready approach.\n\n\nQuestions\n\nWhat data type did Task 2.2 return?\n\n\n\n\n\nWhat is the function of reset_index() in Task 2.3 and when might you choose to reset (or not)?"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#create-pivot-table",
    "href": "practicals/Practical-12-Grouping_Data.html#create-pivot-table",
    "title": "Practical 12: Grouping Data",
    "section": "Create Pivot Table",
    "text": "Create Pivot Table\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe can make use of the pivot table function to generate counts by MSOA in a ‘wide’ format.\npivot = cleaned.groupby(\n                ['MSOA11CD','room_type'], observed=False\n        ).listing_url.agg(Count='count').reset_index().pivot(\n                index='???', columns=['???'], values=['???'])\npivot.head(3)\nThe formatting will look a tiny bit different, but you should get something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCount\n\n\n\n\nroom_type\nEntire home/apt\nHotel room\nPrivate room\nShared room\n\n\n\nMSOA11CD\n\n\n\n\n\n\n\nE02000001\n466\n0\n55\n1\n\n\n\nE02000002\n4\n0\n2\n0\n\n\n\nE02000003\n12\n0\n13\n0"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#check-counts",
    "href": "practicals/Practical-12-Grouping_Data.html#check-counts",
    "title": "Practical 12: Grouping Data",
    "section": "Check Counts",
    "text": "Check Counts\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\npivot.sum()\nJust to reassure you that the pivot results ‘make sense’:\nprint(cleaned[cleaned.room_type=='Entire home/apt'].listing_url.count())\nprint(cleaned[cleaned.room_type=='Private room'].listing_url.count())"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#tidy-normalise",
    "href": "practicals/Practical-12-Grouping_Data.html#tidy-normalise",
    "title": "Practical 12: Grouping Data",
    "section": "Tidy & Normalise",
    "text": "Tidy & Normalise\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nMy instinct at this point is that, looking at the pivot table, we see quite different levels of Airbnb penetration and it is hard to know how handle this difference: share would be unstable because of the low counts in some places and high counts in others; a derived variable that tells us something about density or mix could be interesting (e.g. HHI or LQ) but wouldn’t quite capture the pattern of mixing.\n\nTidy\nPersonally, based on the room type counts above I think we can drop Hotel Rooms and Shared Rooms from this since the other two categories are so dominant.\n# Flatten the column index\npivot.columns = ['Entire home/apt','Hotel room','Private room','Shared room']\n# Drop the columns\npivot.drop(???, inplace=True)\npivot.head()\nYou should have only the Entire home/apt and Private room columns now.\n\n\nNormalise\npivot_norm = pd.DataFrame(index=pivot.index)\nfor c in pivot.columns.to_list():\n    # Power Transform\n    pivot_norm[c] = pts.???(pivot[c].to_numpy().reshape(???,???))\n\npivot_norm.head()\nYou should have something like:\n\n\n\n\nEntire home/apt\nPrivate room\n\n\n\n\nMSOA11CD\n\n\n\n\nE02000001\n2.20\n1.06\n\n\nE02000002\n-1.29\n-1.85\n\n\n\n\n\nPlot\npnm = pd.merge(msoas.set_index('MSOA11CD'), pivot_norm, left_index=True, right_index=True)\npnm.plot(column='Entire home/apt', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#pca",
    "href": "practicals/Practical-12-Grouping_Data.html#pca",
    "title": "Practical 12: Grouping Data",
    "section": "PCA",
    "text": "PCA\n\n\n\n\n\n\nDifficulty: Moderate, though you might find the questions hard.\n\n\n\n\n\n\n\n\n\nYou can merge the output of this next step back on to the rddf data frame as part of a clustering process, though we’d really want to do some more thinking about what this data means and what transformations we’d need to do in order to make them meaningful.\nFor instance, if we went back to last week’s code, we could have appended this InsideAirbnb data before doing the dimensionality reduction, or we could apply it now to create a new measure that could be used as a separate part of the clustering process together with the reduced dimensionality of the demographic data.\n\nPerform Reduction\npcomp = PCA(n_components=???, random_state=42)\nrd    = pcomp.???(pivot_norm)\nprint(f\"The explained variance of each component is: {', '.join([f'{x*100:.2f}%' for x in pcomp.explained_variance_ratio_])}\")\nTake the first component and convert to a series to enable the merge:\nairbnb_pca = pd.DataFrame(\n                {'Airbnb Component 1': mms.fit_transform(rd[:,1].reshape(-1,1)).reshape(1,-1)[0]}, \n                index=pivot.index)\n\nairbnb_pca.head()\nYou should have something like: | | Airbnb Component 1 | | :—- | —-: | | MSOA11CD | | | E02000001 | 0.47 | | E02000002 | 0.19\npcanm = pd.merge(msoas.set_index('MSOA11CD'), airbnb_pca, left_index=True, right_index=True)\npcanm.plot(column='Airbnb Component 1', cmap='viridis', edgecolor='none', legend=True, figsize=(12,8));\n\n\nWrite to Data Frame\n# Result Set from merge\nrs = pd.merge(rddf, airbnb_pca, left_index=True, right_index=True)\nGrab the PCA, UMAP, and Airbnb outputs for clustering and append rescaled price:\n# Merge the reducded dimensionality data frame with the PCA-reduced Airbnb data\n# to create the *cl*uster *d*ata *f*rame\ncldf = pd.merge(rddf.loc[:,'Component 1':], airbnb_pca, \n                left_index=True, right_index=True)\n\n# Append median price from cleaned listings grouped by MSOA too!\ns1 = cleaned.groupby(by='MSOA11CD').price.agg('median')\ncldf['median_price'] = pd.Series(np.squeeze(mms.fit_transform(s1.values.reshape(-1,1))), index=s1.index)\n\n# Append mean price from cleaned listings grouped by MSOA too!\ns2 = cleaned.groupby(by='MSOA11CD').price.agg('mean')\ncldf['mean_price'] = pd.Series(np.squeeze(mms.fit_transform(s2.values.reshape(-1,1))), index=s2.index)\n\ncldf.drop(columns=['Subregion','Borough'], inplace=True)\n\ncldf.head()\n\n\nQuestions\n\nHave a think about why you might want to keep the Airbnb data separate from the MSOA data when doing PCA (or any other kind of dimensionality reduction)!\n\n\n\n\n\nWhy might it be interesting to add both mean and median MSOA prices to the clustering process? Here’s a hint (but it’s very subtle): sns.jointplot(x=s1, y=s2, s=15, alpha=0.6)"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#perform-clustering",
    "href": "practicals/Practical-12-Grouping_Data.html#perform-clustering",
    "title": "Practical 12: Grouping Data",
    "section": "Perform Clustering",
    "text": "Perform Clustering\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nc_nm   = 'KMeans' # Clustering name\nk_pref = ??? # Number of clusters\n\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(cldf.drop(columns=['Dimension 1','Dimension 2'])) # The process\nHere are the results:\nprint(kmeans.labels_) # The results"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#save-clusters-to-data-frame",
    "href": "practicals/Practical-12-Grouping_Data.html#save-clusters-to-data-frame",
    "title": "Practical 12: Grouping Data",
    "section": "Save Clusters to Data Frame",
    "text": "Save Clusters to Data Frame\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\n\nWrite Series and Assign\nNow capture the labels (i.e. clusters) and write them to a data series that we store on the result set df (rs):\nrs[c_nm] = pd.Series(kmeans.labels_, index=cldf.index)\n\n\nHistogram of Cluster Members\nHow are the clusters distributed?\nsns.histplot(data=???, x=c_nm, bins=k_pref);\n\n\nMap Clusters\nAnd here’s a map!\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=???, ax=ax, linewidth=0, zorder=0, categorical=???, legend=True);\n\n\nQuestions\n\nWhat critical assumption did we make when running this analysis?\n\n\n\n\n\nWhy did I not use the UMAP dimensions here?\n\n\n\n\n\nWhy do we have the c_nm='kMeans' when we know what kind of clustering we’re doing?\n\n\n\n\n\nDoes this look like a good clustering?"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#whats-the-right-number-of-clusters",
    "href": "practicals/Practical-12-Grouping_Data.html#whats-the-right-number-of-clusters",
    "title": "Practical 12: Grouping Data",
    "section": "What’s the ‘Right’ Number of Clusters?",
    "text": "What’s the ‘Right’ Number of Clusters?\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nThere’s more than one way to find the ‘right’ number of clusters. In Singleton’s Geocomputation chapter they use WCSS to pick the ‘optimal’ number of clusters. The idea is that you plot the average WCSS for each number of possible clusters in the range of interest (2…n) and then look for a ‘knee’ (i.e. kink) in the curve. The principle of this approach is that you look for the point where there is declining benefit from adding more clusters. The problem is that there is always some benefit to adding more clusters (the perfect clustering is k==n), so you don’t always see a knee.\nAnother way to try to make the process of selecting the number of clusters a little less arbitrary is called the silhouette plot and (like WCSS) it allows us to evaluate the ‘quality’ of the clustering outcome by examining the distance between each observation and the rest of the cluster. In this case it’s based on Partitioning Around the Medoid (PAM).\nEither way, to evaluate this in a systematic way, we want to do multiple k-means clusterings for multiple values of k and then we can look at which gives the best results…\nkcldf = cldf.drop(columns=['Dimension 1','Dimension 2'])\n\nRepeated Clustering\nLet’s try clustering across a wider range. Because we repeatedly re-run the clustering code (unlike with Hierarchical Clustering) this can take a few minutes. I got nearly 5 minutes on a M2 Mac.\n%%time \n\n# Adapted from: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n\nx = []\ny = []\n\n# For resolutions of 'k' in the range 2..40\nfor k in range(2,41):\n    \n    #############\n    # Do the clustering using the main columns\n    kmeans = KMeans(n_clusters=k, n_init=25, random_state=42).fit(kcldf)\n    \n    # Calculate the overall silhouette score\n    silhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n    \n    y.append(k)\n    x.append(silhouette_avg)\n    \n    print('.', end='')\n\n\nPlot Silhouette Scores\nprint()\nprint(f\"Largest silhouette score was {max(x):6.4f} for k={y[x.index(max(x))]}\")\n\nplt.plot(y, x)\nplt.gca().xaxis.grid(True);\nplt.gcf().suptitle(\"Average Silhouette Scores\");\n\n\n\n\n\n\nWarning\n\n\n\n**&#9888; Note**: Had we used the UMAP dimensions here you'd likely see more instability in the silhouette plot because the distribution is not remotely Gaussian, though a lot depends on the magnitude of the columns and the number of UMAP vs. PCA components.\n\n\nWe can use the largest average silhouette score to determine the ‘natural’ number of clusters in the data, but that that’s only if we don’t have any kind of underlying theory, other empirical evidence, or even just a reason for choosing a different value… Again, we’re now getting in areas where your judgement and your ability to communicate your rationale to readers is the key thing."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#final-clustering",
    "href": "practicals/Practical-12-Grouping_Data.html#final-clustering",
    "title": "Practical 12: Grouping Data",
    "section": "Final Clustering",
    "text": "Final Clustering\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nSo although we should probably pick the largest silhouette scores, that’s k=3 which kind of defeats the purpose of clustering in the first place. In the absence of a compelling reason to pick 2 or 3 clusters, let’s have a closer look at the next maximum silhouetted score:\n\nPerform Clustering\nk_pref=???\n\n#############\n# Do the clustering using the main columns\nkmeans = KMeans(n_clusters=k_pref, n_init=25, random_state=42).fit(kcldf)\n\n# Convert to a series\ns = pd.Series(kmeans.labels_, index=kcldf.index, name=c_nm)\n\n# We do this for plotting\nrs[c_nm] = s\n    \n# Calculate the overall silhouette score\nsilhouette_avg = silhouette_score(kcldf, kmeans.labels_)\n\n# Calculate the silhouette values\nsample_silhouette_values = silhouette_samples(kcldf, kmeans.labels_)\n\n\nPlot Diagnostics\n#############\n# Create a subplot with 1 row and 2 columns\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(9, 5)\n\n# The 1st subplot is the silhouette plot\n# The silhouette coefficient can range from -1, 1\nax1.set_xlim([-1.0, 1.0]) # Changed from -0.1, 1\n    \n# The (n_clusters+1)*10 is for inserting blank space between silhouette\n# plots of individual clusters, to demarcate them clearly.\nax1.set_ylim([0, kcldf.shape[0] + (k_pref + 1) * 10])\n    \ny_lower = 10\n    \n# For each of the clusters...\nfor i in range(k_pref):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        sample_silhouette_values[kmeans.labels_ == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n        \n    # Set the color ramp\n    color = plt.cm.Spectral(i/k_pref)\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                        0, ith_cluster_silhouette_values,\n                        facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=0.5)\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks(np.arange(-1.0, 1.1, 0.2)) # Was: [-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]\n\n    # 2nd Plot showing the actual clusters formed --\n    # we can only do this for the first two dimensions\n    # so we may not see fully what is causing the \n    # resulting assignment\n    colors = plt.cm.Spectral(kmeans.labels_.astype(float) / k_pref)\n    ax2.scatter(kcldf[kcldf.columns[0]], kcldf[kcldf.columns[1]], \n                marker='.', s=30, lw=0, alpha=0.7, c=colors)\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    \n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"Visualization of the clustered data\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\nplt.suptitle((\"Silhouette results for KMeans clustering \"\n                \"with %d clusters\" % k_pref),\n                fontsize=14, fontweight='bold')\n\nplt.show()\n\n\n\n\n\n\nWarning\n\n\n\n**&#9888; Stop**: Make sure that you understand how the silhouette plot and value work, and why your results _may_ diverge from mine.\n\n\n\n\nMap Clusters\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results (k={k_pref})\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, categorical=True, legend=True);"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#representative-centroids",
    "href": "practicals/Practical-12-Grouping_Data.html#representative-centroids",
    "title": "Practical 12: Grouping Data",
    "section": "‘Representative’ Centroids",
    "text": "‘Representative’ Centroids\n\n\n\n\n\n\nDifficulty: Moderate since, conceptually, there’s a lot going on.\n\n\n\n\n\n\n\n\n\nTo get a sense of how these clusters differ we can try to extract ‘representative’ centroids (mid-points of the multi-dimensional cloud that constitutes a cluster). In the case of k-means this will work quite will since the clusters are explicitly built around mean centroids. There’s also a k-medoids clustering approach built around the median centroid.\nThese are columns that we want to suppress from our sample:\nto_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n             'Airbnb Component 1']\nTake a sample of the full range of numeric columns:\ncols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\nprint(cols)\nCalculate the mean of these columns for each cluster:\n# Empty data frame with the columns we'll need\ncentroids = pd.DataFrame(columns=cols)\n\n# For each cluster...\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n    \n    # Select rows where the cluster name matches the cluster number\n    clust = rs[rs[c_nm]==k]\n    \n    # Append the means to the centroids data frame\n    centroids.loc[k] = clust[cols].mean()\ncentroids\ncentroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    for r in row.index:\n        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n        centroids_long = pd.concat([centroids_long, d], ignore_index=True)\ng = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.bar, \"Cluster\", \"Std. Value\")\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: The above centroid outputs are a way to think about how each cluster is 'loaded' on to the data. We can't show all of the variables in the data, so we've randomly selected a subset and can then look at how different clusters are more (or less) associated with the standardised value of a particular column/variable."
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#work-out-the-neighbour-distance",
    "href": "practicals/Practical-12-Grouping_Data.html#work-out-the-neighbour-distance",
    "title": "Practical 12: Grouping Data",
    "section": "Work out the Neighbour Distance",
    "text": "Work out the Neighbour Distance\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nWe normally look for some kind of ‘knee’ to set the distance.\nnbrs = NearestNeighbors(n_neighbors=6).fit(cldf2)\ndistances, indices = nbrs.kneighbors(cldf2)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#derive-approximate-knee",
    "href": "practicals/Practical-12-Grouping_Data.html#derive-approximate-knee",
    "title": "Practical 12: Grouping Data",
    "section": "Derive Approximate Knee",
    "text": "Derive Approximate Knee\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nfrom kneed import knee_locator\n\nkn = knee_locator.KneeLocator(np.arange(distances.shape[0]), distances, S=12,\n                              curve='convex', direction='increasing')\nprint(f\"Knee detected at: {kn.knee}\")\nkn.plot_knee()\nkn.plot_knee_normalized()\nprint(f\"Best guess at epsilon for DBSCAN is {distances[kn.knee]:0.4f}\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#explore-epsilons",
    "href": "practicals/Practical-12-Grouping_Data.html#explore-epsilons",
    "title": "Practical 12: Grouping Data",
    "section": "Explore Epsilons",
    "text": "Explore Epsilons\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\nThere are two values that need to be specified: eps and min_samples. Both seem to be set largely by trial and error, though we can use the above result as a target. It’s easiest to set min_samples first since that sets a floor for your cluster size and then eps is basically a distance metric that governs how far away something can be from a cluster and still be considered part of that cluster.\n\nIterate Over Range\n\n\n\n\n\n\nCaution\n\n\n\n**&#9888; Warning**: Depending on the data volume, this next step may take quite a lot of time since we are iterating through many, many values of Epsilon to explore how the clustering result changes and how well this matches up with (or doesn't) the graph above.\n\n\n%%time \n\nc_nm = 'DBSCAN'\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.4f}'.format(x))\n\nel  = []\n\nmax_clusters  = 10\ncluster_count = 1\n\niters = 0\n\nfor e in np.arange(0.025, 0.76, 0.01): # &lt;- You might want to adjust these!\n    \n    if iters % 25==0: print(f\"{iters} epsilons explored.\") \n    \n    # Run the clustering\n    dbs = DBSCAN(eps=e, min_samples=cldf2.shape[1]+1).fit(cldf2)\n    \n    # See how we did\n    s = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\n    \n    row = [e]\n    data = s.value_counts()\n    \n    for c in range(-1, max_clusters+1):\n        try:\n            if np.isnan(data[c]):\n                row.append(None)\n            else: \n                row.append(data[c])\n        except KeyError:\n            row.append(None)\n    \n    el.append(row)\n    iters+=1\n\nedf = pd.DataFrame(el, columns=['Epsilon']+[\"Cluster \" + str(x) for x in list(range(-1,max_clusters+1))])\n\n# Make numeric display a bit neater\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\nprint(\"Done.\")\n\n\nExamine Clusters\nedf.head() # Notice the -1 cluster for small epsilons\nepsilon_long = pd.DataFrame(columns=['Epsilon','Cluster','Count'])\n\nfor i in range(0,len(edf.index)):\n    row = edf.iloc[i,:]\n    for c in range(1,len(edf.columns.values)):\n        if row[c] != None and not np.isnan(row[c]):\n            d = pd.DataFrame({'Epsilon':row[0], 'Cluster':f\"Cluster {c-2}\", 'Count':row[c]}, index=[1])\n            epsilon_long = pd.concat([epsilon_long, d], ignore_index=True)\n\nepsilon_long['Count'] = epsilon_long.Count.astype(float)\n\n\nPlot Cluster Sizes\nOne of the really big problems with DBSCAN and this kind of data is that you have no practical way of specifying epsilon (whereas if you were doing walkability analysis then you could cluster on walking distance!). So you can look at the data (as above) to get a reasoanble value, but look what the output below shows about the stability of the clusters for different values of epsilon!\nfig, ax = plt.subplots(figsize=(12,8))\nsns.lineplot(data=epsilon_long, x='Epsilon', y='Count', hue='Cluster');\nplt.vlines(x=distances[kn.knee], ymin=0, ymax=epsilon_long.Count.max(), color=(1, .7, .7, .8), linestyles='dashed')\nplt.gcf().suptitle(f\"Cluster sizes for various realisations of Epsilon\");\nplt.tight_layout()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#final-clustering-1",
    "href": "practicals/Practical-12-Grouping_Data.html#final-clustering-1",
    "title": "Practical 12: Grouping Data",
    "section": "Final Clustering",
    "text": "Final Clustering\n\n\n\n\n\n\nDifficulty: Moderate.\n\n\n\n\n\n\n\n\n\n###: Perform Clustering\nUse the value from kneed…\ndbs = DBSCAN(eps=distances[kn.knee], min_samples=cldf2.shape[1]+1).fit(cldf2.values)\ns = pd.Series(dbs.labels_, index=cldf2.index, name=c_nm)\nrs[c_nm] = s\nprint(s.value_counts())\n###: Map Clusters\nfig, ax = plt_ldn(water, boros)\nfig.suptitle(f\"{c_nm} Results\", fontsize=20, y=0.92)\nrs.plot(column=c_nm, ax=ax, linewidth=0, zorder=0, legend=True, categorical=True);\n\n‘Representative’ Centroids\nto_suppress=['OBJECTID', 'BNG_E', 'BNG_N', 'LONG', 'LAT', \n             'Shape__Are', 'Shape__Len', 'geometry', 'Component 1', \n             'Component 2', 'Component 3', 'Component 4', 'Component 5', \n             'Component 6', 'Component 7', 'Dimension 1', 'Dimension 2', \n             'Airbnb Component 1']\nTake a sample of the full range of numeric columns:\ncols = random.sample(rs.select_dtypes(exclude='object').drop(columns=to_suppress).columns.to_list(), 12)\nprint(cols)\nCalculate the mean of these columns for each cluster:\n# Empty data frame with the columns we'll need\ncentroids = pd.DataFrame(columns=cols)\n\n# For each cluster...\nfor k in sorted(rs[c_nm].unique()):\n    print(f\"Processing cluster {k}\")\n    \n    # Select rows where the cluster name matches the cluster number\n    clust = rs[rs[c_nm]==k]\n    \n    # Append the means to the centroids data frame\n    centroids.loc[k] = clust[cols].mean()\n# Drop the unclustered records (-1)\ncentroids.drop(labels=[-1], axis=0, inplace=True)\ncentroids\ncentroids_long = pd.DataFrame(columns=['Variable','Cluster','Std. Value'])\nfor i in range(0,len(centroids.index)):\n    row = centroids.iloc[i,:]\n    for r in row.index:\n        d = pd.DataFrame({'Variable':r, 'Cluster':i, 'Std. Value':row[r]}, index=[1])\n        centroids_long = pd.concat([centroids_long, d], ignore_index=True)\ng = sns.FacetGrid(centroids_long, col=\"Variable\", col_wrap=3, height=3, aspect=1.5, margin_titles=True, sharey=True)\ng = g.map(plt.bar, \"Cluster\", \"Std. Value\")"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#additional-setup",
    "href": "practicals/Practical-12-Grouping_Data.html#additional-setup",
    "title": "Practical 12: Grouping Data",
    "section": "Additional Setup",
    "text": "Additional Setup\n\n\n\n\n\n\nDifficulty: Hard, as I’ve left out quite a bit of code.\n\n\n\n\n\n\n\n\n\n\nImport Libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.inspection import permutation_importance\n\n\nSet Up Data\nI’m taking a fairly brutal approach here: anything that is not inherently numeric is gone (bye, bye, text), and I’m not bothering to convert implicitly numeric values either: dates could be converted to ‘months since last review’, for instance, while amenities could be One-Hot Encoded after some pruning of rare amenities. This leaves us with a much smaller number of columns to feed in to the classifier.\nprint(f\"Cleaned columns: {', '.join(cleaned.columns.to_list())}.\")\nclassifier_in = cleaned.drop(columns=['listing_url','last_scraped','name','description',\n                                      'host_name', 'host_location', 'property_type', \n                                      'bathrooms_text', 'amenities', 'geometry', 'MSOA11CD',\n                                      'host_since', 'first_review', 'last_review',\n                                      'host_verifications', 'review_scores_rating',\n                                      'reviews_per_month'])\n\n\nRemove NAs\nNot all classifiers have this issue, but some will struggle to make predictions (or not be able to do so at all) if there are NAs in the data set. The classifier we’re using can’t deal with NAs, so we have to strip these out, but before we do let’s check the effect:\nclassifier_in.isna().sum()\nWe can safely drop these now, and you should end up with about 54,000 rows to work with.\nclassifier_in = classifier_in.dropna(axis=0, how='any')\nprint(f\"Now have {classifier_in.shape[0]:,} rows of data to work with (down from {cleaned.shape[0]:,}).\")\nprint()\nprint(f\"Classifier training columns: {', '.join(classifier_in.columns.to_list())}.\")\nclassifier_in.head()\n\n\nRemap Non-Numeric Columns\nWe do still have a couple of non-numeric columns to deal with: booleans and the thing we’re actually trying to predict (the room type)!\nclassifier_in['host_is_superhost'] = classifier_in.host_is_superhost.replace({True:1, False:0}).astype('int')\nle = LabelEncoder()\nclassifier_in['room_class'] = le.fit_transform(classifier_in.room_type)\nA quick check: we should only have one type per class and vice versa.\nclassifier_in.groupby(by=['room_type','room_class']).host_id.agg('count').reset_index()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#random-forest-classification",
    "href": "practicals/Practical-12-Grouping_Data.html#random-forest-classification",
    "title": "Practical 12: Grouping Data",
    "section": "Random Forest Classification",
    "text": "Random Forest Classification\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\nWe’re going to use a Random Forest Classifier but the nice thing about sklearn is that you can quite easily swap in other classifiers if you’d like to explore further. This is one big advantage of Python over R in my book: whereas R tends to get new algorithms first, they are often implemented independently by many people and you can end up with incompatible data structures that require a lot of faff to reorganise for a different algorithm. Python is a bit more ‘managed’ and the dominance of numpy and sklearn and pandas means that people have an incentive to contribute to this library or, if it’s genuinely novel, to create an implementation that works like it would if it were part of sklearn!\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: So here's an _actual_ Machine Learning implementation, but you'll have seen a lot of parts of the code before! \n\n\n\nTrain/Test Split\ntrain, test = train_test_split(classifier_in, test_size=0.2, random_state=42)\nprint(f\"Train contains {train.shape[0]:,} records.\")\nprint(f\"Test contains {test.shape[0]:,} records.\")\ny_train = train.room_class\nX_train = train.drop(columns=['room_class','room_type'])\n\ny_test  = test.room_class\nX_test  = test.drop(columns=['room_class','room_type'])\n\n\nClassifier Setup\nrfc = RandomForestClassifier(\n    max_depth=8,\n    min_samples_split=7,\n    n_jobs=4,\n    random_state=42\n)\n\n\nFit and Predict\nrfc.fit(X_train, y_train)\ny_hat = rfc.predict(X_test)"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#validate",
    "href": "practicals/Practical-12-Grouping_Data.html#validate",
    "title": "Practical 12: Grouping Data",
    "section": "Validate",
    "text": "Validate\n\n\n\n\n\n\nDifficulty: Hard.\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nc_matrix = pd.DataFrame(confusion_matrix(y_test, y_hat))\nc_matrix.index = le.inverse_transform(c_matrix.index)\nc_matrix.columns = le.inverse_transform(c_matrix.columns)\nc_matrix\n\n\nFeature Importance\nCompare the Random Forest’s built-in ‘feature importance’ with Permutation Feature Importance as documented here.\n\n\n\n\n\n\nNote\n\n\n\n**&#128279; Connections**: This next section is the reason you shouldn't blindly run ML algorithms on your data. It turns out that the Random Forest is seriously affected by the scale of different variables, and of binary variables in particular. You will tend to get erroneous feature importance values back from `sklearn`'s RF implementation and should _normally_ look at Permutation Feature Importance values instead. But this little demonstration also shows (above) a more subtle issue: imbalanced data. There are far fewer hotels than there are private rooms, and far fewer of _those_ than there are entire home/apt listings in the sample. So you'll see that the RF has trouble predicting the classes correctly: that's because with a data set like this it's hard to to _better_ than just predicting entire home/apt _Every Single Time_.\n\n\nmdi_importances = pd.Series(\n    rfc.feature_importances_, index=rfc.feature_names_in_\n).sort_values(ascending=True)\n\nax = mdi_importances.plot.barh()\nax.set_title(\"Random Forest Feature Importances (MDI)\")\nax.figure.tight_layout()\n\n\nPermutation Feature Importance\nresult = permutation_importance(\n    rfc, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X_test.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (Test Set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#shapely-values",
    "href": "practicals/Practical-12-Grouping_Data.html#shapely-values",
    "title": "Practical 12: Grouping Data",
    "section": "Shapely Values",
    "text": "Shapely Values\nShapely values are a big part of explainable AI and they work (very broadly) by permuting the data to explore how sensitive the predictions made by the model are to the results that you see. For these we need to install two libraries: shap (to do the heavy lifting) and slicer to deal with the data.\n\nInstall Libraries\nWe should now have this already available in Docker, but just in case…\ntry:\n    import shap\nexcept ModuleNotFoundError:\n    ! pip install slicer shap\n    import shap\n\n\nCheck for Data Types\nYou are looking for anything other than int64 or float64 for the most part. Boolean should be fine, but pandas’ internal, nullable integer type will give you a ufunc error.\nX_test.info() \nX_test['beds'] = X_test.beds.astype('int')\n\n\nPlot Partial Dependence\nshap.partial_dependence_plot(\n    \"price\", rfc.predict, X_test, ice=False,\n    model_expected_value=True, feature_expected_value=True\n)\n\n\nCalculate Shapely Values\nThis can take a long time: 4-5 hours (!!!) without developing a strategy for tackling it. See the long discussion here. I’ve taken the approach of subsetting the data substantially (the model is already trained so it won’t impact the model’s predictions) with a 20% fraction of the test data and an explainer sample of 5%. On my laptop the ‘Permutation explainer’ stage took about 14 minutes, but your results may obviously be rather different.\nXsample = shap.utils.sample(X_test.sample(frac=0.2, random_state=41), 10)\nexplainer = shap.Explainer(rfc.predict, Xsample)\nNow we calculate the shap values for the 5% sample from X_test.\n\n\n\n\n\n\nCaution\n\n\n\n**&#9888; Warning**: This next block is the one that takes a long time to run. I got between 3mn and 4mn.\n\n\n%%time\nshap_values = explainer(X_test.sample(frac=.05, random_state=42))\n\n\nSingle Observation\nNow you can take any random record (sample_ind) and produce a shap plot to show the role that each attribute played in its classification. Note that getting these plots to save required some searching on GitHub.\nsample_ind=250\nshap.plots.waterfall(shap_values[sample_ind], max_display=14, show=False);\nplt.title(f\"Shapely values for observation #{sample_ind} ({X_test.sample(frac=.05, random_state=42).iloc[sample_ind].name})\")\nplt.tight_layout()\n#plt.savefig('practical-09-waterfall.png', dpi=150)\n\n\n\nShapely Feature Plot for Feature 250\n\n\n\n\nAll Observations\nshap.plots.beeswarm(shap_values, show=False)\nplt.title(f\"Shapely Swarm Plot for Sample\")\nplt.tight_layout()\nplt.savefig('practical-09-swarm.png', dpi=150)\n\n\n\nShapely Swarm Plot"
  },
  {
    "objectID": "practicals/Practical-12-Grouping_Data.html#wrap-up",
    "href": "practicals/Practical-12-Grouping_Data.html#wrap-up",
    "title": "Practical 12: Grouping Data",
    "section": "Wrap-Up",
    "text": "Wrap-Up\n\nFind the appropriate eps value: Nearest Neighbour Distance Functions or Interevent Distance Functions\nClustering Points\nRegionalisation algorithms with Aglomerative Clustering\n\nYou’ve reached the end, you’re done…\nEr, no. This is barely scratching the surface! I’d suggest that you go back through the above code and do three things: 1. Add a lot more comments to the code to ensure that really have understood what is going on. 2. Try playing with some of the parameters (e.g. my thresholds for skew, or non-normality) and seeing how your results change. 3. Try outputting additional plots that will help you to understand the quality of your clustering results (e.g. what is the makeup of cluster 1? Or 6? What has it picked up? What names would I give these clsuters?).\nIf all of that seems like a lot of work then why not learn a bit more about machine learning before calling it a day?\nSee: Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html",
    "href": "practicals/Practical-08-Textual_Data.html",
    "title": "Practical 8: Working with Text",
    "section": "",
    "text": "A lot of the content here is provided to help you understand what text-cleaning does and how it generates tokens that can be processed by the various analytical approaches commonly-used in NLP. The best way to think about this is as a practical in two parts, with a bonus ‘Part 2’ that you should not expect to complete unless you probably shouldn’t be taking CASA0013 in the first place:\nThe later parts in Part 2 are largely complete and ready to run; however, that doesn’t mean you should just skip over them and think you’ve grasped what’s happening and it will be easy to apply in your own analyses. I would not pay as much attention to LDA topic mining since I don’t think it’s results are that good, but I’ve included it here as it’s still commonly-used in the Digital Humanities and by Marketing folks. Word2Vec is much more powerful and forms the basis of the kinds of advances seen in ChatGPT and other LLMs."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#required-modules",
    "href": "practicals/Practical-08-Textual_Data.html#required-modules",
    "title": "Practical 8: Working with Text",
    "section": "Required Modules",
    "text": "Required Modules\n\n\n\n\n\n\nNote\n\n\n\nNotice that the number of modules and functions that we import is steadily increasing week-on-week, and that for text processing we tend to draw on quite a wide range of utilies! That said, the three most commonly used are: sklearn, nltk, and spacy.\n\n\nStandard libraries we’ve seen before.\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport re\nimport math\nimport matplotlib.pyplot as plt\n\nThis next is just a small utility function that allows us to output Markdown (like this cell) instead of plain text:\n\nfrom IPython.display import display_markdown\n\ndef as_markdown(head='', body='Some body text'):\n    if head != '':\n        display_markdown(f\"##### {head}\\n\\n&gt;{body}\\n\", raw=True)\n    else:\n        display_markdown(f\"&gt;{body}\\n\", raw=True)\n\nas_markdown('Result!', \"Here's my output...\")\n\nResult!\n\nHere’s my output…"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#loading-data",
    "href": "practicals/Practical-08-Textual_Data.html#loading-data",
    "title": "Practical 8: Working with Text",
    "section": "Loading Data",
    "text": "Loading Data\n\n\n\n\n\n\n🔗 Connections\n\n\n\nBecause I generally want each practical to stand on its own (unless I’m trying to make a point), I’ve not moved this to a separate Python file (e.g. utils.py, but in line with what we covered back in the lectures on Functions and Packages, this sort of thing is a good candidate for being split out to a separate file to simplify re-use.\n\n\nRemember this function from last week? We use it to save downloading files that we already have stored locally. But notice I’ve made some small changes… what do these do to help the user?\n\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -&gt; str:\n    \"\"\"Downloads and caches a remote file locally.\n    \n    The function sits between the 'read' step of a pandas or geopandas\n    data frame and downloading the file from a remote location. The idea\n    is that it will save it locally so that you don't need to remember to\n    do so yourself. Subsequent re-reads of the file will return instantly\n    rather than downloading the entire file for a second or n-th itme.\n    \n    Parameters\n    ----------\n    src : str\n        The remote *source* for the file, any valid URL should work.\n    dest : str\n        The *destination* location to save the downloaded file.\n        \n    Returns\n    -------\n    str\n        A string representing the local location of the file.\n    \"\"\"\n    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[-1] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    # Check if dest+filename does *not* exist -- \n    # that would mean we have to download it! We\n    # also check for *very* small files that are \n    # likely to represent an incomplete download.\n    if not os.path.isfile(dfn) or os.stat(dfn).st_size &lt; 250:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        # Convert the path back into a list (without)\n        # the filename -- we need to check that directories\n        # exist first.\n        path = os.path.split(dest)\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how\n        # to use the 'splat' operator! os.makedirs creates missing\n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        # Download and write the file\n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(response.content)\n            \n        print(\"\\tDone downloading...\")\n\n        # What's this doing???\n        f_size = os.stat(dfn).st_size\n        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n        # And why is it here as well???\n        f_size = os.stat(dfn).st_size\n        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n        \n    return dfn\n\n\n\n\n\n\n\nTip\n\n\n\nFor very large non-geographic data sets, remember that you can use_cols (or columns depending on the file type) to specify a subset of columns to load.\n\n\nLoad the main data set:\n\nymd  = '20240614'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.geoparquet'\n\n\ngdf = gpd.read_parquet( cache_data(url, os.path.join('data','geo')), \n                      columns=['geometry', 'listing_url', 'name', \n                               'description', 'amenities', 'price'])\n\ngdf = gdf.to_crs('epsg:27700')\n\nprint(f\"gdf has {gdf.shape[0]:,} rows and CRS is {gdf.crs.name}.\")\n\nFound data/geo/20240614-London-listings.geoparquet locally!\n    Size is 42 MB (44,000,824 bytes)\ngdf has 85,127 rows and CRS is OSGB36 / British National Grid.\n\n\nLoad supporting Geopackages:\n\nddir  = os.path.join('data','geo') # destination directory\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\n\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( cache_data(spath+'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( cache_data(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n\nFound data/geo/Boroughs.gpkg locally!\n    Size is 1 MB (905,216 bytes)\nFound data/geo/Water.gpkg locally!\n    Size is 0 MB (208,896 bytes)\nFound data/geo/Greenspace.gpkg locally!\n    Size is 1 MB (1,146,880 bytes)\nDone."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#the-description-field",
    "href": "practicals/Practical-08-Textual_Data.html#the-description-field",
    "title": "Practical 8: Working with Text",
    "section": "The Description Field",
    "text": "The Description Field\n\n\n\n\n\n\nDifficulty level: Moderate, because of the questions.\n\n\n\n\n\n\n\n\n\nTo explore the description field properly you’ll need to filter out any NA/NaN descriptions before sampling the result. Hint: you’ll need to think about negation (~) of a method output that tells you if a field is NA.\n\nQuestion\n\ngdf[???].sample(5, random_state=42)[['description']]\n\n\n\n\n\n\n\nStop\n\n\n\nWhat do you notice about the above? Are they simple text? Are there patterns of problems? Are there characters that represent things other than words and simple punctuation?\n\n\n\n\nQuestions\n\nWhat patterns can you see that might need ‘dealing with’ for text-mining to work?\nWhat non-text characters can you see? (Things other than A-Z, a-z, and simple punctuation!)"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#the-amenities-field",
    "href": "practicals/Practical-08-Textual_Data.html#the-amenities-field",
    "title": "Practical 8: Working with Text",
    "section": "The Amenities Field",
    "text": "The Amenities Field\n\n\n\n\n\n\nDifficulty level: Moderate, because of the questions.\n\n\n\n\n\n\n\n\n\nThis field presents a subtle issue that might not be obvious here:\n\ngdf.amenities.sample(5, random_state=42)\n\n17638    [\"Free dryer \\u2013 In unit\", \"Hot water kettle\", \"Shampoo\", \"Luggage dropoff allowed\", \"Microwave\", \"Dining table\", \"Bed linens\", \"Hot water\", \"Iron\", \"Host greets you\", \"First aid kit\", \"Garden view\", \"Wine glasses\", \"Outdoor furniture\", \"Porta...\n84107    [\"Bathtub\", \"Clothing storage: closet\", \"Portable heater\", \"Microwave\", \"Hot water\", \"Host greets you\", \"First aid kit\", \"Security cameras on property\", \"Washer\", \"Fire extinguisher\", \"Freezer\", \"Lock on bedroom door\", \"Refrigerator\", \"Room-darke...\n53166                                                        [\"Hot water\", \"Iron\", \"Outdoor dining area\", \"Lock on bedroom door\", \"Wifi\", \"Shampoo\", \"Luggage dropoff allowed\", \"Smoke alarm\", \"Hangers\", \"Pets allowed\", \"Essentials\", \"Long term stays allowed\"]\n72159    [\"Free dryer \\u2013 In unit\", \"Clothing storage: closet\", \"Hot water kettle\", \"Shampoo\", \"Microwave\", \"Self check-in\", \"Dining table\", \"Bed linens\", \"Hot water\", \"Iron\", \"Conditioner\", \"Wine glasses\", \"Washer\", \"Coffee maker: Nespresso\", \"Inducti...\n84441    [\"Bathtub\", \"Clothing storage: closet\", \"Elevator\", \"Hot water\", \"Carbon monoxide alarm\", \"Central heating\", \"Paid street parking off premises\", \"Single oven\", \"Wifi\", \"Free washer \\u2013 In unit\", \"Refrigerator\", \"Smoke alarm\", \"Kitchen\", \"Dryer...\nName: amenities, dtype: object\n\n\nBut look what happens now, can you see the issue a little more easily?\n\ngdf.amenities.iloc[0]\n\n'[\"Heating\", \"TV with standard cable\", \"Wifi\", \"Smoke alarm\", \"Dryer\", \"Kitchen\", \"Washer\", \"Essentials\"]'\n\n\n\nQuestions\n\nWhat’s the implicit format of the Amenities columns?\nHow could you represent the data contained in the column?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#remove-nan-values",
    "href": "practicals/Practical-08-Textual_Data.html#remove-nan-values",
    "title": "Practical 8: Working with Text",
    "section": "Remove NaN Values",
    "text": "Remove NaN Values\n\n\n\n\n\n\nNote\n\n\n\nI would be wary of doing the below in a ‘proper’ application without doing some careful research first, but to make our lives easier, we’re going to drop rows where one of these values is NaN now so it will simplify the steps below. In reality, I would spend quite a bit more time investigating which values are NaN and why before simply dropping them.\n\n\nAnyway, drop all rows where either the description or amenities (or both) are NA:\n\nQuestion\n\ngdf = gdf.dropna(???)\nprint(f\"Now gdf has {gdf.shape[0]:,} rows.\")\n\nYou should get that there are 84,266 rows."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#luxury-listings",
    "href": "practicals/Practical-08-Textual_Data.html#luxury-listings",
    "title": "Practical 8: Working with Text",
    "section": "Luxury Listings",
    "text": "Luxury Listings\n\n\n\n\n\n\nDifficulty level: Hard, because of the regular expression and questions.\n\n\n\n\n\n\n\n\n\nI would like you to find listings that might (on the basis of word choice) indicate ‘luxury’ accommodation.\n\nCreate the Regular Expression\nYou should start with variations on ‘luxury’ (i.e. luxurious, luxuriate, …) and work out a single regular expression that works for variations on this one word. Later, I would encourage you to come back to this and consider what other words might help to signal ‘luxury’… perhaps words like ‘stunning’ or ‘prestigious’? Could you add those to the regex as well?\nHints: this is a toughy, but…\n\nAll regular expressions work best using the r'...' (which means raw string) syntax.\nYou need to be able to group terms. Recall, however, that in Python a ‘group’ of the form r'(some text)' refers to matching (some text will be ‘memoized’/remembered), whereas what you need here is a “non-capturing group” of the positive lookahead type. That’s a Google clue right there, but you’ve also seen this in the lecture.\n\nIn fact, in my real-world applications you might even need more than one group/non-capturing group in a nested structure.\n\nQuestion\n\ngdf[\n    gdf.description.str.contains(r'???', regex=True, flags=re.IGNORECASE) # &lt;-- The regex\n].sample(3, random_state=42)[['description']]\n\n\n\n\nApply it to Select Data\nAssign it to a new data frame called lux:\n\nQuestion\n\nlux = gdf[gdf.description.str.contains(r'???', regex=True, flags=re.IGNORECASE)].copy()\nlux.to_parquet(os.path.join('data','clean','luxury.geopackage'))\nprint(f\"Found {lux.shape[0]:,} records for 'luxury' flats\")\n\nYou should get that there are 10,367 rows.\n\n\n\nPlot the Data\nNow we are going to create a more complex plot that will give space to both the spatial and price distributions using subplot2grid.\nhelp(plt.subplot2grid)\nNotice that there are two ways to create the plot specified above. I chose route 1, but in some ways route 2 (where you specify a gridspec object and then add the axes might be a bit simpler to work out if you’re starting from scratch.\nThe critical thing here is to understand how we’er initialising a plot that has 4 rows and 1 column even though it is only showing 2 plots. What we’re going to do is set the first plot to span 3 rows so that it takes up 75% of the plot area (3/4), while the second plot only takes up 25% (1/4). They will appear one above the other, so there’s only 1 column. Here’s how to read the key parts of subplot2grid:\n\nnrows – how many rows of plots in the figure.\nncols – how many columns of plots in the figure.\nrow – what row of the figure does this plot start on (0-indexed like a list in Python).\ncol – what column of the figure does this plot start on (0-indexed like a list in Python).\nrowspan – how many rows of the figure does this plot span (not 0-indexed because it’s not list-like).\ncolspan – how many columns of the figure does this plot span (not 0-indexed because it’s not list-like).\n\nEvery time you call subplot2grid you are initialising a new axis-object into which you can then draw with your geopackage or pandas plotting methods.\n\nQuestion\n\n\nf,ax = plt.subplots(1,1,figsize=(9,6))\nax.remove()\n\n# The first plot \nax1 = plt.subplot2grid((4, 1), (???), rowspan=???)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nlux.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((???), (???), rowspan=1)\nlux.price.plot.hist(bins=250, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Luxury\") # &lt;-- How does this differ from title? Change it and see!\nplt.tight_layout() # &lt;-- Try creating the plot *without* this to see what it changes\nplt.show()\n\nYour result should look similar to:\n\n\n\n\n\n‘Luxury’ listings in London\n\n\n\n\n\n\nQuestion\n\nWhat does suptitle do and how is it different from title? Could you use this as part of your plot-making process?\nWhat does tight_layout do?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#budget-listings",
    "href": "practicals/Practical-08-Textual_Data.html#budget-listings",
    "title": "Practical 8: Working with Text",
    "section": "Budget Listings",
    "text": "Budget Listings\n\n\n\n\n\n\nDifficulty level: Easy, because you’ve worked out the hard bits already.\n\n\n\n\n\n\n\n\n\n\nCreate the Regular Expression\nWhat words can you think of that might help you to spot affordable and budget accommodation? Start with just a couple of words and then I would encourage you to consider what other words might help to signal ‘affordability’… perhaps words like ‘cosy’ or ‘charming’ and then think about how you could you add those to the regex?\nHints: this just builds on what you did above with one exception:\n\nI’d try adding word boundary markers to the regex (\\b) where appropriate…\n\n\nQuestion\n\ngdf[\n    gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)\n].sample(5, random_state=42)[['description']]\n\n\n\n\nApply it to Select Data\n\nQuestion\n\naff = gdf[gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)].copy()\naff.to_parquet(os.path.join('data','clean','affordable.geopackage'))\nprint(f\"There are {aff.shape[0]:,} rows flagged as 'affordable'.\")\n\nYou should get that there are 8,937 rows.\n\n\n\nPlot the Data\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\naff.plot(markersize=2, column='price', cmap='viridis', alpha=0.2, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\naff.price.plot.hist(bins=100, ax=ax2)\n\nplt.suptitle(\"Listings Advertising Affordability\")\nplt.tight_layout()\n#plt.savefig(\"Affordable_Listings.png\", dpi=150)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nDo you think that this is a good way to select affordable options?\nDo you understand what dpi means and how savefig works?\nCopy the code from above but modify it to constrain the histogram on a more limited distribution by filtering out the outliers before drawing the plot. I would copy the cell above to one just below here so that you keep a working copy available and can undo any changes that break things."
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#near-bluespace",
    "href": "practicals/Practical-08-Textual_Data.html#near-bluespace",
    "title": "Practical 8: Working with Text",
    "section": "Near Bluespace",
    "text": "Near Bluespace\n\n\n\n\n\n\nDifficulty level: Medium, because you’re still learning about regexes.\n\n\n\n\n\n\n\n\n\nNow see if you can work out a regular expression to find accommodation that emphasises accessibility to the Thames and other ‘blue spaces’ as part of the description? One thing you’ll need to tackle is that some listings seem to say something about Thameslink and you wouldn’t want those be returned as part of a regex looking for rivers. So by way of a hint:\n\nYou probably need to think about the Thames, rivers, and water.\nThese will probably be followed by a qualifier like a ‘view’ (e.g. Thames-view) or a front (e.g. water-front).\nBut you need to rule out things like “close the Thameslink station…”\n\n\nCreate the regular Expression\n\nQuestion\n\ngdf[\n    gdf.description.str.contains(???, regex=???, flags=???)\n].sample(5, random_state=42)[['description']]\n\n\n\n\nApply it to the Select Data\n\nQuestion\n\nbluesp = gdf[\n    (gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE)) |\n    (gdf.description.str.contains(???, regex=True, flags=re.IGNORECASE))\n].copy()\nbluesp.to_parquet(os.path.join('data','clean','bluespace.geopackage'))\nprint(f\"Found {bluesp.shape[0]:,} rows.\")\n\nYou should get that there are 408 rows.\n\n\n\nPlot the Data\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax.remove()\n\n# The first plot\nax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=3)\nwater.plot(edgecolor='none', facecolor=(.25, .25, .7, .25), ax=ax1)\nboros.plot(edgecolor='red', facecolor='none', linewidth=1, alpha=0.75, ax=ax1)\nbluesp.plot(markersize=2, column='price', cmap='viridis', alpha=0.5, scheme='Fisher_Jenks_Sampled', ax=ax1)\n\nax1.set_xlim([500000, 565000])\nax1.set_ylim([165000, 195000]);\n\n# The second plot\nax2 = plt.subplot2grid((4, 1), (3, 0), rowspan=1)\nbluesp.price.plot.hist(bins=100, ax=ax2)\n\nplt.suptitle(\"Bluespace Listings\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nQuestions\n\nHow else might you select listings with a view of the Thames or other bluespaces?"
  },
  {
    "objectID": "practicals/Practical-08-Textual_Data.html#resources",
    "href": "practicals/Practical-08-Textual_Data.html#resources",
    "title": "Practical 8: Working with Text",
    "section": "Resources",
    "text": "Resources\nThere is a lot more information out there, including a whole book and your standard O’Reilly text.\nAnd some more useful links:\n\nPandas String Contains Method\nUsing Regular Expressions with Pandas\nSummarising Chapters from Frankenstein using TF/IDF"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html",
    "href": "practicals/Practical-05-Objects.html",
    "title": "Practical 5: Objects",
    "section": "",
    "text": "This is a very challenging notebook because it takes you both through the process of building a function incrementally and through a ‘simple’ example of how Python classes actually work. You will need to understand these two very different elements in order to make the most of the remaining 6 weeks of term, because we both improve our code incrementally and make use of objects and their inheritances extensively. You also get an extra chance to revisit the differences between LoLs and DoLs because you will undoubtedly encounter and make use of these data structures even after you become a skillfull Python programmer."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#the-way-that-doesnt-work",
    "href": "practicals/Practical-05-Objects.html#the-way-that-doesnt-work",
    "title": "Practical 5: Objects",
    "section": "The Way That Doesn’t Work",
    "text": "The Way That Doesn’t Work\nRecall that this is how four rows of ‘data’ for city sizes organised by row as a list-of-lists look:\n\nmyData = [\n    ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n    ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n    ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n    ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\n\nTo print out a list of every city in the data set when we don’t know where the Name column is in the file we have to jump through some hoops:\n\ncities = []\n\ncol    = myData[0].index('Name')\nfor i in range(1, len(myData)):\n    cities.append(myData[i][col])\n\nprint(\"The cities in the data set are: \" + \", \".join(cities))\n\nThe cities in the data set are: Greater London, Greater Manchester, West Midlands\n\n\nAnd it’s the same kind of faff if we want to find out if Edinburgh is included in the data set:\n\ncol   = myData[0].index('Name')\nfound = False\nfor i in range(1, len(myData)):\n    if myData[i][col] == 'Edinburgh':\n        print(\"Found Edinburgh in the data set!\")\n        found = True\n        break\n\nif found == False:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nDidn't find Edinburgh in the data set."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#the-way-that-does-work",
    "href": "practicals/Practical-05-Objects.html#the-way-that-does-work",
    "title": "Practical 5: Objects",
    "section": "The Way That Does Work",
    "text": "The Way That Does Work\nCompare that code to how it works for a dictionary-of-lists organised by column. Now try printing out the cities in the data:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['Greater London', 'Greater Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nTo print out a list of every city in the data set:\n\nprint(\", \".join(myData['Name']))\n\nGreater London, Greater Manchester, Birmingham, Edinburgh, Inverness, Lerwick\n\n\nTo find out if Edinburgh is included in the list of data:\n\nif 'Edinburgh' in myData['Name']:\n    print(\"Found Edinburgh in the data set!\")\nelse:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nFound Edinburgh in the data set!\n\n\nSee how even basic questions like “Is Edinburgh in our list of data?” are suddenly easy to answer? We no longer need to loop over the entire data set in order to find one data point. In addition, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already. But let’s test this out and see how it works."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#calculate-mean",
    "href": "practicals/Practical-05-Objects.html#calculate-mean",
    "title": "Practical 5: Objects",
    "section": "Calculate Mean",
    "text": "Calculate Mean\nLet’s start by calculating the sample mean (use Google: Python numpy mean...):\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nmean = np.mean(myData['Population'])\nprint(f\"City distribution has a mean of {mean:,.0f}.\")\n\nCity distribution has a mean of 2,435,442."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#calculate-standard-deviation",
    "href": "practicals/Practical-05-Objects.html#calculate-standard-deviation",
    "title": "Practical 5: Objects",
    "section": "Calculate Standard Deviation",
    "text": "Calculate Standard Deviation\n\n\n\n\n\n\nDifficulty level: Low-ish.\n\n\n\n\n\n\n\n\n\nNow let’s do the standard deviation:\n\nQuestion\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nstd  = np.??(??)\nprint(f\"City distribution has a standard deviation of {std:,.2f}.\")\n\nSo the numpy package gives us a way to calculate the mean and standard deviation quickly and without having to reinvent the wheel. The other potentially new thing here is {std:,.2f}. This is about string formatting and the main thing to recognise is that this means ‘format this float with commas separating the thousands/millions and 2 digits to the right’. The link I’ve provided uses the slightly older approach of &lt;str&gt;.format() but the formatting approach is the same."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#for-loops-without-for-loops",
    "href": "practicals/Practical-05-Objects.html#for-loops-without-for-loops",
    "title": "Practical 5: Objects",
    "section": "For Loops Without For Loops",
    "text": "For Loops Without For Loops\n\n\n\n\n\n\nDifficulty level: Medium.\n\n\n\n\n\n\n\n\n\nNow we’re going to see something called a List Comprehension.\nIn Python you will see code like this a lot: [x for x in list]. This syntax is known as a ‘list comprehension’ and is basically a for loop on one line with the output being assigned to a list. So we can apply an operation (converting to a string, subtracting a value, etc.) to every item in a list without writing out a full for loop.\nHere’s a quick example just to show you what’s going on:\n\ndemo = range(0,10) # &lt;- a *range* of numbers between 0 and 9 (stop at 10)\nprint([x**2 for x in demo]) # square every element of demo\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNow let’s apply this to our problem. We calculated the the mean and standard deviation above, so now we want to apply the z-score formula to every element of the Population list… Remember that the format for the z-score (when dealing with a sample) is:\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\]\nAnd the population standard deviation (by which I mean, if you are dealing with all the data, and not a subsample as we are here) is:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\nQuestion\n\nrs = [(x - ??)/?? for x in myData['Population']] # rs == result set\nprint([f\"{x:.3f}\" for x in rs])"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#appending",
    "href": "practicals/Practical-05-Objects.html#appending",
    "title": "Practical 5: Objects",
    "section": "Appending",
    "text": "Appending\n\n\n\n\n\n\nDifficulty level: trivial\n\n\n\n\n\n\n\n\n\nAnd now let’s add it to the data set:\n\nmyData['Std. Population'] = rs\nprint(myData['Std. Population'])\n\n[np.float64(2.1579383252868527), np.float64(0.0791199354729932), np.float64(-0.3797024575689938), np.float64(-0.45025269939207097), np.float64(-0.6942995760276591), np.float64(-0.7128035277711219)]\n\n\nAnd just to show how everything is in a single data structure:\n\nfor c in myData['Name']:\n    idx = myData['Name'].index(c)\n    print(f\"{c} has a population of {myData['Population'][idx]:,} and standardised score of {myData['Std. Population'][idx]:.3f}\")\n\nGreater London has a population of 9,787,426 and standardised score of 2.158\nGreater Manchester has a population of 2,705,000 and standardised score of 0.079\nBirmingham has a population of 1,141,816 and standardised score of -0.380\nEdinburgh has a population of 901,455 and standardised score of -0.450\nInverness has a population of 70,000 and standardised score of -0.694\nLerwick has a population of 6,958 and standardised score of -0.713"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#downloading-from-a-url",
    "href": "practicals/Practical-05-Objects.html#downloading-from-a-url",
    "title": "Practical 5: Objects",
    "section": "Downloading from a URL",
    "text": "Downloading from a URL\nLet’s focus on the first part first because that’s the precondition for everything else. If we can get the ‘download a file from a URL’ working then the rest will gradually fall into place through iterative improvments!\n\nFinding an Existing Answer\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nFirst, let’s be sensibly lazy–we’ve already written code to read a file from the Internet and turn it into a list of lists. So I’ve copy+pasted that into the code block below since we’re going to start from this point; however, just to help you check your own understanding, I’ve removed a few bits and replaced them with ??. Sorry, it’s good practice. 😈\n\nQuestion\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n\nurlData = [] # Somewhere to store the data\n\nresponse = urlopen(url) # Get the data using the urlopen function\ncsvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n\nfor row in csvfile:\n    urlData.append(??)\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\n\nYou should get:\nurlData has 11 rows and 4 columns.  [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]\n\n\n\nGetting Organised\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nLet’s take the code above and modify it so that it is:\n\nA function that takes two arguments: a URL; and a destination filename.\nImplemented as a function that checks if a file exists already before downloading it again.\n\nYou will find that the os module helps here because of the path function. And you will need to Google how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an explanation included of why a particular answer is a ‘good one’. I also look at which answers got the most votes (not always the same as the one that was the ‘accepted answer’). In this particular case, I also found this answer useful.\nI would start by setting my inputs:\n\nimport os\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\nout = os.path.join('data','Wikipedia-Cities.csv') # Print `out` if you aren't sure what this has done!\n\n\n\nSketching the Function\n\n\n\n\n\n\nDifficulty level: Low, if you’ve watched the videos…\n\n\n\n\n\n\n\n\n\nThen I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:\n\nQuestion\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n    \n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(??):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n        \nget_url(url, out)\n\n\n\n\nFleshing Out the Function\n\n\n\n\n\n\nDifficulty level: Medium\n\n\n\n\n\nIf you really explore what’s going on in the function rather than just running it and moving on.\n\n\n\nI would then flesh out the code so that it downloads the file if it isn’t found and then, either way, returns the local file path for our CSV reader to extract:\n\ndef get_url(src, dest):\n    \n    # Check if dest does *not* exist -- that\n    # would mean we had to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found locally!\")\n    else:\n        print(f\"{dest} not found, downloading!\")\n        \n        # Get the data using the urlopen function\n        response = urlopen(src) \n        filedata = response.read().decode('utf-8')\n        \n        # Extract the part of the dest(ination) that is *not*\n        # the actual filename--have a look at how \n        # os.path.split works using `help(os.path.split)`\n        path = list(os.path.split(dest)[:-1])\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how \n        # to use the 'splat' operator! os.makedirs creates missing \n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n        \n        with open(dest, 'w') as f:\n            f.write(filedata)\n            \n        print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#decorating",
    "href": "practicals/Practical-05-Objects.html#decorating",
    "title": "Practical 5: Objects",
    "section": "Decorating!",
    "text": "Decorating!\nLet’s now look into simplifying this code using a dectorator! Our function has become a bit unwieldy and we want to look at how we can simplify that.\nThe ‘obvious’ (i.e. not obvious) way to do this is to implement the check for a local copy as a decorator on the downloading function. So we have a function that downloads, and a decorator function that checks if the download should even be triggered.\n\nfrom functools import wraps\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        src  = args[0]\n        dest = args[1]\n        if os.path.isfile(dest):\n            print(f\"{dest} found locally!\")\n            return(dest)\n        else:\n            print(f\"{dest} not found, downloading!\")\n            return(f(src, dest))\n    return wrapper\n\n@check_cache\ndef get_url(src, dest):    \n    # Get the data using the urlopen function\n    response = urlopen(src) \n    filedata = response.read().decode('utf-8')\n     \n    # Extract the part of the dest(ination) that is *not*\n    # the actual filename--have a look at how \n    # os.path.split works using `help(os.path.split)`\n    path = list(os.path.split(dest)[:-1])\n     \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how \n    # to use the 'splat' operator! os.makedirs creates missing \n    # directories in a path automatically.\n    if len(path) &gt;= 1 and path[0] != '':\n        os.makedirs(os.path.join(*path), exist_ok=True)\n     \n    with open(dest, 'w') as f:\n        f.write(filedata)\n         \n    print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!\n\n\nI’m not going to pretend that’s the best use of a decorator, but it does neatly separate the downloading function from the caching function. In fact, there is already a cache decorator and some of these have unlimited capacity; however, they are intended to run in a ‘live’ context, so you’d still need to download the file again any time you start a new notebook or restart Docker. This caching function saves the actual data locally to dest.\n\n\n\n\n\n\nStop!\n\n\n\nIt really would be a good idea to put in the effort to make sense of how this function works. There is a lot going on here and understanding how this function works will help you to understand how to code. You should notice that we don’t try to check if the data file contains any useful data! So if you download or create an empty file while testing, you won’t necessarily get an error until you try to turn it into data afterwards!"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#parse-the-csv-file",
    "href": "practicals/Practical-05-Objects.html#parse-the-csv-file",
    "title": "Practical 5: Objects",
    "section": "Parse the CSV File",
    "text": "Parse the CSV File\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nNow we turn to the next task: parsing the file if it’s a CSV. This implies that it might not be so that’s something we should also consider!\n\nQuestion\n\nimport csv\n\ndef read_csv(src):\n    \n    csvdata = []\n    with open(src, 'r') as f:\n        csvr = csv.??(f)\n        \n        for r in csvr:\n            csvdata.append(??)\n    \n    # Return list of lists\n    return ??\n\nread_csv(src)\n#read_csv('foo.bar') # &lt;- Notice what happens if you try to run this code\n#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!\n\nYou should get:\n [[‘City’, ‘Population’, ‘Latitude’, ‘Longitude’], [‘Perth’, ‘45770’, ‘56.39583’, ‘-3.43333’], [‘Armagh’, ‘14777’, ‘54.3499’, ‘-6.6546’], [‘Dundee’, ‘147268’, ‘56.462’, ‘-2.9707’], [‘Colchester’, ‘194706’, ‘51.88861’, ‘0.90361’], [‘Salisbury’, ‘40302’, ‘51.07’, ‘-1.79’], [‘Portsmouth’, ‘205056’, ‘50.80583’, ‘-1.08722’], [‘Wakefield’, ‘325837’, ‘53.683’, ‘-1.499’], [‘Bradford’, ‘522452’, ‘53.792’, ‘-1.754’], [‘Lancaster’, ‘138375’, ‘54.047’, ‘-2.801’], [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]]"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#convert-the-csv-into-a-dol",
    "href": "practicals/Practical-05-Objects.html#convert-the-csv-into-a-dol",
    "title": "Practical 5: Objects",
    "section": "Convert the CSV into a DoL",
    "text": "Convert the CSV into a DoL\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow we can focus on converting the CSV data to a dictionary-of-lists! We’re going to start with the same function name but expand what the function does. This kind of iteration is common in software development.\n\nQuestion\n\ndef read_csv(src):\n    \n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(??, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in ??: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[??].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nread_csv(src)\n\nYou should get something that starts:\n\n\n{'City': ['Perth', 'Armagh', 'Dundee', 'Colchester', 'Salisbury', 'Portsmou..."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#adding-docstring",
    "href": "practicals/Practical-05-Objects.html#adding-docstring",
    "title": "Practical 5: Objects",
    "section": "Adding Docstring",
    "text": "Adding Docstring\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nWe’ve assumed that the first row of our data set is always a header (i.e. list of column names). If it’s not then this code is going to have problems. A robust function would allow us to specify column names, skip rows, etc. when we create the data structure, but let’s not get caught up in that level of detail. Notice that I’ve also, for the first time:\n\nUsed the docstring support offered by Python. You’ll be able to use help(...) and get back the docstring help!\nProvided hints to Python about the expected input and output data types. This can help to ensure consistency and is also critical in testing / continuous integration when working with others on a codebase.\n\n\ndef read_csv(src:str) -&gt; dict:\n    \"\"\"\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n    \n    param src: a local CSV file\n    returns: a dictionary-of-lists\n    \"\"\"\n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in csvr: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[c].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nds = read_csv(src)\n\n\nhelp(read_csv)\n\nHelp on function read_csv in module __main__:\n\nread_csv(src: str) -&gt; dict\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n    \n    param src: a local CSV file\n    returns: a dictionary-of-lists\n\n\n\n\nprint(\"Columns are: \" + \", \".join(ds.keys()))\nprint(f\"First two cities are: {ds['City'][:2]}\")\nprint(f\"First two populations are: {ds['Population'][:2]}\")\nprint(f\"First two latitudes are: {ds['Latitude'][:2]}\")\n\nColumns are: City, Population, Latitude, Longitude\nFirst two cities are: ['Perth', 'Armagh']\nFirst two populations are: ['45770', '14777']\nFirst two latitudes are: ['56.39583', '54.3499']"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#creating-a-package",
    "href": "practicals/Practical-05-Objects.html#creating-a-package",
    "title": "Practical 5: Objects",
    "section": "Creating a Package",
    "text": "Creating a Package\nWe’re not going to tackle this now, but it’s important that you understand how what we’ve done connects to what we’re about to do, and the concept of a package is the bridge. We’ve already covered this in the pre-recorded lectures, but if you want to actually try to create your own package, the simplest way to do this is to:\n\nCopy the read_csv into a new file called, for instance, utils.py.\nMake sure you delete this function from the current ‘namespace’ (del(read_csv)) by which I mean that the read_csv function no longer exists (running help(read_csv) should give you an error!).\nTry importing the function from the file: from utils import read_csv and run the help(read_csv) code again.\n\nAssuming that you’ve done everything correctly, we’ve now brought in code from another file without having to write it into our main Python script file. In Python, many of the most complex libraries are spread across the equivalent of many utils.py files, but on top of that when we import and run them they are also creating objects from classes defined in those files.\nWhat we now want to do is use a fairly simple example using different ‘shapes’ (pyramids, cubes, etc.) that allow us to explore how classes work through inheritance from parents and can extend of overwrite the functionality provided by the parent class. We’ll need this understanding in order to grasp how Pandas and GeoPandas work specifically, but also how Python works more generally."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#abstract-base-class",
    "href": "practicals/Practical-05-Objects.html#abstract-base-class",
    "title": "Practical 5: Objects",
    "section": "Abstract Base Class",
    "text": "Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\nWe can now create a new shape object (an instance of the shape class) but we can’t do much that is useful with it:\n\ns = shape(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nI am a Shape\nError: Unimplmented method error."
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#cube",
    "href": "practicals/Practical-05-Objects.html#cube",
    "title": "Practical 5: Objects",
    "section": "Cube",
    "text": "Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nQuestion\nCan you work out the missing elements that will allow you to create a cube class?\n\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return (3 * self.??**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return ??*(self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = cube(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#sphere",
    "href": "practicals/Practical-05-Objects.html#sphere",
    "title": "Practical 5: Objects",
    "section": "Sphere",
    "text": "Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2r\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nQuestion\nCan you work out the missing elements that will allow you to create a cube class?\n\n# Sphere class\nfrom math import pi\nclass sphere(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        # Something...\n\n    def diameter(self):\n        # Something...\n\n    def volume(self):\n        # Something\n\n    def surface(self):\n        # Something\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = sphere(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#regular-pyramid",
    "href": "practicals/Practical-05-Objects.html#regular-pyramid",
    "title": "Practical 5: Objects",
    "section": "Regular Pyramid",
    "text": "Regular Pyramid\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\n\n\n\n\n\n\nClass Variables\n\n\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them! This class variable is set automatically for all instances of the pyramid class. Changing this variable can have weird effects so they’re not often changed.\n\n\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n\n    has_mummies = True # This is for *all* regular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n\n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n\n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n\n    def volume(self):\n        return self.dim**2 * self.height() / 3\n\n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#triangular-pyramid",
    "href": "practicals/Practical-05-Objects.html#triangular-pyramid",
    "title": "Practical 5: Objects",
    "section": "Triangular Pyramid",
    "text": "Triangular Pyramid\nWe have chosen for triangular pyramid to inherit from regular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nJust to note, as well, that since all sides are equal this is an equilateral triangular pyramid. Anyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\nTriangular pyramids do not have a problem with mummies.\nWhy don’t you add some documentation to this class and the regular pyramid class so that we know how to use them correctly?\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n\n    has_mummies = False # This is for all triangular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n\n    def diameter(self):\n        return self.dim\n\n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n\n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n\n    def volume(self):\n        return (1/3) * self.base() * self.height()\n\n    def surface(self):\n        return 4 * self.base()"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#testing-your-classes",
    "href": "practicals/Practical-05-Objects.html#testing-your-classes",
    "title": "Practical 5: Objects",
    "section": "Testing Your Classes",
    "text": "Testing Your Classes\nIf you’ve implemented everything correctly then the following code should run.\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\nI get the following output:\n\n\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaargh!\n\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'"
  },
  {
    "objectID": "practicals/Practical-05-Objects.html#packaging-it-up",
    "href": "practicals/Practical-05-Objects.html#packaging-it-up",
    "title": "Practical 5: Objects",
    "section": "Packaging It Up",
    "text": "Packaging It Up\nWait, you’re still working on this practical and haven’t thrown up your hands in disgust yet? OK, in that case you can have one more thing to do: turn all the shapes into a package that can be loaded via an import statement.\n\nCell Magic\nThis code allows Jupyter to reload external libraries if they are edited after you import them. When you are working on your own packages this is rather useful since you tend to make a lot of mistakes when packaging code up this way and it’s handy not to have to restart the entire notebook every time you fix a typo or change a function.\n%load_ext autoreload\n%autoreload 2\n\n\nImport Shapes\nMy suggestion is that you create a directory called shapes and copy all of the shape code (that’s the code for shape, cube, sphere, pyramid, tpyramid) into a file called __init__.py inside the shapes directory. You should then able to run the following:\nfor s in ['shape','sphere','cube','pyramid','t_pyramid']:\n    if s in locals():\n        del(s)\nfrom shapes import *\nWe need those first three lines of code to delete the existing classes from Python’s ‘memory’ so that we can be sure we’re importing the versions we saved to shapes/__init__.py.\n\n\nAdding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\nThis would then allow you to run:\nfrom shapes import * # &lt;-- Change this if you didn't call your package `shapes`!\nhelp(shape)\nhelp(shape.type)"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html",
    "href": "practicals/Practical-01-Getting_Started.html",
    "title": "Practical 1: Getting Started",
    "section": "",
    "text": "This week’s practical is focussed on getting you set up with the tools and accounts that you’ll need to across many of the CASA modules in Terms 1 and 2, and familiarising you with ‘how people do data science’. Outside of academia, it’s rare to find a data scientist who works entirely on their own: most code is collaborative, as is most analysis! But collaborating effectively requires tools that: get out of the way of doing ‘stuff’; support teams in negotating conflicts in code; make it easy to share results; and make it easy to ensure that everyone is ‘on the same page’.\nYou will find things confusing this week, but they will start to make more sense as we move further into the module. The key is to keep trying things out and to ask for help when you get stuck.\nIf you haven’t done this already, you need to follow the instructions for installing all of the tools listed on on the CASA Computing Environment web page. This process will take time, so please keep reading or work on other things to do while you wait!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#starting-up-right",
    "href": "practicals/Practical-01-Getting_Started.html#starting-up-right",
    "title": "Practical 1: Getting Started",
    "section": "Starting Up ‘Right’",
    "text": "Starting Up ‘Right’\nOne of the most confusing things about starting a Docker container with a “local volume mount” (i.e. a location on your computer that Docker connects to the container’s file system) is that it seems like magic and it’s often hard to understand why you’re seeing what you are under the work directory.\nSo before you do anything else please spend a minute in the Terminal (macOS) or Power Shell (Windows) learning how to get to your home directory and, within that, to a CASA directory where you can store your work and keep Docker from accessing data that it shouldn’t.\nWhat we are doing is creating a directory on your computer that you can access from Docker. This is where you will store your notebooks, data, and any other files that you need to work with. The ultimate structure we’ll produce this:\n\n\n\n‘Target’ Directory Structure\n\n\nOn both a Mac and a PC you should be able to run the following:\n\ncd $HOME – this will change directory to your home directory (on a Mac it will be /Users/&lt;your username&gt;, on a PC it will be something like C:\\Users\\&lt;your username&gt;). Hint: cd means ‘change directory’!\ncd Documents – this will move you into your ‘Documents’ folder. Note: on Windows this might be called My\\ Documents, in which case it’s cd My\\ Documents! If you have set up your computer in another language this might be called something else, but Power Shell still ‘knows’ which folder should contain your documents.\nmkdir CASA – this will create a CASA folder in your home directory.\ncd CASA – you are now changing into the CASA directory.\necho $pwd (PC) or pwd (Mac) – this should show you the ‘full path’ to your new CASA directory (e.g. /Users/&lt;your username&gt;/Documents/CASA or something like that).\n\nLeave the Terminal or PowerShell window open! You will need it in a moment.\nPerhaps a video will help clarify?"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#configuring-the-image",
    "href": "practicals/Practical-01-Getting_Started.html#configuring-the-image",
    "title": "Practical 1: Getting Started",
    "section": "Configuring the sds2024 Image",
    "text": "Configuring the sds2024 Image\nDuring the ‘install festival’ you should have installed Docker and, time permitting, ‘pulled’ the image appropriate to your system. If you haven’t, then you should do so as a priority now."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#running-docker-1",
    "href": "practicals/Practical-01-Getting_Started.html#running-docker-1",
    "title": "Practical 1: Getting Started",
    "section": "Running Docker",
    "text": "Running Docker\nBy default, the best way to start Docker is from the Terminal or Power Shell.\n\nOn Windows\nUsing the same Power Shell copy and paste the following all on one line:\ndocker run --rm -d --name sds2024 -p 8888:8888 \n  -v \"$(pwd):/home/jovyan/work\" jreades/sds:2025-amd start.sh jupyter lab \n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\n\n\n\n\n\n\nWindows Commands and Docker\n\n\n\n$(pwd) is actually a command, you are asking the Power Shell to use the current working directory* (pwd == print working directory) as the ‘mount point’ for the work directory. The Command Prompt doesn’t support pwd, but the Power Shell should. You can check this by simply typing pwd and hitting enter (⏎) to see if you get an error.\n\n\n\n\nOn macOS\nUsing the same Terminal copy and past the following (change the docker image to jreades/sds:2025-amd if using an older Intel Mac):\ndocker run --rm -d --name sds2024 -p 8888:8888 \\\n   -v \"$(pwd):/home/jovyan/work\" \\\n  jreades/sds:2025-arm start.sh jupyter lab \\\n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#how-do-i-know-it-worked",
    "href": "practicals/Practical-01-Getting_Started.html#how-do-i-know-it-worked",
    "title": "Practical 1: Getting Started",
    "section": "How do I Know it Worked?",
    "text": "How do I Know it Worked?\nWith Docker running, you will mainly interact with Python through a web page. To check if it’s running, we just have to visit the web page and see what happens: http://localhost:8888/lab/tree/work/. We’ll talk more about exactly what is going on over the next few weeks, but this should show you a page that looks something like this (probably with fewer files listed on the left-hand side):\n\n\n\nScreenshot of Jupyter Lab\n\n\n\n\n\n\n\n\nSee Docker Run (and Run)…\n\n\n\nOnce you have started a Docker container using any of the above, the machine will continue running until you either restart the computer or tell Docker to stop the container. This can consume memory and battery power indefinitely."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#your-new-github-account",
    "href": "practicals/Practical-01-Getting_Started.html#your-new-github-account",
    "title": "Practical 1: Getting Started",
    "section": "Your New GitHub Account",
    "text": "Your New GitHub Account\nIt doesn’t really matter which way you do this, but we recommend that you set up your new GitHub account with both your personal and your UCL email addresses. GitHub ‘knows’ about educational users and will give you access to more features for free if you associate a .ac.uk email address to your account. So choose one email address to start with and then add the other one later.\nFrom a security standpoint you should also enable 2-factor authentication so that you receive a text message when you log in on a new machine and are asked to confirm a code."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#creating-a-private-repository",
    "href": "practicals/Practical-01-Getting_Started.html#creating-a-private-repository",
    "title": "Practical 1: Getting Started",
    "section": "Creating a Private Repository",
    "text": "Creating a Private Repository\nTo create a repository, click on the + at the upper-right corner of the GitHub web page and select New Repository. You might as well call your ‘repo’ fsds or foundations since that’s a lot shorter than foundations_of_spatial_data_science.\n\n\n\n\n\n\nYour ‘Repo’ Name\n\n\n\nFor the purposes of this tutorial (and all subsequent tutorials) I will assume that your repository is called fsds. You can call it whatever you like, in which case you will always need to substitute the name that you chose wherever you see me write fsds.\n\n\nIt’s always helpful to provide some basic information about what’s in the project (e.g. your notes and practicals for the Foundations module). And finally, make sure you:\n\nChange the visibility from Public to Private,\nTick Add a README file,\nChange Add .gitignore from None to template: Python.\n\nClick Create Repository and you should end up on a page that looks like this:\n\n\n\nRepository created\n\n\nYour new repository has been created (on GitHub)!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#exclude-data-files",
    "href": "practicals/Practical-01-Getting_Started.html#exclude-data-files",
    "title": "Practical 1: Getting Started",
    "section": "Exclude Data Files",
    "text": "Exclude Data Files\nWe want to make it hard to accidentally add a large data file to our repository. Git/GitHub isn’t designed for large, binary files (you can’t ‘read’ a Parquet file) and we assume that data is backed up or available elsewhere, but our code is not! So as a first step we want to exclude files that are likely to just be ‘data’:\n\n\n\nFile Type\nExtension\n\n\n\n\nCSV\n.csv\n\n\nExcel\n.xls, .xlsx\n\n\nZip\n.zip\n\n\nGZip\n.gzip\n\n\nFeather\n.feather, .geofeather\n\n\nParquet\n.parquet, .geoparquet\n\n\n\nHere’s another hint: C extensions are already excluded by the .gitignore file, so maybe look to see how that’s done to help you figure out how to exlcude .zip, .gz, and .csv files…"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#exclude-a-data-directory",
    "href": "practicals/Practical-01-Getting_Started.html#exclude-a-data-directory",
    "title": "Practical 1: Getting Started",
    "section": "Exclude a Data Directory",
    "text": "Exclude a Data Directory\nTo make it even less like that we accidentally include data, let’s also exclude a data directory from our repository. As a clue, nearly everything in the Distribution / packaging section of the .gitignore file is a directory to be excluded from Git.\nSo how would you indicate that data is a directory? Once you’re sure, add the data directory!\nWhen you are done, don’t forget to add a ‘commit message’ (e.g. ‘Added data files to .gitignore’) at the bottom and then click Commit changes.\n\n\n\n\n\n\nQuick Answer\n\n\n\n\n\nI don’t want you to get hung up on this one thing in Practical 1, so if you just can’t make sense of what you’re being asked to do here, have a look at the Answers at the bottom of this page."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#check-your-changes",
    "href": "practicals/Practical-01-Getting_Started.html#check-your-changes",
    "title": "Practical 1: Getting Started",
    "section": "Check Your Changes",
    "text": "Check Your Changes\nOnce you have committed your changes, you should be back to the default view of the .gitignore file but there should be a message to the effect of Latest commit &lt;some hexadecimal number&gt; 10 seconds ago and, next to that, a History button.\nClick on ‘History’ and let’s go back in time!\n\n\n\nThe Gitignore history\n\n\nOn the history page you can browse every edit to your file. Whenever you commit a file, this like taking a snapshot at a point in time. Using the ‘History’ you can compare two different snapshots in order to see what has changed. This would help you to work out how you broke something, check that requested changes have been made, or see how an error might have been introduced.\n\n\n\n\n\n\nViewing Your Commit History\n\n\n\nYou can mouseover the buttons to see what they do. Why don’t you try to find See commmit details and check what edits you made to the .gitignore file? You should see at least three plusses in the history view representing three new lines in the .gitignore file."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#working-on-your-markdown",
    "href": "practicals/Practical-01-Getting_Started.html#working-on-your-markdown",
    "title": "Practical 1: Getting Started",
    "section": "Working on Your Markdown",
    "text": "Working on Your Markdown\nWrite your README file using at least the following Markdown features:\n\nA level-1 header (#)\nA level-3 header (###)\nItalic text (_this is italicised_)\nBold text (**this is bold**)\nA link ([link text](url))\nAn image (![Alt text](image_location))\n\nIf you’re unsure how these work, just double-click on this text and you’ll see Markdown in a Jupyter notebook. Here’s some sample text to get you started:\n### Foundations of Spatial Data Science\n\nThis repository contains practicals and notes from the _Foundations_ module. \n\nYou can find the original [here](https://jreades.github.io/fsds/).\nDon’t forget to check out the “Preview” tab!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#commiting-a-change",
    "href": "practicals/Practical-01-Getting_Started.html#commiting-a-change",
    "title": "Practical 1: Getting Started",
    "section": "Commiting a Change",
    "text": "Commiting a Change\nOnce you’re happy with how your text looks and works, it’s time to commit! Scroll down to where you see something like this (you will see your own GitHub username, not mine):\n\n\n\nGitHub Commit\n\n\nYou can just accept the description (e.g. Create README.md) or you can write your own. You can also provide an extended description if you choose. Then click Commit new file and you will see your new README appear."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#configuring-defaults",
    "href": "practicals/Practical-01-Getting_Started.html#configuring-defaults",
    "title": "Practical 1: Getting Started",
    "section": "Configuring Defaults",
    "text": "Configuring Defaults\nThe first thing to do is set up the default username and email for GitHub. These can be changed on a project-by-project basis, but to begin with it’s best to set up the global defaults. Using either the Terminal or Bash enter the following (replacing &lt;...&gt; with your details):\ngit config --global user.email '&lt;your GitHub email address&gt;'\ngit config --global user.name '&lt;your GibHub username&gt;'\n\n\n\n\n\n\nRecall: Convention!\n\n\n\nAs a reminder, &lt;...&gt; is a convention in programming to indicate that you should replace the text between the &lt; and &gt; with something that makes sense in your context. For example, if you see '&lt;your GitHub email address&gt;' you should type, for example, 'j.reades@ucl.ac.uk'. You do not include the &lt; or &gt; characters!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#creating-a-personal-access-token",
    "href": "practicals/Practical-01-Getting_Started.html#creating-a-personal-access-token",
    "title": "Practical 1: Getting Started",
    "section": "Creating a Personal Access Token",
    "text": "Creating a Personal Access Token\nFor copying changes up to/down from GitHub, you must use a Personal Access Token. This is like issuing special passwords that allow only limited access to parts of your GitHub account.\nTo create a Personal Access Token:\n\nVisit your GitHub User Page (e.g. github.com/jreades)\nClick on your user icon (at the top-right corner of the page) and click the Settings link (near the bottom on the right side of the page).\nScroll down the settings page until you get to Developer settings (near the bottom on the left side of the page).\nClick the Developer settings link to reach the ‘apps’ page and then click on the Personal access tokens link.\nSelect Tokens (classic) from the left-hand menu and then click the Generate new token button.\n\n\n\n\n\n\n\nTypes of Personal Tokens\n\n\n\nYou now need to choose the type of token to generate. I personally find the old type of tokens easier to work with because the ‘new’ fine-grained tokens are intended to support complex workflows when all we’re trying to do is allow one computer to push/pull from Git.\n\n\n\nGenerate new token (classic) for general use token and then specify the following:\n\nI’d suggest writing FSDS Token or something similar in the Note section.\nSet the expiration to 90 days\nClick the repo tickbox for ‘Full control of private repositories’.\n\nSave the resulting token somewhere safe as you will need it again! (e.g. a Note on your phone, a password manager, etc.).\n\n\n\n\n\n\n\nKeep your Personal Token Safe\n\n\n\nYou will need it at least twice in this tutorial and may want to use it again on other computers. You can always create a new one, but then you’ll need to update every computer where you access your GitHub repositories."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#cloning-your-repository",
    "href": "practicals/Practical-01-Getting_Started.html#cloning-your-repository",
    "title": "Practical 1: Getting Started",
    "section": "Cloning Your Repository",
    "text": "Cloning Your Repository\nNow we are going to clone (i.e. copy) the repository that you just created on to your own computer. This is surprisingly straightforward provided that you have installed the command line tools.\nOn your private repository page, click on the green button labeled Code visible in the screenshot below:\n\n\n\nScreenshot of cloning interface\n\n\nYou should then copy your HTTPS URL (in my screenshot it’s https://github.com/jreades/i2p.git).\nNow go back to the Terminal or PowerShell that you left open earlier and type the following (replacing &lt;the_url_that_you_copied_from_the_browser&gt; with the URL that you copied from GitHub):\ngit clone &lt;the_url_that_you_copied_from_the_browser&gt;\nThe first time that you do this, you will need to provide login information. Use your GitHub username and the Personal Access Token that you just created. On your computer you should now see a new directory with the same name as your repository. For example: Documents/CASA/fsds."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#storing-credentials-pulling",
    "href": "practicals/Practical-01-Getting_Started.html#storing-credentials-pulling",
    "title": "Practical 1: Getting Started",
    "section": "Storing Credentials & ‘Pulling’",
    "text": "Storing Credentials & ‘Pulling’\nYou can now activate the credtial helper that will store your Personal Access Token (though you should still keep your secure note!):\ncd fsds\ngit config credential.helper store\ngit pull\nWhen you type git pull you should be asked again for your username and password. You should (again) use the Personal Access Token as your password. You should not be asked again for pushing or pulling data into this GitHub repository. If you are not asked for your Personal Access Token then this likely means that your token is already saved and ready to use on all future ‘actions’."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#jupyter---your-computer",
    "href": "practicals/Practical-01-Getting_Started.html#jupyter---your-computer",
    "title": "Practical 1: Getting Started",
    "section": "Jupyter < - > Your Computer",
    "text": "Jupyter &lt; - &gt; Your Computer\nIf you now go back to Jupyter, you should now see that the work directory is actually the same as the CASA directory that you created earlier; you know this because you can now see an fsds directory containing that wasn’t there before.\nThis is because Docker is ‘mounting’ the work directory in the container to the CASA directory on your computer. This means that you can save files in the work directory in Jupyter and they will be saved in the CASA directory on your computer.\n\n\n\n\n\n\nOrganising Your Work\n\n\n\nThere are any number of ways to organise your CASA work, what’s important is that you are logical about things like names and hierarchy. This will make it much easier to access files and notebooks using Docker, Quarto, and Python."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#adding-a-local-file-to-your-repository",
    "href": "practicals/Practical-01-Getting_Started.html#adding-a-local-file-to-your-repository",
    "title": "Practical 1: Getting Started",
    "section": "Adding a Local File to Your Repository",
    "text": "Adding a Local File to Your Repository\nIn order to tie together the different concepts covered above, we are now going add Practical 1 (this practical) to your GitHub repo. The easiest way to do this is to download the notebook by clicking on the Jupyter link on the right side of this page. So the process is:\n\nClick on the Jupyter link to save this file to your computer as a notebook with the extension .ipynb.\nMove the file to your new repository folder (e.g. $HOME/work/Documents/CASA/fsds/).\n\n\n\n\n\n\n\nFile Extensions\n\n\n\nIt is highly likely that your browser automatically added a .txt extension when you saved the Notebook file to your computer. You need to remove that ending to your file name or Jupyter won’t be able to run it. You can rename a file by either doing it directly in the Finder/Windows Explorer, or by moving (bash: mv) the file: mv &lt;notebook_name&gt;.ipynb.txt &lt;notebook_name&gt;.ipynb\n\n\nIn the Terminal/PowerShell we now need add this file to Git so that it knows to keep track of it. Unlike Dropbox or OneDrive, just putting a file in a repo directory does not mean that Git will pay attention to it:\n# Assuming that you are 'in' the 'fsds' directory...\ngit add Practical-01-Getting_Started.ipynb\ngit commit -m \"Adding notebook 1 to repo.\"\n\n\n\n\n\n\nAdd, Commit, Push, Repeat\n\n\n\nUnless you have added and committed a file to Git then it is not version controlled.\nUnless you have pushed your committed files to GitHub they are only backed up locally."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#status-check",
    "href": "practicals/Practical-01-Getting_Started.html#status-check",
    "title": "Practical 1: Getting Started",
    "section": "Status Check",
    "text": "Status Check\nWe now want to check that the file has been successfully added to Git. We do this with a status check in the repository directory (i.e. cd $HOME/work/Documents/CASA/fsds/):\ngit status\nYou should see something like:\nOn branch master\nYour branch is ahead of 'origin/master' by 1 commit.\n  (use \"git push\" to publish your local commits)\nThis is telling you that your local computer is 1 commit (the one that you just completed) ahead of the ‘origin’, which is on GitHub. GitHub doesn’t have to be the origin (nor does the repository have to be one that we created in order to be an origin) but conceptually and practically it’s easier to create new repositories on GitHub and clone them to our computer."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#keep-pushing",
    "href": "practicals/Practical-01-Getting_Started.html#keep-pushing",
    "title": "Practical 1: Getting Started",
    "section": "Keep Pushing",
    "text": "Keep Pushing\nTo synchronise the changes we just made, let’s follow Git’s advice:\ngit push\nYou should see something like the below series of messages (the exact details will differ, but the ‘Enumerating, Counting, etc’ messages will be the same):\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 306 bytes | 306.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nremote: This repository moved. Please use the new location:\nremote:   https://github.com/jreades/fsds.git\nTo https://github.com/jreades/fsds.git\n   7410d0e..45aa80a  master -&gt; master\nIf you now go over to your browser and visit your GitHub repo page (e.g. https://github.com/jreades/fsds) — pressing the Reload button if you had the page open already — then you should see that the file you added on your computer is also showing up on the GitHub site as well! This means it’s now fully version-controlled and backed-up.\n\n\n\n\n\n\nKeep Pushing\n\n\n\nUnless have pushed your commits to GitHub they are only stored on your computer. So your files can be properly version-controlled, but without a push if you lose your computer you still lose everything!"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#more-about-git",
    "href": "practicals/Practical-01-Getting_Started.html#more-about-git",
    "title": "Practical 1: Getting Started",
    "section": "More About Git",
    "text": "More About Git\nFrom here on out you can keep changes made either directly on GitHub or locally on your computer (or any other computer to which you clone your repository) in synch by using git push (to push changes from a local computer up to the origin on GitHub) and git pull (to pull changes available on the origin down to the local computer).\nThat said, you can do a lot more than just push/pull to your own repository and this Twitter thread leads to a lot of useful additional resources to do with Git:\n\nIntroduction to Version Control with Git on the Programming Historian web site is written for digital humanities researchers so it’s intended to be accessible.\nOh My Git is an ‘open source game’ to help you learn Git.\nGit Meets Minesweeper? is apparently a ‘thing’.\nVisual Git Reference if you think visually or just want to check your understanding.\nVersion Control with Git is a Software Carpentries lesson that takes you quickly through the important elements of getting set up and started. It would be a good refresher.\nAltassian’s Documentation provides more detailed explanations of the commands and options.\nLearn Git Branching focusses on a key concept for software collaboration.\nGit Immersion provides a ‘guided tour’ of the fundamentals.\n\n\n\n\n\n\n\nTip\n\n\n\nFor the Group Work every member of your group will need to make contributions to a GitHub repository. This will require learning how to invite others to be contributors, how to merge changes, and how to deal with conflicts of the coding kind."
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#other-useful-resources",
    "href": "practicals/Practical-01-Getting_Started.html#other-useful-resources",
    "title": "Practical 1: Getting Started",
    "section": "Other Useful Resources",
    "text": "Other Useful Resources\n\nGitHub Markdown Guide\nCommon Mark\nMarkdown Guide, which helpfully includes do’s and don’ts.\n\nFinally, these are a bit overkill but the bits about setting up and installing git, bash/zsh, and so on may come in handy later:\n\nSetting Up a New Mac\nBeginner’s Guide to Setting Up Windows 10\nSetting up Windows without Linux\nMicrosoft Python Setup Guide"
  },
  {
    "objectID": "practicals/Practical-01-Getting_Started.html#gitignore",
    "href": "practicals/Practical-01-Getting_Started.html#gitignore",
    "title": "Practical 1: Getting Started",
    "section": ".Gitignore",
    "text": ".Gitignore\nThe main thing you should notice is the pattern: * means ‘anything’, while / at the end of a line implies a directory. So the following four lines should be added to your .gitignore file:\n*.zip\n*.gz\n*.csv\n*.gzip\n*.feather\n*.geofeather\n*.parquet\n*.geoparquet\ndata/\nThat’s it."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#section",
    "href": "lectures/9.2-Linking_Spatial_Data.html#section",
    "title": "Linking (Spatial) Data",
    "section": "",
    "text": "Data Set 1\n\n\n\nSensorID\nLatitude\nLongitude\n\n\n\n\n1\n51.5070\n-0.1347\n\n\n2\n51.5071\n-0.0042\n\n\n3\n51.5074\n-0.1223\n\n\n4\n51.5073\n-0.1122\n\n\n5\n51.5072\n0.1589\n\n\n\ndf = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Latitude': [51.5070, 51.5071, 51.5074, 51.5073, 51.5073],\n  'Longitude': [-0.1347, -0.0042, -0.1223, -0.1122, 0.1589]\n})\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1\nTemperature\n5ºC\n\n\n1\nHumidity\n15%\n\n\n3\nTemperature\n7ºC\n\n\n4\nTemperature\n7ºC\n\n\n6\nHumidity\n18%\n\n\n\ndf1 = pd.DataFrame({\n  'SensorID': [1,2,3,4,5],\n  'Parameter': ['Temperature','Humidity','Temperature','Temperature','Humidity'], \n  'Value': ['5ºC', '15%', '7ºC', '7ºC', '18%']\n})\n\n\n\n\n\nThis wouldn’t be a particularly good design for a data structure… why?\n\nObviously, we can use non-spatial operations on spatial data sets."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "href": "lectures/9.2-Linking_Spatial_Data.html#sjoin-vs.-join",
    "title": "Linking (Spatial) Data",
    "section": "Sjoin vs. Join",
    "text": "Sjoin vs. Join\nSjoin adds an operator (['intersects','contains','within']) and example code can be found on GitHub.\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(df.longitude, df.latitude,\n            crs='epsg:4326')).to_crs('epsg:27700')\nhackney = boros[boros.NAME=='Hackney']\nrs = gpd.sjoin(gdf, hackney, op='within')"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "href": "lectures/9.2-Linking_Spatial_Data.html#combining-operators-how",
    "title": "Linking (Spatial) Data",
    "section": "Combining Operators & How",
    "text": "Combining Operators & How\nChanging how to left, right, or inner changes the join’s behaviour:\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = boros[boros.NAME=='Hackney'].plot(edgecolor='k', facecolor='none', figsize=(8,8))\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "href": "lectures/9.2-Linking_Spatial_Data.html#merging-data",
    "title": "Linking (Spatial) Data",
    "section": "Merging Data",
    "text": "Merging Data\nThese merge operators apply where a is the left set of features (in a GeoSeries or GeoDataFrame) and b is the right set:\n\nContains: Returns True if no points of b lie outside of a and at least one point of b lies inside a.\nWithin: Returns True if a’s boundary and interior intersect only with the interior of b (not its boundary or exterior).\nIntersects: Returns True if the boundary or interior of a intersects in any way with those of b.\n\nAll binary predicates are supported by features of GeoPandas, though only these three options are available in sjoin directly.\n\nBehaviour of operaions may vary with how you set up left and right tables, but you can probably think your way through it by asking: “Which features of x fall within features of y?” or “Do features of x contain y?” You will probably get it wrong at least a few times. That’s ok."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "href": "lectures/9.2-Linking_Spatial_Data.html#additional-spatial-operations",
    "title": "Linking (Spatial) Data",
    "section": "Additional Spatial Operations",
    "text": "Additional Spatial Operations\nThese operators apply to the GeoSeries where a is a GeoSeries and b is one or more spatial features:\n\nContains / Covers: Returns a Series of dtype('bool') with value True for each geometry in a that contains b. These are different.\nCrosses: An object is said to cross other if its interior intersects the interior of the other but does not contain it, and the dimension of the intersection is less than the dimension of the one or the other.\nTouches: Returns a Series indicating which elements of a touch a point on b.\nDistance: Returns a Series containing the distance from all a to some b.\nDisjoint: Returns a Series indicating which elements of a do not intersect with any b.\nGeom Equals / Geom Almost Equals: strict and loose tests of equality between a and b in terms of their geometry.\nBuffer, Simplify, Centroid, Representative Point: common transformations.\nRotate, Scale, Affine Transform, Skew, Translate: less common transformations.\nUnary Union: aggregation of all the geometries in a"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "href": "lectures/9.2-Linking_Spatial_Data.html#rtm",
    "title": "Linking (Spatial) Data",
    "section": "RTM",
    "text": "RTM\n\nIn particular, “contains” (and its converse “within”) has an aspect of its definition which may produce unexpected behaviour. This quirk can be expressed as “Polygons do not contain their boundary”. More precisely, the definition of contains is: Geometry A contains Geometry B iff no points of B lie in the exterior of A, and at least one point of the interior of B lies in the interior of A That last clause causes the trap – because of it, a LineString which is completely contained in the boundary of a Polygon is not considered to be contained in that Polygon! This behaviour could easily trip up someone who is simply trying to find all LineStrings which have no points outside a given Polygon. In fact, this is probably the most common usage of contains. For this reason it’s useful to define another predicate called covers, which has the intuitively expected semantics: Geometry A covers Geometry B iff no points of B lie in the exterior of A"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "href": "lectures/9.2-Linking_Spatial_Data.html#set-operations-with-overlay",
    "title": "Linking (Spatial) Data",
    "section": "Set Operations with Overlay",
    "text": "Set Operations with Overlay\nIt is also possible to apply GIS-type ‘overlay’ operations:\n\nThese operations return indexes for gdf1 and gdf2 (either could be a NaN) together with a geometry and (usually?) columns from both data frames:\nrs_union = geopandas.overlay(gdf1, gdf2, how='union')\nThe set of operations includes: union, intersection, difference, symmetric_difference, and identity.\n\nA notebook example can be found here."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "href": "lectures/9.2-Linking_Spatial_Data.html#think-it-through",
    "title": "Linking (Spatial) Data",
    "section": "Think it Through",
    "text": "Think it Through\nAs your data grows in volume, the consequences of choosing the ‘wrong’ approach become more severe. Making a plan of attack becomes essential and it boils down to the following:\n\nSpatial joins are hard\nNon-spatial joins are easy\nKey-/Index-based joins are easiest\nAddition conditions to joins makes them harder.\n\nSo, when you have multiple joins…\n\nDo the easy ones first (they will run quickly on large data sets).\nDo the hard ones last (they will benefit most from the filtering process)."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "href": "lectures/9.2-Linking_Spatial_Data.html#whats-the-right-order",
    "title": "Linking (Spatial) Data",
    "section": "What’s the ‘Right’ Order?",
    "text": "What’s the ‘Right’ Order?\nQ. Find me a nearby family-style Italian restaurant…\nA. Here’s how I’d do this:\n\nCity = New York (probably a key)\n\nCuisine = Italian (probably a key)\n\nStyle = Family (probably an enumeration/free text)\n\nLocation = Within Distance of X from Request (probably a buffered spatial query)"
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "href": "lectures/9.2-Linking_Spatial_Data.html#mis-matched-scales",
    "title": "Linking (Spatial) Data",
    "section": "Mis-matched Scales",
    "text": "Mis-matched Scales\nKeep in mind:\n\nWith complex geometries and mis-matched scales, converting the smaller geometry to centroids or representative points can speed things up a lot (within, contains, etc. become much simpler).\n\nAnd that:\n\nWith large data sets, rasterising the smaller and more ‘abundant’ geometry can speed things up a lot."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "href": "lectures/9.2-Linking_Spatial_Data.html#web-services",
    "title": "Linking (Spatial) Data",
    "section": "Web Services",
    "text": "Web Services\n\n\n\n\n\n\n\n\nAcronym\nMeans\nDoes\n\n\n\n\nWMS\nWeb Map Service\nTransfers map images within an area specified by bounding box; vector formats possible, but rarely used.\n\n\nWFS\nWeb Feature Service\nAllows interaction with features; so not about rendering maps directly, more about manipulation.\n\n\nWCS\nWeb Coverage Service\nTransfers data about objects covering a geographical area.\n\n\nOWS\nOpen Web Services\nSeems to be used by QGIS to serve data from a PC or server.\n\n\n\n\n\nSee also Carto and competitors."
  },
  {
    "objectID": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "href": "lectures/9.2-Linking_Spatial_Data.html#spatial-databases",
    "title": "Linking (Spatial) Data",
    "section": "Spatial Databases",
    "text": "Spatial Databases\nThere are many flavours:\n\nOracle: good enterprise support; reasonably feature-rich, but £££ for commercial use.\nMySQL: free, unless you want dedicated support; was feature-poor (though this looks to have changed with MySQL8); heavyweight.\nPostgreSQL: free, unless you want dedicated support; standards-setting/compliant; heavyweight (requires PostGIS).\nMicrosoft Access: um, no.\nSpatiaLite: free; standards-setting/compliant; lightweight\nGeoParquet+DuckDB: not as full-featured as Postgres, but evolving quickly and much simpler to configure.\n\nGenerally:\n\nAd-hoc, modestly-sized, highly portable == SpatiaLite\nPermanent, large, weakly portable == Postgres+PostGIS"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html",
    "href": "lectures/8.4-Analysing_Text.html",
    "title": "Analysing Text",
    "section": "",
    "text": "The concept of ‘dummy variables’ in economics/regression is a useful point to start thinking about text:\n\n\n\nTopic\nDummy\n\n\n\n\nNews\n0\n\n\nCulture\n1\n\n\nPolitics\n2\n\n\nEntertainment\n3\n\n\n\n\nWhat’s the problem with this approach when you’re thinking about the topics in a document? You either have to assign each document to one, and only one, topic, or you need a lot of dummies."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#dummy-variables",
    "href": "lectures/8.4-Analysing_Text.html#dummy-variables",
    "title": "Analysing Text",
    "section": "",
    "text": "The concept of ‘dummy variables’ in economics/regression is a useful point to start thinking about text:\n\n\n\nTopic\nDummy\n\n\n\n\nNews\n0\n\n\nCulture\n1\n\n\nPolitics\n2\n\n\nEntertainment\n3\n\n\n\n\nWhat’s the problem with this approach when you’re thinking about the topics in a document? You either have to assign each document to one, and only one, topic, or you need a lot of dummies."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#one-hot-encoders",
    "href": "lectures/8.4-Analysing_Text.html#one-hot-encoders",
    "title": "Analysing Text",
    "section": "One-Hot Encoders",
    "text": "One-Hot Encoders\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n1\n1\n0\n1\n\n\nCulture item\n0\n1\n1\n0\n\n\nPolitics item\n1\n0\n0\n1\n\n\nEntertainment item\n1\n1\n1\n1\n\n\n\n\nOne-Hot encoders are not often used this way, but for keyword detection or keyword-based classification this might be appropriate: i.e. this keyword was used in this document!\nSo the big difference is One Hot == \\(n\\) variables, Dummy == \\(n-1\\).\nDefinitely some ‘gotchas’ in deployment: one-hot models shouldn’t have an intercept unless you apply a ‘ridge shrinkage penalty’. Standardisation affects whether or not an intercept is needed."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#the-bag-of-words",
    "href": "lectures/8.4-Analysing_Text.html#the-bag-of-words",
    "title": "Analysing Text",
    "section": "The ‘Bag of Words’",
    "text": "The ‘Bag of Words’\nJust like a one-hot (binarised approach) on preceding slide but now we count occurences:\n\n\n\nDocument\nUK\nTop\nPop\nCoronavirus\n\n\n\n\nNews item\n4\n2\n0\n6\n\n\nCulture item\n0\n4\n7\n0\n\n\nPolitics item\n3\n0\n0\n3\n\n\nEntertainment item\n3\n4\n8\n1"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#bow-in-practice",
    "href": "lectures/8.4-Analysing_Text.html#bow-in-practice",
    "title": "Analysing Text",
    "section": "BoW in Practice",
    "text": "BoW in Practice\nEnter, stage left, scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\n# Non-reusable transformer\nvectors = vectorizer.fit_transform(texts)\n\n# Reusable transformer\nvectorizer.fit(texts)\nvectors1 = vectorizer.transform(texts1)\nvectors2 = vectorizer.transform(texts2)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'All vectors: {vectors.toarray()}')"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#tfidf",
    "href": "lectures/8.4-Analysing_Text.html#tfidf",
    "title": "Analysing Text",
    "section": "TF/IDF",
    "text": "TF/IDF\nBuilds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:\n\\[\nW(t,d) = TF(t,d) / log(N/DF_{t})\n\\]\nFor example:\n\nIf the term ‘cat’ appears 3 times in a document of 100 words then Term Frequency given by: \\(TF(t,d)=3/100\\), and\nIf there are 10,000 documents and cat appears in 1,000 documents then Normalised Document Frequency given by: \\(N/DF_{t}=10000/1000\\) so the Inverse Document Frequency is \\(log(10)=1\\),\nSo IDF=1 and TF/IDF=0.03."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#tfidf-in-practice",
    "href": "lectures/8.4-Analysing_Text.html#tfidf-in-practice",
    "title": "Analysing Text",
    "section": "TF/IDF in Practice",
    "text": "TF/IDF in Practice\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\n# Non-reusable form:\nvectors=vectorizer.fit_transform(texts)\n\n# Reusable form:\nvectorizer.fit(texts)\nvectors = vectorizer.transform(texts)\n\nprint(f'Vocabulary: {vectorizer.vocabulary_}')\nprint(f'Full vector: {vectors.toarray()}')\n\nWhat do you notice about how this code differs from the CountVectorizer?"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "href": "lectures/8.4-Analysing_Text.html#term-co-occurence-matrix-tcm",
    "title": "Analysing Text",
    "section": "Term Co-Occurence Matrix (TCM)",
    "text": "Term Co-Occurence Matrix (TCM)\nThree input texts with a distance weighting (\\(d/2\\), where \\(d&lt;3\\)):\n\nthe cat sat on the mat\nthe cat sat on the fluffy mat\nthe fluffy ginger cat sat on the mat\n\n\n\n\n\nfluffy\nmat\nginger\nsat\non\ncat\nthe\n\n\n\n\nfluffy\n\n1\n1\n\n0.5\n0.5\n2.0\n\n\nmat\n\n\n\n\n0.5\n\n1.5\n\n\nginger\n\n\n\n0.5\n0.5\n1.0\n1.5\n\n\nsat\n\n\n\n\n3.0\n3.0\n2.5\n\n\non\n\n\n\n\n\n1.5\n3.0\n\n\ncat\n\n\n\n\n\n\n2.0\n\n\nthe"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#how-big-is-a-tcm",
    "href": "lectures/8.4-Analysing_Text.html#how-big-is-a-tcm",
    "title": "Analysing Text",
    "section": "How Big is a TCM?",
    "text": "How Big is a TCM?\nThe problem:\n\nA corpus with 10,000 words has a TCM of size \\(10,000^{2}\\) (100,000,000)\nA corpus with 50,000 words has a TCM of size \\(50,000^{2}\\) (2,500,000,000)\n\nCleaning is necessary, but it’s not sufficient to create a tractable TCM on a large corpus.\n\nAlthough usually used in the context of clustering, there’s also a curse of dimensionality here!"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#enter-document-embeddings",
    "href": "lectures/8.4-Analysing_Text.html#enter-document-embeddings",
    "title": "Analysing Text",
    "section": "Enter Document Embeddings",
    "text": "Enter Document Embeddings\nTypically, some kind of 2 or 3-layer neural network that ‘learns’ how to embed the TCM into a lower-dimension representation: from \\(m \\times m\\) to \\(m \\times n, n &lt;&lt; m\\).\nSimilar to PCA in terms of what we’re trying to achieve, but the process is utterly different.\n\nMany different approaches, but GloVe (Stanford), word2vec (Google), fastText (Facebook), and ELMo (Allen) or BERT (Google) are probably the best-known."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#sentiment-analysis",
    "href": "lectures/8.4-Analysing_Text.html#sentiment-analysis",
    "title": "Analysing Text",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nRequires us to deal in great detail with bi- and tri-grams because negation and sarcasm are hard. Also tends to require training/labelled data.\n\nSource."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#clustering",
    "href": "lectures/8.4-Analysing_Text.html#clustering",
    "title": "Analysing Text",
    "section": "Clustering",
    "text": "Clustering\n\n\n\n\n\n\n\n\n\n\n\nCluster\nGeography\nEarth Science\nHistory\nComputer Science\nTotal\n\n\n\n\n1\n126\n310\n104\n11,018\n11,558\n\n\n2\n252\n10,673\n528\n126\n11,579\n\n\n3\n803\n485\n6,730\n135\n8,153\n\n\n4\n100\n109\n6,389\n28\n6,626\n\n\nTotal\n1,281\n11,577\n13,751\n11,307\n37,916"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#topic-modelling",
    "href": "lectures/8.4-Analysing_Text.html#topic-modelling",
    "title": "Analysing Text",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nLearning associations of words (or images or many other things) to hidden ‘topics’ that generate them:"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#word-clouds",
    "href": "lectures/8.4-Analysing_Text.html#word-clouds",
    "title": "Analysing Text",
    "section": "Word Clouds",
    "text": "Word Clouds"
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#additional-resources",
    "href": "lectures/8.4-Analysing_Text.html#additional-resources",
    "title": "Analysing Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOne-Hot vs Dummy Encoding\nCategorical encoding using Label-Encoding and One-Hot-Encoder\nCount Vectorization with scikit-learn\nCorpus Analysis with Spacy\nThe TF*IDF Algorithm Explained\nHow to Use TfidfTransformer and TfidfVectorizer\nSciKit Learn Feature Extraction\nYour Guide to LDA\nMachine Learning — Latent Dirichlet Allocation LDA\nA Beginner’s Guide to Latent Dirichlet Allocation(LDA)\nAnalyzing Documents with TF-IDF\n\nBasically any of the lessons on The Programming Historian."
  },
  {
    "objectID": "lectures/8.4-Analysing_Text.html#additional-resources-1",
    "href": "lectures/8.4-Analysing_Text.html#additional-resources-1",
    "title": "Analysing Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nIntroduction to Word Embeddings\nThe Current Best of Universal Word Embeddings and Sentence Embeddings\nUsing GloVe Embeddings\nWorking with Facebook’s FastText Library\nWord2Vec and FastText Word Embedding with Gensim\nSentence Embeddings. Fast, please!\n\n\n\nPlasticityAI Embedding Models\nClustering text documents using k-means\nTopic extraction with Non-negative Matrix Factorization and LDA\nTopic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html",
    "href": "lectures/8.2-Patterns_in_Text.html",
    "title": "Patterns in Text",
    "section": "",
    "text": "Consider the following character sequences:\n\nfoo@bar.com\nhttps://www.ucl.ac.uk/bartlett/casa/\n(555) 102-1111\nE17 5RS\nNow, fair Hippolyta, our nuptial hour / Draws on apace. Four happy days bring in / Another moon. But, oh, methinks how slow / This old moon wanes. She lingers my desires, / Like to a stepdame or a dowager / Long withering out a young man’s revenue. (I.i.)\n\n\nWe need ways to distinguish: Upper and Lower Case, Digits, Space Characters, Other Characters, Repetition, Type… Can you do those with strings alone?"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#can-we-describe-text",
    "href": "lectures/8.2-Patterns_in_Text.html#can-we-describe-text",
    "title": "Patterns in Text",
    "section": "",
    "text": "Consider the following character sequences:\n\nfoo@bar.com\nhttps://www.ucl.ac.uk/bartlett/casa/\n(555) 102-1111\nE17 5RS\nNow, fair Hippolyta, our nuptial hour / Draws on apace. Four happy days bring in / Another moon. But, oh, methinks how slow / This old moon wanes. She lingers my desires, / Like to a stepdame or a dowager / Long withering out a young man’s revenue. (I.i.)\n\n\nWe need ways to distinguish: Upper and Lower Case, Digits, Space Characters, Other Characters, Repetition, Type… Can you do those with strings alone?"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "href": "lectures/8.2-Patterns_in_Text.html#strings-methods-are-not-enough",
    "title": "Patterns in Text",
    "section": "Strings Methods are Not Enough",
    "text": "Strings Methods are Not Enough\n'123foo456'.index('foo') # 2\n'123foo456'.split('foo') # ['123', '456']\n' 123 foo 456 '.strip()  # '123 foo 456'\n'HOW NOW BROWN COW?'.lower() # 'how now brown cow?'\n'How now brown cow?'.replace('brown ','green-')\n# 'How now green-cow?'\nSee: dir(str) for full list of string methods."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#regular-expressions",
    "href": "lectures/8.2-Patterns_in_Text.html#regular-expressions",
    "title": "Patterns in Text",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegexes are a way for talking about patterns observed in text, although their origins are rooted in philosophy and linguistics.\nImplemented in Python as:\nimport re\n# re.search(&lt;regex&gt;, &lt;str&gt;)\ns = '123foo456'\nif re.search('123',s):\n  print(\"Found a match.\")\nelse:\n  print(\"No match.\")\nPrints 'Found a match.'"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#capturing-matches",
    "href": "lectures/8.2-Patterns_in_Text.html#capturing-matches",
    "title": "Patterns in Text",
    "section": "Capturing Matches",
    "text": "Capturing Matches\nm = re.search('123',s)\nprint(m.start())\nprint(m.end())\nprint(m.span())\nprint(m.group())\nOutputs:\n0\n3\n(0,3)\n123\n\nSo, we have None if a search fails, but if it succeeds then we have attributes of the match objection like start, end, span, and group (this last is going to be particularly interesting since it tells us what matched)."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#configuring-matches",
    "href": "lectures/8.2-Patterns_in_Text.html#configuring-matches",
    "title": "Patterns in Text",
    "section": "Configuring Matches",
    "text": "Configuring Matches\ns = '123foo456'\nm = re.search('FOO',s)\nprint(m)\nm = re.search('FOO',s,re.IGNORECASE)\nprint(m)\nOutputs:\nNone\n&lt;re.Match object; span=(3, 6), match='foo'&gt;\nThe third parameter allows us to: match newlines (re.DOTALL), ignore case (re.IGNORECASE), take language into account (re.LOCALE), match across lines (re.MULTILINE), and write patterns across multiple lines (re.VERBOSE). If you need multiple options it’s re.DOTALL | re.IGNORECASE. Bitwise again!"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#more-than-one-match",
    "href": "lectures/8.2-Patterns_in_Text.html#more-than-one-match",
    "title": "Patterns in Text",
    "section": "More Than One Match",
    "text": "More Than One Match\ns = '123foo456foo789'\nlst = re.findall('foo',s)\nprint(lst)\nlst = re.finditer('foo',s)\n[x for x in lst]\nrs  = re.sub('foo',' ',s)\nprint(rs)\nrs  = re.split(' ',rs)\nprint(rs)\nOutputs:\n['foo','foo']\n[&lt;re.Match object; span=(3, 6), match='foo'&gt;, &lt;re.Match object; span=(9, 12), match='foo'&gt;]\n'123 456 789'\n['123', '456', '789']"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "href": "lectures/8.2-Patterns_in_Text.html#regular-expressions-do-much-more",
    "title": "Patterns in Text",
    "section": "Regular Expressions Do Much More",
    "text": "Regular Expressions Do Much More\nimport re\nm = re.search(r'\\$((\\d+,){2,}\\d+)',\n        \"'That will be $1,000,000 he said...'\")\nprint(m.group(1)) # '1,000,000'\nThis looks for sequences of 1-or-more digits followed by a comma… and for those sequences to repeat two or more times:\n# Look for a literal '$'\nre.search(r'\\$') \n# Group of &gt;=1 digits followed by a comma...\nre.search(r'(\\d+,)') \n# Repeated two or more times...\nre.search(r'(\\d+,){2,}') \n\n\nAlso notice the r'&lt;regex&gt;' with an r in front of the string. This means ‘raw’ and is often a required modifier for regular expression patterns. Simple ones don’t need it, but from here on out you will."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#character-classes",
    "href": "lectures/8.2-Patterns_in_Text.html#character-classes",
    "title": "Patterns in Text",
    "section": "Character Classes",
    "text": "Character Classes\n\n\n\n\n\n\n\n\nCharacters\nRegex Meta Class Options\n‘Antonyms’\n\n\n\n\na…z\n[a-z], \\w (word-like characters)\n[^a-z], \\W\n\n\nA…Z\n[A-Z], \\w (word-like characters)\n[^A-Z], \\W\n\n\n0…9\n[0-9], \\d (digits)\n[^0-9], \\D\n\n\n' ', \\n, \\t, \\r, \\f, \\v\n\\s\n\\S\n\n\n., [, ], +, $, ^, \\|, {, }, *, (, ), ?\nFor safety always precede character with a \\.\nNone\n\n\n\n\n\n\\w will include _. And \\ is, once again, important as it ‘escapes’ various characters, and options."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#metacharacters",
    "href": "lectures/8.2-Patterns_in_Text.html#metacharacters",
    "title": "Patterns in Text",
    "section": "Metacharacters",
    "text": "Metacharacters\n\n\n\nMetacharacter\nMeaning\nExample\n\n\n\n\n.\nAny character at all\nc.t\n\n\n^\nStart of a string/line\n^start\n\n\n$\nEnd of a string/line\nend$\n\n\n*\n0 or more of something\n-*\n\n\n+\n1 or more of something\n-+\n\n\n?\n0 or 1 of something; also lazy modifier\n,?\n\n\n{m,n}\nRepeat between m and n times\n\\d{1,4}\n\n\n[ ]\nA set of character literals\n[1-5]\n\n\n( )\nGroup/remember this sequence of characters\n(\\d+)\n\n\n|\nOr\n(A|B)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#building-blocks",
    "href": "lectures/8.2-Patterns_in_Text.html#building-blocks",
    "title": "Patterns in Text",
    "section": "Building Blocks",
    "text": "Building Blocks\n\n\n\n\n\n\n\nRegex\nInterpretation\n\n\n\n\nr'\\s*'\n0 or more spaces\n\n\nr'\\d+'\n1 or more digits\n\n\nr'[A-Fa-f0-7]{5}'\nExactly 5 hexadecimal ‘digits’\n\n\nr'\\w+\\.\\d{2,}'\n1 or more ‘wordish’ characters, followed by a full-stop, then 2 or more digits\n\n\nr'^[^@]+@\\w+'\nOne more non-@ characters at the start of a line, followed by a ‘@’ then 1 or more ‘wordish’ characters.\n\n\nr'(uk|eu|fr)$'\nThe characters ‘uk’ or ‘eu’ or ‘fr’ at the end of a line."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#exploring",
    "href": "lectures/8.2-Patterns_in_Text.html#exploring",
    "title": "Patterns in Text",
    "section": "Exploring",
    "text": "Exploring\nRegex101 can be a useful way to build a regex interactively:"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^[^@]+@([a-z0-9\\-]+\\.){1,5}[a-z0-9\\-]+$', s)\n\ns should be replaced with any string you want to check."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-1",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-1",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'\\d{4}-\\d{2}-\\d{2}', s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-2",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-2",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^\\s*$', s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-3",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-3",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'^(http|https|ftp):[\\/]{2}([a-zA-Z0-9\\-]+\\.){1,4}[a-zA-Z]{2,5}(:[0-9]+)?\\/?([a-zA-Z0-9\\-\\._\\?\\'\\/\\\\\\+\\&\\%\\$#\\=~]*)',s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#whats-this-4",
    "href": "lectures/8.2-Patterns_in_Text.html#whats-this-4",
    "title": "Patterns in Text",
    "section": "What’s This?",
    "text": "What’s This?\nre.match(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\\s?[0-9][A-Za-z]{2})',s)"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#to-help",
    "href": "lectures/8.2-Patterns_in_Text.html#to-help",
    "title": "Patterns in Text",
    "section": "To Help…",
    "text": "To Help…\nre.VERBOSE to the rescue:\nregex = r\"\"\"\n([GIR] 0[A]{2})|    # Girobank \n(\n  (\n    ([A-Z][0-9]{1,2})| # e.g A00...Z99\n      (\n        ([A-Z][A-HJ-Y][0-9]{1,2})|  # e.g. AB54...ZX11\n          (([A-Z][0-9][A-Z])|  # e.g. A0B...Z9Z \n          ([A-Z][A-HJ-Y][0-9][A-Z]?))  # e.g. WC1 or WC1H\n        )\n      )\n    \\s?[0-9][A-Z]{2} # e.g. 5RX\n  )\n\"\"\"\nre.match(regex,s,re.VERBOSE|re.IGNORECASE) # Can also use: re.X|re.I\n\nThis is the government’s own regex but is probably not 100% accurate."
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "href": "lectures/8.2-Patterns_in_Text.html#applications-of-regular-expressions",
    "title": "Patterns in Text",
    "section": "Applications of Regular Expressions",
    "text": "Applications of Regular Expressions\nIf our problem follows some set of articulable rules about permissible sequences of characters then we can probably validate it using a regex:\n\n\n\n\n\n\n\nExamples\nMore Examples\n\n\n\n\nEmail\nPassword\n\n\nPostcode\nPhone number\n\n\nDate\nCredit cards\n\n\nWeb scraping\nSyntax highlighting\n\n\nSentence structure\nData wrangling\n\n\nSearching for/withinfiles/content\nLexical analysis/Language detection\n\n\n\n\nThese are all good problems…"
  },
  {
    "objectID": "lectures/8.2-Patterns_in_Text.html#additional-resources",
    "href": "lectures/8.2-Patterns_in_Text.html#additional-resources",
    "title": "Patterns in Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRegex 101\nPython Documentation\nReal Python: Regular Expressions 1\nReal Python: Regular Expressions 2\nData Camp RegEx Tutorial\nIntroduction to Regex\nUnderstanding RegExes in Python\nDemystifying RegExes in Python\nPython RegExes\nMastering String Methods in Python\n\nThanks to Yogesh Chavan and Nicola Pietroluongo for examples."
  },
  {
    "objectID": "lectures/7.5-ESDA.html",
    "href": "lectures/7.5-ESDA.html",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "",
    "text": "For this one, you really need to read the docs."
  },
  {
    "objectID": "lectures/7.5-ESDA.html#getting-spatial-with-boroughs",
    "href": "lectures/7.5-ESDA.html#getting-spatial-with-boroughs",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Getting Spatial (with Boroughs)",
    "text": "Getting Spatial (with Boroughs)\nimport geopandas as gpd\nurl = 'https://bit.ly/3neINBV'\nboros = gpd.read_file(url, driver='GPKG')\nboros.plot(color='none', edgecolor='red');"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#convex-hull",
    "href": "lectures/7.5-ESDA.html#convex-hull",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Convex Hull",
    "text": "Convex Hull\nboros['hulls'] = boros.geometry.convex_hull\nboros = boros.set_geometry('hulls')\nboros.plot(column='NAME', categorical=True, alpha=0.5);"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#dissolve",
    "href": "lectures/7.5-ESDA.html#dissolve",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Dissolve",
    "text": "Dissolve\nboros['region'] = 'London'\nboros = boros.set_geometry('geometry') # Set back to original geom\nldn   = boros.dissolve(by='region')    # And dissolve to a single poly\n\nf,ax = plt.subplots(figsize=(10,8))    # New plot\nldn.plot(ax=ax)                        # Add London layer to axis"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#simplify",
    "href": "lectures/7.5-ESDA.html#simplify",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Simplify",
    "text": "Simplify\nldn.simplify(500).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#buffer",
    "href": "lectures/7.5-ESDA.html#buffer",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Buffer",
    "text": "Buffer\nldn.buffer(500).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#buffer-simplify",
    "href": "lectures/7.5-ESDA.html#buffer-simplify",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Buffer & Simplify",
    "text": "Buffer & Simplify\nldn.buffer(1000).simplify(1000).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#difference",
    "href": "lectures/7.5-ESDA.html#difference",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Difference",
    "text": "Difference\nAnd some nice chaining…\nldn.buffer(3000).simplify(2500).difference(ldn.geometry).plot()"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#legendgrams",
    "href": "lectures/7.5-ESDA.html#legendgrams",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Legendgrams",
    "text": "Legendgrams\n\n\n\nCode begins on next slide."
  },
  {
    "objectID": "lectures/7.5-ESDA.html#handy-recall",
    "href": "lectures/7.5-ESDA.html#handy-recall",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Handy Recall",
    "text": "Handy Recall\nLoad the CSV file and convert to GeoDataFrame:\nimport pandas as pd\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url) \ndf['price'] = df.price.str.replace('$','',regex=False).astype('float')\ndf.set_index('id',inplace=True)\ngdf = gpd.GeoDataFrame(df, \n        geometry=gpd.points_from_xy(\n          df['longitude'], df['latitude'], crs='epsg:4326'\n        )\n      )\ngdf = gdf.to_crs('epsg:27700')"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-legendgrams",
    "href": "lectures/7.5-ESDA.html#implementing-legendgrams",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing Legendgrams",
    "text": "Implementing Legendgrams\nimport pysal as ps\n# https://github.com/pysal/mapclassify\nimport mapclassify as mc\n# https://jiffyclub.github.io/palettable/\nimport palettable.matplotlib as palmpl\nfrom legendgram import legendgram\n\nf,ax = plt.subplots(figsize=(10,8))\ngdf.plot(column='price', scheme='Quantiles', cmap='magma', k=5, ax=ax)\nq = mc.Quantiles(gdf.price.array, k=5)\n\n# https://github.com/pysal/legendgram/blob/master/legendgram/legendgram.py\nlegendgram(f, ax, \n               gdf.price, q.bins, pal=palmpl.Magma_5,\n               legend_size=(.4,.2), # legend size in fractions of the axis\n               loc = 'upper left', # mpl-style legend loc\n               clip = (0,500), # clip range of the histogram\n               frameon=True)\n\nNote that the number of colours need to match k, which is 5 in this case.\nIt should be possible to set up the colormap and bins such that they can be passed to both GeoPandas and Legendgram."
  },
  {
    "objectID": "lectures/7.5-ESDA.html#knn-weights",
    "href": "lectures/7.5-ESDA.html#knn-weights",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "KNN Weights",
    "text": "KNN Weights"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-knn",
    "href": "lectures/7.5-ESDA.html#implementing-knn",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing KNN",
    "text": "Implementing KNN\nfrom pysal.lib import weights\nw = weights.KNN.from_dataframe(gdf, k=3)\ngdf['w_price'] = weights.lag_spatial(w, gdf.price)\ngdf[['name','price','w_price']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nname\nprice\nw_price\n\n\n\n\n83\nSouthfields Home\n85.0\n263.0\n\n\n53\nFlat in Islington, Central London\n55.0\n190.0\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\n470.0\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\n186.0\n\n\n44\nAvni Kensington Hotel\n430.0\n821.0"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#spatial-lag-of-distance-band",
    "href": "lectures/7.5-ESDA.html#spatial-lag-of-distance-band",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Spatial Lag of Distance Band",
    "text": "Spatial Lag of Distance Band"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-db",
    "href": "lectures/7.5-ESDA.html#implementing-db",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing DB",
    "text": "Implementing DB\nw2 = weights.DistanceBand.from_dataframe(gdf, threshold=2000, alpha=-0.25)\ngdf['price_std'] = (gdf.price - gdf.price.mean()) / gdf.price.std()\ngdf['w_price_std'] = weights.lag_spatial(w2, gdf.price_std)\ngdf[['name','price_std','w_price_std']].sample(5, random_state=42)\n\n\n\n\nname\nprice_std\nw_price_std\n\n\n\n\n83\nSouthfields Home\n-0.27\n0.00\n\n\n53\nFlat in Islington, Central London\n-0.51\n-0.58\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n0.83\n0.46\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n-0.07\n-0.82\n\n\n44\nAvni Kensington Hotel\n2.52\n3.25"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#morans-i",
    "href": "lectures/7.5-ESDA.html#morans-i",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Moran’s I",
    "text": "Moran’s I\nmi = esda.Moran(gdf['price'], w)\nprint(f\"{mi.I:0.4f}\")\nprint(f\"{mi.p_sim:0.4f}\")\nmoran_scatterplot(mi)"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#local-morans-i",
    "href": "lectures/7.5-ESDA.html#local-morans-i",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#implementing-local-morans-i",
    "href": "lectures/7.5-ESDA.html#implementing-local-morans-i",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Implementing Local Moran’s I",
    "text": "Implementing Local Moran’s I\nlisa = esda.Moran_Local(gdf.price, w)\n# Break observations into significant or not\ngdf['sig'] = lisa.p_sim &lt; 0.05\n# Store the quadrant they belong to\ngdf['quad'] = lisa.q\ngdf[['name','price','sig','quad']].sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\n\nname\nprice\nsig\nquad\n\n\n\n\n83\nSouthfields Home\n85.0\nFalse\n3\n\n\n53\nFlat in Islington, Central London\n55.0\nFalse\n3\n\n\n70\n3bedroom Family Home minutes from Kensington Tube\n221.0\nFalse\n1\n\n\n453\nBed, 20 min to Liverpool st, EAST LONDON\n110.0\nFalse\n3\n\n\n44\nAvni Kensington Hotel\n430.0\nFalse\n1"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#full-lisa",
    "href": "lectures/7.5-ESDA.html#full-lisa",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Full LISA",
    "text": "Full LISA\nplot_local_autocorrelation(lisa, gdf, 'price')"
  },
  {
    "objectID": "lectures/7.5-ESDA.html#additional-resources",
    "href": "lectures/7.5-ESDA.html#additional-resources",
    "title": "Exploratory (Spatial) Data Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\nThere’s so much more to find, but:\n\nPandas Reference\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nVisualising Missing Data\nChoosing Map Colours"
  },
  {
    "objectID": "lectures/7.3-EDA.html#epicyclic-feedback",
    "href": "lectures/7.3-EDA.html#epicyclic-feedback",
    "title": "ExploratoryData Analysis",
    "section": "Epicyclic Feedback",
    "text": "Epicyclic Feedback\nPeng and Matsui, The Art of Data Science, p.8\n\n\n\n\n\n\n\n\n\n\nSet Expectations\nCollect Information\nRevise Expectations\n\n\n\n\nQuestion\nQuestion is of interest to audience\nLiterature search/experts\nSharpen question\n\n\nEDA\nData are appropriate for question\nMake exploratory plots\nRefine question or collect more data\n\n\nModelling\nPrimary model answers question\nFit secondary models / analysis\nRevise model to include more predictors\n\n\nInterpretation\nInterpretation provides specific and meaningful answer\nInterpret analyses with focus on effect and uncertainty\nRevise EDA and/or models to provide more specific answers\n\n\nCommunication\nProcess & results are complete and meaningful\nSeek feedback\nRevises anlyses or approach to presentation"
  },
  {
    "objectID": "lectures/7.3-EDA.html#approaching-eda",
    "href": "lectures/7.3-EDA.html#approaching-eda",
    "title": "ExploratoryData Analysis",
    "section": "Approaching EDA",
    "text": "Approaching EDA\nThere’s no hard and fast way of doing EDA, but as a general rule you’re looking to:\n\nClean\nCanonicalise\nClean More\nVisualise & Describe\nReview\nClean Some More\n…\n\nThe ‘joke’ is that 80% of Data Science is data cleaning.\n\nCleaning Part 1: testing validity of records (possibly while tracking rejected records for subsequent analysis)\nCanonicalisation: controling for variation (e.g. typos, capitalisation, formatting, leading/trailing whitespace, different types of NULL values, etc.) and in a spatial context deal with projection and geo-data issues.\nCleaning Part 2: further testing of records (e.g. deciding what to do with NaNs, missing values, outside of study area, etc.)\nVisualise & Describe: covered in QM but we’ll take a high-level look at this."
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-related-take",
    "href": "lectures/7.3-EDA.html#a-related-take",
    "title": "ExploratoryData Analysis",
    "section": "A Related Take",
    "text": "A Related Take\nEDA—Don’t ask how, ask what:\n\nDescriptive Statistics: get a high-level understanding of your dataset.\nMissing values: come to terms with how bad your dataset is.\nDistributions and Outliers: and why countries that insist on using different units make our jobs so much harder.\nCorrelations: and why sometimes even the most obvious patterns still require some investigating."
  },
  {
    "objectID": "lectures/7.3-EDA.html#another-take",
    "href": "lectures/7.3-EDA.html#another-take",
    "title": "ExploratoryData Analysis",
    "section": "Another Take",
    "text": "Another Take\nHere’s another view of how to do EDA:\n\nPreview data randomly and substantially\nCheck totals such as number of entries and column types\nCheck nulls such as at row and column levels\nCheck duplicates: do IDs recurr, did the servers fail\nPlot distribution of numeric data (univariate and pairwise joint distribution)\nPlot count distribution of categorical data\nAnalyse time series of numeric data by daily, monthly and yearly frequencies"
  },
  {
    "objectID": "lectures/7.3-EDA.html#getting-started",
    "href": "lectures/7.3-EDA.html#getting-started",
    "title": "ExploratoryData Analysis",
    "section": "Getting Started",
    "text": "Getting Started\nYou can follow along by loading the Inside Airbnb sample:\nimport pandas as pd\nimport geopandas as gpd\nurl='https://bit.ly/3I0XDrq'\ndf = pd.read_csv(url)\ndf.set_index('id', inplace=True)\ndf['price'] = df.price.str.replace('$','',regex=False).astype('float')\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.to_file('Airbnb_Sample.gpkg', driver='GPKG')\n\nNote that this (re)loads the sampled Airbnb data from GitHub every time you run it. For a large data set on someone else’e server you might want to save and (re)load it locally. A simple helper function would then just check if the file already existed locally before trying to download the file again: that would allow you to work while offline and speed up your code substantially too!"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-can-we-do-series",
    "href": "lectures/7.3-EDA.html#what-can-we-do-series",
    "title": "ExploratoryData Analysis",
    "section": "What Can We Do? (Series)",
    "text": "What Can We Do? (Series)\nThis is by no means all that we can do…\n\nSeries-level Methods.\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(f\"Host count is {gdf.host_name.count()}\")\nprint(f\"Mean is {gdf.price.mean():.0f}\")\nprint(f\"Max price is {gdf.price.max()}\")\nprint(f\"Min price is {gdf.price.min()}\")\nprint(f\"Median price is {gdf.price.median()}\")\nprint(f\"Standard dev is {gdf.price.std():.2f}\")\nprint(f\"25th quantile is {gdf.price.quantile(q=0.25)}\")\nCount of non-nulls\nMean\nHighest value\nLowest value\nMedian\nStandard deviation\n25th quantile"
  },
  {
    "objectID": "lectures/7.3-EDA.html#what-can-we-do-data-frame",
    "href": "lectures/7.3-EDA.html#what-can-we-do-data-frame",
    "title": "ExploratoryData Analysis",
    "section": "What Can We Do? (Data Frame)",
    "text": "What Can We Do? (Data Frame)\n\n\n\n\n\n\n\nCommand\nReturns\n\n\n\n\nprint(df.mean())\nprint(df.count())\nprint(df.max())\n# ...\nprint(df.corr())\nprint(df.describe())\nMean of each column\nNumber of non-null values in each column\nHighest value in each column\n$\\vdots$\nCorrelation between columns\nSummarise\n\n\n\n\nNotice how we have the same functionality, but it operates at the level of the data set itself now. We gain a few new functions as well that relate to interactions between columns (a.k.a. data series)."
  },
  {
    "objectID": "lectures/7.3-EDA.html#measures",
    "href": "lectures/7.3-EDA.html#measures",
    "title": "ExploratoryData Analysis",
    "section": "Measures",
    "text": "Measures\nSo pandas provides functions for commonly-used measures:\nprint(f\"{df.price.mean():.2f}\")\nprint(f\"{df.price.median():.2f}\")\nprint(f\"{df.price.quantile(0.25):.2f}\")\nOutput:\n118.4542\n80.50\n40.75"
  },
  {
    "objectID": "lectures/7.3-EDA.html#more-complex-measures",
    "href": "lectures/7.3-EDA.html#more-complex-measures",
    "title": "ExploratoryData Analysis",
    "section": "More Complex Measures",
    "text": "More Complex Measures\nBut Pandas also makes it easy to derive new variables… Here’s the z-score:\n\\[ z = \\frac{x - \\bar{x}}{s}\\]\ndf['zscore'] = (df.price - df.price.mean())/df.price.std()\ndf.plot.box(column='zscore')"
  },
  {
    "objectID": "lectures/7.3-EDA.html#and-even-more-complex",
    "href": "lectures/7.3-EDA.html#and-even-more-complex",
    "title": "ExploratoryData Analysis",
    "section": "And Even More Complex",
    "text": "And Even More Complex\nAnd here’s the Interquartile Range Standardised score:\n\\[ x_{iqrs} = \\frac{x - \\widetilde{x}}{Q_{75} - Q_{25}} \\]\ndf['iqr_std'] = (df.price - df.price.median())/ \\\n      (df.price.quantile(q=0.75)-df.price.quantile(q=0.25))\ndf.plot.box(column='iqr_std')"
  },
  {
    "objectID": "lectures/7.3-EDA.html#the-plot-thickens",
    "href": "lectures/7.3-EDA.html#the-plot-thickens",
    "title": "ExploratoryData Analysis",
    "section": "The Plot Thickens",
    "text": "The Plot Thickens\nWe’ll get to more complex plotting over the course of the term, but here’s a good start for exploring the data! All plotting depends on matplotlib which is the ogre in the attic to R’s ggplot.\nimport matplotlib.pyplot as plt\nGet used to this import as it will allow you to save and manipulate the figures created in Python. It is not the most intuitive approach (unless you’ve used MATLAB before) but it does work."
  },
  {
    "objectID": "lectures/7.3-EDA.html#boxplot",
    "href": "lectures/7.3-EDA.html#boxplot",
    "title": "ExploratoryData Analysis",
    "section": "Boxplot",
    "text": "Boxplot\ndf.price.plot.box()\nplt.savefig('pboxplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#frequency",
    "href": "lectures/7.3-EDA.html#frequency",
    "title": "ExploratoryData Analysis",
    "section": "Frequency",
    "text": "Frequency\ndf.room_type.value_counts().plot.bar()\nplt.savefig('phistplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-correlation-heatmap",
    "href": "lectures/7.3-EDA.html#a-correlation-heatmap",
    "title": "ExploratoryData Analysis",
    "section": "A Correlation Heatmap",
    "text": "A Correlation Heatmap\nWe’ll get to these in more detail in a couple of weeks, but here’s some output…"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-map",
    "href": "lectures/7.3-EDA.html#a-map",
    "title": "ExploratoryData Analysis",
    "section": "A ‘Map’",
    "text": "A ‘Map’\ndf.plot.scatter(x='longitude',y='latitude')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#a-fancy-map",
    "href": "lectures/7.3-EDA.html#a-fancy-map",
    "title": "ExploratoryData Analysis",
    "section": "A Fancy ‘Map’",
    "text": "A Fancy ‘Map’\ndf.plot.scatter(x='longitude',y='latitude',\n                c='price',colormap='viridis',\n                figsize=(10,5),title='London',\n                grid=True,s=24,marker='x')\nplt.savefig('pscatterplot.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#an-actual-map",
    "href": "lectures/7.3-EDA.html#an-actual-map",
    "title": "ExploratoryData Analysis",
    "section": "An Actual ‘Map’",
    "text": "An Actual ‘Map’\ngdf.plot(column='price', cmap='viridis', \n         scheme='quantiles', markersize=8, legend=True)"
  },
  {
    "objectID": "lectures/7.3-EDA.html#additional-resources",
    "href": "lectures/7.3-EDA.html#additional-resources",
    "title": "ExploratoryData Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\nThere’s so much more to find, but:\n\n\n\nPandas Reference\nA Guide to EDA in Python (Looks very promising)\nEDA with Pandas on Kaggle\nEDA Visualisation using Pandas\nPython EDA Analysis Tutorial\nBetter EDA with Pandas Profiling [Requires module installation]\nEDA: DataPrep.eda vs Pandas-Profiling [Requires module installation]\n\n\n\nA Data Science Project for Beginners (EDA)\nEDA: A Pracitcal Guide and Template for Structured Data\nEDA—Don’t ask how, ask what (Part 1)\nPreparing your Dataset for Modeling – Quickly and Easily (Part 2)\nHandling Missing Data\nIntroduction to Exploratory Data Analysis (EDA)"
  },
  {
    "objectID": "lectures/7.1-Mapping.html",
    "href": "lectures/7.1-Mapping.html",
    "title": "Computers+Maps",
    "section": "",
    "text": "Both analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\n\n\n\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#two-cultures",
    "href": "lectures/7.1-Mapping.html#two-cultures",
    "title": "Computers+Maps",
    "section": "",
    "text": "Both analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\n\n\n\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#the-challenge",
    "href": "lectures/7.1-Mapping.html#the-challenge",
    "title": "Computers+Maps",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#constituency-cards",
    "href": "lectures/7.1-Mapping.html#constituency-cards",
    "title": "Computers+Maps",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nClone and reproduce: github.com/alasdairrae/wpc and explanation: cconstituency cards."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/7.1-Mapping.html#short-term-lets-in-scotland",
    "title": "Computers+Maps",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#every-building-in-america",
    "href": "lectures/7.1-Mapping.html#every-building-in-america",
    "title": "Computers+Maps",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#think-it-through",
    "href": "lectures/7.1-Mapping.html#think-it-through",
    "title": "Computers+Maps",
    "section": "Think it Through!",
    "text": "Think it Through!"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/7.1-Mapping.html#a-deceptively-simple-problem",
    "title": "Computers+Maps",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#classification",
    "href": "lectures/7.1-Mapping.html#classification",
    "title": "Computers+Maps",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment",
    "title": "Computers+Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment-1",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment-1",
    "title": "Computers+Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#six-views-of-employment-2",
    "href": "lectures/7.1-Mapping.html#six-views-of-employment-2",
    "title": "Computers+Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#consider",
    "href": "lectures/7.1-Mapping.html#consider",
    "title": "Computers+Maps",
    "section": "Consider",
    "text": "Consider\nWe want to:\n\nGroup features with similar values together.\nShow these in a way that doesn’t mislead the viewer.\n\nBut we have the following problems:\n\nToo many classes confuse the viewer.\nToo few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#choices-choices",
    "href": "lectures/7.1-Mapping.html#choices-choices",
    "title": "Computers+Maps",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#look-at-the-data",
    "href": "lectures/7.1-Mapping.html#look-at-the-data",
    "title": "Computers+Maps",
    "section": "Look at the Data!",
    "text": "Look at the Data!\nDifferent colour and break schemes not only give us different views of the data, they give us different understandings of the data! Each scheme changes how the data looks and, consequently, how we perceive the distribution."
  },
  {
    "objectID": "lectures/7.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/7.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Computers+Maps",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/7.1-Mapping.html#additional-resources",
    "href": "lectures/7.1-Mapping.html#additional-resources",
    "title": "Computers+Maps",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation\nWorking with Spatial Data in Python\nWeb Mapping Notes"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#why-pandas",
    "href": "lectures/6.4-Pandas.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy, numpy, and sklearn) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-data-frame",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-index",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-series",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#pandas-terminology-slice",
    "href": "lectures/6.4-Pandas.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#using-pandas",
    "href": "lectures/6.4-Pandas.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nHere’s code to read a (remote) CSV file:\nimport pandas as pd      # import package\n# Bitly: https://raw.githubusercontent.com/jreades/fsds/master/data/2019-sample-crime.csv\nurl='https://bit.ly/39SJpfp'\ndf = pd.read_csv(url)       # load a (remote) CSV\nprint(type(df))             # not simple data type\nprint(df.columns.to_list()) # column names\nprint(df.columns.values)    # Also works but deprecated\nOutput:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n['ID' 'Case Number' 'Date' 'Primary Type' 'Description'\n 'Location Description' 'Arrest' 'Domestic' 'Year' 'Latitude' 'Longitude']"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#summarise-a-data-frame",
    "href": "lectures/6.4-Pandas.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\ndf.describe() # Information about each Series\ndf.info()     # Information about each Series and the df\ndf.info is more about data types and memory usage. df.describe is for summarising information about the distribution of values in every series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#familiar",
    "href": "lectures/6.4-Pandas.html#familiar",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\nThis should be looking eerily familiar:\nprint(type(df['Latitude']))          # type for column\nprint(type(df['Latitude'].array))    # type for values\nprint(df['Latitude'].array[:5])     # first five values\nprint(f\"1: {df['Latitude'].mean()}\") # summarise a series/column\nprint(f\"2: {df.Latitude.mean()}\")    # if no spaces in name\nProduces:\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'numpy.ndarray'&gt;\n[41.75130706 41.90399688 41.88032861 41.92438396 41.75579713]\n1: 41.84550008439\n2: 41.84550008439\n\nNotice that we’ve got two ways of accessing a pandas Series:\n\nThe dictionary-like way: df['Latitude']; this works for all columns, always.\nThe method-like way: df.Latitude; this works for ‘reading’ columns without spaces in their names."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#jupyter-formatting",
    "href": "lectures/6.4-Pandas.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter in ‘fancy’ ways:"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#familiar-1",
    "href": "lectures/6.4-Pandas.html#familiar-1",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                       # First 3 rows of df\ndf[['ID','Date','Year']].tail(3) # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nOn one level, this is what we’ve been building towards! We’ve got head and tail which we saw in the Command Line lecture. We’ve got random sampling with seeds which we saw in the Randomness lecture. We’ve even got LoLs, which we saw way back in the Lists of Lists lecture!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#data-frames-vs-series",
    "href": "lectures/6.4-Pandas.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nPandas operates on two principles:\n\nAny operation on a Data Frame returns a Data Frame.\nAny operation on a Series returns a Series.\n\n\nWe’ll see in a moment why this is useful!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#shallow-copies",
    "href": "lectures/6.4-Pandas.html#shallow-copies",
    "title": "Pandas",
    "section": "‘Shallow’ Copies",
    "text": "‘Shallow’ Copies\nMore subtly, operations on a Series or Data Frame return a shallow copy, which is like a ‘view’ in a database…\n\nThe original is unchanged unless you specify inplace=True (where supported).\nAttempts to change a subset of the data frame will often trigger a SettingWithCopyWarning warning.\n\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy()).\n\n\nDataQuest has a nice overview of how SettingWithCopyWarning is triggered and what to do about it."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#putting-these-ideas-together",
    "href": "lectures/6.4-Pandas.html#putting-these-ideas-together",
    "title": "Pandas",
    "section": "Putting These Ideas Together",
    "text": "Putting These Ideas Together\n# Returns a series but not a column\ndf.Latitude - 1 \n# Saves returned series as a new column\ndf['lat'] = df.Latitude - 1\n# Returns a new data frame w/o 'lat' \ndf.drop(columns=['lat']) \n# Modifies df directly\ndf.drop(columns=['lat'], inplace=True) \n# Try to modify a view of df (triggers warning)\ndf[df['Primary Type']=='BURGLARY'].Latitude = 41.7"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#chaining",
    "href": "lectures/6.4-Pandas.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\ndf.sort_values(by=['Year','ID'], ascending=False).sample(frac=0.5).head(20).median()"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#selection",
    "href": "lectures/6.4-Pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n# Returns a selection (Boolean series)\ndf['Primary Type']=='ASSAULT'\n\n# All rows where Primary Type is ASSAULT\ndf[ df['Primary Type']=='ASSAULT' ]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['Primary Type']=='ASSAULT'][['Longitude','Latitude']].mean()\n\n# Two conditions with a bit-wise AND\ndf[\n  (df['Primary Type']=='ASSAULT') &\n  (df['Description']=='AGGRAVATED: HANDGUN')\n]\n\n# Two conditions with a bit-wise OR\ndf[\n  (df['Primary Type']=='ASSAULT') |\n  (df['Primary Type']=='THEFT')\n]"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#dealing-with-types",
    "href": "lectures/6.4-Pandas.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\n\n\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns (including arrays)\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#changing-the-type",
    "href": "lectures/6.4-Pandas.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\nprint(df['Primary Type'].unique())   # Find unique values\nprint(df['Primary Type'].dtype.name) # Confirm is 'object'\ndf['Primary Type'] = df['Primary Type'].astype('category')\nprint(df['Primary Type'].dtype.name) # Confirm is 'category'\nprint(df['Primary Type'].describe()) # Category column info\nOutputs:\n['BURGLARY' 'DECEPTIVE PRACTICE' 'BATTERY'...]\nobject   # &lt; before `as type`\ncategory # &lt; after `as type`\ncount       100\nunique       15\ntop       THEFT\nfreq         28\nName: Primary Type, dtype: object # category==special class of object"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#datetime-data",
    "href": "lectures/6.4-Pandas.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\nprint(df.Date.dtype.name)\n# object\ndf.Date.to_list()[:3]\n# ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nThis shows that Date is currently a string of dates+times.\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.array, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nprint(df.dt.dtype.name)\n# datetime64[ns]\ndf.dt.to_list()[:3]\n# [Timestamp('2019-04-20 11:00:00'), Timestamp('2019-12-02 10:35:00'), Timestamp('2019-10-06 04:50:00')]\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#datetime-formats",
    "href": "lectures/6.4-Pandas.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM\n\n\n\nSo that is why:\npd.to_datetime(df.Date.array, format=\"%m/%d/%Y %H:%M:%S %p\")\nNote the other things happening here:\n\npd.to_datetime(...) is not a method, it’s a function from the pandas package.\ndf.Date.array (and df.Date.to_numpy() and df.Data.tolist()) gives access to the data directly, whereas df.Date gives access to the Series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#deprecation-warning",
    "href": "lectures/6.4-Pandas.html#deprecation-warning",
    "title": "Pandas",
    "section": "Deprecation Warning!",
    "text": "Deprecation Warning!\nFrom time to time, real-world software projects will change the way things work. Pandas is just such a project!\n\n\n\n\n\n\nWarning\n\n\n\nWe recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array. See API Documenation.\n\n\nSo while Series.values still works, and will continue to work for some time, you are being advised to start using Series.array or Series.to_numpy() instead. Meaning, we should consider using df.Date.array."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#tidying-up",
    "href": "lectures/6.4-Pandas.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many options and subtleties…\n# Fix categories\nmapping = {}\n\n# df['Primary Type'].unique().to_list() also works\nfor x in df['Primary Type'].cat.categories.to_list():\n  mapping[x]=x.title()\n\n# And update\ndf['Primary Type'] = df['Primary Type'].cat.rename_categories(mapping)\nHow would you work out what this code does? 1\nTo deal with pricing information treated as a string:\ndf2['price'].str.replace('$','').astype(float)\nMany more examples accessible via Google!\n\nAnother thing you might notice here: adding .cat allows us to access category methods for the Series; adding .str allows us to access string methods for the Series."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#dropping-rows-and-columns",
    "href": "lectures/6.4-Pandas.html#dropping-rows-and-columns",
    "title": "Pandas",
    "section": "Dropping Rows and Columns",
    "text": "Dropping Rows and Columns\nThere are multiple ways to drop ‘stuff’:\ndf2 = df.copy()\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf2.drop(index=range(5,10), inplace=True) # Row 'numbers' or index values\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\ndf.drop(columns=['Year'], inplace=True)   # Column name(s)\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df.shape[1]:,} cols.\")\nThere is also df.dropna() which can apply to rows or columns with NULL or np.nan values.\nI often prefer df = df[df.index &gt; 15] (negative selection) to df.drop(index=range(0,14)) (positive selection).\n\nWhy might you want the default to not be in_place?"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#accessing-data-by-location",
    "href": "lectures/6.4-Pandas.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index:\n\ndf.loc returns the rows labeled 0, 1, and 2 ([0..2]), whereas\ndf.iloc returns the range 0..2 ([0..2))!"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#indexes",
    "href": "lectures/6.4-Pandas.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you print out the head:\ndf.head(3)\nThe left-most column (without) a name is the index.\ndf.set_index('ID', inplace=True)\ndf.head(3)\nNow we see:\n         Case Number                    Date  ...  Longitude                  dt\nID                                            ...\n11667185    JC237601  04/20/2019 11:00:00 PM  ... -87.603468 2019-04-20 11:00:00\n11909178    JC532226  12/02/2019 10:35:00 AM  ... -87.643230 2019-12-02 10:35:00\n11852571    JC462365  10/06/2019 04:50:00 PM  ... -87.758473 2019-10-06 04:50:00\n\nSo ID is now the index and is not accessible as a column: df.ID will now throw an error because it’s not longer part of the Column Index."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#indexes-contd",
    "href": "lectures/6.4-Pandas.html#indexes-contd",
    "title": "Pandas",
    "section": "Indexes (cont’d)",
    "text": "Indexes (cont’d)\nNotice the change to the data frame:\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nAnd now:\nprint(df.loc[11667185,:])\nprint(df.loc[11667185:11852571,'Case Number':'Date'])\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/6.4-Pandas.html#saving",
    "href": "lectures/6.4-Pandas.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, here are some of the more popular ones:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(&lt;path&gt;)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(&lt;path&gt;)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_feather(&lt;path&gt;)\nDirectly usable by R. Requires pyarrow to be installed to access the options.\n\n\ndf.to_parquet(&lt;path&gt;)\nDirectly usable by many languages. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(&lt;path&gt;))\nWrite a LaTeX-formatted table to a file. Display requires booktabs. Could do copy+paste with print(df.to_latex()).\n\n\ndf.to_markdown(&lt;path&gt;)\nWrite a Markdown-formatted table to a file. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is detected automatically (e.g. df.to_csv('file.csv.gz')) but you can also specify it (e.g. df.to_csv('file.csv.gz', compression='gzip')).2"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#additional-resources",
    "href": "lectures/6.4-Pandas.html#additional-resources",
    "title": "Pandas",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply"
  },
  {
    "objectID": "lectures/6.4-Pandas.html#footnotes",
    "href": "lectures/6.4-Pandas.html#footnotes",
    "title": "Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are at least two ways: 1) print out mapping; 2) before running the code comment out the ‘update’ line and print out x and x.title(); 3) search for title python.↩︎\nFor instance, a bit.ly link to a Gzipped file requires compression='gzip' because there’s nothing in the link itself to tell Pandas what to expect.↩︎"
  },
  {
    "objectID": "lectures/6.2-Randomness.html",
    "href": "lectures/6.2-Randomness.html",
    "title": "Randomness",
    "section": "",
    "text": "Benford’s Law, which has applications in data science and fraud detection.\n\nGenuine randomness is hard. Most of us, both humans and computers, tend to use shortcuts when generating random numbers.\nPeople are really bad at generating random numbers. Bendford’s law is used by auditors to look for abnormalities in financial returns.\nComputers are better, but they are also lazy.\n\n\n\nDepends on the problem:\n\nBanking and encryption?\nSampling and testing?\nReproducing research/documentation?\n\n\nOK, technically, even encryption needs to be reproducible to allow for decryption, but you sure don’t want it to be easy.\n\n\n\n\n\n\n\nCyphertext\nOutput\n\n\n\n\nROT0\nTo be or not to be, That is the question\n\n\nROT1\nUp cf ps opu up cf, Uibu jt uif rvftujpo\n\n\nROT2\nVq dg qt pqv vq dg, Vjcv ku vjg swguvkqp\n\n\n…\n…\n\n\nROT9\nCx kn xa wxc cx kn, Cqjc rb cqn zdnbcrxw\n\n\n\nROT is known as the Caesar Cypher, but since the transformation is simple (A..Z+=x) decryption is easy now. How can we make this harder?\n\n\n\nimport random\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nSee also: random.randrange, random.choice, random.sample, random.random, random.gauss, etc.\n\n\n\nimport random\nsize = 10\nresults = [0] * size\n\ntests = 100000\nwhile tests &gt; 0:\n    results[random.randint(0,len(results)-1)] += 1\n    tests -= 1\n\nfor i in range(0,len(results)):\n    print(f\"{i} -&gt; {results[i]}\")\n\nWhat will this return?\nWill it hold for more than 10 numbers?\n\n\n\n\nimport random \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nsize = 1000\ndata = [0] * size\n\ntests = 10000000\nwhile tests &gt; 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nfig = plt.figure()\nplt.bar(np.arange(0,len(data)), data)\nfig.savefig('Random.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#reproducibility-good-or-bad",
    "href": "lectures/6.2-Randomness.html#reproducibility-good-or-bad",
    "title": "Randomness",
    "section": "",
    "text": "Depends on the problem:\n\nBanking and encryption?\nSampling and testing?\nReproducing research/documentation?\n\n\nOK, technically, even encryption needs to be reproducible to allow for decryption, but you sure don’t want it to be easy."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#not-very-good-encryption",
    "href": "lectures/6.2-Randomness.html#not-very-good-encryption",
    "title": "Randomness",
    "section": "",
    "text": "Cyphertext\nOutput\n\n\n\n\nROT0\nTo be or not to be, That is the question\n\n\nROT1\nUp cf ps opu up cf, Uibu jt uif rvftujpo\n\n\nROT2\nVq dg qt pqv vq dg, Vjcv ku vjg swguvkqp\n\n\n…\n…\n\n\nROT9\nCx kn xa wxc cx kn, Cqjc rb cqn zdnbcrxw\n\n\n\nROT is known as the Caesar Cypher, but since the transformation is simple (A..Z+=x) decryption is easy now. How can we make this harder?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#python-is-random",
    "href": "lectures/6.2-Randomness.html#python-is-random",
    "title": "Randomness",
    "section": "",
    "text": "import random\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nrandom.randint(0,10)\nSee also: random.randrange, random.choice, random.sample, random.random, random.gauss, etc."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#and-repeat",
    "href": "lectures/6.2-Randomness.html#and-repeat",
    "title": "Randomness",
    "section": "",
    "text": "import random\nsize = 10\nresults = [0] * size\n\ntests = 100000\nwhile tests &gt; 0:\n    results[random.randint(0,len(results)-1)] += 1\n    tests -= 1\n\nfor i in range(0,len(results)):\n    print(f\"{i} -&gt; {results[i]}\")\n\nWhat will this return?\nWill it hold for more than 10 numbers?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat",
    "href": "lectures/6.2-Randomness.html#aaaaaaaaaaand-repeat",
    "title": "Randomness",
    "section": "",
    "text": "import random \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nsize = 1000\ndata = [0] * size\n\ntests = 10000000\nwhile tests &gt; 0:\n    data[random.randint(0,len(data)-1)] += 1\n    tests -= 1\n\nfig = plt.figure()\nplt.bar(np.arange(0,len(data)), data)\nfig.savefig('Random.png', dpi=150, transparent=True)"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#just-add-salt",
    "href": "lectures/6.2-Randomness.html#just-add-salt",
    "title": "Randomness",
    "section": "Just add salt",
    "text": "Just add salt"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#hashing",
    "href": "lectures/6.2-Randomness.html#hashing",
    "title": "Randomness",
    "section": "Hashing",
    "text": "Hashing\nChecking for changes (usally in a security context).\nimport hashlib # Can take a 'salt' (similar to a 'seed')\n\nr1 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r1 is: {r1.hexdigest()}\")\n\nr2 = hashlib.md5('CASA Intro to Programming '.encode())\nprint(f\"The hashed equivalent of r2 is: {r2.hexdigest()}\")\n\nr3 = hashlib.md5('CASA Intro to Programming'.encode())\nprint(f\"The hashed equivalent of r3 is: {r3.hexdigest()}\")\nOutputs:\n\"The hashed equivalent of r1 is: acd601db5552408851070043947683ef\"\n\"The hashed equivalent of r2 is: 4458e89e9eb806f1ac60acfdf45d85b6\"\n\"The hashed equivalent of r3 is: acd601db5552408851070043947683ef\"\n\nThis is like generating a ‘fingerprint’ of an application or file. In fact, it’s what is going on behind the scenes when you download something to install on macOS or Windows and you’re told that the installer is being ‘verified’ before it will run: the computer is generating a hash of the application’s codebase, and sending that to the Apple Store or Windows Store or direct to the developer in order to check that the file hasn’t been tampered with."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#and-note",
    "href": "lectures/6.2-Randomness.html#and-note",
    "title": "Randomness",
    "section": "And Note…",
    "text": "And Note…\nimport requests\nnight = requests.get(\"http://www.gutenberg.org/ebooks/1514.txt.utf-8\")\nprint(f\"The text is {night.text[30:70]}\")\nprint(f\"The text is {len(night.text):,} characters long\")\nhash = hashlib.md5(night.text.encode())\nprint(f\"This can be hashed into: {hash.hexdigest()}\")\nOutputs:\n\"The text is A Midsummer Night's Dream by Shakespeare\"\n\"The text is 112,127 characters long\"\n\"This can be hashed into: cce0d35b8b2c4dafcbde3deb983fec0a\"\n\nCan be applied to anything: even one byte’s difference (e.g. in a application) can lead to a different hash output.\nBut notice that hashes are always the same length. This property is quite useful for databases and verifying the integrity of applications (MD5 Checksums)."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#jupyterlab-password",
    "href": "lectures/6.2-Randomness.html#jupyterlab-password",
    "title": "Randomness",
    "section": "JupyterLab Password",
    "text": "JupyterLab Password\nTo set a password in JupyterLab you need something like this:\n'sha1:5b1c205a53e14e:0ce169b9834984347d62b20b9a82f6513355f72d'\nHow this was generated:\nimport uuid, hashlib\nsalt = uuid.uuid4().hex[:16] # Truncate salt\npassword = 'casa2021'        # Set password\n\n# Here we combine the password and salt to \n# 'add complexity' to the hash\nhashed_password = hashlib.sha1(password.encode() + \n                  salt.encode()).hexdigest()\nprint(':'.join(['sha1',salt,hashed_password]))\n\n\nDon’t set your passwords this way.\n\nThen you can replace the JUPYTER_PWD parameter in the start-up string for Podman/Docker if you want to set a password."
  },
  {
    "objectID": "lectures/6.2-Randomness.html#encryption-security",
    "href": "lectures/6.2-Randomness.html#encryption-security",
    "title": "Randomness",
    "section": "Encryption & Security",
    "text": "Encryption & Security\nSimple hashing algorithms are not normally secure enough for full encryption. Genuine security training takes a whole degree + years of experience.\nAreas to look at if you get involved in applications:\n\nPublic and Private Key Encryption (esp. OpenSSL)\nPrivileges used by Applications (esp. Podman vs Docker)\nRevocable Tokens (e.g. for APIs)\nInjection Attacks (esp. for SQL using NULL-byte and similar)"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#other-types-of-seeds",
    "href": "lectures/6.2-Randomness.html#other-types-of-seeds",
    "title": "Randomness",
    "section": "Other Types of Seeds",
    "text": "Other Types of Seeds\nTwo main libraries where seeds are set:\nimport random\nrandom.seed(42)\n\nimport numpy as np\nnp.random.seed(42)\n\nTurning now from the security aspects, let’s look at other types seeds.\nWhy do you often see 42 used as a seed?"
  },
  {
    "objectID": "lectures/6.2-Randomness.html#seeds-and-state",
    "href": "lectures/6.2-Randomness.html#seeds-and-state",
    "title": "Randomness",
    "section": "Seeds and State",
    "text": "Seeds and State\nimport random\nrandom.seed(42)\nst = random.getstate()\nfor r in range(0,3):\n    random.setstate(st)\n    print(f\"Repetition {r}:\")\n    ints = []\n    for i in range(0,10):\n        ints.append(random.randint(0,10))\n    print(f\"\\t{ints}\")"
  },
  {
    "objectID": "lectures/5.4-Errors.html",
    "href": "lectures/5.4-Errors.html",
    "title": "Errors",
    "section": "",
    "text": "It’s how we deal with them that matters. Same in Python: understanding and handling errors is the key to good code.\n\n\nPython will always try to tell you what it thinks went wrong: “I didn’t understand what you meant by this…” or “I’m sorry, I can’t let you do that Dave…”\nThe challenges are:\n\nPython tends to give you a lot of information about the error: this can be very helpful for programmers dealing with complex problems and totally overwhelming for beginners.\nThat what Python thinks the problem is doesn’t always line up with where the problem actually is. In cases of syntax, for instance, the problem could be an unclosed parenthesis three lines earlier!\n\n\n\n\nThat the ‘error’ isn’t always the error…\ntotal = 0\nprint(\"About to start loop\"\nfor i in range(1,10):\n  total += i\nprint(total)\nThis outputs:\nprint(\"About to start loop\"\n... for i in range(1,10):\n  File \"&lt;stdin&gt;\", line 2\n    for i in range(1,10):\n                        ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "lectures/5.4-Errors.html#helpfully",
    "href": "lectures/5.4-Errors.html#helpfully",
    "title": "Errors",
    "section": "",
    "text": "Python will always try to tell you what it thinks went wrong: “I didn’t understand what you meant by this…” or “I’m sorry, I can’t let you do that Dave…”\nThe challenges are:\n\nPython tends to give you a lot of information about the error: this can be very helpful for programmers dealing with complex problems and totally overwhelming for beginners.\nThat what Python thinks the problem is doesn’t always line up with where the problem actually is. In cases of syntax, for instance, the problem could be an unclosed parenthesis three lines earlier!"
  },
  {
    "objectID": "lectures/5.4-Errors.html#challenge-1",
    "href": "lectures/5.4-Errors.html#challenge-1",
    "title": "Errors",
    "section": "",
    "text": "That the ‘error’ isn’t always the error…\ntotal = 0\nprint(\"About to start loop\"\nfor i in range(1,10):\n  total += i\nprint(total)\nThis outputs:\nprint(\"About to start loop\"\n... for i in range(1,10):\n  File \"&lt;stdin&gt;\", line 2\n    for i in range(1,10):\n                        ^\nSyntaxError: invalid syntax"
  },
  {
    "objectID": "lectures/5.4-Errors.html#errors-have-types",
    "href": "lectures/5.4-Errors.html#errors-have-types",
    "title": "Errors",
    "section": "Errors Have Types",
    "text": "Errors Have Types\nIn the same way that variables have types, so do errors:\n\nModuleNotFoundError\nIndexError\nKeyError\nOSError\n…\n\nWe can add our own messages:\nraise Exception(\"Sorry, I can't let you do that, Dave.\")\n\n\nLots more built-in error types in the Python documentation and imported packages will provide their own as well.\n\nWhy might different types of errors be useful?\nWe might reasonably want to distinguish between errors that we could reasonably expect or that are not serious, from those that we did not expect or that call the results of the program into question."
  },
  {
    "objectID": "lectures/5.4-Errors.html#custom-errors",
    "href": "lectures/5.4-Errors.html#custom-errors",
    "title": "Errors",
    "section": "Custom Errors",
    "text": "Custom Errors\nWe can create our own types (classes) of error:\nclass CustomError(Exception):\n  pass # We do nothing except create a new type\nThis can then be triggered with:\nraise CustomError(\"Our custom error\")\nAnd (very importantly) this can be caught with:\nexcept CustomError: \n  #... do something with CustomError ..."
  },
  {
    "objectID": "lectures/5.4-Errors.html#why-customise",
    "href": "lectures/5.4-Errors.html#why-customise",
    "title": "Errors",
    "section": "Why Customise?",
    "text": "Why Customise?\nCustom exceptions can perform a variety of tasks:\n\nDistinguish between generic issues and those specific to your application.\nTriage issues based on your understanding of your application and the severity.\nProvide detailed insight based on fuller access to the state of your application.\nPerform important tidying-up or rollback operations, etc."
  },
  {
    "objectID": "lectures/5.4-Errors.html#so-errors-can-be-trapped",
    "href": "lectures/5.4-Errors.html#so-errors-can-be-trapped",
    "title": "Errors",
    "section": "So Errors can be Trapped",
    "text": "So Errors can be Trapped\nPython calls errors exceptions, so this leads to:\ntry:\n  #... some code that might fail...\nexcept &lt;Named_Error_Type&gt;:\n  #... what do it if it fails for a specific reason...\nexcept:\n  #... what to do if it fails for any other reason...\nfinally:\n  #... always do this, even if it fails...\nYou can use any or all of these together: you can have multiple named excepts to handle different types of errors from a single block of code; you do not have to have a catch-all except or a finally.\n\nSo it makes sense to think: “Well, let’s try this and see what happens. If we have a problem of this type then it’s not serious and we should carry on. But if we have a problem that type then we need to stop what we’re doing right away.”\nSome of the intricacies of errors can seem quite confusing. What’s the point of having a finally (which we’ll get to in a moment), for instance? Well, if your application is connected to a database then finally gives your application a chance to disconnect cleanly (freeing up resources for the database) and even to rollback incomplete changes (e.g. a new user whose details were only partially inputted when the application crashed)."
  },
  {
    "objectID": "lectures/5.4-Errors.html#trapping-errors",
    "href": "lectures/5.4-Errors.html#trapping-errors",
    "title": "Errors",
    "section": "Trapping Errors",
    "text": "Trapping Errors\nThis code fails:\nx,y = 10,0\nprint(x/y)\nAnd it generates this error:\n&gt; Traceback (most recent call last):\n&gt;   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n&gt; ZeroDivisionError: division by zero"
  },
  {
    "objectID": "lectures/5.4-Errors.html#trapping-errors-contd",
    "href": "lectures/5.4-Errors.html#trapping-errors-contd",
    "title": "Errors",
    "section": "Trapping Errors (cont’d)",
    "text": "Trapping Errors (cont’d)\nBut if you ‘trap’ the error using except then:\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\nexcept:\n  print(\"Something has gone very wrong.\")\nfinally: \n  print(\"Division is fun!\")\nThis will print\n&gt; You can't divide by zero!\n&gt; Division is fun!\n\n\nNote: if we need to access the actual exception: except ZeroDivisionError as e:"
  },
  {
    "objectID": "lectures/5.4-Errors.html#raising-hell",
    "href": "lectures/5.4-Errors.html#raising-hell",
    "title": "Errors",
    "section": "Raising Hell",
    "text": "Raising Hell\nYou can trigger your own exceptions using raise.\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")"
  },
  {
    "objectID": "lectures/5.4-Errors.html#understanding-multiple-errors",
    "href": "lectures/5.4-Errors.html#understanding-multiple-errors",
    "title": "Errors",
    "section": "Understanding Multiple Errors",
    "text": "Understanding Multiple Errors\nx,y = 10,0\ntry:\n  print(x/y)\nexcept ZeroDivisionError:\n  print(\"You can't divide by zero!\")\n  raise Exception(\"Please don't do that again!\")\nfinally: \n  print(\"Division is fun!\")\n\nThe code we try triggers the ZeroDivisionError block.\nThis prints \"You can't divide by zero!\"\nWe then raise a new exception that is not caught.\nThe finally code executes because it always does before Python exits.\nPython exits with the message from our newly raised Exception.\n\nThus: ‘During handling of above (ZeroDivisionError) another exception (our Exception) occurred…’"
  },
  {
    "objectID": "lectures/5.4-Errors.html#test-based-development",
    "href": "lectures/5.4-Errors.html#test-based-development",
    "title": "Errors",
    "section": "Test-Based Development",
    "text": "Test-Based Development\nWe can actually think of exceptions as a way to develop our code.\nHere’s some ‘pseudo-code’:\n# Testing the 'addition' operator\ntest(1+1, 2)           # Should equal 2\ntest(1+'1', TypeError) # Should equal TypeError\ntest('1'+'1', '11')    # Should equal '11'\ntest(-1+1, 0)          # Should equal 0 \nOur test(A,B) function takes an input (A) and the expected output (B) and then compares them. The test returns True if A==B and False otherwise."
  },
  {
    "objectID": "lectures/5.4-Errors.html#unit-tests",
    "href": "lectures/5.4-Errors.html#unit-tests",
    "title": "Errors",
    "section": "Unit Tests",
    "text": "Unit Tests\nEach test is a Unit Test because it tests one thing and one thing only. So if you had three functions to ‘do stuff’ then you’d need at least three unit tests.\nA Unit Test may be composed of one or more assertions. Our pseudo-code on the previous slide contained 4 assertions.\nA Unit Test does not mean that your code is correct or will perform properly under all circumstances. It means that your code returns the expected value for a specified input.\nPython considers this approach so important that it’s built in."
  },
  {
    "objectID": "lectures/5.4-Errors.html#approach-1",
    "href": "lectures/5.4-Errors.html#approach-1",
    "title": "Errors",
    "section": "Approach 1",
    "text": "Approach 1\nThis is an explict assertion to test fun:\nimport unittest\n\ndef fun(x):\n  return x + 1\n\nclass MyTest(unittest.TestCase):\n  def test(self):\n    self.assertEqual(fun(3), 4)\n    print(\"Assertion 1 passed.\")\n    self.assertEqual(fun(3), 5)\n    print(\"Assertion 2 passed.\")\n\nm = MyTest()\nm.test()\nThe critical output is:\nAssertionError: 4 != 5"
  },
  {
    "objectID": "lectures/5.4-Errors.html#approach-2",
    "href": "lectures/5.4-Errors.html#approach-2",
    "title": "Errors",
    "section": "Approach 2",
    "text": "Approach 2\nThis approach uses the ‘docstring’ (the bits between \"\"\") to test the results of the function. This is intended to encourage good documentation of functions using examples:\ndef square(x):\n    \"\"\"Return the square of x.\n\n    &gt;&gt;&gt; square(2)\n    4\n    &gt;&gt;&gt; square(-2)\n    4\n    &gt;&gt;&gt; square(-1)\n    2\n    \"\"\"\n    return x * x\n\nif __name__ == '__main__':\n    import doctest\n    doctest.testmod()\n\n\nNotice the __name__ (what could that possibly mean???) is tested to see if it’s '__main__'. The doctest code only runs when this code is executed in '__main__'."
  },
  {
    "objectID": "lectures/5.4-Errors.html#collaboration-continuous-integration",
    "href": "lectures/5.4-Errors.html#collaboration-continuous-integration",
    "title": "Errors",
    "section": "Collaboration & Continuous Integration",
    "text": "Collaboration & Continuous Integration\nThe Unit Test approach is often used on collaborative projects, especially in the Open Source world. PySAL, for instance, asks for unit tests with every new feature or integration.\nThe running of all tests for multiple components is called ‘integration testing’.\nA commit, merge, or pull on GitHub can trigger the unit testing process for the entire software ‘stack’. This is known as Continuous Integration because you are always checking that the code works as expected, rather than leaving testing to the end.\n\nThis is heavily used by PySAL and other robust FOSS projects since TravisCI is free for FOSS projects!"
  },
  {
    "objectID": "lectures/5.4-Errors.html#additional-resources",
    "href": "lectures/5.4-Errors.html#additional-resources",
    "title": "Errors",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nHandling exceptions\nReporting errors\nPython Custom Exceptions\nWriting and Using Custom Exceptions in Python\nPython Documentation\nHow to Define Custom Exception Classes\n\n\n\nUnit Testing in Python\nUnderstanding Unit Testing\nTesting Your Code\nGetting Started with Testing in Python\nPython’s unittest Library\nVideo: Unit Testing Your Code"
  },
  {
    "objectID": "lectures/5.4-Errors.html#footnotes",
    "href": "lectures/5.4-Errors.html#footnotes",
    "title": "Errors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource↩︎"
  },
  {
    "objectID": "lectures/5.2-Classes.html",
    "href": "lectures/5.2-Classes.html",
    "title": "Classes",
    "section": "",
    "text": "It is the template for producing new objects…\n\n\n\nObjects are instantiated versions of classes:\n\n\"hello world\" is an instance of a string, and\n['A','B',1,3] is an instance of a list.\n\nThe class is your recipe, the object is your 🍕…\n\n\n\nclass pizza(object):\n  base = 'sourdough'\n  \n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\n    \n  def add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\n  \n  def get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients\n\n\n\nclass pizza(object):\n\n    base = 'sourdough'\n    ...\nFollows the pattern: class &lt;name&gt;(&lt;parent class&gt;).\nYou can find many examples in: /opt/conda/envs/sds2020/lib/python3.7/site-packages (Docker).\n\n\n\n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce    = sauce\n    self.cheese   = cheese\nFollows the pattern: def __init__(self, &lt;params&gt;)\n\n\nNotice also the namespace: the parameters sauce and cheese are the same as the instance variables self.sauce and self.cheese because they occupy different namespaces.\n\n\n\ndef add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\nFollows the pattern: def &lt;function&gt;(self, &lt;params&gt;):\n\n\n\ndef get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients\n\n\n\np = pizza(sauce='white')\np.add_topping('peppers')\np.add_topping('chillis')\np.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chillis']\n\n\n\np1 = pizza(sauce='white')\np1.add_topping('peppers')\np1.add_topping('chilis')\n\np2 = pizza()\np2.base = \"Plain old base\"\np2.add_topping('pineapple')\np2.add_topping('ham')\n\np1.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()\n&gt; ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']\n\n\n\npizza.base='Crusty' # Like changing a package var!\np1.get_pizza()      # Base has changed\n# ['Crusty', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()      # Base has not changed!\n# ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']\n\np3 = pizza()\np3.get_pizza()      # Base has changed\n# ['Crusty', 'tomato', 'mozzarella']\n\n\n\n\nA class is defined by:\nclass &lt;name&gt;(&lt;parent class):\n  ...\nA class is initialised by:\n  def __init__(self, &lt;any_parameters&gt;):\n    ...\nAll methods have to have this:\n  def &lt;method&gt;(self, &lt;any_parameters&gt;):\n    ...\n\n\n\nThis is an instance variable:\n  self.&lt;var&gt; = &lt;something&gt;\nThis is a class variable (in the class definition):\n  &lt;var&gt; = &lt;something&gt;"
  },
  {
    "objectID": "lectures/5.2-Classes.html#whats-an-object",
    "href": "lectures/5.2-Classes.html#whats-an-object",
    "title": "Classes",
    "section": "",
    "text": "Objects are instantiated versions of classes:\n\n\"hello world\" is an instance of a string, and\n['A','B',1,3] is an instance of a list.\n\nThe class is your recipe, the object is your 🍕…"
  },
  {
    "objectID": "lectures/5.2-Classes.html#really-like-a-pizza",
    "href": "lectures/5.2-Classes.html#really-like-a-pizza",
    "title": "Classes",
    "section": "",
    "text": "class pizza(object):\n  base = 'sourdough'\n  \n  def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce = sauce\n    self.cheese = cheese\n    \n  def add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\n  \n  def get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/5.2-Classes.html#class-definition",
    "href": "lectures/5.2-Classes.html#class-definition",
    "title": "Classes",
    "section": "",
    "text": "class pizza(object):\n\n    base = 'sourdough'\n    ...\nFollows the pattern: class &lt;name&gt;(&lt;parent class&gt;).\nYou can find many examples in: /opt/conda/envs/sds2020/lib/python3.7/site-packages (Docker)."
  },
  {
    "objectID": "lectures/5.2-Classes.html#the-constructor",
    "href": "lectures/5.2-Classes.html#the-constructor",
    "title": "Classes",
    "section": "",
    "text": "def __init__(self, sauce:str='tomato', cheese:str='mozzarella'):\n    self.toppings = []\n    self.sauce    = sauce\n    self.cheese   = cheese\nFollows the pattern: def __init__(self, &lt;params&gt;)\n\n\nNotice also the namespace: the parameters sauce and cheese are the same as the instance variables self.sauce and self.cheese because they occupy different namespaces."
  },
  {
    "objectID": "lectures/5.2-Classes.html#adding-toppings",
    "href": "lectures/5.2-Classes.html#adding-toppings",
    "title": "Classes",
    "section": "",
    "text": "def add_topping(self, topping:str) -&gt; None:\n    self.toppings.insert(len(self.toppings), topping)\nFollows the pattern: def &lt;function&gt;(self, &lt;params&gt;):"
  },
  {
    "objectID": "lectures/5.2-Classes.html#getting-the-pizza",
    "href": "lectures/5.2-Classes.html#getting-the-pizza",
    "title": "Classes",
    "section": "",
    "text": "def get_pizza(self) -&gt; list:\n    ingredients = [self.base, self.sauce, self.cheese]\n    ingredients.extend(self.toppings)\n    return ingredients"
  },
  {
    "objectID": "lectures/5.2-Classes.html#pizza-in-action",
    "href": "lectures/5.2-Classes.html#pizza-in-action",
    "title": "Classes",
    "section": "",
    "text": "p = pizza(sauce='white')\np.add_topping('peppers')\np.add_topping('chillis')\np.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chillis']"
  },
  {
    "objectID": "lectures/5.2-Classes.html#check-it-out",
    "href": "lectures/5.2-Classes.html#check-it-out",
    "title": "Classes",
    "section": "",
    "text": "p1 = pizza(sauce='white')\np1.add_topping('peppers')\np1.add_topping('chilis')\n\np2 = pizza()\np2.base = \"Plain old base\"\np2.add_topping('pineapple')\np2.add_topping('ham')\n\np1.get_pizza()\n&gt; ['sourdough', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()\n&gt; ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']\n\n\n\npizza.base='Crusty' # Like changing a package var!\np1.get_pizza()      # Base has changed\n# ['Crusty', 'white', 'mozzarella', 'peppers', 'chilis']\np2.get_pizza()      # Base has not changed!\n# ['Plain old base', 'tomato', 'mozzarella', 'pineapple', 'ham']\n\np3 = pizza()\np3.get_pizza()      # Base has changed\n# ['Crusty', 'tomato', 'mozzarella']"
  },
  {
    "objectID": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza",
    "href": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza",
    "title": "Classes",
    "section": "",
    "text": "A class is defined by:\nclass &lt;name&gt;(&lt;parent class):\n  ...\nA class is initialised by:\n  def __init__(self, &lt;any_parameters&gt;):\n    ...\nAll methods have to have this:\n  def &lt;method&gt;(self, &lt;any_parameters&gt;):\n    ..."
  },
  {
    "objectID": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "href": "lectures/5.2-Classes.html#recap-how-to-make-a-pizza-contd",
    "title": "Classes",
    "section": "",
    "text": "This is an instance variable:\n  self.&lt;var&gt; = &lt;something&gt;\nThis is a class variable (in the class definition):\n  &lt;var&gt; = &lt;something&gt;"
  },
  {
    "objectID": "lectures/5.2-Classes.html#decorating-a-class",
    "href": "lectures/5.2-Classes.html#decorating-a-class",
    "title": "Classes",
    "section": "Decorating a Class",
    "text": "Decorating a Class\nClasses have additional decorators you can use:\n\n@staticmethod: binds any function to a class so objects can use it (but no object or class information is passed to static methods).\n@classmethod: decorates a function for use by a class, not an object (accesses and modifies class state).\n@property: create ‘managed’ attributes in classes to ‘hide’ them from users but keep code clean."
  },
  {
    "objectID": "lectures/5.2-Classes.html#class-and-static-methods",
    "href": "lectures/5.2-Classes.html#class-and-static-methods",
    "title": "Classes",
    "section": "Class and Static Methods1",
    "text": "Class and Static Methods1\n\nfrom datetime import date\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    # a class method to create a Person object by birth year.\n    @classmethod\n    def fromBirthYear(cls, name, year):\n        return cls(name, date.today().year - year)\n\n    # a static method to check if a Person is adult or not.\n    @staticmethod\n    def isAdult(age):\n        return age &gt; 18\n\nperson1 = Person('Andy', 31)\nperson2 = Person.fromBirthYear('Jon', 1976)\n\nprint(person1.age)\nprint(person2.age)\nprint(Person.isAdult(22))\n\n31\n49\nTrue"
  },
  {
    "objectID": "lectures/5.2-Classes.html#properties",
    "href": "lectures/5.2-Classes.html#properties",
    "title": "Classes",
    "section": "Properties2",
    "text": "Properties2\n\nclass Circle:\n    def __init__(self, radius):\n        self._radius = radius\n\n    @property\n    def radius(self):\n        \"\"\"The radius property.\"\"\"\n        print(\"Get radius\")\n        return self._radius\n\n    @radius.setter\n    def radius(self, value):\n        print(\"Set radius\")\n        self._radius = value\n\n    @radius.deleter\n    def radius(self):\n        print(\"Delete radius\")\n        del self._radius\n\ncircle = Circle(42.0)\nprint(circle.radius)\ncircle.radius = 100\nprint(circle.radius)\ndel(circle.radius)\n\nGet radius\n42.0\nSet radius\nGet radius\n100\nDelete radius\n\n\n\nThis is a simple example, but notice how we are accessing an attribute with the potential to ‘do things’ before or after accessing the value of interest. This could have all sorts of useful applications in the real world where lots of things are happening to an application at the same time."
  },
  {
    "objectID": "lectures/5.2-Classes.html#additional-resources",
    "href": "lectures/5.2-Classes.html#additional-resources",
    "title": "Classes",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nClasses\nObjects\nBasic class definition\nInstance methods and attributes\nChecking instance types\nClass methods and members\n\n\n\nCreating a class\nConstructing an object\nClass methods\nClass vs Instance Variables\nObject data\nInheritance\nAn Introduction to Object-Oriented Programming (by a CASA alum)"
  },
  {
    "objectID": "lectures/5.2-Classes.html#footnotes",
    "href": "lectures/5.2-Classes.html#footnotes",
    "title": "Classes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Geeks for Geeks.↩︎\nExample from Real Python.↩︎"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#many-models",
    "href": "lectures/4.5-Group_Working.html#many-models",
    "title": "Group Working",
    "section": "Many Models…",
    "text": "Many Models…\n\nArtist Collectives — shared responsibilities for ideas and outputs; a lot of freedom and fuzziness in roles, but propensity for struggles over direction and power.\nTheatre — every member has a specific role; there is freedom within the role and clear lines of responsibility for delivery, but can blow up spectacularly.\nCo-Creation — emphasis on participation and recognition of diverse strengths; problem- and communication-focused; lots of effort and uncertainty, but results can be much more meaningful and durable.\n…\n\n\nThe arts world is particularly innovative when it comes to project work because teams are always being formed and reformed around individual outputs (a play, an artistic collaboration, a theatre production)…\nFields such as consultancy and software development have their own norms, but they are for the most part less experimental in their structure.\nHowever, the point is that there is no one-size-fits-all pattern for a successful piece of group work."
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#challenges",
    "href": "lectures/4.5-Group_Working.html#challenges",
    "title": "Group Working",
    "section": "Challenges",
    "text": "Challenges\n\nWe make superficial assessments of the flaws/strengths of others.\nWe make a range of assumptions about the motivations of others.\nWe make a range of assumptions about the situations of others.\nWe respond differently to stimuli and stresses.\nWe have a hard time talking about any of this.\n\n\nThese are just some of the challenges to setting up your group.\nIn particular, when we meet a new group, it’s often easy to be impressed by someone who is self-confident and well-spoken: they must know what they’re doing! Group dynamics can be like a popularity contest in which the person who is easy-going or ‘fun’ is making a wonderful contribution, while the one who asks difficult questions is ‘not constructive’ and ‘not contributing’. Someone who is ‘quiet’ doesn’t seem to have much to say…"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#identifying-your-strengths",
    "href": "lectures/4.5-Group_Working.html#identifying-your-strengths",
    "title": "Group Working",
    "section": "Identifying Your Strengths",
    "text": "Identifying Your Strengths\n\n\nDominance\n\nResults-oriented\nInnovative\nCompetitive\nDirect\n\nInfluence\n\nPeople-oriented\nEnthusiastic\nOptimistic\nCreative\n\n\nSteadiness\n\nSincere\nDependable\nPatient\nModest\n\nConscientiousness\n\nAccurate\nCautious\nPrecise\nAnalytical\n\n\n\n\nHere is one way of moving beyond the stereotype that the person doing the talking is also doing the thinking. The DISC model is apparently connected back to Wonder Woman by her creator W.M Marston. Like Wonder Woman, this is a model that reflects its cultural context: it was created for American teams and if you aren’t American then you might find that these four personality types don’t ‘fit’ very well. But it’s still a good place to start talking about preferences, behaviours, and the way they impact others."
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#too-much-of-a-good-thing",
    "href": "lectures/4.5-Group_Working.html#too-much-of-a-good-thing",
    "title": "Group Working",
    "section": "Too Much of a Good Thing",
    "text": "Too Much of a Good Thing\n\n\nDominance\n\nFails to involve others\nImpatient\nOffensive\n\nInfluence\n\nToo social\nEasily distracted\nOverly optimistic\n\n\nSteadiness\n\nIndirect\nAvoids conflict\nDelays difficult decisions\n\nConscientiousness\n\nPerfectionist\nAvoids unsystematic people\nDelays decisions over risks\n\n\n\n\nAnything that is a strength in one context can become a weakness in another.\nFor instance, a dominant personality might assume that the fact no one else is objecting means that there is support for their proposal; however, it could just be that the rest of the group wants to avoid conflict even though they think there are significant risks.\nOr someone who is motivated by the social component might spend so long bringing people together to talk about directions (and constantly adding new ideas to the mix) that very little progress is made on the actual project!"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#consequences",
    "href": "lectures/4.5-Group_Working.html#consequences",
    "title": "Group Working",
    "section": "Consequences1",
    "text": "Consequences1"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#every-good-plan",
    "href": "lectures/4.5-Group_Working.html#every-good-plan",
    "title": "Group Working",
    "section": "Every Good Plan2…",
    "text": "Every Good Plan2…"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#nothing-ever-goes-to-plan",
    "href": "lectures/4.5-Group_Working.html#nothing-ever-goes-to-plan",
    "title": "Group Working",
    "section": "Nothing Ever Goes to Plan",
    "text": "Nothing Ever Goes to Plan\nFrom relational to practical…\n\nAgile principles3: iterative delivery of successful projects focussed on individuals and interactions; working software, customer collaboration; and responding to change.\nScrum methodology4: organised around stand ups, sprints, sprint review, and retrospectives.\nKanban methodology: use of ‘sticky notes’ in columns to organise and prioritise visually.\nXP methdology: one person writes/codes while a partner researches/solves/feeds back.\n\n\nOne challenge that teams face in the world of software is failure due to complexity and change.\nYou may have seen ‘Waterfall charts’ of the sort produced by project management software where one task cascades into the next until the project is delivered at the end. This is recipe for failure. The Mythical Man Month comprehensively debunked this approach in 1975.\nThe general conclusion was that “Adding more people to a project that is late makes it later.” The reason this happens is that you cannot break a complex task into discrete parts, send someone away to work on it in isolation, and then have them report back at the end with a finished product. I have seen this happen in previous years’ group work too: one person did a lit review while someone else was writing code but they didn’t line up at the end!\nI saw this in graphic design too: a client writes a brief for exactly what they want. You deliver something exactly like what they asked for. And they say that’s not what they meant.\nThe Agile philosophy emerged in response to these kinds of failures, with the approach that the product should be finished as quickly as possible, but as provisionally as possible. So you always have a working application, even if it only does a fraction of what you intend for it to do in the long run.\nThis has been elevated by companies like Facebook to slogans along the lines of ‘Fail Fast’, but the point is to recognise as quickly as possible when something isn’t working or isn’t headed in the direction you planned.\nFor the group work at the heart of this module, the point of the iterative process that begins in Week 6 is to help you develop competency in using the tools required to make your submission, to start you thinking along the lines that are required for the more complex questions, and to tease out points of failure in the group’s ability to work together long before you are under the tight deadline at the end of term!"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#additional-resources",
    "href": "lectures/4.5-Group_Working.html#additional-resources",
    "title": "Group Working",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nBartlett Guide to Group Work\nThe Five Dysfunctions of a Team\nThe Mythical Man Month"
  },
  {
    "objectID": "lectures/4.5-Group_Working.html#footnotes",
    "href": "lectures/4.5-Group_Working.html#footnotes",
    "title": "Group Working",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource↩︎\nSource: Zentao↩︎\nSource: SimpleLearn↩︎\nSource: Northeastern University↩︎"
  },
  {
    "objectID": "lectures/4.3-Packages.html",
    "href": "lectures/4.3-Packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages (also called modules or libraries) are bundles of useful, related functions and constants.\n\n\n\n\n\n\n\nPackage\nDoes\n\n\n\n\nos\nOperating System stuff (paths, processes, etc.)\n\n\ncsv\nReading and Writing CSV files.\n\n\nmath\nMaths constants and functions.\n\n\nstatistics\nSimple statistical tests & measures.\n\n\nurllib\nURL Reading (e.g. web pages).\n\n\nnumpy\nNumerical Python (scientific computing).\n\n\nscipy\nScientific Python (more scientific computing)\n\n\nsklearn\nMachine learning, clustering, etc.\n\n\n\n\n\n\nIf a package is installed, then it’s as simple as:\nimport &lt;packagename&gt;\nYou normally do this at the start of a program so that it’s easy to see what the program requires to run:\nimport math\nprint(math.pi) # Prints 3.141592653589793\n\n\n\nThere are many ways to find this out:\n\nRead the documentation\nSearch Google, and\nSearch StackOverflow\n\nThere’s even a web site python.readthedocs.io.\nBut we can also ask the package:\nimport math\ndir(math)\n# ['__doc__', '__file__', '__name__', '__package__', ..., \n# 'log', 'log10', 'log1p', 'modf', 'pi', ...]\nhelp(math.log10)\n\nSo, remember what I said in the talk about functions: that the numbers and total variables in the function aren’t the same as the ones outside the function? I said the answer had to do with something called the namespace. Here is the math namespace.\n\n\n\n\n\ndir(&lt;package name&gt;) lists all ‘things’ that &lt;package&gt; contains.\nBy convention, things that start with __ are ‘private’ (you shouldn’t change them) and things that start and end with __ are metadata (e.g. __version__).\nEverything else you can interrogate with help(&lt;package name&gt;.&lt;thing in package&gt;).\n\nWith help(math.log10) you get an answer like this:\nHelp on built-in function log10 in module math:\n\nlog10(x, /)\n    Return the base 10 logarithm of x.\nWith help(math.pi) you get an answer Help on float object…\n\nThis tells you that pi is a float, it doesn’t tell you what Pi is (an irrational number). So here’s another case where the computer gives you a technically correct but not always helpful answer. In the context of the math package, Pi is a float constant.\n\n\n\n\nConsider this:\nimport math\npi = 4\nprint(math.pi)\nprint(pi)\nSo math.pi and pi are not the same variable!\n\nThe latter is implicitly main.pi but Python doesn’t have a ‘main’ programme namespace. Everything is ‘main’ unless you put it in a package.\n\n\n\n\nProgrammers hate typing more than they have to:\nimport math\nr = 5\narea = math.pi * r**2\nln = math.log(area)\nprint(ln)\nSo we can use an alias instead:\nimport math as m\nr = 5\narea = m.pi * r**2\nln = m.log(area)\nprint(ln)\nYou will see this used a lot with more complex libraries like Pandas (pd), Geopandas (gpd), and PySAL (ps).\n\n\n\nSometimes even that is too much typing… or sometimes we only really want one or two things from a much larger package. In that case we can select these specifically:\nfrom math import pi, log10\nprint(pi)\nhelp(log10)\nThis import pi and log10 from math into the ‘main’ namespace.\n\n\n\n\nNotice the subtle differences here:\n\n\n\n\npi = 3.1415\nprint(pi)      # 3.1415\n\nimport math as m\nprint(m.pi)    # 3.141592...\nprint(pi)      # 3.1415\nprint(math.pi) # Error!\n\n\n\n\npi = 3.1415\nprint(pi) # 3.1415\n\nfrom math import pi\nprint(pi)      # 3.141592...\nprint(m.pi)    # Error!\nprint(math.pi) # Error!\n\n\n\n\n\n\n\n\n\n\nA bit of a mish-mash of different explanations:\n\n\n\nLearnPython.org\nRealPython: Namespaces and Scope\nReal Python: Modules and Packages\nProgramiz\nPythonCourse.eu\nTutorialsTeacher.com\nTutorialsPoint.com\n\n\n\nHow to Make a Package in Python\nHow to Create Your First Python Package\nCreate Python Packages for Your Python Code [This is more for distributing your own packages via Pip]"
  },
  {
    "objectID": "lectures/4.3-Packages.html#popular-packages",
    "href": "lectures/4.3-Packages.html#popular-packages",
    "title": "Packages",
    "section": "",
    "text": "Package\nDoes\n\n\n\n\nos\nOperating System stuff (paths, processes, etc.)\n\n\ncsv\nReading and Writing CSV files.\n\n\nmath\nMaths constants and functions.\n\n\nstatistics\nSimple statistical tests & measures.\n\n\nurllib\nURL Reading (e.g. web pages).\n\n\nnumpy\nNumerical Python (scientific computing).\n\n\nscipy\nScientific Python (more scientific computing)\n\n\nsklearn\nMachine learning, clustering, etc."
  },
  {
    "objectID": "lectures/4.3-Packages.html#importing-a-package",
    "href": "lectures/4.3-Packages.html#importing-a-package",
    "title": "Packages",
    "section": "",
    "text": "If a package is installed, then it’s as simple as:\nimport &lt;packagename&gt;\nYou normally do this at the start of a program so that it’s easy to see what the program requires to run:\nimport math\nprint(math.pi) # Prints 3.141592653589793"
  },
  {
    "objectID": "lectures/4.3-Packages.html#what-can-a-package-do",
    "href": "lectures/4.3-Packages.html#what-can-a-package-do",
    "title": "Packages",
    "section": "",
    "text": "There are many ways to find this out:\n\nRead the documentation\nSearch Google, and\nSearch StackOverflow\n\nThere’s even a web site python.readthedocs.io.\nBut we can also ask the package:\nimport math\ndir(math)\n# ['__doc__', '__file__', '__name__', '__package__', ..., \n# 'log', 'log10', 'log1p', 'modf', 'pi', ...]\nhelp(math.log10)\n\nSo, remember what I said in the talk about functions: that the numbers and total variables in the function aren’t the same as the ones outside the function? I said the answer had to do with something called the namespace. Here is the math namespace."
  },
  {
    "objectID": "lectures/4.3-Packages.html#so",
    "href": "lectures/4.3-Packages.html#so",
    "title": "Packages",
    "section": "",
    "text": "dir(&lt;package name&gt;) lists all ‘things’ that &lt;package&gt; contains.\nBy convention, things that start with __ are ‘private’ (you shouldn’t change them) and things that start and end with __ are metadata (e.g. __version__).\nEverything else you can interrogate with help(&lt;package name&gt;.&lt;thing in package&gt;).\n\nWith help(math.log10) you get an answer like this:\nHelp on built-in function log10 in module math:\n\nlog10(x, /)\n    Return the base 10 logarithm of x.\nWith help(math.pi) you get an answer Help on float object…\n\nThis tells you that pi is a float, it doesn’t tell you what Pi is (an irrational number). So here’s another case where the computer gives you a technically correct but not always helpful answer. In the context of the math package, Pi is a float constant."
  },
  {
    "objectID": "lectures/4.3-Packages.html#why-namespaces-matter",
    "href": "lectures/4.3-Packages.html#why-namespaces-matter",
    "title": "Packages",
    "section": "",
    "text": "Consider this:\nimport math\npi = 4\nprint(math.pi)\nprint(pi)\nSo math.pi and pi are not the same variable!\n\nThe latter is implicitly main.pi but Python doesn’t have a ‘main’ programme namespace. Everything is ‘main’ unless you put it in a package."
  },
  {
    "objectID": "lectures/4.3-Packages.html#more-laziness-aliases",
    "href": "lectures/4.3-Packages.html#more-laziness-aliases",
    "title": "Packages",
    "section": "",
    "text": "Programmers hate typing more than they have to:\nimport math\nr = 5\narea = math.pi * r**2\nln = math.log(area)\nprint(ln)\nSo we can use an alias instead:\nimport math as m\nr = 5\narea = m.pi * r**2\nln = m.log(area)\nprint(ln)\nYou will see this used a lot with more complex libraries like Pandas (pd), Geopandas (gpd), and PySAL (ps)."
  },
  {
    "objectID": "lectures/4.3-Packages.html#importing-part-of-a-package",
    "href": "lectures/4.3-Packages.html#importing-part-of-a-package",
    "title": "Packages",
    "section": "",
    "text": "Sometimes even that is too much typing… or sometimes we only really want one or two things from a much larger package. In that case we can select these specifically:\nfrom math import pi, log10\nprint(pi)\nhelp(log10)\nThis import pi and log10 from math into the ‘main’ namespace."
  },
  {
    "objectID": "lectures/4.3-Packages.html#gotcha",
    "href": "lectures/4.3-Packages.html#gotcha",
    "title": "Packages",
    "section": "",
    "text": "Notice the subtle differences here:\n\n\n\n\npi = 3.1415\nprint(pi)      # 3.1415\n\nimport math as m\nprint(m.pi)    # 3.141592...\nprint(pi)      # 3.1415\nprint(math.pi) # Error!\n\n\n\n\npi = 3.1415\nprint(pi) # 3.1415\n\nfrom math import pi\nprint(pi)      # 3.141592...\nprint(m.pi)    # Error!\nprint(math.pi) # Error!"
  },
  {
    "objectID": "lectures/4.3-Packages.html#additional-resources",
    "href": "lectures/4.3-Packages.html#additional-resources",
    "title": "Packages",
    "section": "",
    "text": "A bit of a mish-mash of different explanations:\n\n\n\nLearnPython.org\nRealPython: Namespaces and Scope\nReal Python: Modules and Packages\nProgramiz\nPythonCourse.eu\nTutorialsTeacher.com\nTutorialsPoint.com\n\n\n\nHow to Make a Package in Python\nHow to Create Your First Python Package\nCreate Python Packages for Your Python Code [This is more for distributing your own packages via Pip]"
  },
  {
    "objectID": "lectures/4.1-Functions.html",
    "href": "lectures/4.1-Functions.html",
    "title": "Functions",
    "section": "",
    "text": "Functions ‘encapsulate’ a task (they combine many instructions into a single line of code). Most programming languages provide many built-in functions that would otherwise require many steps to accomplish, for example computing the square root of a number. In general, we don’t care how a function does what it does, only that it ‘does it’!\n\n\nWhy do any work you don’t need to?\nFunctions allow us to re-use our code!\nBonus: they can also make our code more legible.\n\n\n\n\n\nSo len(...) encapsulates the process of figuring out how long something with ‘countable units’ actually is, whether it’s a string or a list.\n\n\n\nBecause print(...) encapsulates the process of sending output to the command line, a file, or even a database or API!\n\nlen(123) is a Type Error.\nlen(‘123’) is not.\nCan you think why?\n\n\n\n\n\nAll function ‘calls’ looking something like this:\nfunction_name(...)\nWhere the ‘...’ are the inputs to the function; it could be one variable, 25 variables, a list, even another function!\nAnd if the function ‘returns’ something it will look like this:\nreturn_data = function_name(...)\n\n\n\n\n\ndata = [1,25,-4,14,7,9]\ntotal = 0.0\ncount = len(data)\nfor i in data:\n  total += i\n\nprint(total/count)\n\n\n\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data))\n\n\n\n\nThe advantage of the function is that I can keep sending new lists to calc_mean without writing any new code. Whereas the other approach requires me to copy+paste repeatedly.\nWhere this really makes a difference is when you realise you made a mistake in your code: if you’ve made good use of functions this will mean correction only the place where you made the logical mistake. If you’ve not used functions then this is a tedious search and replace that is likely to be error-prone.\n\n\n\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data)) # 8.666666666666666\ndata2 = [200000,2500000,-4,1400000,70,900000]\nprint(calc_mean(data2)) # 833344.3333333334\n\n\n\ndata    = [1,25,-4,14,7,9]\ntotal   = 1\nnumbers = []\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\nprint(calc_mean(data))\n\n# Why haven't these changed????\nprint(total)\nprint(numbers)\n\nFunctions encapsulate information: the total and numbers used within the function are different from the variables with the same name that we created outside the function. When we get to libraries and packages you’ll understand why, but the key concept here is ‘namespace’ and that these variables might have the same name but they sit in different namespaces.\n\n\n\n\nBy ‘simple’ I don’t mean easy, I mean it does one thing only:\ndef hello():\n  print(\"Hello world!\")\nWe then run it with:\nhello()\nAnd that produces:\nHello world!\n\n\n\nWe can pass information to a function if we tell the function what to expect:\ndef hello(name:str):\n  print(f\"Hello {name}!\")\nNow we can do this:\nhello(\"new programmers\")\nAnd that produces:\nHello new programmers!\n\n\n\nWe can also get information out of them!\ndef hello(name:str) -&gt; str:\n  return f\"Hello {name}!\"\nNow we can do this:\noutput = hello(\"new programmers\")\nprint(output.title())\n# Same as: print(hello(\"new programmers\").title())\nAnd this produces:\n'Hello New Programmers!'\n\n\n\ndef &lt;function_name&gt;(&lt;var_name&gt;: &lt;var_type&gt;) -&gt; &lt;var_type&gt;:\n  ...\n  return &lt;var&gt;\nThis can also be written:\ndef &lt;function_name&gt;(&lt;var_name&gt;):\n  ...\n  return &lt;var&gt;\nPython is ‘friendly’ in the sense that all of the &lt;var_type&gt; information is optional, but it will help you (and Python) to know what you were expecting to see happen.\n\n\n\nds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz': [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ndef get_city_info(city:str, field:str, city_lookup:str='name', data:dict=ds2) -&gt; str:\n  return str(data[field][ data[city_lookup].index(city) ])\n\ncity = 'New York'\nprint(f\"The latitude of {city} is {get_city_info(city,'lat')}\")\n# The latitude of New York is 40.71"
  },
  {
    "objectID": "lectures/4.1-Functions.html#what-does-a-function-look-like",
    "href": "lectures/4.1-Functions.html#what-does-a-function-look-like",
    "title": "Functions",
    "section": "",
    "text": "So len(...) encapsulates the process of figuring out how long something with ‘countable units’ actually is, whether it’s a string or a list.\n\n\n\nBecause print(...) encapsulates the process of sending output to the command line, a file, or even a database or API!\n\nlen(123) is a Type Error.\nlen(‘123’) is not.\nCan you think why?"
  },
  {
    "objectID": "lectures/4.1-Functions.html#so-what-does-a-function-look-like",
    "href": "lectures/4.1-Functions.html#so-what-does-a-function-look-like",
    "title": "Functions",
    "section": "",
    "text": "All function ‘calls’ looking something like this:\nfunction_name(...)\nWhere the ‘...’ are the inputs to the function; it could be one variable, 25 variables, a list, even another function!\nAnd if the function ‘returns’ something it will look like this:\nreturn_data = function_name(...)\n\n\n\n\n\ndata = [1,25,-4,14,7,9]\ntotal = 0.0\ncount = len(data)\nfor i in data:\n  total += i\n\nprint(total/count)\n\n\n\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data))\n\n\n\n\nThe advantage of the function is that I can keep sending new lists to calc_mean without writing any new code. Whereas the other approach requires me to copy+paste repeatedly.\nWhere this really makes a difference is when you realise you made a mistake in your code: if you’ve made good use of functions this will mean correction only the place where you made the logical mistake. If you’ve not used functions then this is a tedious search and replace that is likely to be error-prone."
  },
  {
    "objectID": "lectures/4.1-Functions.html#in-action",
    "href": "lectures/4.1-Functions.html#in-action",
    "title": "Functions",
    "section": "",
    "text": "def calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\ndata = [1,25,-4,14,7,9]\nprint(calc_mean(data)) # 8.666666666666666\ndata2 = [200000,2500000,-4,1400000,70,900000]\nprint(calc_mean(data2)) # 833344.3333333334"
  },
  {
    "objectID": "lectures/4.1-Functions.html#but-notice",
    "href": "lectures/4.1-Functions.html#but-notice",
    "title": "Functions",
    "section": "",
    "text": "data    = [1,25,-4,14,7,9]\ntotal   = 1\nnumbers = []\n\ndef calc_mean(numbers):\n  total = 0.0\n  count = len(numbers)\n  for i in numbers:\n    total += i\n  return total/count\n\nprint(calc_mean(data))\n\n# Why haven't these changed????\nprint(total)\nprint(numbers)\n\nFunctions encapsulate information: the total and numbers used within the function are different from the variables with the same name that we created outside the function. When we get to libraries and packages you’ll understand why, but the key concept here is ‘namespace’ and that these variables might have the same name but they sit in different namespaces."
  },
  {
    "objectID": "lectures/4.1-Functions.html#simple-function",
    "href": "lectures/4.1-Functions.html#simple-function",
    "title": "Functions",
    "section": "",
    "text": "By ‘simple’ I don’t mean easy, I mean it does one thing only:\ndef hello():\n  print(\"Hello world!\")\nWe then run it with:\nhello()\nAnd that produces:\nHello world!"
  },
  {
    "objectID": "lectures/4.1-Functions.html#passing-in-information",
    "href": "lectures/4.1-Functions.html#passing-in-information",
    "title": "Functions",
    "section": "",
    "text": "We can pass information to a function if we tell the function what to expect:\ndef hello(name:str):\n  print(f\"Hello {name}!\")\nNow we can do this:\nhello(\"new programmers\")\nAnd that produces:\nHello new programmers!"
  },
  {
    "objectID": "lectures/4.1-Functions.html#getting-information-out",
    "href": "lectures/4.1-Functions.html#getting-information-out",
    "title": "Functions",
    "section": "",
    "text": "We can also get information out of them!\ndef hello(name:str) -&gt; str:\n  return f\"Hello {name}!\"\nNow we can do this:\noutput = hello(\"new programmers\")\nprint(output.title())\n# Same as: print(hello(\"new programmers\").title())\nAnd this produces:\n'Hello New Programmers!'"
  },
  {
    "objectID": "lectures/4.1-Functions.html#writing-a-function",
    "href": "lectures/4.1-Functions.html#writing-a-function",
    "title": "Functions",
    "section": "",
    "text": "def &lt;function_name&gt;(&lt;var_name&gt;: &lt;var_type&gt;) -&gt; &lt;var_type&gt;:\n  ...\n  return &lt;var&gt;\nThis can also be written:\ndef &lt;function_name&gt;(&lt;var_name&gt;):\n  ...\n  return &lt;var&gt;\nPython is ‘friendly’ in the sense that all of the &lt;var_type&gt; information is optional, but it will help you (and Python) to know what you were expecting to see happen."
  },
  {
    "objectID": "lectures/4.1-Functions.html#complicating-things",
    "href": "lectures/4.1-Functions.html#complicating-things",
    "title": "Functions",
    "section": "",
    "text": "ds2 = {\n  'lat':[51.51,40.71,35.69],\n  'lon':[0.13,74.01,139.68],\n  'tz': [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ndef get_city_info(city:str, field:str, city_lookup:str='name', data:dict=ds2) -&gt; str:\n  return str(data[field][ data[city_lookup].index(city) ])\n\ncity = 'New York'\nprint(f\"The latitude of {city} is {get_city_info(city,'lat')}\")\n# The latitude of New York is 40.71"
  },
  {
    "objectID": "lectures/4.1-Functions.html#additional-resources",
    "href": "lectures/4.1-Functions.html#additional-resources",
    "title": "Functions",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nWhat is a function?\nPython functions\nBuilt-in functions\nDefine your own functions\nTypes of functions\nDefining a function\n\n\n\nFunction arguments\nArgument lists\nKeyword arguments\nReturn values\nDecorators\nVariable Scopes\nRobust Python with Type Hints"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html",
    "href": "lectures/3.3-DOLs_to_Data.html",
    "title": "Data Structures",
    "section": "",
    "text": "cities = {\n  'London': [[51.5072, 0.1275], +0], \n  'New York': [[40.7127, 74.0059], -5], \n  'Tokyo': [[35.6833, 139.6833], +8]\n}\nSo:\nprint(cities['London'][0]) # Prints [51.5072, 0.1275]\n\n\nConsider how these two data structures differ:\ncities = [\n  {'name': 'London', 'loc': [51.5072, 0.1275], 'tz': +0}, \n  {'name': 'New York', 'loc': [40.7127, 74.0059], 'tz': -5}, \n  {'name': 'Tokyo', 'loc': [35.6833, 139.6833], 'tz': +8}\n]\nOr:\ncities = {\n  'London': {'loc': [51.5072, 0.1275], 'tz': +0}, \n  'New York': {'loc': [40.7127, 74.0059], 'tz': -5}, \n  'Tokyo': {'loc': [35.6833, 139.6833], 'tz': +8}\n}\n\nWhy don’t you copy the code and then see how to access different fields/values? What might be the pros and cons of each?\n\n\n\n\n\nSo we can mix and match dictionaries and lists in whatever way we need to store… ‘data’. The question is then: what’s the right way to store our data?\n\n\nAnswer: the way that makes the most sense to a human while also being the most robust for coding."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare",
    "title": "Data Structures",
    "section": "",
    "text": "Consider how these two data structures differ:\ncities = [\n  {'name': 'London', 'loc': [51.5072, 0.1275], 'tz': +0}, \n  {'name': 'New York', 'loc': [40.7127, 74.0059], 'tz': -5}, \n  {'name': 'Tokyo', 'loc': [35.6833, 139.6833], 'tz': +8}\n]\nOr:\ncities = {\n  'London': {'loc': [51.5072, 0.1275], 'tz': +0}, \n  'New York': {'loc': [40.7127, 74.0059], 'tz': -5}, \n  'Tokyo': {'loc': [35.6833, 139.6833], 'tz': +8}\n}\n\nWhy don’t you copy the code and then see how to access different fields/values? What might be the pros and cons of each?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#implications",
    "href": "lectures/3.3-DOLs_to_Data.html#implications",
    "title": "Data Structures",
    "section": "",
    "text": "So we can mix and match dictionaries and lists in whatever way we need to store… ‘data’. The question is then: what’s the right way to store our data?\n\n\nAnswer: the way that makes the most sense to a human while also being the most robust for coding."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "href": "lectures/3.3-DOLs_to_Data.html#but-compare-1",
    "title": "Data Structures",
    "section": "But Compare…",
    "text": "But Compare…\nHow do these data structures differ?\n\n\n\nOption 1\nds1 = [\n  ['lat','lon','name','tz'],\n  [51.51,0.13,'London',+0],\n  [40.71,74.01,'New York',-5],\n  [35.69,139.68,'Tokyo',+8]\n]\n\n\n\nOption 2\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\n\n\n\nTo understand why I’m asking this question, here are two example questions I’d like you to try to answer:\n\nWhat’s the average latitude of these three cities?\nWhat’s the time zone of Tokyo?"
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "href": "lectures/3.3-DOLs_to_Data.html#thinking-it-through",
    "title": "Data Structures",
    "section": "Thinking it Through",
    "text": "Thinking it Through\nWhy does this work for both computers and people?\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nWe are doing away with the idea that the order of columns matters (humans don’t care that a city’s name is in the first column, and a city’s latitude in the second). We just want to find the column. But because we have a dictionary-of-lists we can ensure that the row order is preserved. Let’s see this in action."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#examples",
    "href": "lectures/3.3-DOLs_to_Data.html#examples",
    "title": "Data Structures",
    "section": "Examples",
    "text": "Examples\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\nprint(ds2['name'][0]) # London\nprint(ds2['lat'][0])  # 51.51\nprint(ds2['tz'][0])   # 0\nSo 0 always returns information about London, and 2 always returns information about Tokyo. But it’s also easy to ask for the latitude (ds2['lat'][0]) or time zone (ds2['tz'][0]) value once you know that 0 is London!\n\nBut there’s another advantage that’s not quite so obvious: for the computer because everything of type ‘lat’ is a float, everything of type ‘tz’ is an integer, and everything of type ‘name’ is a string, it’s a lot easier to work with each column as data."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "href": "lectures/3.3-DOLs_to_Data.html#how-is-that-easier",
    "title": "Data Structures",
    "section": "How is that easier???",
    "text": "How is that easier???\nRemember that we can use any immutable ‘thing’ as a key. This means…\nds2 = {\n  'lat': [51.51,40.71,35.69],\n  'lon': [0.13,74.01,139.68],\n  'tz':  [+0,-5,+8],\n  'name':['London','New York','Tokyo']\n}\n\ncity_nm = 'Tokyo'\ncity_idx = ds2['name'].index(city_nm)\n\nprint(f\"The time zone of {city_nm} is {ds2['tz'][city_idx]}\")\nWe can re-write this into a single line as:\ncity_nm = 'New York'\nprint(f\"The time zone of {city_nm} is {ds2['tz'][ ds2['name'].index(city_nm)]}\")\n\nThis achieves several useful things:\n\nIt is fast: faster than iterating over a list-of-lists or dictionary-of-dictionaries. In other words, there is no iteration at all!\nAll data in a list is of the same type so we can easily add checks to make sure that it’s valid.\nWe can also easily calculate an average/max/min/median and so on (as we’ll see later) without even having to look at any other columns!\nWe can add more columns instantly and the process of finding something is just as fast as it is now. And adding more rows doesn’t make it much slower either!\n\nAlso, notice how in these two examples we don’t try to write the second example in one go: first, we work it out as a set of steps: how do we figure out what ‘row’ (position in the list) Tokyo is in? Now that we’ve got that, how do we retrieve the time zone value for Tokyo? We know that code works, now let’s do variable substitution, as we would if we were doing maths: we can replace the city_idx in the time zone lookup with ds2['name'].index('Tokyo')."
  },
  {
    "objectID": "lectures/3.3-DOLs_to_Data.html#additional-resources",
    "href": "lectures/3.3-DOLs_to_Data.html#additional-resources",
    "title": "Data Structures",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n8 Data Structures Every Data Scientist Should Know (by a CASA alum)"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html",
    "href": "lectures/3.1-Dictionaries.html",
    "title": "Dictionaries",
    "section": "",
    "text": "What advantages might this have over a list?\n\n\n\nThe key can be almost anything that is immutable (aka. hashable). So these are all ‘legit’:\nlookup[1]             # Int\nlookup(52.1)          # Float\nlookup['1']           # String\nlookup['Jon Reades']  # String\nk = 'Jon Reades'\nlookup[k]             # String variable\nlookup[(52.1, -0.04)] # Tuple\nBut this is not:\nlookup[['Jon','Reades']] # Error, unhashable type\nThat’s because a list is not immutable.\n\nAgain, just like a real dictionary: you don’t have multiple entries for ‘dog’, otherwise the dictionary wouldn’t work. You might have multiple definitions: which is to say, the key might return multiple values.\n\n\n\n\n\nNotice the differences when creating them, and the absence of difference when accessing them.\n\n\n\n\ncities = [\n  'San Francisco',\n  'London',\n  'Paris',\n  'Beijing']\n  \n# Prints London\nprint(cities[2]) \n\n\n\n\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\n# Prints pop of London\nprint(cities['London'])\n\n\n\n\nSo why might we prefer the dictionary?\n\nDicts are created using: d = { key: value, key: value }\nDicts are accessed using: d[key]\n\nSo the only difference between lists and dicts is: [...] and {...} when they are created.\nOver the next couple of weeks we’ll see ways that you can store more information in a list and also why lists are sometimes more powerful than you realise… if you can think about your data in an entirely new way. But for simple key/value stuff it’s hard to beat a dictionary!\n\n\n\n\nSpecifically: do we need sequential or random access?\n\n\n\n\n\n\n\nindex\nvalue\n\n\n\n\n0\nSan Francisco\n\n\n1\nLondon\n\n\n2\nParis\n\n\n3\nBeijing\n\n\n\n\n\n\n\n\n\n\nkey\nvalue\n\n\n\n\nSan Francisco\n837442\n\n\nLondon\n8673713\n\n\nParis\n2229621\n\n\nBeijing\n21700000\n\n\n\n\n\n\n\n\n\nThere are two ways to retrieve values from a dictionary:\n\ncities['Beijing']\ncities.get('Beijing')\n\nWhy have two? Consider:\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nprint(cities['Sao Paulo'])     # Throws KeyError\nprint(cities.get('Sao Paulo')) # Returns None\nprint(cities.get('Sao Paulo','No Data')) # Returns 'No Data'\n\nThe first triggers an error, the second returns None. Errors can be unfriendly: do you want your entire Python program to fail because a single city is missing, or would you rather than it did something a little more sensible such as… skipping the row or returning a sensible default?\n\n\n\n\nIf we want to think about whether a value is in the dictionary (as opposed to just retrieving it) then notice these options:\nc = cities.get('Sao Paulo')\nif not c:\n  print(\"Sorry, no city by that name.\")\n\nif 'Beijing' in cities:\n  print(\"Found Beijing!\")\n\nThe first example works because cities.get returns None, which is the same as ‘undefined’ for Python. So we can use ‘not’ to imply ‘if c is not defined then do something…’\nThe second example works because we are implicitly treating the keys in the cities dictionary as a list and looking to see if Beijing is one of the values in that list.\nPython often benefits and suffers from TMTOWTDI (There’s More Than One Way To Do It): think of these as being different ways to say the same thing, but depending on where you want to put the emphasis you would choose one or the other.\n\n\n\n\nIt’s the same process to update an existing value or create a new one:\ncities = {}  # Empty dictionary\ncities['Beijing'] = 21716620    # Sets key-&gt;value\ncities['Toronto'] = 2930000     # Sets key-&gt;value\n\nprint(cities['Toronto'])        # Prints 2930000\ndel cities['Toronto']           # Deletes Toronto key (and value)\ncities.pop('Toronto','Default') # Prints 'Default' b/c key not found\nprint(cities)\nThis last command outputs:\n{'Beijing': '0.17'16620}\n\n\n\nSimilar to iterating over lists but…\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nfor c in cities:\n  print(c)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nOne really important point to note: here, the cities are printed out in the same order that they were added to the dictionary, but that is not guaranteed! Unlike lists, dictionaries are unordered.\nAlso, how would we print out the population of each city?\n\n\n\n\n\n\n\n\n\nfor k in cities.keys():\n  print(k)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\n\n\n\nfor v in cities.values():\n  print(v)\nPrints:\n837442\n8673713\n2229621\n21716620\n\n\n\n\nfor k,v in cities.items():\n  print(f\"{k} -&gt; {v}\")\nPrints:\nSan Francisco -&gt; 837442\nLondon -&gt; 8673713\nParis -&gt; 837442\nBeijing -&gt; 21700000\n\n\n\n\n\n\nValues can be almost anything, including a dictionary or list! This opens up some interesting possibilities:\n\n\ncities = {\n  'San Francisco': \n    [37.77, -122.43, 'SFO']\n}\n\ncities = {\n  'San Francisco': {\n    'lat': 37.77,\n    'lon': -122.43,\n    'airport':'SFO'}\n}\nprint(cities['San Francisco']['lat'])\n\n\nSpoiler: you’re going to encounter this kind of thing a lot.\n\nWhat is this starting to look like? This is basically what JSON is."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#so-key---value",
    "href": "lectures/3.1-Dictionaries.html#so-key---value",
    "title": "Dictionaries",
    "section": "",
    "text": "The key can be almost anything that is immutable (aka. hashable). So these are all ‘legit’:\nlookup[1]             # Int\nlookup(52.1)          # Float\nlookup['1']           # String\nlookup['Jon Reades']  # String\nk = 'Jon Reades'\nlookup[k]             # String variable\nlookup[(52.1, -0.04)] # Tuple\nBut this is not:\nlookup[['Jon','Reades']] # Error, unhashable type\nThat’s because a list is not immutable.\n\nAgain, just like a real dictionary: you don’t have multiple entries for ‘dog’, otherwise the dictionary wouldn’t work. You might have multiple definitions: which is to say, the key might return multiple values."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "href": "lectures/3.1-Dictionaries.html#deliberately-similar",
    "title": "Dictionaries",
    "section": "",
    "text": "Notice the differences when creating them, and the absence of difference when accessing them.\n\n\n\n\ncities = [\n  'San Francisco',\n  'London',\n  'Paris',\n  'Beijing']\n  \n# Prints London\nprint(cities[2]) \n\n\n\n\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\n# Prints pop of London\nprint(cities['London'])\n\n\n\n\nSo why might we prefer the dictionary?\n\nDicts are created using: d = { key: value, key: value }\nDicts are accessed using: d[key]\n\nSo the only difference between lists and dicts is: [...] and {...} when they are created.\nOver the next couple of weeks we’ll see ways that you can store more information in a list and also why lists are sometimes more powerful than you realise… if you can think about your data in an entirely new way. But for simple key/value stuff it’s hard to beat a dictionary!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "href": "lectures/3.1-Dictionaries.html#its-all-about-access",
    "title": "Dictionaries",
    "section": "",
    "text": "Specifically: do we need sequential or random access?\n\n\n\n\n\n\n\nindex\nvalue\n\n\n\n\n0\nSan Francisco\n\n\n1\nLondon\n\n\n2\nParis\n\n\n3\nBeijing\n\n\n\n\n\n\n\n\n\n\nkey\nvalue\n\n\n\n\nSan Francisco\n837442\n\n\nLondon\n8673713\n\n\nParis\n2229621\n\n\nBeijing\n21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values",
    "href": "lectures/3.1-Dictionaries.html#getting-values",
    "title": "Dictionaries",
    "section": "",
    "text": "There are two ways to retrieve values from a dictionary:\n\ncities['Beijing']\ncities.get('Beijing')\n\nWhy have two? Consider:\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nprint(cities['Sao Paulo'])     # Throws KeyError\nprint(cities.get('Sao Paulo')) # Returns None\nprint(cities.get('Sao Paulo','No Data')) # Returns 'No Data'\n\nThe first triggers an error, the second returns None. Errors can be unfriendly: do you want your entire Python program to fail because a single city is missing, or would you rather than it did something a little more sensible such as… skipping the row or returning a sensible default?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "href": "lectures/3.1-Dictionaries.html#getting-values-contd",
    "title": "Dictionaries",
    "section": "",
    "text": "If we want to think about whether a value is in the dictionary (as opposed to just retrieving it) then notice these options:\nc = cities.get('Sao Paulo')\nif not c:\n  print(\"Sorry, no city by that name.\")\n\nif 'Beijing' in cities:\n  print(\"Found Beijing!\")\n\nThe first example works because cities.get returns None, which is the same as ‘undefined’ for Python. So we can use ‘not’ to imply ‘if c is not defined then do something…’\nThe second example works because we are implicitly treating the keys in the cities dictionary as a list and looking to see if Beijing is one of the values in that list.\nPython often benefits and suffers from TMTOWTDI (There’s More Than One Way To Do It): think of these as being different ways to say the same thing, but depending on where you want to put the emphasis you would choose one or the other."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#setting-values",
    "href": "lectures/3.1-Dictionaries.html#setting-values",
    "title": "Dictionaries",
    "section": "",
    "text": "It’s the same process to update an existing value or create a new one:\ncities = {}  # Empty dictionary\ncities['Beijing'] = 21716620    # Sets key-&gt;value\ncities['Toronto'] = 2930000     # Sets key-&gt;value\n\nprint(cities['Toronto'])        # Prints 2930000\ndel cities['Toronto']           # Deletes Toronto key (and value)\ncities.pop('Toronto','Default') # Prints 'Default' b/c key not found\nprint(cities)\nThis last command outputs:\n{'Beijing': '0.17'16620}"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#iterating",
    "href": "lectures/3.1-Dictionaries.html#iterating",
    "title": "Dictionaries",
    "section": "",
    "text": "Similar to iterating over lists but…\ncities = {\n  'San Francisco': 837442,\n  'London': 8673713,\n  'Paris': 837442,\n  'Beijing': '0.17'00000}\n\nfor c in cities:\n  print(c)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\nOne really important point to note: here, the cities are printed out in the same order that they were added to the dictionary, but that is not guaranteed! Unlike lists, dictionaries are unordered.\nAlso, how would we print out the population of each city?"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#section",
    "href": "lectures/3.1-Dictionaries.html#section",
    "title": "Dictionaries",
    "section": "",
    "text": "for k in cities.keys():\n  print(k)\nPrints:\n'San Francisco'\n'London'\n'Paris'\n'Beijing'\n\n\n\n\nfor v in cities.values():\n  print(v)\nPrints:\n837442\n8673713\n2229621\n21716620\n\n\n\n\nfor k,v in cities.items():\n  print(f\"{k} -&gt; {v}\")\nPrints:\nSan Francisco -&gt; 837442\nLondon -&gt; 8673713\nParis -&gt; 837442\nBeijing -&gt; 21700000"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#a-final-note",
    "href": "lectures/3.1-Dictionaries.html#a-final-note",
    "title": "Dictionaries",
    "section": "",
    "text": "Values can be almost anything, including a dictionary or list! This opens up some interesting possibilities:\n\n\ncities = {\n  'San Francisco': \n    [37.77, -122.43, 'SFO']\n}\n\ncities = {\n  'San Francisco': {\n    'lat': 37.77,\n    'lon': -122.43,\n    'airport':'SFO'}\n}\nprint(cities['San Francisco']['lat'])\n\n\nSpoiler: you’re going to encounter this kind of thing a lot.\n\nWhat is this starting to look like? This is basically what JSON is."
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "href": "lectures/3.1-Dictionaries.html#danger-will-robinson",
    "title": "Dictionaries",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\n\nRemember: in most programming languages dictionaries/hashes are unordered and there is no guarantee that things come out in the same order they went in! They complement ordered lists, they don’t replace them!"
  },
  {
    "objectID": "lectures/3.1-Dictionaries.html#additional-resources",
    "href": "lectures/3.1-Dictionaries.html#additional-resources",
    "title": "Dictionaries",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDictionaries and sets\nComprehensions\nThe Complete Guide to Dictionaries (by a CASA alum)"
  },
  {
    "objectID": "lectures/2.7-On_Writing.html",
    "href": "lectures/2.7-On_Writing.html",
    "title": "On Writing & Coding",
    "section": "",
    "text": "If writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial. ~ Graham (2022)\n\n \n\nWriting is thinking. To write well is to think clearly. That’s why it’s so hard. ~ McCullough (2002)\n\n\nA story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#writing-coding-thinking",
    "href": "lectures/2.7-On_Writing.html#writing-coding-thinking",
    "title": "On Writing & Coding",
    "section": "",
    "text": "If writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial. ~ Graham (2022)\n\n \n\nWriting is thinking. To write well is to think clearly. That’s why it’s so hard. ~ McCullough (2002)\n\n\nA story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#large-language-models-llms",
    "href": "lectures/2.7-On_Writing.html#large-language-models-llms",
    "title": "On Writing & Coding",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\n\nChatGPT from OpenAI (an increasingly ‘ironic’ name) is simply the most famous of a growing number of Large Language Models that draw on information found on the web and in open texts to perform sophisticated summarisation tasks."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-for-coding",
    "href": "lectures/2.7-On_Writing.html#why-use-it-for-coding",
    "title": "On Writing & Coding",
    "section": "Why Use it for Coding?",
    "text": "Why Use it for Coding?\nMany programmers use LLMs in coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\n. . .\nThese accelerate code production, but there are significant doubts about code quality."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-code",
    "href": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-code",
    "title": "On Writing & Coding",
    "section": "Why Use it to Learn to Code?",
    "text": "Why Use it to Learn to Code?\nMany students use LLMs to write their code for three reasons:\n\nThey are available at all hours as a personal tutor and advisor.\nThey write better code, more quickly than a learner.\nThey can employ advanced techniques right away.\n\n. . .\n\n\n\n\n\n\nPro-Tip\n\n\n\nOnly one of these is a good reason."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-for-writing",
    "href": "lectures/2.7-On_Writing.html#why-use-it-for-writing",
    "title": "On Writing & Coding",
    "section": "Why Use It for Writing?",
    "text": "Why Use It for Writing?\nMany writers use LLMs for three reasons:\n\nThey can help to ensure consistency and suggest changes in tone or word choice (i.e. feedback).\nThey can provide information about different strategies the writer can use to tackle a problem (i.e. guidance or training).\nThey can help to ‘translate’ text between languages (i.e. re-use).\n\n. . .\nThese accelerate word production, but there are significant doubts about quality."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-write-academic-english",
    "href": "lectures/2.7-On_Writing.html#why-use-it-to-learn-to-write-academic-english",
    "title": "On Writing & Coding",
    "section": "Why Use it to Learn to Write (Academic English)?",
    "text": "Why Use it to Learn to Write (Academic English)?\nMany students use LLMs to write their documents for three reasons:\n\nThey are available at all hours as a personal tutor and advisor.\nThey write better sentences, more quickly than a learner.\nThey draw on a wider range of sources and styles than a learner.\n\n. . .\n\n\n\n\n\n\nPro-Tip\n\n\n\nOnly one of these is a good reason."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#danger-will-robinson",
    "href": "lectures/2.7-On_Writing.html#danger-will-robinson",
    "title": "On Writing & Coding",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nHere’s what we’ve noticed about LLM use so far:\n\n\nOver-use of flowery language/complex code.\nOver-confidence in recommending solutions.\nLack of overall structure/coherence.\nPoor referencing, including the use of non-existent or irrelevant references/coding libraries.\n\n\n. . .\n\n\n\n\n\n\nThe Underlying Issue\n\n\n\n\nLack of actual learning by the student.\nUndermining of learning by other students.\nPointless work by staff.\n\n\n\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly.\n\nWe think that ‘intricate methodologies’ and ‘exhaustive reviews’ aren’t just overblown, they actually invite you to be marked down because youre methodology or review are not intricate or exhaustive.\nWe find individual paragraphs that seem reasonable but the whole doesn’t ‘work’ as a single output.\nOne of our PhD students has created a tool to help us find the papers in your bibliography that don’t actually exist, or that seem superficially useful but are not relevant in practice.\nFundamentally, if you’re substituting the LLM for thinking then you’re not going to learn and that means that you are not going to be able to draw the wider connections that mark out strong submissions on assessments and the kind of person that we, or any company, might want to hire!\n\nBasically, they are overconfident and, as a result, tend to either: a) lead the student to think it’s not worth the effort of learning (leading the student into deep trouble); or b) lead the student to overconfidence in their self-assessment of submissions or skills (leading the student into deep trouble)."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#read-the-fine-print",
    "href": "lectures/2.7-On_Writing.html#read-the-fine-print",
    "title": "On Writing & Coding",
    "section": "Read the Fine Print",
    "text": "Read the Fine Print"
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#we-recommend",
    "href": "lectures/2.7-On_Writing.html#we-recommend",
    "title": "On Writing & Coding",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder or writer by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose and rationale.\n\n\n\n\n\n\nLLMs as co-authors\n\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write."
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#additional-resources",
    "href": "lectures/2.7-On_Writing.html#additional-resources",
    "title": "On Writing & Coding",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nThe Myth of the ‘Genius Programmer’ (by Google Devs)\n“I mostly had one big, ugly, long, unreadable script”\nProgram-Aided Language Models\nChain of Thought Prompting\nChatGPT is a blurry JPEG of the Internet 1\nWhy Meta’s latest large language model survived only three days online 2\n\n\n\nA nice tutorial on distinguishing between plagiarism and paraphrasing.\nGreene, A. E. (2013). Writing science in plain English. University of Chicago Press.3\nFinding your scientific story by writing backwards\nSword, H. (2017). Air & light & time & space: How successful academics write. Harvard University Press.4"
  },
  {
    "objectID": "lectures/2.7-On_Writing.html#footnotes",
    "href": "lectures/2.7-On_Writing.html#footnotes",
    "title": "On Writing & Coding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProbably the best ‘lay person’s’ explanation of how LLMs work/fall apart you’ll ever read.↩︎\nAnd this one was trained on scientific articles!↩︎\nNot seemingly available for free, but I found a nice little summary (with typos) here.↩︎\nAvailable for free via JStor.↩︎"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html",
    "href": "lectures/2.5-The_Command_Line.html",
    "title": "The CLI",
    "section": "",
    "text": "The first question most new programmers want to ask about the Command Line is: why?\n\n\n\n\nWhy are you torturing me with this arcane knowledge?\nWhy do I need to do this when we have slick IDEs now?\n\n\n\n\nNo matter how long you try to avoid it, eventually you’ll find things that can only be solved (or that can be much more quickly solved) using the Command Line Interface (CLI).\nThings like:\n\nInteracting with git is actually easier on the Command Line.\nMaking the most of developer-oriented tools (e.g. docker, GDAL, proj4/6).\nPeeking and poking at (large) files efficiently…\nAutomating things that would be hard/annoying to do manually…\n\nA lot of this ties back to data and servers.\n\nTrue story: 25 years ago I used to process more than 40GB of compressed plain-text data every day from my Titanium PowerBook. But that’s because it was all running on a server in New Jersey while I was in Manhattan. Everything was done using the Command Line and SSH (secure shell).\nMore recently, processing OSM data for the entire UK was possible on my MacBook Air using GDAL and bash scripts but not possible using R/RStudio directly. Basically, the work took so long (&gt; 13 hours) that RStudio thought the script had died and tried to kill it.\n\n\n\n\ncurl -L http://bit.ly/2vrUFKi | \n  head -3 |  \n  awk -F\",\" '{ print $2, $4, $6; }' &gt; results.txt\nThis command does four things in one ‘line’ on the CLI:\n\ncurl downloads the file and passes the contents to…\nhead which takes the first three rows and passes those to…\nawk which splits the rows on \",\" and takes the 2nd, 4th, and 6th fields and directs them into…\nA file called results.txt\n\n\nNote that results.txt is created if it doesn’t already exist, or overwritten if it does.\nIf you wanted to append to an existing file you would use &gt;&gt; instead of &gt;."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#as-in",
    "href": "lectures/2.5-The_Command_Line.html#as-in",
    "title": "The CLI",
    "section": "",
    "text": "Why are you torturing me with this arcane knowledge?\nWhy do I need to do this when we have slick IDEs now?"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#the-answer",
    "href": "lectures/2.5-The_Command_Line.html#the-answer",
    "title": "The CLI",
    "section": "",
    "text": "No matter how long you try to avoid it, eventually you’ll find things that can only be solved (or that can be much more quickly solved) using the Command Line Interface (CLI).\nThings like:\n\nInteracting with git is actually easier on the Command Line.\nMaking the most of developer-oriented tools (e.g. docker, GDAL, proj4/6).\nPeeking and poking at (large) files efficiently…\nAutomating things that would be hard/annoying to do manually…\n\nA lot of this ties back to data and servers.\n\nTrue story: 25 years ago I used to process more than 40GB of compressed plain-text data every day from my Titanium PowerBook. But that’s because it was all running on a server in New Jersey while I was in Manhattan. Everything was done using the Command Line and SSH (secure shell).\nMore recently, processing OSM data for the entire UK was possible on my MacBook Air using GDAL and bash scripts but not possible using R/RStudio directly. Basically, the work took so long (&gt; 13 hours) that RStudio thought the script had died and tried to kill it."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#anatomy-of-a-command",
    "href": "lectures/2.5-The_Command_Line.html#anatomy-of-a-command",
    "title": "The CLI",
    "section": "",
    "text": "curl -L http://bit.ly/2vrUFKi | \n  head -3 |  \n  awk -F\",\" '{ print $2, $4, $6; }' &gt; results.txt\nThis command does four things in one ‘line’ on the CLI:\n\ncurl downloads the file and passes the contents to…\nhead which takes the first three rows and passes those to…\nawk which splits the rows on \",\" and takes the 2nd, 4th, and 6th fields and directs them into…\nA file called results.txt\n\n\nNote that results.txt is created if it doesn’t already exist, or overwritten if it does.\nIf you wanted to append to an existing file you would use &gt;&gt; instead of &gt;."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#interacting-with-files",
    "href": "lectures/2.5-The_Command_Line.html#interacting-with-files",
    "title": "The CLI",
    "section": "Interacting with Files",
    "text": "Interacting with Files\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nls\nList\nls .\n\n\ncd\nChange Directory\ncd $HOME or cd ~\n\n\npwd\nPrint Working Directory\npwd\n\n\nmv\nRename/Move file a to b\nmv a.txt b.txt\n\n\nfind\nFind files matching some criteria\nfind . -name \"*.md\"\n\n\n\n\nNotice that most commands on the Command Line involve typing mnemonics (the shortest possible combination of letters that is unique memorable)."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#common-shortcuts",
    "href": "lectures/2.5-The_Command_Line.html#common-shortcuts",
    "title": "The CLI",
    "section": "Common Shortcuts",
    "text": "Common Shortcuts\n\n\n\n\n\n\n\n\nShortcut\nMeans\nExample\n\n\n\n\n.\nThe current working directory\nls .\n\n\n..\nThe directory above the current working one\ncd ..\n\n\n~1\nThe current user’s home directory.\ncd ~\n\n\n/\nThe ‘root’ directory for the entire computer\nls /\n\n\n\"*\"\nA ‘wildcard’ meaning any number of characters in a filename\nfind . -name \"*.md\"\n\n\n\"?\"\nA ‘wildcard’ meaning one character in a filename\nfind . -name \"2.?-*.md\"\n\n\n\n\nThe main reason we care about all this is that all data is stored somewhere and all code executes somewhere. So we want a way to traverse the device efficiently when looking in directories, creating new files, writing different types of data to different places, and so forth. These shortcuts therefore crop up all over the place ‘in the wild’–if you don’t know what they’re telling you then you’ll wonder why your code doesn’t run or you can’t find the data you saved!"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "href": "lectures/2.5-The_Command_Line.html#a-simulated-walk-across-my-laptop",
    "title": "The CLI",
    "section": "A Simulated Walk Across My Laptop",
    "text": "A Simulated Walk Across My Laptop\ncd /\npwd\n&gt; /\nls\n&gt; Applications  Library  System  Users Volumes ...\ncd $HOME\npwd\n&gt; /Users/casa\nls\n&gt; Applications  Desktop  Dropbox  ...\ncd Dropbox\npwd\n&gt; /Users/casa/Dropbox\nls\n&gt; CASA  Lectures  Practicals ...\n\nModern computers (especially if you’ve grown up around iPhones and Android phones/tablets) are really good at hiding this fact, but that’s because people using phones or tablets really don’t want to be thinking about where their data is being stored, they just want to click save. But when you start coding then you need to start caring a lot more about where something is happening."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#finding-things-in-files",
    "href": "lectures/2.5-The_Command_Line.html#finding-things-in-files",
    "title": "The CLI",
    "section": "Finding Things in Files",
    "text": "Finding Things in Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\nless\nPeek at contents of a text file\nless file.txt\n\n\ngrep\nFind lines matching a ‘pattern’ in a file\ngrep 'pattern' file.txt\n\n\nhead\nPeek at first x rows of a text file\nhead -n 10 file.txt\n\n\ntail\nPeek at last x rows of a text file\ntail -n 10 file.txt\n\n\nwc\nCount things (rows, words, etc.)\nwc -l file.txt\n\n\nsed/awk\nComplicated, but powerful, things\nawk -F\",\" '{ print $1, $3; }' file.csv\n\n\n\n\nThe really crucial thing about all of these utilities is that they don’t load the entire file into memory. So you can ‘peek’ into a 15GB text file instantly without waiting four hours for it to load into memory (and then crash your machine). It’s kind of like the anti-Excel."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#time-to-escape",
    "href": "lectures/2.5-The_Command_Line.html#time-to-escape",
    "title": "The CLI",
    "section": "Time to Escape!",
    "text": "Time to Escape!\nSome characters are ‘special’ and need to be escaped. You’ll encounter these both in the shell (a.k.a. command line) and in Python:\n\n\n\n\n\n\n\n\nEscape\nDoes\nExample\n\n\n\n\n\\\nAllows spaces in file names\nless My\\ File\\ with\\ Spaces.txt\n\n\n\\t\nCreates/matches a tab character\n\\tThe start of a paragraph...\n\n\n\\n\nCreates/matches a newline character\nThe end of a row/para...\\n\n\n\n\\r\nCreates/matches a carriage return\nThe end of a row/para...\\r\\n\n\n\n\\$\nLiteral dollar sign (since $ often marks a variable)\nIt costs \\$1,000,000\n\n\n\\!\nLiteral exclamation mark (since ! can mean a number of things)\nDon't forget me\\!\n\n\n\nThis also becomes relevant when you’re dealing with quotes:\n\"\"This is a problem,\" she said.\"\nvs. \n\"\\\"This is a problem,\\\" she said.\"\n\nThe carriage return is only ever encountered on files that have been opened on Windows machines."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#compressingdecompressing-files",
    "href": "lectures/2.5-The_Command_Line.html#compressingdecompressing-files",
    "title": "The CLI",
    "section": "Compressing/Decompressing Files",
    "text": "Compressing/Decompressing Files\n\n\n\n\n\n\n\n\nCommand\nDoes\nExample\n\n\n\n\ngzip\nCompress/Decompress files\ngzip file.txt\n\n\ngunzip\nDecompress files\ngunzip file.txt.gz2"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#chaining-commands",
    "href": "lectures/2.5-The_Command_Line.html#chaining-commands",
    "title": "The CLI",
    "section": "Chaining Commands",
    "text": "Chaining Commands\nThe CLI becomes much useful with command chaining:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\"\nThe ‘pipe’ (|) takes output from command and ‘pipes’ (aka. passes) it to another.\n\nThis will give you an ‘answer’ much, much, much faster than trying to open the whole file in, say, Excel, Numbers, or even Python."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#redirecting-output",
    "href": "lectures/2.5-The_Command_Line.html#redirecting-output",
    "title": "The CLI",
    "section": "Redirecting Output",
    "text": "Redirecting Output\nWe can redirect outputs in to new files with &gt;, and inputs out of existing files using &lt;:\ngzip -cd very_lg_file.txt.gz | \n  head -n 500 | \n  grep \"pattern\" &gt; matches.txt\nSo the output from the previous commands goes into matches.txt as plain-text. The reverse &lt; is only used in very special circumstances so you probably won’t encounter it very often."
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#practical-applications",
    "href": "lectures/2.5-The_Command_Line.html#practical-applications",
    "title": "The CLI",
    "section": "Practical Applications",
    "text": "Practical Applications\nMost developers will use one or more of these on a daily basis:"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#a-complex-example",
    "href": "lectures/2.5-The_Command_Line.html#a-complex-example",
    "title": "The CLI",
    "section": "A (Complex) Example",
    "text": "A (Complex) Example\nI do not expect you to understand this, but I do want you to understand why this is important:\ndocker run -v conda:/home/jovyan/work --rm ${DOCKER_NM} start.sh \\\n   conda env export -n ${ENV_NM} | sed '1d;$d' | sed '$d' \\\n   | perl -p -e 's/^([^=]+=)([^=]+)=.+$/$1$2/m' \\\n   | grep -Ev '\\- _|cpp|backports|\\- lib|\\- tk|\\- xorg' &gt; conda/environment_py.yml\n\nThis is how I generated the YAML file used by Anaconda Python installers: it is running a command on a virtual machine, collecting the output, filtering out lines by both row number and textual pattern, and directing this all in the environment_py.yml file. This can be run as part of my ‘build’ of the programming environment. It’s all automated!"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#getting-help",
    "href": "lectures/2.5-The_Command_Line.html#getting-help",
    "title": "The CLI",
    "section": "Getting Help",
    "text": "Getting Help\nThe Software Carpentry people have a whole set of lessons around working with ‘the shell’ (a.k.a. Command Line) that might help you.\n\nThe UNIX Shell.\nMIT’s ‘Missing Semester’ on Vim\n\nIndeed all of MIT’s Missing Semester content could be useful!"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#additional-resources",
    "href": "lectures/2.5-The_Command_Line.html#additional-resources",
    "title": "The CLI",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\nThe Shell/Terminal in general:\n\nAbsolute BEGINNER Guide to the Mac OS Terminal\nLinux Bash Shell for Beginners: Tutorial 1\nBeginner’s Guide to the Bash Terminal\nShell Novice\nHow to use the Command Line\n\n\nSome specific commands:\n\nCat\nGzip/Tar (also a good point about spaces in a file name!)\nGrep\nFind\n\n\n\nAnd lots more here on using the file system and shell commands"
  },
  {
    "objectID": "lectures/2.5-The_Command_Line.html#footnotes",
    "href": "lectures/2.5-The_Command_Line.html#footnotes",
    "title": "The CLI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis may be easier to remember and write as cd $HOME, which does the same thing.↩︎\nThis can also be done using ‘switches’ passed to gzip: gzip -cd (where -d means ‘decompress’).↩︎"
  },
  {
    "objectID": "lectures/2.3-Lists.html",
    "href": "lectures/2.3-Lists.html",
    "title": "Lists",
    "section": "",
    "text": "In the same way that a paper shopping list holds many ‘types’ of shopping in one place, a Python list holds many ‘types’ of data in one place.\nmyList = [1, 3, 5, 7]     # homogenous list\nmyList = [1, \"dog\", 7.01] # heterogenous list \nmyList = []               # empty list\nPython lists are always recognisable by their “square brackets”: [...]"
  },
  {
    "objectID": "lectures/2.3-Lists.html#whats-in-a-list",
    "href": "lectures/2.3-Lists.html#whats-in-a-list",
    "title": "Lists",
    "section": "",
    "text": "In the same way that a paper shopping list holds many ‘types’ of shopping in one place, a Python list holds many ‘types’ of data in one place.\nmyList = [1, 3, 5, 7]     # homogenous list\nmyList = [1, \"dog\", 7.01] # heterogenous list \nmyList = []               # empty list\nPython lists are always recognisable by their “square brackets”: [...]"
  },
  {
    "objectID": "lectures/2.3-Lists.html#whats-in-a-list-part-2",
    "href": "lectures/2.3-Lists.html#whats-in-a-list-part-2",
    "title": "Lists",
    "section": "What’s in a List? (Part 2)",
    "text": "What’s in a List? (Part 2)\nIn fact, when I say lists can hold many types of data, I should have said that they can hold any type of data:\nx = 3\ny = \"Foo\"\nz = [\"A\", \"list\", 42]\n\na = [x, y, z] # Holds x, y, *and* list z\nThe output of print(a) is:\n[3, 'Foo', ['A', 'list', 42]]\n\nWe’re going to come back to this a lot later, but for now notice that a list can hold lists!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#using-list-indexes",
    "href": "lectures/2.3-Lists.html#using-list-indexes",
    "title": "Lists",
    "section": "Using List Indexes",
    "text": "Using List Indexes\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nLists are ‘indexed’ numerically from the zero-th element:\n\n\n\n\n\n\n\n\n\n\ngeographers [\n0\n1\n2\n]\n\n\n\n\n\nMassey 1\nHarvey 2\nRose 3\n\n\n\n\nprint(geographers[1]) # Harvey\nprint(geographers[2]) # Rose\nprint(geographers[3]) # Error: List index out of range\n\nAnd notice this error: Python tells you waht the problem is. The issue is understanding what the message means if you don’t know the vocabulary."
  },
  {
    "objectID": "lectures/2.3-Lists.html#interpolation",
    "href": "lectures/2.3-Lists.html#interpolation",
    "title": "Lists",
    "section": "Interpolation",
    "text": "Interpolation\nWe can also use variables as list indexes:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 0\nprint(geographers[i]) # Massey\nAnything that evaluates (i.e. resolves) to a number can be used as an index:\ni = 1\nprint(geographers[i+1]) # Rose\nprint(geographers[ (i-2+1)*2 ]) # Massey"
  },
  {
    "objectID": "lectures/2.3-Lists.html#countdown",
    "href": "lectures/2.3-Lists.html#countdown",
    "title": "Lists",
    "section": "Countdown!",
    "text": "Countdown!\nWe can ‘count’ backwards from the end of the list using negative numbers:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[-1] ) # Rose\nprint( geographers[-2] ) # Harvey"
  },
  {
    "objectID": "lectures/2.3-Lists.html#does-not-compute",
    "href": "lectures/2.3-Lists.html#does-not-compute",
    "title": "Lists",
    "section": "Does Not Compute!",
    "text": "Does Not Compute!\nErrors can be scary… but informative!\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[4] )\n\nIndexError: list index out of range\n\nAnd then try:\nprint( geographers[1.25] )\n\nTypeError: list indices must be integers or slices, not float\n\nNotice that Python gives us important hints about the source of the problem!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#slicing-dicing-lists",
    "href": "lectures/2.3-Lists.html#slicing-dicing-lists",
    "title": "Lists",
    "section": "Slicing & Dicing Lists",
    "text": "Slicing & Dicing Lists\nYou can access more than one element at a time using a slice:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\nprint( geographers[0:2] ) # ['Massey','Harvey']\nprint( geographers[1:] )  # ['Harvey', 'Rose']\nprint( geographers[-2:] ) # ['Harvey', 'Rose']\nThe syntax for a slice is: list[ &lt;start_idx&gt;, &lt;end_idx&gt; ], but end_idx is not included in the slice. And notice:\nprint( geographers[1:2] ) # ['Harvey']\nprint( geographers[1] )   #   Harvey\n\nIt’s really subtle, but notice that a slice always returns a list, even if it’s just a list containing one thing. So geographers[1]=='Harvey' but geographers[1:2]==['Harvey']. Not the same thing!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself",
    "href": "lectures/2.3-Lists.html#test-yourself",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat do you think this will produce?\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ni = 2\nprint( geographers[ (i-3)**2-4:-1 ] )\nSee if you can work out in your head before typing it!"
  },
  {
    "objectID": "lectures/2.3-Lists.html#wheres-wally",
    "href": "lectures/2.3-Lists.html#wheres-wally",
    "title": "Lists",
    "section": "Where’s Wally?",
    "text": "Where’s Wally?\nlist.index(...) tells you where something can be found in a list:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index(\"Harvey\") # 1\ngeographers.index(\"Massey\") # 0\nCombining ideas that will become very useful later:\nprint(geographers[ geographers.index(\"Massey\") ])\nWhat do you think this prints? Why does it work at all?\n\nThis last example looks a little strange, but what if I had a separate list with first names, or Wikipedia links, or other information about these geographers? Because list.index(x) returns an integer we can use it as an index for accessing another list."
  },
  {
    "objectID": "lectures/2.3-Lists.html#wheres-wally-part-2",
    "href": "lectures/2.3-Lists.html#wheres-wally-part-2",
    "title": "Lists",
    "section": "Where’s Wally (Part 2)",
    "text": "Where’s Wally (Part 2)\nlist.index(...) has one flaw:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.index('Batty')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: 'Batty' is not in list\nIf ‘throwing an error’ is overkill, then here’s another way:\nif 'Batty' in geographers:\n    print(\"Found Mike!\")\nelse:\n    print(\"Not a geographer!\")"
  },
  {
    "objectID": "lectures/2.3-Lists.html#sorting",
    "href": "lectures/2.3-Lists.html#sorting",
    "title": "Lists",
    "section": "Sorting",
    "text": "Sorting\nWe can sort lists in alpha-numerical order:\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers.sort()\nprint(geographers) # ['Harvey', 'Massey', 'Rose']\nAnd we can reverse-sort too:\ngeographers.sort(reverse=True)\nprint(geographers) # ['Rose', 'Massey', 'Harvey']"
  },
  {
    "objectID": "lectures/2.3-Lists.html#lists-are-mutable",
    "href": "lectures/2.3-Lists.html#lists-are-mutable",
    "title": "Lists",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\nMutable == “liable or subject to change or alteration”\nLet’s replace Rose with Jefferson4 in the list.\ngeographers = [\"Massey\", \"Harvey\", \"Rose\"]\ngeographers[2] = \"Jefferson\"\nprint(geographers) # ['Massey','Harvey','Jefferson']"
  },
  {
    "objectID": "lectures/2.3-Lists.html#addingremoving-items",
    "href": "lectures/2.3-Lists.html#addingremoving-items",
    "title": "Lists",
    "section": "Adding/Removing Items",
    "text": "Adding/Removing Items\nWhen we insert() items into, or pop() items out of, a list we normally need to specify the index.\ngeographers = [\"Massey\", \"Harvey\", \"Jefferson\"]\ngeographers.insert(0,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.insert(3,\"von Humboldt\")\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'von Humboldt', 'Jefferson']\nAnd in ‘reverse’:\ngeographers.pop(3) # 'von Humboldt'\nprint(geographers) \n# ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\n\nNotice also that insert modifies the list and returns nothing, while pop modifies the list and returns the value that you ‘popped’."
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself-1",
    "href": "lectures/2.3-Lists.html#test-yourself-1",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nThere are two ways to remove David Harvey from the list of geographers without writing this:\ngeographers = ['von Humboldt', 'Massey', 'Harvey', 'Jefferson']\ngeographers.pop(2) # Do not use this answer!\n\nYou can adapt an example we saw earlier in ‘Finding Things’.\nYou can use Google to see if there are list operations we’ve not covered.\n\n\nHints: remove and del are both options for 2."
  },
  {
    "objectID": "lectures/2.3-Lists.html#concatenating",
    "href": "lectures/2.3-Lists.html#concatenating",
    "title": "Lists",
    "section": "Concatenating",
    "text": "Concatenating\nWe combine lists using addition:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = female_geographers + male_geographers\nprint(all_geographers)    # ['Rose', ..., 'Hägerstrand']\nprint(all_geographers[0]) # Rose"
  },
  {
    "objectID": "lectures/2.3-Lists.html#appending",
    "href": "lectures/2.3-Lists.html#appending",
    "title": "Lists",
    "section": "Appending",
    "text": "Appending\nNote that this is not the same!\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers   = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\n all_geographers.append(male_geographers)\nprint(all_geographers) # [['Rose',...], [..., 'Hägerstrand']]\nprint(all_geographers[0]) # ['Rose', ..., 'Jefferson']\nWhat do you think has happened here?"
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself-2",
    "href": "lectures/2.3-Lists.html#test-yourself-2",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nmale_geographers.append('Batty')\nprint(male_geographers)\nWhat do you think this will produce? And why do you think that append appears to do something different in these two examples?"
  },
  {
    "objectID": "lectures/2.3-Lists.html#how-many-geographers-do-i-know",
    "href": "lectures/2.3-Lists.html#how-many-geographers-do-i-know",
    "title": "Lists",
    "section": "How many geographers do I know?",
    "text": "How many geographers do I know?\nlen(...) gives you the length of ‘countable’ things:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nlen(geographers) # 3\nBut…\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nprint( len(all_geographers) ) # 2"
  },
  {
    "objectID": "lectures/2.3-Lists.html#whos-on-the-list",
    "href": "lectures/2.3-Lists.html#whos-on-the-list",
    "title": "Lists",
    "section": "Who’s on the List?",
    "text": "Who’s on the List?\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers) # True\nprint(\"Batty\" in geographers)  # False\nBut…\ngeographers.index('Batty')\nis a ValueError that causes your Python code to fail.\n\nWhy might you choose one of these over the other?"
  },
  {
    "objectID": "lectures/2.3-Lists.html#test-yourself-3",
    "href": "lectures/2.3-Lists.html#test-yourself-3",
    "title": "Lists",
    "section": "Test Yourself",
    "text": "Test Yourself\nHow would you change this code:\ngeographers = [\"Massey\",\"Harvey\",\"Rose\"]\nprint(\"Massey\" in geographers)\nprint(\"Batty\" in geographers)\nSo that it prints:\nFalse\nTrue\nYou will have seen the answer to this in Code Camp, but you can also Google it†!\n\n\n† I’d suggest looking first at Stack Overflow answers in most cases."
  },
  {
    "objectID": "lectures/2.3-Lists.html#tuples-not-actually-a-list",
    "href": "lectures/2.3-Lists.html#tuples-not-actually-a-list",
    "title": "Lists",
    "section": "Tuples: Not Actually a List",
    "text": "Tuples: Not Actually a List\nBecause they come up a lot in geo-data, it’s worth knowing about tuples, which are basically immutable lists:\nt = (52.124021, -0.0012012)\nprint(type(t)) # &lt;class 'tuple'&gt;\nprint(t)       # (52.124021, -0.0012012)\nprint(t[0])    # 52.124021\nBut this…\nt[0] = 25.1203210\nwill throw an error:\n\nTypeError: ‘tuple’ object does not support item assignment"
  },
  {
    "objectID": "lectures/2.3-Lists.html#additional-resources",
    "href": "lectures/2.3-Lists.html#additional-resources",
    "title": "Lists",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nLists in Python\nTuples in Python\nRange and lists\nSequence types\nThe Complete Guide to Lists (by a CASA alum!)"
  },
  {
    "objectID": "lectures/2.3-Lists.html#footnotes",
    "href": "lectures/2.3-Lists.html#footnotes",
    "title": "Lists",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Doreen_Massey_(geographer)↩︎\nhttps://en.wikipedia.org/wiki/David_Harvey↩︎\nhttps://en.wikipedia.org/wiki/Gillian_Rose_(geographer)↩︎\nthe-women-cartographers-who-mapped-art-and-science-in-the-20th-century↩︎"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html",
    "href": "lectures/2.1-Python_the_Basics-1.html",
    "title": "Python: the Basics",
    "section": "",
    "text": "At heart, all programming is about:\n\nStoring data (or references to data) in variables,\nFeeding those data into operations,\nReporting back.\n\nOver time we just get better and better at each of those steps!\nLet’s start with storing data…\n\n\n\n\n\n\nName\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object.\n\n\n\n\nMessage starts as a string:\nmsg = '42'\ntype(msg) # str\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg) # change to int\nAnd back to a string:\nmsg = str(msg)\ntype(msg) # back to str\nAnd notice:\nprint(str(int('42'))) # string from int from string\n\nThat last line of code is intended to start familiarising you with Python syntax: programmers rarely do one operation per line of code if they can do more than one, so you’ll often see nested parentheses like this and you need to learn how to read this kind of code starting from the inner-most parentheses (the int()) and working outwards from there to str() and finally print().\n\n\n\n\nOne to remember for the session on objects and classes:\nisinstance(msg,object)     # True\nisinstance(answer,object)  # True\nisinstance(pi,object)      # True\nisinstance(c,object)       # True\nisinstance(correct,object) # True\n\nLike str(), int(), and print(), you see here another command — or ‘function’ in Python terminology — called ininstance. An ‘instance’ of something is just a way of asking ‘is it a type of’: so the first line asks if string msg is a kind of object, and the answer to that is: True. But it’s the same for answer (an integer), pi (a floating point number), and so on. They are all a kind of object, and we’ll see why that is a useful answer in a few weeks’ time.\n\n\n\n\nRules for variable names:\n\nThey cannot start with a number: item3 is valid, 3items is not.\nWhite space and symbols are not allowed, but _ is allowed: my_variable is valid, my-variable, my$variable, and my variable are not.\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used; but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves.\n\n\n\n\nDo not try to use any of these as variable names. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#variables-have-types",
    "href": "lectures/2.1-Python_the_Basics-1.html#variables-have-types",
    "title": "Python: the Basics",
    "section": "",
    "text": "Name\nValue\nType\n\n\n\n\nmsg\n‘Hello world’\ntype(msg)==str\n\n\nanswer\n42\ntype(answer)==int\n\n\npi\n3.14159\ntype(pi)==float\n\n\nc\ncomplex(5,2)\ntype(c)==complex\n\n\ncorrect\nTrue\ntype(correct)==bool\n\n\n\n\nAs we’ll see in Week 4, everything is also an object."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#but-we-can-change-that",
    "href": "lectures/2.1-Python_the_Basics-1.html#but-we-can-change-that",
    "title": "Python: the Basics",
    "section": "",
    "text": "Message starts as a string:\nmsg = '42'\ntype(msg) # str\nBut we can change it to an integer like this:\nmsg = int(msg)\ntype(msg) # change to int\nAnd back to a string:\nmsg = str(msg)\ntype(msg) # back to str\nAnd notice:\nprint(str(int('42'))) # string from int from string\n\nThat last line of code is intended to start familiarising you with Python syntax: programmers rarely do one operation per line of code if they can do more than one, so you’ll often see nested parentheses like this and you need to learn how to read this kind of code starting from the inner-most parentheses (the int()) and working outwards from there to str() and finally print()."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#and-they-are-all-objects",
    "href": "lectures/2.1-Python_the_Basics-1.html#and-they-are-all-objects",
    "title": "Python: the Basics",
    "section": "",
    "text": "One to remember for the session on objects and classes:\nisinstance(msg,object)     # True\nisinstance(answer,object)  # True\nisinstance(pi,object)      # True\nisinstance(c,object)       # True\nisinstance(correct,object) # True\n\nLike str(), int(), and print(), you see here another command — or ‘function’ in Python terminology — called ininstance. An ‘instance’ of something is just a way of asking ‘is it a type of’: so the first line asks if string msg is a kind of object, and the answer to that is: True. But it’s the same for answer (an integer), pi (a floating point number), and so on. They are all a kind of object, and we’ll see why that is a useful answer in a few weeks’ time."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#variables-have-names",
    "href": "lectures/2.1-Python_the_Basics-1.html#variables-have-names",
    "title": "Python: the Basics",
    "section": "",
    "text": "Rules for variable names:\n\nThey cannot start with a number: item3 is valid, 3items is not.\nWhite space and symbols are not allowed, but _ is allowed: my_variable is valid, my-variable, my$variable, and my variable are not.\nCase matters: myVar is different from both myvar and MYVAR\nBe consistent: my_var is more ‘Pythonic’, though myVar is also widely used; but don’t mix these!\nVariable names should be long enough to be clear but not so long as to be impractical: bldg_height vs. bh vs. max_building_height_at_eaves."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#but-not-all-words-are-allowed",
    "href": "lectures/2.1-Python_the_Basics-1.html#but-not-all-words-are-allowed",
    "title": "Python: the Basics",
    "section": "",
    "text": "Do not try to use any of these as variable names. Python may not complain, but strange things will happen when you run your code.\n\n\n\nand\ndel\nfrom\nnot\nwhile\n\n\nas\nelif\nglobal\nor\nwith\n\n\nassert\nelse\nif\npass\nyield\n\n\nbreak\nexcept\nimport\nprint\n\n\n\nclass\nexec\nin\nraise\n\n\n\ncontinue\nfinally\nis\nreturn\n\n\n\ndef\nfor\nlambda\ntry"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#simple-operations-on-variables",
    "href": "lectures/2.1-Python_the_Basics-1.html#simple-operations-on-variables",
    "title": "Python: the Basics",
    "section": "Simple Operations on Variables",
    "text": "Simple Operations on Variables\nLet’s start with x=10 and y=5…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n15\n\n\nDifference\nx - y\n5\n\n\nProduct\nx * y\n50\n\n\nQuotient\nx / y\n2.0\n\n\n‘Floored’ Quotient\nx // y\n2\n\n\nRemainder\nx % y\n0\n\n\nPower\npow(x,y) or x**y\n100000\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#strings-are-different",
    "href": "lectures/2.1-Python_the_Basics-1.html#strings-are-different",
    "title": "Python: the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nWhen you do things with strings the answers can look a little different. Let’s start with x=\"Hello\" and y=\"You\" and z=2…\n\n\n\nOperator\nInput\nResult\n\n\n\n\nSum\nx + y\n'HelloYou'\n\n\nDifference\nx - y\nTypeError\n\n\nProduct\nx * z\nHelloHello\n\n\nEquality\nx==y\nFalse"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#using-strings-to-output-information",
    "href": "lectures/2.1-Python_the_Basics-1.html#using-strings-to-output-information",
    "title": "Python: the Basics",
    "section": "Using Strings to Output Information",
    "text": "Using Strings to Output Information\nPython has no fewer than three ways output information:\n\nstring concatenation using +;\nstring formatting using &lt;str&gt;.format(&lt;variables&gt;); and\nf-strings using f\"{variable_1} some text {variable_n}\".\n\nThere are pros and cons to each:\nx = 24\ny = 'Something'\nprint(\"The value of \" + y + \" is \" + str(x))\nprint(\"The value of {0} is {1}\".format(y, x))\nprint(f\"The value of {y} is {x}\")\n\nObviously, a really common requirement that programmers have is ‘output a nicely formatted string containing information about the variables in my program’.\nI rather like f-strings because they can actually contain any code you like (you could, for instance, write `f”The square root of {y} is {x**(1/2)}” and it would work. However, concatenation is the easiest to learn."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#operators-assemble",
    "href": "lectures/2.1-Python_the_Basics-1.html#operators-assemble",
    "title": "Python: the Basics",
    "section": "Operators Assemble!",
    "text": "Operators Assemble!\nAlways pay attention to precedence:\nx, y = 10, 5\nx + y * 2         # == 20\n(x + y) * 2       # == 30\nx + y * 2 / 3     # == 13.3333333333334\nx + y * (2/3)     # == 13.3333333333332\n(x + y) * (2/3)   # == 10.0\nAnd here’s a subtle one:\n(x * y) ** 2/3    # == 833.333333333334\n(x * y) ** (2/3)  # == 13.5720880829745\nThe full list is here.\n\nIf you’re a little rusty on exponents, that last example is the cube root of (x*y)**2. But the key point is that formatting is not what matters here, it’s operator precedence: one has parentheses, the other does not, so they are evaluated differently. To make the first one more clear we might use (x * y)**2 / 3 or even ((x*y)**2)/3.\nAlso notice that with the two floats in the first block you do not always get the same result from operations that should give the same answer. Non-terminating decimals (e.g. 1/3) will always be rounded, unpredictably, by the computer because it doesn’t have infinite memory. The process of rounding means that you need to be very careful comparing floats (more on this later)."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#comparing-variables",
    "href": "lectures/2.1-Python_the_Basics-1.html#comparing-variables",
    "title": "Python: the Basics",
    "section": "Comparing Variables",
    "text": "Comparing Variables\nFor numeric variables comparisons are easy.\n\n\n\nOperator\nInput\nResult\n\n\n\n\n==\n10 == 5\nFalse\n\n\n!=\n10 != 5\nTrue\n\n\n&lt;, &lt;=\n10 &lt; 5\nFalse\n\n\n&gt;, &gt;=\n10 &gt; 5\nTrue\n\n\n\n\nThe result of any (successful) comparison is a Boolean (True/False). We can save the output of this comparison to a new variable (e.g. z = x &gt; y).\nThis last example has to do with the way that strings are compared."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#strings-are-different-1",
    "href": "lectures/2.1-Python_the_Basics-1.html#strings-are-different-1",
    "title": "Python: the Basics",
    "section": "Strings Are Different",
    "text": "Strings Are Different\nBut notice:\nw, x, y, z = '4a','4a','365','42'\nw == x  # True\nw != y  # True\nx &gt; y   # True\nx &gt; z   # True\nWhy is 4a greater than both 365 and 42?"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#danger-will-robinson",
    "href": "lectures/2.1-Python_the_Basics-1.html#danger-will-robinson",
    "title": "Python: the Basics",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nNotice the very subtle visual difference between = and ==!\nx = 5\ny = 10\nx = y   # Probably a mistake: setting x to the value of y\nx == y  # True, because x and z are now both set to 10\nRemember this!\n\nConfusing these two operators is the most common source of mistakes early on when learning to code in Python! One (=) does assignment, the other (==) does comparison."
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#common-mistakes",
    "href": "lectures/2.1-Python_the_Basics-1.html#common-mistakes",
    "title": "Python: the Basics",
    "section": "Common Mistakes",
    "text": "Common Mistakes\nHere’s the output from some attempts at comparison:\nx, y = '42', 42\nx==y   # False\nx&gt;y    # An error!\nThis last line produces:\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: '&gt;' not supported between instances of 'str' and 'int'\nIf we want to compare them then we’ll need to change their type:\nx &gt; str(y)   # False\nint(x) &lt;= y  # True\nx &gt; = str(y) # Also True\n\nA really common mistake is to think that string (str) \"42\" is the same as the integer (int) 42.\nNotice that in the first example we can say that 42 is clearly not the same as ‘42’, but we can’t say whether it’s more or less because that’s non-sensical in this context. So this is a computer being totally logical but not always sensible.\nAlso notice the syntax for this: we have str(&lt;something&gt;) and int(&lt;something&gt;) to convert between types. These are functions, which we’ll spend a lot more time on next week!\nWhy might it be (fractionally) faster to compare integers than strings?"
  },
  {
    "objectID": "lectures/2.1-Python_the_Basics-1.html#additional-resources",
    "href": "lectures/2.1-Python_the_Basics-1.html#additional-resources",
    "title": "Python: the Basics",
    "section": "Additional Resources",
    "text": "Additional Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\nTypes of Data\nVariables and expressions\nStrings\nThe string type\nCommon string methods\nFormatting strings\nSplitting and joining\nNumeric types\nThe bool type\nStoring Data in Variables"
  },
  {
    "objectID": "lectures/12.3-Clustering.html",
    "href": "lectures/12.3-Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Allocates n samples to k groups\nWorks for different values of k\nDifferent algorithms (A) present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in 1–2 dimensions\n\n\n\n\n\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms A present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in &lt; 9 dimensions\n\n\n\n\n\nClustering algorithms can suffer from the ‘curse of dimensionality’ such that high-dimensional spaces cluster poorly without either dimensionality reduction or the use of specialist algorithms such as Spherical k-Means."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#spot-the-difference",
    "href": "lectures/12.3-Clustering.html#spot-the-difference",
    "title": "Clustering",
    "section": "",
    "text": "Allocates n samples to k groups\nWorks for different values of k\nDifferent algorithms (A) present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in 1–2 dimensions\n\n\n\n\n\n\nAllocates n samples to k groups\nWorks for different values of k\nDifferent algorithms A present different views of group relationships\nPoor choices of A and k lead to weak understanding of data\nTypically works best in &lt; 9 dimensions\n\n\n\n\n\nClustering algorithms can suffer from the ‘curse of dimensionality’ such that high-dimensional spaces cluster poorly without either dimensionality reduction or the use of specialist algorithms such as Spherical k-Means."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#the-first-geodemographic-classification",
    "href": "lectures/12.3-Clustering.html#the-first-geodemographic-classification",
    "title": "Clustering",
    "section": "The First Geodemographic Classification?",
    "text": "The First Geodemographic Classification?\n\nSource: booth.lse.ac.uk/map/"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#more-than-100-years-later",
    "href": "lectures/12.3-Clustering.html#more-than-100-years-later",
    "title": "Clustering",
    "section": "More than 100 Years Later",
    "text": "More than 100 Years Later\n\nSource: vis.oobrien.com/booth/"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "href": "lectures/12.3-Clustering.html#intimately-linked-to-rise-of-the-state",
    "title": "Clustering",
    "section": "Intimately Linked to Rise of The State",
    "text": "Intimately Linked to Rise of The State\n\nGeodemographics only possible in context of a State – without a Census it simply wouldn’t work… until now?\nClearly tied to social and economic ‘control’ and intervention: regeneration, poverty & exclusion, crime, etc.\nPresumes that areas are the relevant unit of analysis; in geodemographics these are usually called neighbourhoods… which should ring a few bells.\nIn practice, we are in the realm of ‘homophily’, a.k.a. Tobler’s First Law of Geography"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#where-is-it-used",
    "href": "lectures/12.3-Clustering.html#where-is-it-used",
    "title": "Clustering",
    "section": "Where is it used?",
    "text": "Where is it used?\nAnything involving grouping individuals, households, or areas into larger ‘groups’…\n\nStrategic marketing (above the line, targeted, etc.)\nRetail analysis (store location, demand modelling, etc.)\nPublic sector planning (resource allocation, service development, etc.)\n\nCould see it as a subset of customer segmentation."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#problem-domains",
    "href": "lectures/12.3-Clustering.html#problem-domains",
    "title": "Clustering",
    "section": "Problem Domains",
    "text": "Problem Domains\n\n\n\n\nContinuous\nCategorical\n\n\n\n\nSupervised\nRegression\nClassification\n\n\nUnsupervised\nDimensionality Reduction\nClustering\n\n\n\n\n\nIn classification we ‘know’ the answer (we test against labels).\nIn clustering we don’t ‘know’ the answer (we look for clusters)."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#measuring-fit",
    "href": "lectures/12.3-Clustering.html#measuring-fit",
    "title": "Clustering",
    "section": "Measuring ‘Fit’",
    "text": "Measuring ‘Fit’\n\nUsually working towards an ‘objective criterion’ for quality… these are known as cohesion and separation measures."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#how-your-data-looks",
    "href": "lectures/12.3-Clustering.html#how-your-data-looks",
    "title": "Clustering",
    "section": "How Your Data Looks…",
    "text": "How Your Data Looks…\nClustering is one area where standardisation (and, frequently, normalisation) are essential:\n\nYou don’t (normally) want scale in any one dimension to matter more than scale in another.\nYou don’t want differences between values in one dimension to matter more than differences in another.\nYou don’t want skew in one dimension to matter more than skew in another.\n\nYou also want uncorrelated variables… why?"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#first-steps",
    "href": "lectures/12.3-Clustering.html#first-steps",
    "title": "Clustering",
    "section": "First Steps",
    "text": "First Steps\nYou will normally want a continuous variable… so these types of data are especially problematic:\n\nDummies / One-Hot Encoded\nCategorical / Ordinal\nPossible solutions: k-modes, CCA, etc."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#performance",
    "href": "lectures/12.3-Clustering.html#performance",
    "title": "Clustering",
    "section": "Performance",
    "text": "Performance\nTypically about trade-offs between:\n\n\n\nAccuracy\nGeneralisation"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#trade-offs",
    "href": "lectures/12.3-Clustering.html#trade-offs",
    "title": "Clustering",
    "section": "Trade-Offs",
    "text": "Trade-Offs\nNeed to balance:\n\nAbility to cluster at speed.\nAbility to replicate results.\nAbility to cope with fuzzy/indeterminate boundaries.\nAbility to cope with curse of dimensionality.\nUnderlying representation of group membership…"
  },
  {
    "objectID": "lectures/12.3-Clustering.html#visualising-the-trade-offs",
    "href": "lectures/12.3-Clustering.html#visualising-the-trade-offs",
    "title": "Clustering",
    "section": "Visualising the Trade-Offs",
    "text": "Visualising the Trade-Offs\n\n\n\nDetails on scikit-learn.org.\n\nNotice the limitations to k-means: it may be fast but it’s got problems if your data is non-linear/non-Gaussian.\nAnd this doesn’t even include options like HDBSCAN, HAC/Hierarchical Clustering, and many more!\n\n\n\n\n\nInteractive Demo."
  },
  {
    "objectID": "lectures/12.3-Clustering.html#putting-it-all-into-context",
    "href": "lectures/12.3-Clustering.html#putting-it-all-into-context",
    "title": "Clustering",
    "section": "Putting it All into Context",
    "text": "Putting it All into Context"
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html",
    "href": "lectures/12.1-Diverse_Data.html",
    "title": "Diverse Data",
    "section": "",
    "text": "One of the benefits of grouping is that it enables us to derive measures of density and diversity; here are just a few…"
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#location-quotient",
    "href": "lectures/12.1-Diverse_Data.html#location-quotient",
    "title": "Diverse Data",
    "section": "Location Quotient",
    "text": "Location Quotient\nThe LQ for industry i in zone z is the share of employment for i in z divided by the share of employment of i in the entire region R. \\[\nLQ_{zi} = \\dfrac{Emp_{zi}/Emp_{z}}{Emp_{Ri}/Emp_{R}}\n\\]\n\n\n\n \nHigh Local Share\nLow Local Share\n\n\n\n\nHigh Regional Share\n\\[\\approx 1\\]\n\\[&lt; 1\\]\n\n\nLow Regional Share\n\\[&gt; 1\\]\n\\[\\approx 1\\]\n\n\n\n\nIn other words, this is a type of standardisation that enables to compare the concentration of Investment Bankers with the concentration of Accountants, even if there are many more Accountants than Bankers! But this can also apply to the share of flats to whole-property lettings just as easily.\nNote that this is influenced by small sample sizes (e.g. the number of Fijians in Britain)."
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#herfindahl-hirschman-index",
    "href": "lectures/12.1-Diverse_Data.html#herfindahl-hirschman-index",
    "title": "Diverse Data",
    "section": "Herfindahl-Hirschman index",
    "text": "Herfindahl-Hirschman index\nThe HHI for an industry i is the sum of squared market shares for each company in that industry: \\[\nH = \\sum_{i=1}^{N} s_{i}^{2}\n\\]\n\n\n\n\n\n\n\nConcentration Level\nHHI\n\n\n\n\nMonopolistic: one firm accounts for 100% of the market\n\\[1.0\\]\n\n\nOligopolistic: top five firms account for 60% of the market\n\\[\\approx 0.8\\]\n\n\nCompetitive: anything else?\n\\[&lt; 0.5\\]?\n\n\n\n\nIf \\(s_{i} = 1\\) then \\(s_{i}^{2} = 1\\), while if \\(s_{i} = 0.5\\) then \\(s_{i}^{2} = 0.25\\) and \\(s_{i} = 0.1\\) then \\(s_{i}^{2} = 0.01\\).\nThis can be translated to compare, for instance, local and regional neighbourhood diversity: some cities are ethnically diverse in aggregate but highly segregated at the local level.\nNote that this is influenced by the number of ‘firms’ (or ethnicities or…)."
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#shannon-entropy",
    "href": "lectures/12.1-Diverse_Data.html#shannon-entropy",
    "title": "Diverse Data",
    "section": "Shannon Entropy",
    "text": "Shannon Entropy\nShannon Entropy is an information-theoretic measure: \\[\nH(X) = - \\sum_{i=1}^{n} P(x_{i}) log P(x_{i})\n\\]\n\nI often think of this as ‘surprise’: a high entropy measure means that it’s hard to predict what will happen next. So randomness has high entropy. By extension, high concentration has low entropy (even if the result is surprising on the level of intuition: I wasn’t expecting to see that) because I can predict a 6 on the next roll of the dice fairly easy if all of my previous rolls were 6s."
  },
  {
    "objectID": "lectures/12.1-Diverse_Data.html#additional-resources",
    "href": "lectures/12.1-Diverse_Data.html#additional-resources",
    "title": "Diverse Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nStop aggregating away the signal in your data"
  },
  {
    "objectID": "lectures/11.2-Transformation.html",
    "href": "lectures/11.2-Transformation.html",
    "title": "Transformation",
    "section": "",
    "text": "Data transformation just means changing the raw data in some way to make it more tractable for analysis.\nFor example:\n\nCentering the mean on zero is an obvious example.\nBut we can also do quite complicated things (with caution!) in order to get a distribution that we can work with using statistical tests that have certain expectations about the data we’re feeding them.\n\n\nSo, even though we add or subtract, multiply or divide, square or log the data, because we are doing the same thing to every observation the underlying relationships between the data are ‘unchanged’."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#what-is-it",
    "href": "lectures/11.2-Transformation.html#what-is-it",
    "title": "Transformation",
    "section": "",
    "text": "Data transformation just means changing the raw data in some way to make it more tractable for analysis.\nFor example:\n\nCentering the mean on zero is an obvious example.\nBut we can also do quite complicated things (with caution!) in order to get a distribution that we can work with using statistical tests that have certain expectations about the data we’re feeding them.\n\n\nSo, even though we add or subtract, multiply or divide, square or log the data, because we are doing the same thing to every observation the underlying relationships between the data are ‘unchanged’."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#transformation-in-1d",
    "href": "lectures/11.2-Transformation.html#transformation-in-1d",
    "title": "Transformation",
    "section": "Transformation in 1D",
    "text": "Transformation in 1D\n\n\n\\[\nx-\\bar{x}\n\\]\n\n\n\nInput\nOutput\n\n\n\n\n12\n-2\n\n\n13\n-1\n\n\n14\n0\n\n\n15\n+1\n\n\n16\n+2\n\n\n\n\n\nHow is this any different?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#so",
    "href": "lectures/11.2-Transformation.html#so",
    "title": "Transformation",
    "section": "So…",
    "text": "So…\n\nTransformations are mathematical operations applied to every observation in a data set that preserve some of the relationships between them."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#for-example",
    "href": "lectures/11.2-Transformation.html#for-example",
    "title": "Transformation",
    "section": "For Example",
    "text": "For Example\nIf we subtract the mean from everyone’s height then we can immediately tell if someone is taller or shorter than we would expect.\nIf we subtract the mean from everyone’s income then we cannot immediately tell if someone is earning more or less that we would expect.\nSo what is a useful transformation in one context, may not be in another!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#fleshing-this-out",
    "href": "lectures/11.2-Transformation.html#fleshing-this-out",
    "title": "Transformation",
    "section": "Fleshing This Out",
    "text": "Fleshing This Out\nQuestion: How can you tell if you did better than everyone else on the Quiz or on the Final Report?\n\nAnswer: Just subtracting the mean is not enough because the distributions are not the same. For that we also need to standardise the data in some way.\n\\[\nz = \\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nDivide through by the distribution!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#z-score-standardisation",
    "href": "lectures/11.2-Transformation.html#z-score-standardisation",
    "title": "Transformation",
    "section": "Z-Score Standardisation",
    "text": "Z-Score Standardisation\n\\[\n\\dfrac{x-\\bar{x}}{\\sigma}\n\\]\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nprint(scaler.mean_)\nscaler.transform(x)\n\nThe important thing to note is that if transform data that has not been fit and you get values outside the range used for fitting then you can no longer assume a standard normal."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#interquartile-standardisation",
    "href": "lectures/11.2-Transformation.html#interquartile-standardisation",
    "title": "Transformation",
    "section": "Interquartile Standardisation",
    "text": "Interquartile Standardisation\n\\[\n\\dfrac{x_{i}-x_{Q2}}{x_{Q3}-x_{Q1}}\n\\]\nfrom sklearn.preprocessing import RobustScaler\ntrf = RobustScaler(\n        quantile_range=(25.0,75.0)).fit(x)\ntrf.transform(x)"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#interdecile-standardisation",
    "href": "lectures/11.2-Transformation.html#interdecile-standardisation",
    "title": "Transformation",
    "section": "Interdecile Standardisation",
    "text": "Interdecile Standardisation\n\\[\n\\dfrac{x_{i}-x_{50^{th}}}{x_{90^{th}}-x_{10^{th}}}\n\\]\nprint(\"You've got this...\")\n\nWhy standardise:\n\nWe understand the properties of normal-ish distributions, and can simulate them easily.\nMore ‘power’ in the statistical tools available.\nMany analyses assume that ‘error’ is random and symmetric (homoscedastic, not skewed)."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#group-standardisation",
    "href": "lectures/11.2-Transformation.html#group-standardisation",
    "title": "Transformation",
    "section": "Group Standardisation",
    "text": "Group Standardisation\n\\[\nx'_{a,i} = \\dfrac{x_{ai}}{\\sum_{g} r_{N,g} P_{a,g}}\n\\]\n\nDetails:\n\n\\(x_{a,i}\\) = Value of attribute i in area a.\n\\(P_{a,g}\\) = Population of group g in area a.\n\\(r_{N,g}\\) = National ratio N of group g\n\\(\\sum\\) = Sum for all groups.\n\\(x'_{a,i}\\) = Standardised value of i in area a."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#proportional-normalisation",
    "href": "lectures/11.2-Transformation.html#proportional-normalisation",
    "title": "Transformation",
    "section": "Proportional Normalisation",
    "text": "Proportional Normalisation\n\\[\n\\dfrac{x_{i}}{\\sum{x}_{i=1}^{n}}\n\\]\nimport numpy as np\nx/np.sum(x)\n\nNumpy has a fair few options for implementing this, but note that log means natural log, not log10!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#range-normalisation",
    "href": "lectures/11.2-Transformation.html#range-normalisation",
    "title": "Transformation",
    "section": "Range Normalisation",
    "text": "Range Normalisation\n\\[\n\\dfrac{x_{i}-x_{min}}{x_{max}-x_{min}}\n\\]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(x)\nprint(scaler.data_max_)\nscaler.transform(x)\n\nNormalisation helps in several ways:\n\nScaling is important for comparability\nClustering is particularly sensitive to scale"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#log-transformation",
    "href": "lectures/11.2-Transformation.html#log-transformation",
    "title": "Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\nRecall: logs are the inverse of exponentiation!\n\nSo if \\(10^{3} = 1,000\\) then \\(log_{10}(1,000) = 3\\).\nAnd if \\(10^{0} = 1\\) then \\(log_{10}(1) = 0\\)\n\nimport numpy as np\nxhat = np.log(x)\n\n\nNote that numpy uses the natural log in base \\(e\\).\n\nWhy is this so common? Esp. in social sciences?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#why-log-transform",
    "href": "lectures/11.2-Transformation.html#why-log-transform",
    "title": "Transformation",
    "section": "Why Log Transform?",
    "text": "Why Log Transform?\nLet’s assume that \\(x = \\{10, 100, 1000, 10000\\}\\), consider what happens if:\n\nThe formula for the mean is \\(\\frac{\\sum{x}}{n}\\).\nThe formula for variance is \\(\\frac{(x-\\bar{x})^{2}}{n}\\).\n\nThe Natural Log (\\(e\\)) has certain advantages over other logs and should probably be your default choice for log transformations."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#other-transforms",
    "href": "lectures/11.2-Transformation.html#other-transforms",
    "title": "Transformation",
    "section": "Other Transforms…",
    "text": "Other Transforms…\n\nQuantile (maps the PDF of each feature to a uniform distribution)\nSquare Root (often with count data)\nArcsine/Angular (with percentages, proportions, text)\nRank (with care on extreme distributions)\nBox-Cox and Yeo-Johnson (arbitrary power transformations)\n\n\n\nTo report measures of central tendency it’s usually helpful to convert back to the original units.\nThe more extreme the transformation the less meaningful measures of dispersion.\nCorrelation can be significantly affected in either direction.\nCount data can be tricky because you should not have negative values (especially \\(-\\infty\\))."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#when-transforms-dont-help",
    "href": "lectures/11.2-Transformation.html#when-transforms-dont-help",
    "title": "Transformation",
    "section": "When Transforms Don’t Help",
    "text": "When Transforms Don’t Help\nArbitrarily transforming data isn’t a panacea. ‘Robust’ tests can be another approach when all else fails and two common approaches are:\n\nTrimming: cutting off, say, the top and bottom 5% of scores would start to remove skew and offer a more useful view of the central tendency of the data.\nBootstrapping: taking many sub-samples (usually of \\(n-1\\) data points or similar) we can build a picture of how certain metrics vary."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#one-last-note",
    "href": "lectures/11.2-Transformation.html#one-last-note",
    "title": "Transformation",
    "section": "One Last Note",
    "text": "One Last Note\n\nThe term normalization is used in many contexts, with distinct, but related, meanings. Basically, normalizing means transforming so as to render normal. When data are seen as vectors, normalizing means transforming the vector so that it has unit norm. When data are though of as random variables, normalizing means transforming to normal distribution. When the data are hypothesized to be normal, normalizing means transforming to unit variance.\n\nSource: Stack Exchange"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#whats-wrong-with-this-map",
    "href": "lectures/11.2-Transformation.html#whats-wrong-with-this-map",
    "title": "Transformation",
    "section": "What’s Wrong with this Map?",
    "text": "What’s Wrong with this Map?"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#thats-better",
    "href": "lectures/11.2-Transformation.html#thats-better",
    "title": "Transformation",
    "section": "That’s Better!",
    "text": "That’s Better!"
  },
  {
    "objectID": "lectures/11.2-Transformation.html#whats-a-projection",
    "href": "lectures/11.2-Transformation.html#whats-a-projection",
    "title": "Transformation",
    "section": "What’s a Projection?",
    "text": "What’s a Projection?\n\nSource\n\n\nAre spatial (a.k.a. geometrical) transformations any different from the other mathematical transformations covered in this session?\nData exists in a ‘space’ that we can transform and manipulate in various ways using functions to serve our exploratory and analytical purposes."
  },
  {
    "objectID": "lectures/11.2-Transformation.html#additional-resources",
    "href": "lectures/11.2-Transformation.html#additional-resources",
    "title": "Transformation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nNormalisation vs Standardisation – Quantitative analysis\nTransforming Data with R\nData Transformation and Normality Testing\nIntroduction to Logarithms\nWhat is ‘e’ and where does it come from?\nLogarithms - What is e?\nsklearn API reference\nCompare effects of different scalers on data with outliers\nThe things you’ll find in higher dimensions (useful brief discussion of manifolds)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html",
    "href": "lectures/10.2-Wrap-Up.html",
    "title": "The End is Nigh",
    "section": "",
    "text": "…Engage [with] GIS as an always-interconnected set of technical and social practices… [we approach] critical GIS as an orientation to GIS praxis that ‘does’ GIS from within a questioning stance about how we know. (Elwood and Wilson 2017)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#its-all-connected",
    "href": "lectures/10.2-Wrap-Up.html#its-all-connected",
    "title": "The End is Nigh",
    "section": "",
    "text": "…Engage [with] GIS as an always-interconnected set of technical and social practices… [we approach] critical GIS as an orientation to GIS praxis that ‘does’ GIS from within a questioning stance about how we know. (Elwood and Wilson 2017)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#beta-the-business-problem",
    "href": "lectures/10.2-Wrap-Up.html#beta-the-business-problem",
    "title": "The End is Nigh",
    "section": "\\(\\beta\\) (The Business Problem)",
    "text": "\\(\\beta\\) (The Business Problem)\nRequirements:\n\n\nUnderstanding of the problem domain (Why?).\n\nWhat is the context?\nWhat are the priorities?\nWhat has (not) been done before?\n\nCritical thinking wrt to how the problem can be solved (How?).\n\nNot all problems are data problems.\nNot all problems are coding problems.\n\nAbility to communicate with decision-makers/budget-holders (What?).\n\nDevelop a narrative.\nFocus on the takeaways.\n\n\n\n\n\nHow do you develop a narrative? Through story-telling and analogy/metaphor.\nRefer to experience with telco CRM team."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#understand-the-problem",
    "href": "lectures/10.2-Wrap-Up.html#understand-the-problem",
    "title": "The End is Nigh",
    "section": "Understand the Problem",
    "text": "Understand the Problem\n\nA standard template emerges when reading such work. A cursory review of existing research is reeled off and then set aside. Central questions are reframed in terms of reproducing statistical features of the phenomenon, such as the highly skewed dis- tribution of earthquake magnitudes or city populations. A highly stylized computational, statistical, or mathematical model is demonstrated to exhibit the required statistical features, implying that the model explains the real-world phenomenon. Filling in the details of real-world mechanisms that explain why the model works can be done later by domain-specific experts. The implication is that a general explanatory mechanism is more significant than incidental details of any specific field. (O’Sullivan and Manson 2015)\n\n\n\nDiscussion with telco re: modelling sub-cell level mobility."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#delta-the-data",
    "href": "lectures/10.2-Wrap-Up.html#delta-the-data",
    "title": "The End is Nigh",
    "section": "\\(\\delta\\) (The Data)",
    "text": "\\(\\delta\\) (The Data)\nRequirements:\n\n\nUnderstanding of the DGP (Data Generating Process).\n\nWhere does it come from and how is it generated?\nWhat is not in the data?\nIs the DGP stable?\n\nUnderstanding of the ethics of the data.\n\nHow can it be used?\nHow should it be used?\n\nAbility to map data on to the problem.\n\nCleaning and transforming.\nLinking and integrating."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#whats-a-disaster",
    "href": "lectures/10.2-Wrap-Up.html#whats-a-disaster",
    "title": "The End is Nigh",
    "section": "What’s a Disaster?",
    "text": "What’s a Disaster?\n\nSocial media datasets depict a specific time period, typically defined by the spike in Twitter messages or the use of particular hashtags. This can make it difficult to understand both the causes of disaster and the entire period of aftermath where the impact is realized. This further reifies a problematic short-term conceptions of disaster. In this sense, the analysis of social media during and after a disaster can resemble traditional media coverage, which has been often accused of paying attention to only the most sensational stories in a truncated timeframe… (Crawford and Finn 2015)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#whats-a-poi",
    "href": "lectures/10.2-Wrap-Up.html#whats-a-poi",
    "title": "The End is Nigh",
    "section": "What’s a POI?",
    "text": "What’s a POI?\n\nStephens (2013) for example analyses the gendered politics of spatial data crowdsourcing initiatives like Open Street Map, which enable the creation of endless categorizations associated with particular masculinities (brothels, strip clubs) while often wholly omitting feminized places (baby hatches, after-school childcare centres) from these schemas. (Elwood and Leszczynski 2018)"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#kappa-the-code",
    "href": "lectures/10.2-Wrap-Up.html#kappa-the-code",
    "title": "The End is Nigh",
    "section": "\\(\\kappa\\) (The Code)",
    "text": "\\(\\kappa\\) (The Code)\nRequirements:\n\n\nAbility to develop/use appropriate tools.\n\nWhat can I reuse or recycle?\nIs this solution the right one?\n\nAbility to visualise an abstract goal.\n\nHow does this bit of code serve that goal?\nWhat is the next step towards that goal?\n\nAbility to work withing constraints.\n\nWhat is the budget?\nWhat is the timeline?\nWhat are the limitations?\n\n\n\n\nAll of these are about constraints. The ‘correct answer’ is defined in part by the constraints you are working within, not the ideal set of knowledge, data, and resources!"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#iota-the-infrastructure",
    "href": "lectures/10.2-Wrap-Up.html#iota-the-infrastructure",
    "title": "The End is Nigh",
    "section": "\\(\\iota\\) (The Infrastructure)",
    "text": "\\(\\iota\\) (The Infrastructure)\nRequirements:\n\n\nUnderstanding of the pipeline.\n\nHow is the code deployed?\nHow is the data managed?\nHow is the solution maintained?\n\nUndertanding of the wider ecosystem.\n\nHow are issues with the code identified and resolved?\nHow are changes to the data identified and managed?\nHow are reports generated and shared?\nHow are resources allocated?"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#the-big-picture",
    "href": "lectures/10.2-Wrap-Up.html#the-big-picture",
    "title": "The End is Nigh",
    "section": "The Big Picture",
    "text": "The Big Picture\n\n… we need to enroll the practical and technical aspects of GIS use and development directly into modules that address its history of development and implications of use. In doing so, hardware, software, analytic techniques, and data/representation are always presented as both technical and social. From these origins, critical GIS pedagogies aim beyond only building students’ conceptual apparatus for critiquing GIS or explaining its social and political implications, toward instilling this technopositionality as their orientation to doing GIS. (Elwood and Wilson 2017)\n\n\n\nWhen we think ecosystem, we should think about everything."
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#pi-the-pies",
    "href": "lectures/10.2-Wrap-Up.html#pi-the-pies",
    "title": "The End is Nigh",
    "section": "\\(\\pi\\) (The Pies)",
    "text": "\\(\\pi\\) (The Pies)\nRequirements:\n\n\n🍕🍕🍕"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#so",
    "href": "lectures/10.2-Wrap-Up.html#so",
    "title": "The End is Nigh",
    "section": "So…",
    "text": "So…\n\nLet \\(\\sigma\\) be the spatial data science solution to a business problem that requires the integration of data, code, infrastructure, and pies…"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#but-in-an-urban-context",
    "href": "lectures/10.2-Wrap-Up.html#but-in-an-urban-context",
    "title": "The End is Nigh",
    "section": "But in an urban context…",
    "text": "But in an urban context…\nHave you heard of IBM’s Smarter Cities lab?\n\n\nThis is not easy.\nAfter being a big part of IBM’s strategy in the early 2000s, the Smarter Cities initiative was quietly dropped in 2016.\nImportance of the ‘assemblage’"
  },
  {
    "objectID": "lectures/10.2-Wrap-Up.html#instrumenting-the-city",
    "href": "lectures/10.2-Wrap-Up.html#instrumenting-the-city",
    "title": "The End is Nigh",
    "section": "Instrumenting the City",
    "text": "Instrumenting the City\n\nThe risk here is that the dashboard’s seeming comprehensiveness and seamlessness suggest that we can “govern by Blackberry” — or “fly by instrument” — alone. (Mattern 2015)"
  },
  {
    "objectID": "lectures/1.4-Tools.html",
    "href": "lectures/1.4-Tools.html",
    "title": "Tools of the Trade",
    "section": "",
    "text": "Ideally, we want to ‘do’ data science in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct. ~ Knuth (1996)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#literate-programming",
    "href": "lectures/1.4-Tools.html#literate-programming",
    "title": "Tools of the Trade",
    "section": "",
    "text": "Ideally, we want to ‘do’ data science in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct. ~ Knuth (1996)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#key-tenets",
    "href": "lectures/1.4-Tools.html#key-tenets",
    "title": "Tools of the Trade",
    "section": "Key Tenets",
    "text": "Key Tenets\nWhat we want:\n\nWeaving: the code and its documentation are together.\nTangling: the code can be run directly.\n\nIn an ideal world, these are the same file…\n\n\n\nBut why would we want this?"
  },
  {
    "objectID": "lectures/1.4-Tools.html#and-how-do-we-do-this",
    "href": "lectures/1.4-Tools.html#and-how-do-we-do-this",
    "title": "Tools of the Trade",
    "section": "And how do we do this?",
    "text": "And how do we do this?\nHint: it’s more than just one thing…\n\n\nJupyterLab: how we do ‘data science’.\nVirtualisation: separate your computer from your coding environment.\nVersion Control: manage your code, your data, and even your reports.\nMarkup: focus on the structure while you write!\nRender: creating documents and web pages from code and markup."
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-jupyterlab",
    "href": "lectures/1.4-Tools.html#why-use-jupyterlab",
    "title": "Tools of the Trade",
    "section": "Why Use JupyterLab?",
    "text": "Why Use JupyterLab?\nCoding in JupyterLab has a number of advantages over ‘point-and-click’:\n\n\nCoding requires our instructions to be unambiguous and logical.1\nComputers are infinitely patient so we can re-run as many times as necessary to get it ‘right’.\nThere is nothing to install (runs in your web browser).\nYou can run code from anywhere (runs in your web browser)."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture",
    "href": "lectures/1.4-Tools.html#the-bigger-picture",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nIf we can’t explain it simply enough that a computer can do it, perhaps we don’t actually understand it?\n\n\n\nTogether with the other tools in this talk, you can largely stop worrying about where code is running.\nIt’s easy to forget how you obtained a particular result when you are clicking around inside software like ArcGIS; this is much harder when using code.\nIn analysing the problem so that we can submit it to the computer we often develop a better understanding of the problem ourselves!\nWhy spend your time doing the boring stuff???\n\n\n\nConversely, perhaps the real value of humans over AI lies in what cannot be explained to the computer."
  },
  {
    "objectID": "lectures/1.4-Tools.html#jupyterlab-python",
    "href": "lectures/1.4-Tools.html#jupyterlab-python",
    "title": "Tools of the Trade",
    "section": "JupyterLab + Python",
    "text": "JupyterLab + Python"
  },
  {
    "objectID": "lectures/1.4-Tools.html#two-basic-flavours",
    "href": "lectures/1.4-Tools.html#two-basic-flavours",
    "title": "Tools of the Trade",
    "section": "Two Basic ‘Flavours’",
    "text": "Two Basic ‘Flavours’\nBoth do the same thing: separate the platform from the hardware, but they do this in defferent ways for different reasons.\n\nA ‘full’ Virtual Machine (VM) includes the Operating System and behaves like a separate computer even though it may share hardware with other VMs.\nA ‘container’ is a ‘lightweight’ VM running only the application and its dependencies; everything else is managed by the host Operating System so the resulting ‘image’ is small and easy to distribute.\n\nShort version: if you have to install an Operating System you are using a full VM; otherwise you are probably using a containerisation tool/\nMany things, including storage, networks, CPUs, GPUs, etc. can be virtualised."
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-containers",
    "href": "lectures/1.4-Tools.html#why-use-containers",
    "title": "Tools of the Trade",
    "section": "Why Use Containers?",
    "text": "Why Use Containers?\nWe gain quite a few benefits:\n\nEasier installation and ‘everyone’ has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nEasy to scale up and scale down, or to link them together via ‘microservices’.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-1",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-1",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nRather than having one environment for every project, we have one environment for each project.\n\n\n‘Computing contexts’ are disposable, while data and code are persistent when I need them.\nI don’t care where my code and data are, so long as they’re accessible when I need them.\nI don’t care if containers are created or destroyed, so long as they’re available when I need them.\nI rebuild or update the computing context when I am ready to do so."
  },
  {
    "objectID": "lectures/1.4-Tools.html#podman",
    "href": "lectures/1.4-Tools.html#podman",
    "title": "Tools of the Trade",
    "section": "Podman",
    "text": "Podman\n\n\nPodman is an open source container and image management engine. Podman makes it easy to find, run, build, and share containers."
  },
  {
    "objectID": "lectures/1.4-Tools.html#using-podman",
    "href": "lectures/1.4-Tools.html#using-podman",
    "title": "Tools of the Trade",
    "section": "Using Podman",
    "text": "Using Podman\nPodman makes configuring a development environment (fairly) simple. If a Podman image works for us then we know2 it works for you.\nUse either:\n\njreades/sds:2025-amd (Windows and Older Macs)\njreades/sds:2025-arm (Newer Macs)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-version-control",
    "href": "lectures/1.4-Tools.html#why-use-version-control",
    "title": "Tools of the Trade",
    "section": "Why use Version Control?",
    "text": "Why use Version Control?\n\n… If a mistake is made, developers can turn back the clock and compare earlier versions of the code to help fix the mistake while minimizing disruption to all team members.\nSource: Altassian\n\nIn addition:\n\n\nWe can share code with others (directly) as source code or (indirectly) as the product of compiling that source code.\nWe can rewind, fast forward, and combine changes by different people working on different features.\nWe gain detailed, incremental backups that help us tro track down the changes that introduced a bug when something goes wrong.\n\n\n\nDiscuss comparison with Dropbox or OneDrive. How are they similar, how are they different? File level vs. row-level views."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-2",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-2",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nIn open source projects there may be no one view of what the ‘right’ solution/version of a project is, so differences need to be negotiated.\n\n\nEvery computer with version control might have the ‘right’ version of the code for a given user, so there is no ‘master’ view of a project.\nWe need to be able to choose whether to merge other people’s changes with our changes, rather than having everything forced on us.\nWe still want to be able to share our version of the code / outputs of the code with other people, and a web site is a good way to do that."
  },
  {
    "objectID": "lectures/1.4-Tools.html#git",
    "href": "lectures/1.4-Tools.html#git",
    "title": "Tools of the Trade",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout."
  },
  {
    "objectID": "lectures/1.4-Tools.html#github",
    "href": "lectures/1.4-Tools.html#github",
    "title": "Tools of the Trade",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.4-Tools.html#gitgithub-is-for-anything",
    "href": "lectures/1.4-Tools.html#gitgithub-is-for-anything",
    "title": "Tools of the Trade",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course is on GitHub."
  },
  {
    "objectID": "lectures/1.4-Tools.html#oh-my-git",
    "href": "lectures/1.4-Tools.html#oh-my-git",
    "title": "Tools of the Trade",
    "section": "Oh My Git!",
    "text": "Oh My Git!\n\n\nSource: OhMyGit"
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-use-markup",
    "href": "lectures/1.4-Tools.html#why-use-markup",
    "title": "Tools of the Trade",
    "section": "Why use Markup?",
    "text": "Why use Markup?\n\n\nQuickly sketch out the structure of a document.\nFocus on the substance, not the style.\nWorks well with version control (line-by-line changes + GitHub.io web site).\nCombine code, documentation, and narrative easily."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-3",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-3",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nI spend a lot less time ‘faffing’ writing in Markdown than I used to. Spend more time on what you want to say and worry about the how later.\n\n\n\n### A Subtitle\n\nSome text in **bold** and *italics* with a [link](https://jreades.github.io/).\n\n&gt; A blockquote\n\n\nA Subtitle\nSome text in bold and italics with a link.\n\nA blockquote"
  },
  {
    "objectID": "lectures/1.4-Tools.html#markdown-examples",
    "href": "lectures/1.4-Tools.html#markdown-examples",
    "title": "Tools of the Trade",
    "section": "Markdown Examples",
    "text": "Markdown Examples\nSee CommonMark and the Markdown Guide for more:\n\n\n\n\n\n\n\nFormat\nOutput\n\n\nPlain text...\nPlain text\n\n\n## A Large Heading\nA Large Heading\n\n\n### A Medium Heading\nA Medium Heading\n\n\n- A list\n- More list\n\nA list\nMore list\n\n\n\n1. An ordered list\n2. More ordered list\n\nAn ordered list\nMore ordered list\n\n\n\n[A link](http://casa.ucl.ac.uk)\nA link\n\n\n\n\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…\n\n\n\n\n\n\n\n\n\nFormat\nOutput\n\n\n![Alt Text](casa_logo.jpg)\n\n\n\n`x=y+1`\n\n  x=y+1  \n\n\n\n```{python}\n# A block of Python code\nx = y+1\n```\n\n  # A block of Python code\n  x = y+1  \n\n\n\n\n\n\nEmbedded Maths\n$$\nf(a) = \\frac{1}{2\\pi i}     \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n$$\n\n \n\\[\nf(a) = \\frac{1}{2\\pi i} \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n\\]\n\n\nCustom Markup\n&lt;div style=\"border: dotted 2px red; margin-top: 25px; background-color: rgb(230,230,230)\"&gt;\n  This content has HTML formatting attached.\n&lt;/div&gt;\n\n \n\nThis content has HTML formatting attached."
  },
  {
    "objectID": "lectures/1.4-Tools.html#why-render",
    "href": "lectures/1.4-Tools.html#why-render",
    "title": "Tools of the Trade",
    "section": "Why Render?",
    "text": "Why Render?\n\n\nOutputs can be: web pages, Jupyter notebooks, Word documents, PDFs, presentations…\nIt can be really useful to have a single input and multiple outputs because requirements and needs always change.\nIt teaches you to focus on the process, not the minutiae."
  },
  {
    "objectID": "lectures/1.4-Tools.html#the-bigger-picture-4",
    "href": "lectures/1.4-Tools.html#the-bigger-picture-4",
    "title": "Tools of the Trade",
    "section": "The Bigger Picture",
    "text": "The Bigger Picture\n\nEverything this week was created using these basic tools and techniques. It has transformed the way I teach, do research, and write! It embodies the potential of ‘literate programming’ (Knuth 1984)."
  },
  {
    "objectID": "lectures/1.4-Tools.html#programming-languages-used",
    "href": "lectures/1.4-Tools.html#programming-languages-used",
    "title": "Tools of the Trade",
    "section": "Programming Languages (Used)",
    "text": "Programming Languages (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#databases-used",
    "href": "lectures/1.4-Tools.html#databases-used",
    "title": "Tools of the Trade",
    "section": "Databases (Used)",
    "text": "Databases (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#frameworks-libraries-used",
    "href": "lectures/1.4-Tools.html#frameworks-libraries-used",
    "title": "Tools of the Trade",
    "section": "Frameworks & Libraries (Used)",
    "text": "Frameworks & Libraries (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#virtualisation-other-tools-used",
    "href": "lectures/1.4-Tools.html#virtualisation-other-tools-used",
    "title": "Tools of the Trade",
    "section": "Virtualisation & Other Tools (Used)",
    "text": "Virtualisation & Other Tools (Used)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#programming-languages-desiredadmired",
    "href": "lectures/1.4-Tools.html#programming-languages-desiredadmired",
    "title": "Tools of the Trade",
    "section": "Programming Languages (Desired/Admired)",
    "text": "Programming Languages (Desired/Admired)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#databases-desiredadmired",
    "href": "lectures/1.4-Tools.html#databases-desiredadmired",
    "title": "Tools of the Trade",
    "section": "Databases (Desired/Admired)",
    "text": "Databases (Desired/Admired)"
  },
  {
    "objectID": "lectures/1.4-Tools.html#additional-resources",
    "href": "lectures/1.4-Tools.html#additional-resources",
    "title": "Tools of the Trade",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGetting Started with Markdown\nAn online interactive tutorial for Markdown\nMarkdown Cheatsheet\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nGit for Decolonisation3\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…\n\nAnd once you’re ready to get ‘serious’, check out this tutorial on Sustainable Authorship in Plain Text using Pandoc and Markdown from The Programming Historian! That’s what actually underpins Quarto, but you can do so much more…"
  },
  {
    "objectID": "lectures/1.4-Tools.html#footnotes",
    "href": "lectures/1.4-Tools.html#footnotes",
    "title": "Tools of the Trade",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis does not guarantee that they’ll be correct.↩︎\nUsually, but not always, true.↩︎\nPart art, part activism, part tech!↩︎"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "… we actually have a long history of cutting-edge computer use in these disciplines! Every time there has been a step-change in computing, geography and planning have been there to exploit it!\n\n\n\n\n\n“A computer in every institution”\n“A computer in every office”\n“A computer in every thing”\n\n\n\n\n\n\nRoughly the 1950s–70s\nComputers as ‘super-human’ calculators for data\nData + models as theory-testing tool\n\nRetrospectively: the 1st quantitative revolution. But can see this as incorrect and focus on the theoretical aspect.\n\n\n\n\nRoughly the 1980s–2000s\nComputers as tools for thinking about spatial relationships\nExplicit modelling of local spatial effects\n\nRetrospectively: the GIS revolution. But I personally see this as incorrect because GIS is Wave 1.\nShift from computers as processors of data to integrated, pervasive systems that spew out data on everything.\n\n\n\n\nRoughly the mid-2000s–?\nComputers as tools for generating data (pace ABM researchers)\nGeodata being continuously produced as byproduct of other activities\nShift from researching attributes to behaviours (pace Hägerstrand)\n\nRetrospectively: the big data revolution or 2nd quantitative revolution.\n\n\n\n\n\nWave 1: Computers help me do it faster\n\nGIS is ‘just’ the industrial revolution impacting cartography.\n\n. . .\nWave 2: Computers help me to think\n\nGeocomputation & local stats are qualitatively & meaningfully different.\n\n. . .\nWave 3: Computers help me to learn\n\nNot ‘just’ about the ‘bigness’ of data, though that is important.\n\n\nWave 2 is about implementing ideas such as recursion and iteration – these could, in theory, have been tackled in Wave 1, but in practice that’s not what people were doing.\nWave 3 is about more explicitly allowing computers to learn about data so that we can extract insight from these models – these could also, in theory, have been tackled in Wave 2 but in practice that’s not what people were doing.\nI’m not totally happy about my description of Wave 3 and will try to dig into this in a little more detail but suggestions welcome!\n\n\n\n\n\nI think that the computer can do three different and useful things for us. The first and simplest operation is… descriptive mapping the second… is the analytical one The third kind of service is… to run process models by which we might try to reproduce observed or create hypothetical chains of events of a geographical nature.\nHägerstrand (1967)\n\n\n\n\n\nMore important, there is a clear disparity between the sophisticated theoretical and methodological framework which we are using and our ability to say anything really meaningful about events as they unfold around us. There are too many anomalies between what we purport to explain and manipulate and what actually happens. There is an ecological problem, an urban problem, an international trade problem, and yet we seem incapable of saying anything of any depth or profundity about any of them. When we do say something it appears trite and rather ludicrous.\nHarvey (2008 [1972], 17)\n\n\n\n\n\n\n\nSource: Davenport and Patil (2012)\n\n\nIndustry-led\nSpatially ignorant (often)\nDisciplinarily greedy (often)\n\n\n\n\n\nSource: Schneider (2013)\n\nPersonally, I think that you get to danger zones everywhere here, not just at the intersection between hacking and substantive expertise.\n\n\n\n\nAccording to Donoho (2017) ‘data science’ differs from plain old ‘statistics’ through an interest in:\n\nData gathering, preparation, and exploration;\nData representation and transformation;\nComputing with data;\nData visualisation and presentation;\nData modelling; and\nA reflexive ‘science of data science’.\n\n\n\n\nI think there are several distinguishing features that I encounter in day-to-day (geography) work:\n\nData-driven methods development & deployment\nExplicit tuning/meta-parameterisation\nExplicit feature optimisation/engineering\nExplicit training/testing from ‘one shot’ data\n‘Black boxes’ feature prominently & ‘online learning’ emerging quickly\n\nData science as process and pipeline, not just input to research.\n\n\n\n\nSource: xkcd\n\n\n\n\nThe Data of Urban Spaces — a nice collection of pieces on Towards Data Science"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#waves",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#waves",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "“A computer in every institution”\n“A computer in every office”\n“A computer in every thing”\n\n\n\n\n\n\nRoughly the 1950s–70s\nComputers as ‘super-human’ calculators for data\nData + models as theory-testing tool\n\nRetrospectively: the 1st quantitative revolution. But can see this as incorrect and focus on the theoretical aspect.\n\n\n\n\nRoughly the 1980s–2000s\nComputers as tools for thinking about spatial relationships\nExplicit modelling of local spatial effects\n\nRetrospectively: the GIS revolution. But I personally see this as incorrect because GIS is Wave 1.\nShift from computers as processors of data to integrated, pervasive systems that spew out data on everything.\n\n\n\n\nRoughly the mid-2000s–?\nComputers as tools for generating data (pace ABM researchers)\nGeodata being continuously produced as byproduct of other activities\nShift from researching attributes to behaviours (pace Hägerstrand)\n\nRetrospectively: the big data revolution or 2nd quantitative revolution."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#all-waves-still-going",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#all-waves-still-going",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "Wave 1: Computers help me do it faster\n\nGIS is ‘just’ the industrial revolution impacting cartography.\n\n. . .\nWave 2: Computers help me to think\n\nGeocomputation & local stats are qualitatively & meaningfully different.\n\n. . .\nWave 3: Computers help me to learn\n\nNot ‘just’ about the ‘bigness’ of data, though that is important.\n\n\nWave 2 is about implementing ideas such as recursion and iteration – these could, in theory, have been tackled in Wave 1, but in practice that’s not what people were doing.\nWave 3 is about more explicitly allowing computers to learn about data so that we can extract insight from these models – these could also, in theory, have been tackled in Wave 2 but in practice that’s not what people were doing.\nI’m not totally happy about my description of Wave 3 and will try to dig into this in a little more detail but suggestions welcome!"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#anticipated-by-hägerstrand-1967",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#anticipated-by-hägerstrand-1967",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "I think that the computer can do three different and useful things for us. The first and simplest operation is… descriptive mapping the second… is the analytical one The third kind of service is… to run process models by which we might try to reproduce observed or create hypothetical chains of events of a geographical nature.\nHägerstrand (1967)"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#but-persistent-critiques",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#but-persistent-critiques",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "More important, there is a clear disparity between the sophisticated theoretical and methodological framework which we are using and our ability to say anything really meaningful about events as they unfold around us. There are too many anomalies between what we purport to explain and manipulate and what actually happens. There is an ecological problem, an urban problem, an international trade problem, and yet we seem incapable of saying anything of any depth or profundity about any of them. When we do say something it appears trite and rather ludicrous.\nHarvey (2008 [1972], 17)"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#data-science",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#data-science",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "Source: Davenport and Patil (2012)\n\n\nIndustry-led\nSpatially ignorant (often)\nDisciplinarily greedy (often)\n\n\n\n\n\nSource: Schneider (2013)\n\nPersonally, I think that you get to danger zones everywhere here, not just at the intersection between hacking and substantive expertise."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#what-is-different",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#what-is-different",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "According to Donoho (2017) ‘data science’ differs from plain old ‘statistics’ through an interest in:\n\nData gathering, preparation, and exploration;\nData representation and transformation;\nComputing with data;\nData visualisation and presentation;\nData modelling; and\nA reflexive ‘science of data science’."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#in-practice",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#in-practice",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "I think there are several distinguishing features that I encounter in day-to-day (geography) work:\n\nData-driven methods development & deployment\nExplicit tuning/meta-parameterisation\nExplicit feature optimisation/engineering\nExplicit training/testing from ‘one shot’ data\n‘Black boxes’ feature prominently & ‘online learning’ emerging quickly\n\nData science as process and pipeline, not just input to research."
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#do-we-need-it",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#do-we-need-it",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "Source: xkcd"
  },
  {
    "objectID": "lectures/1.2-Computers_in_Urban_Studies.html#additional-resources",
    "href": "lectures/1.2-Computers_in_Urban_Studies.html#additional-resources",
    "title": "Computers in Urban Studies",
    "section": "",
    "text": "The Data of Urban Spaces — a nice collection of pieces on Towards Data Science"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html",
    "href": "extra/Live-04-Objects-1.html",
    "title": "Intro",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 4: Object-Oriented Programming 1&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with Classes &amp; Objects&lt;/h2&gt;\nYou will be encountering classes and dealing with inheritance from next week and this is fundamental to how Python works. In Python, everything is an object and that means that everything – absolutely everything – is part of the Python class hierarchy, at the bottom of which sits the base object class.\nFor our purposes, the place you’re going to encounter this every day is that Pandas is our main data analysis tool, but when we deal with geography we’ll be using GeoPandas, which inherits from Pandas and adds (we say “extends”) functionality for processing geo-data.\nThere’s even a Moving Pandas for trajectory analysis. These all work on the basis of inheritance."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-1-creating-a-class-hierarchy",
    "href": "extra/Live-04-Objects-1.html#task-1-creating-a-class-hierarchy",
    "title": "Intro",
    "section": "Task 1: Creating a Class Hierarchy",
    "text": "Task 1: Creating a Class Hierarchy\nWe want to create a set of ideal shape classes with methods allowing us to derive various properties of that shape:\n\nDiameter: which we’ll define as the longest line that can be drawn across the inside of the shape.\nVolume: the total volume of the shape.\nSurface Area: the total outside area of the shape.\n\nWe will create all of these shape classes in the notebook so that we know they work and then will move them to an external package file so that they can be imported and re-used easily in other notebooks.\nWe’re also going to make use of a few features of Python:\n\nYou can access the class name of an instance using: self.__class__.__name__. And here’s one key point: self refers to the instance, not to the class… we’ll see why this matters.\nYou can raise your own exceptions easily if you don’t want to implement a particular method yet.\nYou can have an ‘abstract’ base class that does nothing except provide a template for the ‘real’ classes so that they can be used interchangeably.\n\n\nprint(\"Hello world\".__class__.__name__)\n\nstr\n\n\n\nTask 1.1: Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\nfrom math import pi\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        print(\"I'm a shape!\")\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\n\n\nTask 1.2: Demonstrate\nBriefly discuss what’s going on, paying particular attention to:\n\nclass shape(object) – this means that shape extends the object base class.\ndef __init__ – what is self (reference to the instance) and what does dimension:float=None mean?\nCreate a new shape object:\n\n\nWhat happens when you run s = shape()?\nWhat happens when you try to call s.diameter()?\nWhat happens when you try to call s.type()?"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-2-cube",
    "href": "extra/Live-04-Objects-1.html#task-2-cube",
    "title": "Intro",
    "section": "Task 2: Cube",
    "text": "Task 2: Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nTask 2.1: Cube Class\n\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        print(\"I'm a cube!\")\n        return\n    \n    def diameter(self):\n        return (3 * self.dim**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return 6*(self.dim**2)\n\n\n\nTask 2.2: Demonstrate\n\nCreate a cube and do the same things we did with shape (show that it’s been overridden).\nLook at the print statements created from the __init__ and discuss.\nShow how the type method works for Cubes even though we never specified it."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-3-sphere",
    "href": "extra/Live-04-Objects-1.html#task-3-sphere",
    "title": "Intro",
    "section": "Task 3: Sphere",
    "text": "Task 3: Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2d\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nTask 3.1: Sphere Class\n\n# Sphere class\nclass sphere(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return self.dim*2\n    \n    def volume(self):\n        return (4/3) * pi * self.dim**3\n    \n    def surface(self):\n        return 4 * pi * (self.dim**2)\n\n\n\nTask 3.2: Demonstrate how this works\n\nCreate one Cube and one Sphere with the same parameter (dimension)\nShow how they return different values for diameter, volume, surface\nShow that both have a type function that works without being defined."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-4-pyramids",
    "href": "extra/Live-04-Objects-1.html#task-4-pyramids",
    "title": "Intro",
    "section": "Task 4: Pyramids",
    "text": "Task 4: Pyramids\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them!\n\nTask 4.1: Regular Pyramids\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n    \n    has_mummies = True # This is for *all* regular pyramids\n    \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n    \n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n    \n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**2 * self.height() / 3\n    \n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)\n\n\n\nTask 4.2: Demonstrate\nShow how we added a new method for regular pyramids:\n\nDoesn’t work for any other class\nBut all the rest of the methods do\n\nDiscuss: what are the pros and cons of this new method?\nAnswer: in general, we try to extend and never to break functionality of other classes in the hierarchy. So you wouldn’t create a shape sub-class that broke the diameter or surface methods by returning something like “I don’t have this dimension” as a text string, but you could throw an error or return a Null value. It depends on what makes sense.\n\n\nTask 4.3: Triangular Pyramid\nWe want triangular pyramid to inherit from regular pyramid, and all sides are equal so it’s an equilateral triangular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nAnyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n    \n    has_mummies = False # This is for all triangular pyramids\n    \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n    \n    def diameter(self):\n        return self.dim\n    \n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n    \n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n    \n    def volume(self):\n        return (1/3) * self.base() * self.height()\n    \n    def surface(self):\n        return 4 * self.base()\n\n\n\nTask 4.4: Demonstrate\n\nCompare regular and triangular pyramids: notice that the has_mummies class variable has changed its default value.\nNotice too that you can override/change a class variable or an instance variable at run-time. There is a very important difference between doing something like self.has_mummies = has_mummies and t_pyramid.has_mummies = True.\n\nHere’s an example of everything run together:\n\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaagh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaagh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\n\nI'm a shape!\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nI'm a shape!\nI'm a cube!\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nI'm a shape!\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaagh!\n\nI'm a shape!\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'\n\n\n\n\nprint(p2.__class__)\nprint(p2.__class__.__mro__)\n\n&lt;class '__main__.t_pyramid'&gt;\n(&lt;class '__main__.t_pyramid'&gt;, &lt;class '__main__.pyramid'&gt;, &lt;class '__main__.shape'&gt;, &lt;class 'object'&gt;)\n\n\nThis shows the class name and then the full hierarchy, so you know in which order Python will search for a method before throwing an error. Some, such as print, exist at the very basic level of the object."
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-5-packaging",
    "href": "extra/Live-04-Objects-1.html#task-5-packaging",
    "title": "Intro",
    "section": "Task 5: Packaging",
    "text": "Task 5: Packaging\nNow that we’ve created our classes, we want to move them to a separate file that can be imported and re-used by code elsewhere. We need to turn this into a package. Formally packaging things up for distribution requires a lot more work(see this and this). But for something that we’re only going to use ourselves it’s basically as simple as:\n\nCreate a directory that will become the package name (e.g. shapes).\nWrite the code in a file called __init.py__ inside that new directory (i.e. ./shapes/__init__.py).\n\nWe could do this by hand by copy-pasting each cell above into a new file, but because we prefer to be lazy let’s try automating it…\n\nTask 5.1: Shell Commands\nFirst, we need to create the shapes directory. To do this, although you can use Python code to create and delete files and folders, it’s helpful to know that you can actually execute shell commands directly from a notebook. Oviously, the degree to which this works depends on which Operating System you’re using!\nFor instance, here’s our trusty list command:\n\n!ls\n\nLive-02-Foundations_1.qmd             Live-08-Dimensions_in_Shakespeare.qmd\nLive-03-Foundations_2.qmd             Live-09-Grouping_Data.qmd\nLive-03-Foundations_2_Intro.qmd       Live-10-Visualising_Data.qmd\nLive-04-Objects-1.qmd                 Note_Hint_Danger.qmd\nLive-04-Objects-1.quarto_ipynb        XX-Function_Development.ipynb\nLive-04-Objects-2.qmd                 css\nLive-05-Numeric_Data.qmd              img\nLive-06-Spatial_Data.qmd              shapes\nLive-07-Textual_Data.qmd              textual\nLive-08-Dimensions_in_Data.qmd        world.geojson\n\n\nSo we can simply ask mkdir (make directory) to do this for us. The -p means that it creates directory hierarhices if they’re missing and doesn’t complain if a directory already exists.\n\n!mkdir -p shapes\n\n\n\nTask 5.2: Creating an init file\nThe easiest and most intelligible way to create the shapes package is to copy and paste each of the class definitions above into a new file called __init__.py manually.\nI would demonstrate this approach; however…\n\nAside: Notebook Convert\nIt turns out that jupyter also offers a nbconvert (Notebook Convert) utility that helps you to convert notebooks to other formats including HTML, LaTeX, PDF, Markdown, Executable Python and so on! We’re going to take advantage of the fact that __init__.py is just plain old python following a particular naming and placement scheme…\n\n!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \\\n    --to python --output=shapes/__init__.py \\\n    Live-04-Objects-1.ipynb\n\n[NbConvertApp] WARNING | pattern 'Live-04-Objects-1.ipynb' matched no files\nThis application is used to convert notebook files (*.ipynb)\n        to various other formats.\n\n        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n\nOptions\n=======\nThe options below are convenience aliases to configurable class-options,\nas listed in the \"Equivalent to\" description-line of the aliases.\nTo see all configurable class-options for some &lt;cmd&gt;, use:\n    &lt;cmd&gt; --help-all\n\n--debug\n    set log level to logging.DEBUG (maximize logging output)\n    Equivalent to: [--Application.log_level=10]\n--show-config\n    Show the application's configuration (human-readable format)\n    Equivalent to: [--Application.show_config=True]\n--show-config-json\n    Show the application's configuration (json format)\n    Equivalent to: [--Application.show_config_json=True]\n--generate-config\n    generate default config file\n    Equivalent to: [--JupyterApp.generate_config=True]\n-y\n    Answer yes to any questions instead of prompting.\n    Equivalent to: [--JupyterApp.answer_yes=True]\n--execute\n    Execute the notebook prior to export.\n    Equivalent to: [--ExecutePreprocessor.enabled=True]\n--allow-errors\n    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n--stdin\n    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n    Equivalent to: [--NbConvertApp.from_stdin=True]\n--stdout\n    Write notebook output to stdout instead of files.\n    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n--inplace\n    Run nbconvert in place, overwriting the existing notebook (only\n            relevant when converting to notebook format)\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n--clear-output\n    Clear output of current file and save in place,\n            overwriting the existing notebook.\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n--coalesce-streams\n    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n--no-prompt\n    Exclude input and output prompts from converted document.\n    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n--no-input\n    Exclude input cells and output prompts from converted document.\n            This mode is ideal for generating code-free reports.\n    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n--allow-chromium-download\n    Whether to allow downloading chromium if no suitable version is found on the system.\n    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n--disable-chromium-sandbox\n    Disable chromium security sandbox when converting to PDF..\n    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n--show-input\n    Shows code input. This flag is only useful for dejavu users.\n    Equivalent to: [--TemplateExporter.exclude_input=False]\n--embed-images\n    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n    Equivalent to: [--HTMLExporter.embed_images=True]\n--sanitize-html\n    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n    Equivalent to: [--HTMLExporter.sanitize_html=True]\n--log-level=&lt;Enum&gt;\n    Set the log level by value or name.\n    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n    Default: 30\n    Equivalent to: [--Application.log_level]\n--config=&lt;Unicode&gt;\n    Full path of a config file.\n    Default: ''\n    Equivalent to: [--JupyterApp.config_file]\n--to=&lt;Unicode&gt;\n    The export format to be used, either one of the built-in formats\n            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n            or a dotted object name that represents the import path for an\n            ``Exporter`` class\n    Default: ''\n    Equivalent to: [--NbConvertApp.export_format]\n--template=&lt;Unicode&gt;\n    Name of the template to use\n    Default: ''\n    Equivalent to: [--TemplateExporter.template_name]\n--template-file=&lt;Unicode&gt;\n    Name of the template file to use\n    Default: None\n    Equivalent to: [--TemplateExporter.template_file]\n--theme=&lt;Unicode&gt;\n    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n    as prebuilt extension for the lab template)\n    Default: 'light'\n    Equivalent to: [--HTMLExporter.theme]\n--sanitize_html=&lt;Bool&gt;\n    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n    should be set to True by nbviewer or similar tools.\n    Default: False\n    Equivalent to: [--HTMLExporter.sanitize_html]\n--writer=&lt;DottedObjectName&gt;\n    Writer class used to write the\n                                        results of the conversion\n    Default: 'FilesWriter'\n    Equivalent to: [--NbConvertApp.writer_class]\n--post=&lt;DottedOrNone&gt;\n    PostProcessor class used to write the\n                                        results of the conversion\n    Default: ''\n    Equivalent to: [--NbConvertApp.postprocessor_class]\n--output=&lt;Unicode&gt;\n    Overwrite base name use for output files.\n                Supports pattern replacements '{notebook_name}'.\n    Default: '{notebook_name}'\n    Equivalent to: [--NbConvertApp.output_base]\n--output-dir=&lt;Unicode&gt;\n    Directory to write output(s) to. Defaults\n                                  to output to the directory of each notebook. To recover\n                                  previous default behaviour (outputting to the current\n                                  working directory) use . as the flag value.\n    Default: ''\n    Equivalent to: [--FilesWriter.build_directory]\n--reveal-prefix=&lt;Unicode&gt;\n    The URL prefix for reveal.js (version 3.x).\n            This defaults to the reveal CDN, but can be any url pointing to a copy\n            of reveal.js.\n            For speaker notes to work, this must be a relative path to a local\n            copy of reveal.js: e.g., \"reveal.js\".\n            If a relative path is given, it must be a subdirectory of the\n            current directory (from which the server is run).\n            See the usage documentation\n            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n            for more details.\n    Default: ''\n    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n--nbformat=&lt;Enum&gt;\n    The nbformat version to write.\n            Use this to downgrade notebooks.\n    Choices: any of [1, 2, 3, 4]\n    Default: 4\n    Equivalent to: [--NotebookExporter.nbformat_version]\n\nExamples\n--------\n\n    The simplest way to use nbconvert is\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to html\n\n            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n\n            &gt; jupyter nbconvert --to latex mynotebook.ipynb\n\n            Both HTML and LaTeX support multiple output templates. LaTeX includes\n            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n            'classic'. You can specify the flavor of the format used.\n\n            &gt; jupyter nbconvert --to html --template lab mynotebook.ipynb\n\n            You can also pipe the output to stdout, rather than a file\n\n            &gt; jupyter nbconvert mynotebook.ipynb --stdout\n\n            PDF is generated via latex\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to pdf\n\n            You can get (and serve) a Reveal.js-powered slideshow\n\n            &gt; jupyter nbconvert myslides.ipynb --to slides --post serve\n\n            Multiple notebooks can be given at the command line in a couple of\n            different ways:\n\n            &gt; jupyter nbconvert notebook*.ipynb\n            &gt; jupyter nbconvert notebook1.ipynb notebook2.ipynb\n\n            or you can specify the notebooks list in a config file, containing::\n\n                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n\n            &gt; jupyter nbconvert --config mycfg.py\n\nTo see all available configurables, use `--help-all`.\n\n\n\n\n\nTidying Up the Output\nUsing the file browser provided by Jupyter Lab (on the left), open up the new __init__.py file created in the shapes directory. You will want (at the very least) to search for ‘Task 2’ and delete everything in the file after that point.\n\nReally Important: if you do not delete everything from Task 2: Packaging onwards then every time you try to (re)load the shapes you will also output another copy of this notebook because this code is also outputted by nbconvert. In fact, you might find it easier to search for the line “How would you test these changes?” and delete everything from there onwards in the file.\n\n\n\n\nTask 5.3: Importing the Classes\n\nTask 5.3.1: Autoreload Cell Magic\nWhen writing a new package from a notebook, you need to ensure that Python knows to reload the package every time you run the code. Otherwise, Python will keep running the version of the package that you loaded when you first ran import &lt;package name&gt;!\nThis the first time you’ll have seen this special type of code block: % at the start of a line in a codeblock indicates that a ‘magic command’ is being issued. autoreload is one such magic command. There is also the ! at the start of a line which indicates a shell command to be executed (e.g. !ls).\nThere are other magics we will use later like %%time which track the running time of an operation.\n\n%load_ext autoreload\n%autoreload 2\n\n\n\nTask 5.3.2: Import\n\nimport shapes\n\nstr\nI'm a shape!\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nI'm a shape!\nI'm a cube!\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nI'm a shape!\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaagh!\n\nI'm a shape!\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'\n\n&lt;class 'shapes.t_pyramid'&gt;\n(&lt;class 'shapes.t_pyramid'&gt;, &lt;class 'shapes.pyramid'&gt;, &lt;class 'shapes.shape'&gt;, &lt;class 'object'&gt;)\nLive-02-Foundations_1.qmd             Live-08-Dimensions_in_Shakespeare.qmd\nLive-03-Foundations_2.qmd             Live-09-Grouping_Data.qmd\nLive-03-Foundations_2_Intro.qmd       Live-10-Visualising_Data.qmd\nLive-04-Objects-1.qmd                 Note_Hint_Danger.qmd\nLive-04-Objects-1.quarto_ipynb        XX-Function_Development.ipynb\nLive-04-Objects-2.qmd                 css\nLive-05-Numeric_Data.qmd              img\nLive-06-Spatial_Data.qmd              shapes\nLive-07-Textual_Data.qmd              textual\nLive-08-Dimensions_in_Data.qmd        world.geojson\n[NbConvertApp] WARNING | pattern 'Live-04-Objects-1.ipynb' matched no files\nThis application is used to convert notebook files (*.ipynb)\n        to various other formats.\n\n        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n\nOptions\n=======\nThe options below are convenience aliases to configurable class-options,\nas listed in the \"Equivalent to\" description-line of the aliases.\nTo see all configurable class-options for some &lt;cmd&gt;, use:\n    &lt;cmd&gt; --help-all\n\n--debug\n    set log level to logging.DEBUG (maximize logging output)\n    Equivalent to: [--Application.log_level=10]\n--show-config\n    Show the application's configuration (human-readable format)\n    Equivalent to: [--Application.show_config=True]\n--show-config-json\n    Show the application's configuration (json format)\n    Equivalent to: [--Application.show_config_json=True]\n--generate-config\n    generate default config file\n    Equivalent to: [--JupyterApp.generate_config=True]\n-y\n    Answer yes to any questions instead of prompting.\n    Equivalent to: [--JupyterApp.answer_yes=True]\n--execute\n    Execute the notebook prior to export.\n    Equivalent to: [--ExecutePreprocessor.enabled=True]\n--allow-errors\n    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n--stdin\n    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n    Equivalent to: [--NbConvertApp.from_stdin=True]\n--stdout\n    Write notebook output to stdout instead of files.\n    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n--inplace\n    Run nbconvert in place, overwriting the existing notebook (only\n            relevant when converting to notebook format)\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n--clear-output\n    Clear output of current file and save in place,\n            overwriting the existing notebook.\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n--coalesce-streams\n    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n--no-prompt\n    Exclude input and output prompts from converted document.\n    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n--no-input\n    Exclude input cells and output prompts from converted document.\n            This mode is ideal for generating code-free reports.\n    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n--allow-chromium-download\n    Whether to allow downloading chromium if no suitable version is found on the system.\n    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n--disable-chromium-sandbox\n    Disable chromium security sandbox when converting to PDF..\n    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n--show-input\n    Shows code input. This flag is only useful for dejavu users.\n    Equivalent to: [--TemplateExporter.exclude_input=False]\n--embed-images\n    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n    Equivalent to: [--HTMLExporter.embed_images=True]\n--sanitize-html\n    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n    Equivalent to: [--HTMLExporter.sanitize_html=True]\n--log-level=&lt;Enum&gt;\n    Set the log level by value or name.\n    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n    Default: 30\n    Equivalent to: [--Application.log_level]\n--config=&lt;Unicode&gt;\n    Full path of a config file.\n    Default: ''\n    Equivalent to: [--JupyterApp.config_file]\n--to=&lt;Unicode&gt;\n    The export format to be used, either one of the built-in formats\n            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n            or a dotted object name that represents the import path for an\n            ``Exporter`` class\n    Default: ''\n    Equivalent to: [--NbConvertApp.export_format]\n--template=&lt;Unicode&gt;\n    Name of the template to use\n    Default: ''\n    Equivalent to: [--TemplateExporter.template_name]\n--template-file=&lt;Unicode&gt;\n    Name of the template file to use\n    Default: None\n    Equivalent to: [--TemplateExporter.template_file]\n--theme=&lt;Unicode&gt;\n    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n    as prebuilt extension for the lab template)\n    Default: 'light'\n    Equivalent to: [--HTMLExporter.theme]\n--sanitize_html=&lt;Bool&gt;\n    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n    should be set to True by nbviewer or similar tools.\n    Default: False\n    Equivalent to: [--HTMLExporter.sanitize_html]\n--writer=&lt;DottedObjectName&gt;\n    Writer class used to write the\n                                        results of the conversion\n    Default: 'FilesWriter'\n    Equivalent to: [--NbConvertApp.writer_class]\n--post=&lt;DottedOrNone&gt;\n    PostProcessor class used to write the\n                                        results of the conversion\n    Default: ''\n    Equivalent to: [--NbConvertApp.postprocessor_class]\n--output=&lt;Unicode&gt;\n    Overwrite base name use for output files.\n                Supports pattern replacements '{notebook_name}'.\n    Default: '{notebook_name}'\n    Equivalent to: [--NbConvertApp.output_base]\n--output-dir=&lt;Unicode&gt;\n    Directory to write output(s) to. Defaults\n                                  to output to the directory of each notebook. To recover\n                                  previous default behaviour (outputting to the current\n                                  working directory) use . as the flag value.\n    Default: ''\n    Equivalent to: [--FilesWriter.build_directory]\n--reveal-prefix=&lt;Unicode&gt;\n    The URL prefix for reveal.js (version 3.x).\n            This defaults to the reveal CDN, but can be any url pointing to a copy\n            of reveal.js.\n            For speaker notes to work, this must be a relative path to a local\n            copy of reveal.js: e.g., \"reveal.js\".\n            If a relative path is given, it must be a subdirectory of the\n            current directory (from which the server is run).\n            See the usage documentation\n            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n            for more details.\n    Default: ''\n    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n--nbformat=&lt;Enum&gt;\n    The nbformat version to write.\n            Use this to downgrade notebooks.\n    Choices: any of [1, 2, 3, 4]\n    Default: 4\n    Equivalent to: [--NotebookExporter.nbformat_version]\n\nExamples\n--------\n\n    The simplest way to use nbconvert is\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to html\n\n            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n\n            &gt; jupyter nbconvert --to latex mynotebook.ipynb\n\n            Both HTML and LaTeX support multiple output templates. LaTeX includes\n            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n            'classic'. You can specify the flavor of the format used.\n\n            &gt; jupyter nbconvert --to html --template lab mynotebook.ipynb\n\n            You can also pipe the output to stdout, rather than a file\n\n            &gt; jupyter nbconvert mynotebook.ipynb --stdout\n\n            PDF is generated via latex\n\n            &gt; jupyter nbconvert mynotebook.ipynb --to pdf\n\n            You can get (and serve) a Reveal.js-powered slideshow\n\n            &gt; jupyter nbconvert myslides.ipynb --to slides --post serve\n\n            Multiple notebooks can be given at the command line in a couple of\n            different ways:\n\n            &gt; jupyter nbconvert notebook*.ipynb\n            &gt; jupyter nbconvert notebook1.ipynb notebook2.ipynb\n\n            or you can specify the notebooks list in a config file, containing::\n\n                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n\n            &gt; jupyter nbconvert --config mycfg.py\n\nTo see all available configurables, use `--help-all`.\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nI'm a shape!\nI'm a cube!\n&lt;shapes.cube object at 0x1225c0470&gt;\n1000\n17.320508075688775\n600\nI'm a shape!\n150\n122.47448713915888\n397747.5644174329\n38971.143170299736\nHelp on class shape in module shapes:\n\nclass shape(builtins.object)\n |  shape(dimension: float = None)\n |\n |  # Base class shape\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension: float = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |\n |  diameter(self)\n |\n |  surface(self)\n |\n |  type(self)\n |\n |  volume(self)\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\nHelp on function type in module shapes:\n\ntype(self)\n\n\n\n\n\nTask 5.3.3: Demonstrate\nNotice that because we just imported shapes we need to call shapes.cube() below. We could also have done this as from shapes import cube which would allow us to call cube() directly.\nPros and Cons: This is about namespaces. If I dump everything into the main namespace and I have several classes/packages that implement different aspects of cubes the issue is that only the last cube class I imported will be accessible (from shapes import cube), but if I import just shapes then I can use the namespace (shapes) to specify that I mean exactly that class. This is a choice based on what you want to do.\n\nb = shapes.cube(10)\n\nI'm a shape!\nI'm a cube!\n\n\nWhat’s particularly interesting below is the output of dir: notice all of the methods we didn’t create but which are available! And notice that many of them start __ (like __init__ up above). That means the method is ‘private’ and shouldn’t be called directly by the programmer. But what might __gt__ do and why might the person creating the shape classes need to do something here?\n\nprint(b)\nprint(b.volume())\nprint(b.diameter())\nprint(b.surface())\ndir(b)\n\n&lt;shapes.cube object at 0x147b88c80&gt;\n1000\n17.320508075688775\n600\n\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'diameter',\n 'dim',\n 'shape_type',\n 'surface',\n 'type',\n 'volume']\n\n\n\np = shapes.t_pyramid(150)\nprint(p.diameter())\nprint(p.height())\nprint(p.volume())\nprint(p.surface())\n\nI'm a shape!\n150\n122.47448713915888\n397747.5644174329\n38971.143170299736"
  },
  {
    "objectID": "extra/Live-04-Objects-1.html#task-6.-adding-documentation",
    "href": "extra/Live-04-Objects-1.html#task-6.-adding-documentation",
    "title": "Intro",
    "section": "Task 6.: Adding Documentation",
    "text": "Task 6.: Adding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\n\nimport shapes\nhelp(shapes.shape)\nhelp(shapes.shape.type)\n\nHelp on class shape in module shapes:\n\nclass shape(builtins.object)\n |  shape(dimension: float = None)\n |\n |  # Base class shape\n |\n |  Methods defined here:\n |\n |  __init__(self, dimension: float = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |\n |  diameter(self)\n |\n |  surface(self)\n |\n |  type(self)\n |\n |  volume(self)\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __weakref__\n |      list of weak references to the object\n\nHelp on function type in module shapes:\n\ntype(self)"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html",
    "href": "extra/Live-03-Foundations_2.html",
    "title": "Task 0: LoLs and DoLs",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 3: Foundations (Part 2)&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with Dictionaries, LOLs and DOLs, Packages and Functions&lt;/h2&gt;\nCome to class prepared to present/discuss:"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-1-reading-a-remote-file",
    "href": "extra/Live-03-Foundations_2.html#task-1-reading-a-remote-file",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 1: Reading a Remote File",
    "text": "Task 1: Reading a Remote File\n\nTask 1.1: Research Solution\n\nGoogle: read remote CSV file Python\nOooh look, Stack Overflow link\nLet’s review this–note: Python2 vs Python3\n\nLook at dates on answers to see if reasonably useful\nRead responses to what looks like useful answer\nWhich answer was accepted?\n\nOK, so it looks like urllib and csv will be useful.\n\nHow do we work out what’s possible?\nHow do we read help for a function?\n\n\n\nfrom urllib.request import urlopen\n?urlopen\n\n\n\nTask 1.2: Implementing Solution\nOK, so now we know what to do, how do we do it?\n\nSet a url variable\nCapture the response\nRead it, what’s the decoding thing?\nLook at the ‘.’ cropping up: we’ll deal with that later.\n\n\nfrom urllib.request import urlopen\n\n# Given the info you were given above, what do you \n# think the value of 'url' should be? What\n# type of variable is it? int or string? \nurl = 'https://github.com/jreades/fsds/raw/master/data/src/2022-sample-Crime.csv'\n\n# Read the URL stream into variable called 'response'\n# using the function that we imported above\nresponse = urlopen(url)\n\n# Now read from the stream, decoding so that we get actual text\ndatafile = response.read().decode('utf-8')\n\n# You might want to explore what `__class__` and `__name__`\n# offer, but basically the give us a way of finding out what\n# is 'behind' more complex variables\nprint(\"datafile variable is of type: '\" + datafile.__class__.__name__ + \"'.\\n\")\n\ndatafile variable is of type: 'str'.\n\n\n\n\n\nTask 1.3: Checking Solution\nIt’s tempting to just print out the contents of datafile, but what should we do? - Print out the size of the variable (how do we do this for a string?) - Print out some of the top of the file (how do we do this for the first n chars in a string?)\n\nprint(len(datafile))\nprint(datafile[:600])\n\n3809403\nID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location\n11570600,JB377499,08/03/2018 01:16:00 PM,046XX S ST LAWRENCE AVE,2024,NARCOTICS,POSSESS - HEROIN (WHITE),VEHICLE NON-COMMERCIAL,true,false,0221,002,4,38,18,,,2018,09/10/2022 04:50:59 PM,,,\n12457866,JE332953,08/10/2021 04:20:00 PM,016XX W VAN BUREN ST,2018,NARCOTICS,MANUFACTURE / DELIVER - SYNTHETIC DRUGS,VEHICLE NON-COMMERCIAL,true,false,1231,012,27,28,18,,,2021,09/10/2022 04:50:59 PM,,,\n128"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-2-parsing-a-csv-file-using-a-package",
    "href": "extra/Live-03-Foundations_2.html#task-2-parsing-a-csv-file-using-a-package",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 2: Parsing a CSV file using a Package",
    "text": "Task 2: Parsing a CSV file using a Package\nRight, so we’ve got our data in datafile, what are we going to do with it now?\n\nTask 2.1: Research Solution\n\nWe need to turn it into data by reading the CSV\n\nGoogle what to do\nHmmm, this looks useful,\nMaybe also try read csv file python example\n\nOK, so it looks like we need to splitlines first.\nThen let’s read it into a list (What data structure is this?)\nNow, how would we print out the number of rows and columns?\n\n\nimport csv \n\nurlData = [] # Somewhere to store the data\n\ncsvfile = csv.reader(datafile.splitlines())\n\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(f\"urlData has {len(urlData)} rows and {len(urlData[0])} columns.\")\n\nurlData has 16956 rows and 22 columns.\n\n\n\n\nTask 2.2: Selecting a Sample Row\nHow could we print out a random row? Let’s Google it. - Library random seems promising - Which function do we want?\n\nimport random\ndir(random)\nhelp(random.randint)\n\nHelp on method randint in module random:\n\nrandint(a, b) method of random.Random instance\n    Return random integer in range [a, b], including both end points.\n\n\n\n\nurlData[random.randint(1,len(urlData))]\n\n['12812099',\n 'JF377432',\n '05/27/2022 12:00:00 AM',\n '063XX S EBERHART AVE',\n '1153',\n 'DECEPTIVE PRACTICE',\n 'FINANCIAL IDENTITY THEFT OVER $ 300',\n '',\n 'false',\n 'false',\n '0312',\n '003',\n '20',\n '42',\n '11',\n '',\n '',\n '2022',\n '10/07/2022 04:46:51 PM',\n '',\n '',\n '']"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-3-reading-file-as-dictionary-of-lists",
    "href": "extra/Live-03-Foundations_2.html#task-3-reading-file-as-dictionary-of-lists",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 3: Reading File as Dictionary of Lists",
    "text": "Task 3: Reading File as Dictionary of Lists\n\nTask 3.1: Finding the Header Row\nSometimes this is easy (it’s the very first row in a CSV file), but often (especially with Excel data from, e.g., the Office for National Statistics) it’s not. So here is where doing some quick, initial checks using head can be helpful.\n\nurlData[0]\n\n['ID',\n 'Case Number',\n 'Date',\n 'Block',\n 'IUCR',\n 'Primary Type',\n 'Description',\n 'Location Description',\n 'Arrest',\n 'Domestic',\n 'Beat',\n 'District',\n 'Ward',\n 'Community Area',\n 'FBI Code',\n 'X Coordinate',\n 'Y Coordinate',\n 'Year',\n 'Updated On',\n 'Latitude',\n 'Longitude',\n 'Location']\n\n\nOK, some questions: - What does row 0 give us? Are these data, or something else? - If we were making a dictionary-of-lists, how would we use Row 0? - What do we need to do to set this up?\n\n\nTask 3.2: Creating a DOL from Data\nHow would we use the header row to initialise our Dictionary-of-Lists.\n\nds = {}\ncol_names = urlData[0]\nfor c in col_names:\n    ds[c] = []\n\nNext… - How would we print out all of the column names? - How would we go about adding all of the data? - What kind of loop would this use?\n\nprint(ds.keys())\n\ndict_keys(['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude', 'Location'])\n\n\n\nfor r in urlData[1:len(urlData)]:\n    for c in range(0,len(col_names)):\n        ds[ col_names[c] ].append( r[c] )\n\n\n\nTask 3.3: Validating/Checking\nLet’s check a few columns to see if the data makes sense!\n\nprint(ds['Case Number'][:20])\nprint()\nprint(ds['Primary Type'][:20])\nprint()\n\n['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989', 'JF413871', 'JF380003', 'JF378902', 'JF415101', 'JF379014', 'JF378985', 'JF415451', 'JF415893', 'JF378620', 'JF380820', 'JF380344', 'JF380066', 'JF378882', 'JF379811', 'JF381657']\n\n['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n\n\n\nActually, is there a better way to do this? Rather than printing each out in turn, could we do this as a loop?\n\nfor c in ds.keys():\n    print(ds[c][:5])\n\n['11570600', '12457866', '12842196', '12842170', '12842271']\n['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989']\n['08/03/2018 01:16:00 PM', '08/10/2021 04:20:00 PM', '03/01/2020 12:00:00 AM', '02/28/2018 09:00:00 AM', '01/01/2014 08:00:00 PM']\n['046XX S ST LAWRENCE AVE', '016XX W VAN BUREN ST', '069XX S CALUMET AVE', '128XX S LOWE AVE', '035XX S STATE ST']\n['2024', '2018', '1153', '1153', '1153']\n['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n['POSSESS - HEROIN (WHITE)', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300']\n['VEHICLE NON-COMMERCIAL', 'VEHICLE NON-COMMERCIAL', 'RESIDENCE', 'RESIDENCE', 'CTA TRAIN']\n['true', 'true', 'false', 'false', 'false']\n['false', 'false', 'false', 'false', 'false']\n['0221', '1231', '0322', '0523', '0213']\n['002', '012', '003', '005', '002']\n['4', '27', '6', '9', '3']\n['38', '28', '69', '53', '35']\n['18', '18', '11', '11', '11']\n['', '', '', '', '']\n['', '', '', '', '']\n['2018', '2021', '2020', '2018', '2014']\n['09/10/2022 04:50:59 PM', '09/10/2022 04:50:59 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM']\n['', '', '', '', '']\n['', '', '', '', '']\n['', '', '', '', '']\n\n\nHow would we improve this?\n\nfor c in ds.keys():\n    print(f\"{c}:\\t{ds[c][:5]}\")\n    print()\n\nID: ['11570600', '12457866', '12842196', '12842170', '12842271']\n\nCase Number:    ['JB377499', 'JE332953', 'JF413864', 'JF413886', 'JF413989']\n\nDate:   ['08/03/2018 01:16:00 PM', '08/10/2021 04:20:00 PM', '03/01/2020 12:00:00 AM', '02/28/2018 09:00:00 AM', '01/01/2014 08:00:00 PM']\n\nBlock:  ['046XX S ST LAWRENCE AVE', '016XX W VAN BUREN ST', '069XX S CALUMET AVE', '128XX S LOWE AVE', '035XX S STATE ST']\n\nIUCR:   ['2024', '2018', '1153', '1153', '1153']\n\nPrimary Type:   ['NARCOTICS', 'NARCOTICS', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE', 'DECEPTIVE PRACTICE']\n\nDescription:    ['POSSESS - HEROIN (WHITE)', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'FINANCIAL IDENTITY THEFT OVER $ 300']\n\nLocation Description:   ['VEHICLE NON-COMMERCIAL', 'VEHICLE NON-COMMERCIAL', 'RESIDENCE', 'RESIDENCE', 'CTA TRAIN']\n\nArrest: ['true', 'true', 'false', 'false', 'false']\n\nDomestic:   ['false', 'false', 'false', 'false', 'false']\n\nBeat:   ['0221', '1231', '0322', '0523', '0213']\n\nDistrict:   ['002', '012', '003', '005', '002']\n\nWard:   ['4', '27', '6', '9', '3']\n\nCommunity Area: ['38', '28', '69', '53', '35']\n\nFBI Code:   ['18', '18', '11', '11', '11']\n\nX Coordinate:   ['', '', '', '', '']\n\nY Coordinate:   ['', '', '', '', '']\n\nYear:   ['2018', '2021', '2020', '2018', '2014']\n\nUpdated On: ['09/10/2022 04:50:59 PM', '09/10/2022 04:50:59 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM', '09/29/2022 04:48:23 PM']\n\nLatitude:   ['', '', '', '', '']\n\nLongitude:  ['', '', '', '', '']\n\nLocation:   ['', '', '', '', '']"
  },
  {
    "objectID": "extra/Live-03-Foundations_2.html#task-4-fixing-column-types",
    "href": "extra/Live-03-Foundations_2.html#task-4-fixing-column-types",
    "title": "Task 0: LoLs and DoLs",
    "section": "Task 4: Fixing Column Types",
    "text": "Task 4: Fixing Column Types\nOK, so we have a few columns that aren’t really of the right type. We have date-time types in Python that we’re not going to get stuck into now, but we also very obviously have numbers and booleans as well that we need to deal with!\nSo how we would do this? The process for each float would be the same. The process for each int would be the same. The process for each boolean would be the same. Sounds like a good opportunity for a function!\n\nTask 4.1: What Type Am I?\nGiven these data… What Python data type should each one be?\n\nID:\nCase Number:\nDate:\nPrimary Type:\nDescription:\nLocation Description:\nArrest:\nDomestic:\nYear:\nLatitude:\nLongitude:\n\n\n\nTask 4.2 Converting One Column\n\ndef to_bool(col_data):\n    fdata = []\n    for c in col_data:\n        fdata.append( c=='true' )\n    return fdata\n\nbool_cols = ['Arrest','Domestic']\nfor b in bool_cols:\n    ds[ b ] = to_bool( ds[b] )\n\n\nds['Arrest'][:6]\n\n[True, True, False, False, False, False]\n\n\n\nimport numpy as np\n\n\narrest_made = list(np.where(ds['Arrest']))[0]\n\narrest_reasons = set()\n\nfor a in arrest_made:\n    arrest_reasons.add(ds['Description'][a])\n\nprint(arrest_reasons)\nprint()\nprint(\", \".join([i.title() for i in sorted(arrest_reasons)]))\n\n{'POSSESS - CANNABIS MORE THAN 30 GRAMS', 'THEFT OF LABOR / SERVICES', 'MANUFACTURE / DELIVER - CANNABIS OVER 10 GRAMS', 'POSSESS - HALLUCINOGENS', 'MANUFACTURE / DELIVER - SYNTHETIC DRUGS', 'MANUFACTURE / DELIVER - BARBITURATES', 'THEFT OF LOST / MISLAID PROPERTY', 'POSSESS - HEROIN (TAN / BROWN TAR)', 'POSSESS - COCAINE', 'MANUFACTURE / DELIVER - HEROIN (TAN / BROWN TAR)', 'POSSESSION OF DRUG EQUIPMENT', 'BOGUS CHECK', 'CRIMINAL DRUG CONSPIRACY', 'POSSESS - PCP', 'ILLEGAL USE CASH CARD', 'MANUFACTURE / DELIVER -  HEROIN (WHITE)', 'MANUFACTURE / DELIVER - HEROIN (BLACK TAR)', 'SOLICIT NARCOTICS ON PUBLIC WAY', 'POSSESS - SYNTHETIC DRUGS', 'MANUFACTURE / DELIVER - COCAINE', 'FRAUD OR CONFIDENCE GAME', 'POSSESS - BARBITURATES', 'THEFT BY LESSEE, MOTOR VEHICLE', 'MANUFACTURE / DELIVER - CRACK', 'ILLEGAL POSSESSION CASH CARD', 'MANUFACTURE / DELIVER - AMPHETAMINES', 'FORGERY', 'FOUND SUSPECT NARCOTICS', 'POSSESS - AMPHETAMINES', 'STOLEN PROPERTY BUY / RECEIVE / POSSESS', 'COUNTERFEIT CHECK', 'CREDIT CARD FRAUD', 'EMBEZZLEMENT', 'POSSESS - HEROIN (WHITE)', 'FORFEIT PROPERTY', 'COUNTERFEITING DOCUMENT', 'FINANCIAL IDENTITY THEFT OVER $ 300', 'POSSESS - CANNABIS 30 GRAMS OR LESS', 'POSSESS - HEROIN (BLACK TAR)', 'DECEPTIVE COLLECTION PRACTICES', 'ATTEMPT - FINANCIAL IDENTITY THEFT', 'MANUFACTURE / DELIVER - HALLUCINOGEN', 'MANUFACTURE / DELIVER - PCP', 'POSSESS - HYPODERMIC NEEDLE', 'IMPERSONATION', 'POSSESS - CRACK', 'MANUFACTURE / DELIVER - CANNABIS 10 GRAMS OR LESS', 'MANUFACTURE / DELIVER - METHAMPHETAMINE', 'FINANCIAL EXPLOITATION OF AN ELDERLY OR DISABLED PERSON', 'UNAUTHORIZED VIDEOTAPING', 'SALE / DELIVER - DRUG PARAPHERNALIA', 'ATTEMPT POSSESSION NARCOTICS', 'POSSESS - METHAMPHETAMINE', 'ALTER / FORGE PRESCRIPTION'}\n\nAlter / Forge Prescription, Attempt - Financial Identity Theft, Attempt Possession Narcotics, Bogus Check, Counterfeit Check, Counterfeiting Document, Credit Card Fraud, Criminal Drug Conspiracy, Deceptive Collection Practices, Embezzlement, Financial Exploitation Of An Elderly Or Disabled Person, Financial Identity Theft Over $ 300, Forfeit Property, Forgery, Found Suspect Narcotics, Fraud Or Confidence Game, Illegal Possession Cash Card, Illegal Use Cash Card, Impersonation, Manufacture / Deliver -  Heroin (White), Manufacture / Deliver - Amphetamines, Manufacture / Deliver - Barbiturates, Manufacture / Deliver - Cannabis 10 Grams Or Less, Manufacture / Deliver - Cannabis Over 10 Grams, Manufacture / Deliver - Cocaine, Manufacture / Deliver - Crack, Manufacture / Deliver - Hallucinogen, Manufacture / Deliver - Heroin (Black Tar), Manufacture / Deliver - Heroin (Tan / Brown Tar), Manufacture / Deliver - Methamphetamine, Manufacture / Deliver - Pcp, Manufacture / Deliver - Synthetic Drugs, Possess - Amphetamines, Possess - Barbiturates, Possess - Cannabis 30 Grams Or Less, Possess - Cannabis More Than 30 Grams, Possess - Cocaine, Possess - Crack, Possess - Hallucinogens, Possess - Heroin (Black Tar), Possess - Heroin (Tan / Brown Tar), Possess - Heroin (White), Possess - Hypodermic Needle, Possess - Methamphetamine, Possess - Pcp, Possess - Synthetic Drugs, Possession Of Drug Equipment, Sale / Deliver - Drug Paraphernalia, Solicit Narcotics On Public Way, Stolen Property Buy / Receive / Possess, Theft By Lessee, Motor Vehicle, Theft Of Labor / Services, Theft Of Lost / Mislaid Property, Unauthorized Videotaping\n\n\n\n\nTask 4.3: Converting Another Column\nFloats first!\n\nimport numpy as np\n\ndef to_float(col_data):\n    fdata = []\n    for c in col_data:\n        try:\n            fdata.append( float(c) )\n        except (ValueError, TypeError):\n            fdata.append( np.nan )\n    return fdata\n\nfloat_cols = ['Latitude','Longitude']\nfor f in float_cols:\n    ds[ f ] = to_float( ds[f] )\n\n\nds['Latitude'][:6]\n\n[nan, nan, nan, nan, nan, nan]\n\n\n\nprint(f\"Have found {len(list(np.where(~np.isnan(ds['Latitude'])))[0]):,} records with lat/long coordinates\")\n\nHave found 9,801 records with lat/long coordinates\n\n\n\nfor i in list(np.where(~np.isnan(ds['Latitude'])))[0][:15]:\n    print(ds['Latitude'][i])\n\n41.885739108\n41.893676531\n41.885931086\n41.786188054\n41.788414065\n41.701933304\n41.976290414\n41.772998982\n41.881857098\n41.722597651\n41.80184271\n41.763268313\n41.986788545\n41.793924823\n41.894327846\n\n\nThen booleans!\nThen ints! (Drop if time is short)\n\ndef to_int(col_data):\n    fdata = []\n    for c in col_data:\n        fdata.append( int(c) )\n    return fdata\n\nint_cols = ['ID','Year']\nfor i in int_cols:\n    ds[ i ] = to_int( ds[i] )\n\n\nds['Year'][:6]\n\n[2018, 2021, 2020, 2018, 2014, 2020]\n\n\n\nprint(f\"There are {len(np.where(np.asarray(ds['Arrest']))[0]):,} arrests\")\nprint(f\"There are {len(set(np.where(np.asarray(ds['Year']) &gt; 2021)[0])):,} records from 2022 onwards\")\n\nThere are 4,108 arrests\nThere are 13,845 records from 2022 onwards\n\n\n\nintersect = set(np.where(np.asarray(ds['Arrest']))[0]).intersection(set(np.where(np.asarray(ds['Year']) &gt; 2021)[0]))\nprint(f\"There are {len(intersect):,} records that are both.\")\n\nThere are 3,058 records that are both."
  },
  {
    "objectID": "assessments/resources.html",
    "href": "assessments/resources.html",
    "title": "Resources",
    "section": "",
    "text": "To get you started, we’ve created a set of templates that you’ll want to keep together in your Git/GitHub repo. You are free to modify the template and its defaults as needed for stylistic or other purposes.\n\nThe template file (.qmd) that we have provided: Group_Work.qmd. You can see both PDF and HTML output (and may find rendering to HTML faster when writing and testing), but please only submit the raw QMD and rendered PDF in their respective areas. Zip files and other submissions of multiple files will be ignored.\nThe CSL file (.csl) that governs how references are done: harvard-cite-them-right.csl.\nThe BibTeX file (.bib) that will contain any references you use: bio.bib (you may also make use of the one used for the module to simplify the creation/management of your bibliography)\n\nNote that in our reproducibility tests we are starting with only the qmd file since you could choose to change the CSL style and will be producing your own references.\n\n\n\n\n\n\nReally Look at the Template!\n\n\n\nYou’ll notice that the template specifies three fonts (mainfont, monofont, sansfont). These are installed in the Docker image. You are welcome to change the fonts used and can see what’s available in the existing Docker image by asking matplotlib or using the Terminal… which will help you to learn about font-management.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Resources"
    ]
  },
  {
    "objectID": "assessments/resources.html#the-templates",
    "href": "assessments/resources.html#the-templates",
    "title": "Resources",
    "section": "",
    "text": "To get you started, we’ve created a set of templates that you’ll want to keep together in your Git/GitHub repo. You are free to modify the template and its defaults as needed for stylistic or other purposes.\n\nThe template file (.qmd) that we have provided: Group_Work.qmd. You can see both PDF and HTML output (and may find rendering to HTML faster when writing and testing), but please only submit the raw QMD and rendered PDF in their respective areas. Zip files and other submissions of multiple files will be ignored.\nThe CSL file (.csl) that governs how references are done: harvard-cite-them-right.csl.\nThe BibTeX file (.bib) that will contain any references you use: bio.bib (you may also make use of the one used for the module to simplify the creation/management of your bibliography)\n\nNote that in our reproducibility tests we are starting with only the qmd file since you could choose to change the CSL style and will be producing your own references.\n\n\n\n\n\n\nReally Look at the Template!\n\n\n\nYou’ll notice that the template specifies three fonts (mainfont, monofont, sansfont). These are installed in the Docker image. You are welcome to change the fonts used and can see what’s available in the existing Docker image by asking matplotlib or using the Terminal… which will help you to learn about font-management.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Resources"
    ]
  },
  {
    "objectID": "assessments/resources.html#rendering",
    "href": "assessments/resources.html#rendering",
    "title": "Resources",
    "section": "Rendering",
    "text": "Rendering\nThe method for rendering the document will be the same on your computer as on ours, this is how we are testing for reproducibility. From the Terminal in Docker you use the following command to render a document in the current directory:\nquarto render Group_Work.qmd --to pdf \nThis is the command we will be running to generate the PDF output as well, though you can change the name of the input file if you like.\nHowever, there are a few things to note:\n\nWe run this command in the same directory as your QMD file (i.e. in Docker we would run cd work/&lt;your group's name&gt;/ then quarto render &lt;your group's file&gt;.qmd --to pdf).\nWe will have only your raw QMD file and no other files, so you need to ensure that all supporting files, data, and code are downloaded near the start of the QMD file to be available for inclusion in the render pipeline.\nYou will want to test that your QMD file renders correctly on a ‘new’ machine or ‘clean’ folder before submitting it.\n\nGood luck!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Resources"
    ]
  },
  {
    "objectID": "assessments/conflicts.html",
    "href": "assessments/conflicts.html",
    "title": "Dealing with Conflict",
    "section": "",
    "text": "Group Disputes\n\n\n\nIn the event that there is irreconcilable disagreement within a group, we will use GitHub to determine contributions and inform individual marks.\nWorking on a group project through Git and GitHub will invetiably result in version control conflicts: two (or more) students will have made changes that need to be reconciled using Git’s conflict resolution mechanisms. Depending on the nature of the conflict, these can be trivial or very, very hard to resolve. If you think about it, the obvious conflicts will broadly fall into three classes:\nMaking this all a lot harder is that a ‘change’ could be anything from a direct edit to a section of a line, to deleting a file or folder! There’s a lot more to this, and there are undoubtedly lots of good examples of about (e.g. Example 1, Example 2).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#spotting-a-conflict",
    "href": "assessments/conflicts.html#spotting-a-conflict",
    "title": "Dealing with Conflict",
    "section": "Spotting a Conflict",
    "text": "Spotting a Conflict\nYou will most likely discover a conflict when pushing local changes or pulling remote changes. So let’s imagine that you’ve done something like this:\ngit add [file]\ngit commit -m \"[your message]\"\ngit push\nYou then see the message:\nfailed to push some refs to [url].\nUpdates were rejected because the remote contains work that you do not have locally.  \nSince you can’t push, what do you do?",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-1-hope-for-the-best",
    "href": "assessments/conflicts.html#option-1-hope-for-the-best",
    "title": "Dealing with Conflict",
    "section": "Option 1: Hope for the Best",
    "text": "Option 1: Hope for the Best\nIf you’re lucky then some conflicts can be resolved simply with a git pull. This works when there are remote changes that have zero impact on your local changes. Pulling the remote changes will update your local copy and then you can just turn around and push your local changes back to the remote (usually: GitHub).\nWhat do you do if that doesn’t work?",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-2-resolve-conflicts-locally",
    "href": "assessments/conflicts.html#option-2-resolve-conflicts-locally",
    "title": "Dealing with Conflict",
    "section": "Option 2: Resolve Conflicts Locally",
    "text": "Option 2: Resolve Conflicts Locally\nFetch the latest changes from the remote repository:\ngit fetch origin\nMerge the remote version of the repository (that you just fetched) with the local version:\ngit merge origin/main (or git merge origin/master)\n\n\n\n\n\n\nWhat Branch?\n\n\n\nThe command above assumes that you are working in the main branch, which is the most likely branch unless your group is very, very advanced. But how do you know what branch you’re working on?\nRunning git branch --show-current will give you the currently active branch. The other way (git branch) shows you a list of local branches, the one with a * next to it is the one you’re currently using. To exit the list of branches just hit the letter q for ‘Quit’.\n\n\nYou will likely then see a message similar to:\nCONFLICT (content): Merge conflict in your-file.md\n\nFormat of a Conflict\nWhen you have a conflict, the format of a conflict is (roughly) as follows2:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:file.txt\nHello world\n=======\nGoodbye\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 77976da35a11db4580b80ae27e8d65caf5208086:file.txt\nThe first section (from &lt;&lt;&lt;&lt; HEAD to ====) is what you have in your local file. The second section (from ==== to &gt;&gt;&gt;&gt; &lt;hexadecimal number&gt;) is the change that is coming from the remote repository that Git wants to merge on to your local file but can’t because of the changes that you’ve made locally.\nOpen the conflicting files and manually edit them. You will see annotations like this indicating where there are conflicts:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; origin/main\nChange #1\n=======\nChange #2\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\nResolve the conflicts and remove the annotations (the &lt;&lt;&lt;..., ===... and &gt;&gt;&gt;... lines):\nChange #1\nChange #2\nCommit your changes and push the merged results back to the remote repository.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#option-3-open-a-pull-request",
    "href": "assessments/conflicts.html#option-3-open-a-pull-request",
    "title": "Dealing with Conflict",
    "section": "Option 3: Open a Pull Request",
    "text": "Option 3: Open a Pull Request\nTo resolve, create a new branch:\ngit checkout -b new-branch-name\nCheck that you are in the new branch:\ngit branch -v\nCheck that your local changes are committed on to the new branch and then push the new branch to the remote repository:\ngit push --set-upstream origin new-branch-name\nYou then create a Pull Request on Github (you will see a green button pop up “Compare & Pull Request”):\n\nGit will say “Can’t automatically merge”. That’s OK. Click “Create pull request”.\nGit will “Check for ability to merge automatically” and will then say:\n\n“This branch has conflicts that must be resolved”\nClick “Resolve conflicts”\nGit will show you text like this that shows the conflicting changes in each branch and the resolution process is the same as above:\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; new-branch-name\nYet another one\n=======\nAnother one\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n\nNow that you’ve resolved the conflict, click “Mark as resolved” and “Commit merge”.\nThen, “Merge pull request” and “Confirm merge”.\n\nWhen you go back to the main page of your repository, you will see the new changes.\nOne more thing! On your computer, make sure to exit your branch and return to the main (or master) branch.\ngit checkout main\nThen, pull your latest changes from the Github repository.\ngit pull\nCongratulations (🎉), you have successfully resolved a merge conflict!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/conflicts.html#footnotes",
    "href": "assessments/conflicts.html#footnotes",
    "title": "Dealing with Conflict",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example: you’ve deleted lines 6-9 and changed line 10, the other person has modified lines 17-18 of the same file, so Git needs to determine that 17-18 of the other change maps on to 14-15 of your change.↩︎\nTaken from: https://stackoverflow.com/a/7901901↩︎",
    "crumbs": [
      "Elements",
      "Group Work",
      "Dealing with Conflict"
    ]
  },
  {
    "objectID": "assessments/Rubric-Group.html",
    "href": "assessments/Rubric-Group.html",
    "title": "Assessment #2: Reproducible Analysis",
    "section": "",
    "text": "Deadline: Tuesday, 16 December 2025\nWeighting:\n\nReproducibility: 25% of module grade;\nContent: 35%) of module grade.\n\nFormat: PDF and QMD (.qmd file) submitted to Moodle. See online guidance for details."
  },
  {
    "objectID": "assessments/Rubric-Group.html#key-facts",
    "href": "assessments/Rubric-Group.html#key-facts",
    "title": "Assessment #2: Reproducible Analysis",
    "section": "",
    "text": "Deadline: Tuesday, 16 December 2025\nWeighting:\n\nReproducibility: 25% of module grade;\nContent: 35%) of module grade.\n\nFormat: PDF and QMD (.qmd file) submitted to Moodle. See online guidance for details."
  },
  {
    "objectID": "assessments/Rubric-Group.html#overview",
    "href": "assessments/Rubric-Group.html#overview",
    "title": "Assessment #2: Reproducible Analysis",
    "section": "2 Overview",
    "text": "2 Overview\nThe marking scheme for this submission has two parts:\n\nThe Content (35% of total mark for this submission) will be assessed as an essay incorporating analytical elements; however, please note that a great deal of consideration will be given to the language, presentation, and content of the submission as befits a data-led briefing for a busy politician (the Mayor of London) and their policy-makers.\nThe Reproducibility (25% of total mark for this submission) will be assessed on the basis of how well it runs (its portability and robustness) and the quality of code (its efficiency and legibility).\n\nThe evaluation criteria span the skillset expected of a practicing analyst or data scientist: it is not just about writing code, but about selecting, developing, and communicating evidence with your ‘end user’ in mind, as well as demonstrating an understanding of how to give others within your organisation or team confidence in the results that you present.\nThe models provided should assist in determining whether you will do well in this area."
  },
  {
    "objectID": "assessments/Rubric-Group.html#rubric",
    "href": "assessments/Rubric-Group.html#rubric",
    "title": "Assessment #2: Reproducible Analysis",
    "section": "3 Rubric",
    "text": "3 Rubric\n\n3.1 Policy Briefing\n\n3.1.1 Question and Framing (20% of total mark)\nThis aspect is focussed on your ability to select and frame a research question or problem that is relevant to a specified audience.\n\n\n\n\n\n\n\nGuidance\nGrade\n\n\n\n\nThe research problem has been well-developed and -framed for the specified audience. There is a broad range of background research entailing the use of diverse sources to produce a convincing framing of the topic. The briefing is clearly of compelling interest to the audience.\nDistinction (&gt;=70%)\n\n\n\n\n\n\nThe research problem has been developed for the specified audience. There is a range of background research used to produce a relevant framing of the topic. The briefing is clearly of interest to the audience, but makes some assumptions that suggest more limited reasoning about the audience.\nMerit (60–69%)\n\n\n\n\n\n\nThe research problem is of limited interest to the specified audience. There is a limited range of background research used to support the framing of the topic. Parts of the briefing are demonstrably of interest to the audience but this is only weakly-communicated or largely implicit.\nPass (50-59%)\n\n\n\n\n\n\nThe research problem is not obviously of interest to the specified audience. There is a very limited range of background research used to support the framing of the topic. Few, if any, efforts are mode to make the briefing relevant to the audience, and the overall framing is weak and/or illogical.\nFail (&lt;50%)\n\n\n\n\n\n3.1.2 Analysis & Interpretation (30% of total mark)\nThis aspect is focussed on your ability to choose and employ the analytical tools covered in this and other modules (you are free to draw on knowledge developed in Quantitative Methods and GIS) to support your investigation of the research question.\n\n\n\n\n\n\n\nEvaluation\nGrade\n\n\n\n\nThe analysis is excellent and entirely appropriate to the problem. It demonstrates a comprehensive technical understanding of how to select and deploy technical and statistical approaches in support of an analysis. Interpretations of results are very well constructed, clear and focused in relation to the research problem/framing. Maps, figures, and tables are excellent additions to the text and all work together to communicate effectively with the specified audience.\nDistinction (&gt;=70%)\n\n\n\n\n\n\nThe analysis is good and largely appropriate to the problem. It demonstrates a sound technical understanding of how to select and deploy technical and statistical approaches in support of an analysis, but with some opportunities for additional learning. Interpretations of results are logical and contain a well constructed discussion with linkage to the research problem. Maps, figures, and tables are appropriate, but could be improved upon or may contain minor errors.\nMerit (60–69%)\n\n\n\n\n\n\nThe analysis is relevant to the problem but lacks focus and/or clarity. It demonstrates sound understanding of individual technical and statistical approaches, but those selected may not be the most relevant to the research problem or could be better-specified. Interpretations of results are largely correct but need to be better-connected to the research problem. Maps, figures, and tables are adequate but more appropriate visualisations could have been implemented or may containt multiple errors that detract from their usefulness.\nPass (50-59%)\n\n\n\n\n\n\nThe analysis does not address the problem effectively and demonstrates that the material covered in class has not been understood. It demonstrates limited understanding of individual technical and statistical approaches, and those selected may not address the research problem or may be incorrectly-specified. Interpretations of results may not be correct or may not have a bearing on the research problem. Maps, figures, and tables are of poor quality and do not aid understanding in any way.\nFail (&lt;50%)\n\n\n\n\n\n3.1.3 Structure and Presentation (10% of total mark)\nThis aspect focusses on the attention you’ve given to communicating your results in a manner that is appropriate to the target audience.\n\n\n\n\n\n\n\nGuidance\nGrade\n\n\n\n\nThe briefing demonstrates an excellent awareness of its audience, and excellently balances the elements of a model data-led briefing. The linkage between sections is obvious and there is a strong narrative throughout. Tables, figures, and other elements are produced at a very high level with attention to details of content, colour, layout, and related aspects.\nDistinction (&gt;=70%)\n\n\n\n\n\n\nThe briefing demonstrates a good awareness of its audience, and there is a clear and logical structure to the data-led briefing. Attempts to link sections as part of an overall narrative are evident, but could be improved by further reflection about the audience and topic. Tables, figures, and other elements show some attention to details of content, colour, layout, and related aspects.\nMerit (60–69%)\n\n\n\n\n\n\nThe report has an obvious structure but demonstrates a limited awareness of its audience. Connections between sections are weakly-articulated and the overall narrative is lost. Tables, figures, and other elements are of high quality but show little attention to detail.\nPass (50-59%)\n\n\n\n\n\n\nThe report is weakly structured and demonstrates a very limited awareness of its audience. There is no obvious linkage between sections and little narrative throughout. Tables, figures, and other elements are of poor quality and are difficult to read.\nFail (&lt;50%)\n\n\n\n\n\n\n3.2 Reproducible Analysis\n\n3.2.1 Reproducibility (20% of total mark)\nThis aspect focusses on the attention you’ve given to ensuring that your analysis can be run on another system in two ways: 1) without changes being necessary to the code; and 2) with useful guidance being offered in terms of what to do if ‘things go wrong’. So this requires thinking about both ‘portability’ and how to ensure that a user can recover from a predictable failure.\n\n\n\n\n\n\n\nEvaluation\nGrade\n\n\n\n\nYour workbook runs without errors and obvious consideration has been given to ways that your code might fail on another system as well as the processes that could mitigate this. Documentation/comments neatly balance concision with readability.\nDistinction(&gt;=70%)\n\n\n\n\n\n\nYour workbook runs without errors and there is some consideration of ways that your code might fail on another system as well as the processes that could mitigate this. Documentation/comments are generally good but may be uneven or excessive.\nMerit (60–69%)\n\n\n\n\n\n\nYour workbook runs with minor errors (e.g. a path variable or missed library import from the SDS Docker image) that are quickly and easily resolved. Documentation/comments may be largely missing or gnomic when present.\nPass (50-59%)\n\n\n\n\n\n\nYour workbook does not run without significant edits to the code, such as installing a missing library or a failed download. Documentation/comments are minimal or incomprehensible.\nFail (&lt;50%)\n\n\n\nIf you have, for instance, a NLP analysis that would take hours to run then it is acceptable to provide a partially-processed data file (i.e. at the point where the time-consuming analysis is complete) and to ‘comment out’ the cells that generated this data, but you must include the code used to generate this data.\n\n\n3.2.2 Quality (20% of total mark)\nThis aspect focusses on the attention you’ve given to creating an analysis that is intelligible and well-presented such that another user doesn’t just ‘run the code’ but actually understands and has confidence in the how and why of what you’ve done. This is about making maximal use of the tools at your disposal such that the code is efficient, even elegant, and the outputs are clear and legible.\n\n\n\n\n\n\n\nEvaluation\nGrade\n\n\n\n\nAnother data scientist could understand and use your code with minimal effort and would appreciate the overall ‘style’ of the notebook. The code is well-organised, with each step of the analysis undertaken in a logical, structured way with little or no duplication or confusion. The code is highly-efficient, demonstrating a comprehensive understanding of the best practices taught on the course (e.g. intelligent creation of functions where appropriate, iterations to work efficiently with data/loops, and/or includes advanced programming techniques). Clever and creative use of the packages taught on the course.\nDistinction (&gt;=70%)\n\n\n\n\n\n\nAnother data scientist could understand and use your code with some effort but would find the overall ‘style’ of the notebook to be both clear and consistent. The code is generally well-organised, with most steps of the analysis undertaken in a logical, structured way with minimal duplication or confusion. The code is reasonably efficient, demonstrating a good understanding of the best practices taught on the course (e.g. intelligent creation of functions where appropriate, iterations to work efficiently with data/loops, and/or includes advanced programming techniques). Appropriate use of the packages taught on the course.\nMerit (60–69%)\n\n\n\n\n\n\nAnother data scientist could understand and use your code only with significant effort and would find the overall ‘style’ of the notebook to be both unclear and inconsistent. The code is could be better-organised, with greater structure leading to reduced duplication or confusion. The code could be more efficient, suggesting a fairly basic understanding of the best practices taught on the course (e.g. intelligent creation of functions where appropriate, iterations to work efficiently with data/loops, and/or includes advanced programming techniques). Weak use of the packages taught on the course.\nPass (50-59%)\n\n\n\n\n\n\nAnother data scientist could not understand and use your code, would not be inclined to invest the effort in doing so as they would find the overall ‘style’ of the notebook to be fundamentally unclear and confusingly inconsistent. The code is disorganised, with a substantial amount of duplication and confusion. The code is inefficient, suggesting little or no understanding of the best practices taught on the course (e.g. intelligent creation of functions where appropriate, iterations to work efficiently with data/loops, and/or includes advanced programming techniques). Fundamental errors are made in the use of the packages taught on the course.\nFail (&lt;50%)"
  },
  {
    "objectID": "assessments/Project_Outline.html",
    "href": "assessments/Project_Outline.html",
    "title": "Project Outline Form",
    "section": "",
    "text": "This form is intended to help you organise your thoughts and to develop an outline that can be used: a) to ensure that the group as a whole understands and agrees the briefing topic; and b) to share with the lecturer for high-level feedback. Filling in this form is not mandatory and you will not be graded. For each of the questions below, try to anticipate problems, questions that the intended audience might have, and practical issues for your group of deadlines and travel,\nFuthermore, the development of a good outline does not guarantee a good mark on the final submission, nor does robust feedback imply failure; rather, it is a chance to triangulate the briefing’s aims, agree a workplan, and begin collaborating on the final assessment before you disperse at the end of the Term.\nIf your group would like feedback on your outline, please have it ready by midway through the final ‘live’ session."
  },
  {
    "objectID": "assessments/Project_Outline.html#what-is-the-briefing-topic",
    "href": "assessments/Project_Outline.html#what-is-the-briefing-topic",
    "title": "Project Outline Form",
    "section": "What is the briefing topic?",
    "text": "What is the briefing topic?\nIn two sentences, outline your briefing topic for the intended audience."
  },
  {
    "objectID": "assessments/Project_Outline.html#how-will-it-meet-the-assessments-stated-aims",
    "href": "assessments/Project_Outline.html#how-will-it-meet-the-assessments-stated-aims",
    "title": "Project Outline Form",
    "section": "How will it meet the assessment’s stated aims?",
    "text": "How will it meet the assessment’s stated aims?\nWhy would this topic be of interest to the intended audience?"
  },
  {
    "objectID": "assessments/Project_Outline.html#what-data-sets-does-it-use",
    "href": "assessments/Project_Outline.html#what-data-sets-does-it-use",
    "title": "Project Outline Form",
    "section": "What data sets does it use?",
    "text": "What data sets does it use?\nWhat data will this briefing require in order to achieve its aims and what are the strengths and limitations of these data?"
  },
  {
    "objectID": "assessments/Project_Outline.html#what-techniques-do-you-expect-to-use",
    "href": "assessments/Project_Outline.html#what-techniques-do-you-expect-to-use",
    "title": "Project Outline Form",
    "section": "What techniques do you expect to use?",
    "text": "What techniques do you expect to use?\nWhich of the techniques covered in-class (or not in class!) do you expect to use in order to write the briefing and why?"
  },
  {
    "objectID": "assessments/Project_Outline.html#what-findings-do-you-expect-and-how-will-you-present-them",
    "href": "assessments/Project_Outline.html#what-findings-do-you-expect-and-how-will-you-present-them",
    "title": "Project Outline Form",
    "section": "What findings do you expect and how will you present them?",
    "text": "What findings do you expect and how will you present them?\nOn the assumption that you find what you expect, how will you organise and present your findings in a succinct, compelling way?"
  },
  {
    "objectID": "assessments/Project_Outline.html#workplan",
    "href": "assessments/Project_Outline.html#workplan",
    "title": "Project Outline Form",
    "section": "Workplan",
    "text": "Workplan\nDiscuss and agree who will do what, and how you will coordinate your activity to ensure that your briefing is completed in time."
  },
  {
    "objectID": "assessments/Group_Work.html",
    "href": "assessments/Group_Work.html",
    "title": "Group Name’s Group Project",
    "section": "",
    "text": "We, [insert your group’s names], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\nDate:\nStudent Numbers:"
  },
  {
    "objectID": "assessments/Group_Work.html#declaration-of-authorship",
    "href": "assessments/Group_Work.html#declaration-of-authorship",
    "title": "Group Name’s Group Project",
    "section": "",
    "text": "We, [insert your group’s names], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\nDate:\nStudent Numbers:"
  },
  {
    "objectID": "assessments/Group_Work.html#brief-group-reflection",
    "href": "assessments/Group_Work.html#brief-group-reflection",
    "title": "Group Name’s Group Project",
    "section": "Brief Group Reflection",
    "text": "Brief Group Reflection\n\n\n\nWhat Went Well\nWhat Was Challenging\n\n\n\n\nA\nB\n\n\nC\nD"
  },
  {
    "objectID": "assessments/Group_Work.html#priorities-for-feedback",
    "href": "assessments/Group_Work.html#priorities-for-feedback",
    "title": "Group Name’s Group Project",
    "section": "Priorities for Feedback",
    "text": "Priorities for Feedback\nAre there any areas on which you would appreciate more detailed feedback if we’re able to offer it?"
  },
  {
    "objectID": "assessments/Group_Work.html#who-collected-the-insideairbnb-data",
    "href": "assessments/Group_Work.html#who-collected-the-insideairbnb-data",
    "title": "Group Name’s Group Project",
    "section": "1. Who collected the InsideAirbnb data?",
    "text": "1. Who collected the InsideAirbnb data?\n\n( 2 points; Answer due Week 7 )\n\nAn inline citation example: As discussed on “Inside airbnb” (n.d.), there are many…\nA parenthetical citation example: There are many ways to research Airbnb (see, for example, “Inside airbnb,” n.d.)…"
  },
  {
    "objectID": "assessments/Group_Work.html#why-did-they-collect-the-insideairbnb-data",
    "href": "assessments/Group_Work.html#why-did-they-collect-the-insideairbnb-data",
    "title": "Group Name’s Group Project",
    "section": "2. Why did they collect the InsideAirbnb data?",
    "text": "2. Why did they collect the InsideAirbnb data?\n\n( 4 points; Answer due Week 7 )\n\nOne of way to embed output in the text looks like this: after cleaning, we were left with 85,127 rows of data.\nThis way is also supposed to work ({python} f\"{df.shape[0]:,}\") but I’ve found it less reliable."
  },
  {
    "objectID": "assessments/Group_Work.html#how-did-they-collect-it",
    "href": "assessments/Group_Work.html#how-did-they-collect-it",
    "title": "Group Name’s Group Project",
    "section": "3. How did they collect it?",
    "text": "3. How did they collect it?\n\n( 5 points; Answer due Week 8 )"
  },
  {
    "objectID": "assessments/Group_Work.html#how-does-the-method-of-collection-q3-impact-the-completeness-andor-accuracy-of-the-insideairbnb-data-how-well-does-it-represent-the-process-it-seeks-to-study-and-what-wider-issues-does-this-raise",
    "href": "assessments/Group_Work.html#how-does-the-method-of-collection-q3-impact-the-completeness-andor-accuracy-of-the-insideairbnb-data-how-well-does-it-represent-the-process-it-seeks-to-study-and-what-wider-issues-does-this-raise",
    "title": "Group Name’s Group Project",
    "section": "4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?",
    "text": "4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?\n\n( 11 points; Answer due Week 9 )"
  },
  {
    "objectID": "assessments/Group_Work.html#what-ethical-considerations-does-the-use-of-the-insideairbnb-data-raise",
    "href": "assessments/Group_Work.html#what-ethical-considerations-does-the-use-of-the-insideairbnb-data-raise",
    "title": "Group Name’s Group Project",
    "section": "5. What ethical considerations does the use of the InsideAirbnb data raise?",
    "text": "5. What ethical considerations does the use of the InsideAirbnb data raise?\n\n( 18 points; Answer due Tuesday, 16 December 2025 )"
  },
  {
    "objectID": "assessments/Group_Work.html#with-reference-to-the-insideairbnb-data-i.e.-using-numbers-figures-maps-and-descriptive-statistics-what-does-an-analysis-of-hosts-and-the-types-of-properties-that-they-list-suggest-about-the-nature-of-airbnb-lettings-in-london",
    "href": "assessments/Group_Work.html#with-reference-to-the-insideairbnb-data-i.e.-using-numbers-figures-maps-and-descriptive-statistics-what-does-an-analysis-of-hosts-and-the-types-of-properties-that-they-list-suggest-about-the-nature-of-airbnb-lettings-in-london",
    "title": "Group Name’s Group Project",
    "section": "6. With reference to the InsideAirbnb data (i.e. using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London?",
    "text": "6. With reference to the InsideAirbnb data (i.e. using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London?\n\n( 15 points; Answer due Tuesday, 16 December 2025 )"
  },
  {
    "objectID": "assessments/Group_Work.html#drawing-on-your-previous-answers-and-supporting-your-response-with-evidence-e.g.-figures-maps-edaesda-and-simple-statistical-analysismodels-drawing-on-experience-from-e.g.-casa0007-how-could-the-insideairbnb-data-set-be-used-to-inform-the-regulation-of-short-term-lets-stl-in-london",
    "href": "assessments/Group_Work.html#drawing-on-your-previous-answers-and-supporting-your-response-with-evidence-e.g.-figures-maps-edaesda-and-simple-statistical-analysismodels-drawing-on-experience-from-e.g.-casa0007-how-could-the-insideairbnb-data-set-be-used-to-inform-the-regulation-of-short-term-lets-stl-in-london",
    "title": "Group Name’s Group Project",
    "section": "7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how could the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?",
    "text": "7. Drawing on your previous answers, and supporting your response with evidence (e.g. figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how could the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London?\n\n( 45 points; Answer due Tuesday, 16 December 2025 )"
  },
  {
    "objectID": "assessments/Group_Work.html#sustainable-authorship-tools",
    "href": "assessments/Group_Work.html#sustainable-authorship-tools",
    "title": "Group Name’s Group Project",
    "section": "Sustainable Authorship Tools",
    "text": "Sustainable Authorship Tools\nUsing the Terminal in Docker, you compile the Quarto report using quarto render &lt;group_submission_file&gt;.qmd.\nYour QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.\nWritten in Markdown and generated from Quarto. Fonts used: Spectral (mainfont), Roboto (sansfont) and JetBrains Mono (monofont)."
  },
  {
    "objectID": "assessments/Rubric-Audit.html",
    "href": "assessments/Rubric-Audit.html",
    "title": "Assessment #1: Data Audit Marking Rubric",
    "section": "",
    "text": "Deadline: Tuesday, 16 December 2025\nWeighting: 30% of module grade.\nFormat: PDF submitted to Moodle. See online guidance for details."
  },
  {
    "objectID": "assessments/Rubric-Audit.html#key-facts",
    "href": "assessments/Rubric-Audit.html#key-facts",
    "title": "Assessment #1: Data Audit Marking Rubric",
    "section": "",
    "text": "Deadline: Tuesday, 16 December 2025\nWeighting: 30% of module grade.\nFormat: PDF submitted to Moodle. See online guidance for details."
  },
  {
    "objectID": "assessments/Rubric-Audit.html#marking",
    "href": "assessments/Rubric-Audit.html#marking",
    "title": "Assessment #1: Data Audit Marking Rubric",
    "section": "2 Marking",
    "text": "2 Marking\nMarks will be assigned using the following scale:\n\nWeak: Little or no argument or response presented; it is, at best, a gesture towards a critical engagement with the substance of the assessment. There is little or no evidence of doing the assigned readings.\nFair: A sound argument or response; it demonstrates a basic level of critical engagement with the substance of the assessment. There is evidence of completing the assigned readings, but not of the deeper integration that comes from reflection.\nExcellent: An outstanding argument or response; it engages critically and reflectively with the substance of the assessment. Not only is there evidence of the completion of the assigned readings, there is clear evidence of deeper integration through reflection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nPoints\nWord Count\nWeak\nFair\nExcellent\n\n\n\n\n1\nWho collected the data?\n2\n25\n0\n1-2\nN/A\n\n\n2\nWhy did they collect it?\n4\n50\n0-1\n2-4\nN/A\n\n\n3\nHow was it collected?\n7\n75\n0-3\n4-7\nN/A\n\n\n4\nWhat useful information does it contain?\n12\n100\n0-3\n4-8\n9-12\n\n\n5\nTo what extent is the data ‘complete’?\n25\n200\n0-7\n8-17\n18-25\n\n\n6\nWhat kinds of analysis would this support?\n15\n200\n0-5\n6-10\n11-15\n\n\n7\nWhich of these are ethical?\n35\n350\n0-10\n11-24\n25-35\n\n\n\nNote: the word counts provided are indicative."
  },
  {
    "objectID": "assessments/code.html",
    "href": "assessments/code.html",
    "title": "Code",
    "section": "",
    "text": "The Reproducible Analysis is worth 25% of your module grade.\nThe Reproducible Analysis must be written using Python in a Quarto Markdown Document (QMD file). You are free to draw on concepts and methods covered in both Quantitative Methods and GIS, but must still write the code in Python (e.g. adapting something from R in the GIS module to Python).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#preparing-your-submission",
    "href": "assessments/code.html#preparing-your-submission",
    "title": "Code",
    "section": "Preparing Your Submission",
    "text": "Preparing Your Submission\nYou are expected to use sustainable authorship tools for this submission. You may be asked to provide evidence of this. A template will be provided, and you should also look at the Quarto Guide and, in particular, the PDF options in order to customise your project.\nYou are strongly advised to develop and maintain the submission in GitHub so that we can review contributions if necessary. In the absence of a GitHub commit history or everything being committed by only one member of the group we will be unable to determine individual contributions when marking the submission as a whole.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#how-we-measure-reproducibility",
    "href": "assessments/code.html#how-we-measure-reproducibility",
    "title": "Code",
    "section": "How We Measure Reproducibility",
    "text": "How We Measure Reproducibility\nYou will be submitting a runnable markdown document (.qmd file) that we will run on our own computers in order to generate a PDF output. If you have made use of one or more libraries that are not part of the Docker image then you can install these using ! pip install; however, if you take this approach then you should also ‘place nice’ by checking first to see if the library is already installed using try... except code that you can find on Stack Overflow and elsewhere (you will need to look this up).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#data-and-resources-used",
    "href": "assessments/code.html#data-and-resources-used",
    "title": "Code",
    "section": "Data and Resources Used",
    "text": "Data and Resources Used\nIt is also up to you to ensure that all relevant data are available via a valid URL for downloading and running. You may host your data anywhere you like, but please bear in mind that the markers will be based in the U.K. so some servers may be inaccessible. For very small data sets we’d recommend a GitHub repo, but for larger ones a Dropbox or OneDrive link would be more appropriate (you will need to check that the link you’ve created gives permission to anyone to download).",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/code.html#for-largelong-workflows",
    "href": "assessments/code.html#for-largelong-workflows",
    "title": "Code",
    "section": "For Large/Long Workflows",
    "text": "For Large/Long Workflows\nIf your analysis has a particularly time-consuming stage (e.g. Named-Entity Recognition or Part-of-Speech tagging) then you can provide partially-processed data via a download in the QMD file: comment out the code up to the point where you have generated the ‘expensive’ data set but leave it in the markdown document. That way we can see how you generated the data without it being part of the reproducibility stage.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Code"
    ]
  },
  {
    "objectID": "assessments/content.html",
    "href": "assessments/content.html",
    "title": "Content",
    "section": "",
    "text": "The Content is worth 35% of your module mark. You will be writing for a non-technical audience: the incumbent Mayor has asked you to rapidly respond to a political scandal and a proposal for regulation by the opposition candidate.\nThe responses to the set questions may be written without substantially new modelling or coding through the judicious use of descriptive statistics (see, for instance, Housing and Inequality in London and The suburbanisation of poverty in British cities, 2004-16: extent, processes and nature).\nStudents may use data from more than one time period if they wish, but this is not required. You can see the available data sets on Orca.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#set-questions",
    "href": "assessments/content.html#set-questions",
    "title": "Content",
    "section": "Set Questions",
    "text": "Set Questions\nIn the midst of an election, a newspaper has broken the scandal of an advisor on social mobility to the Mayor having three homes, two of which are let on Airbnb (one of which is still council-owned housing!). The opposition has announced a plan to force all professional landlords to register their properties and face higher Council Tax rates saying that Airbnb is ‘out of control’ in the capital. The Mayor wants to understand the scale of the ‘problem’ and the likely impacts of the opposition’s proposal so that they can either adopt it (and show how responsive they are) or demonstrate how poorly thought-through the opposition’s proposal is (and show that they’re not ready to govern). They have come to you—their team of crack data crunchers and advisors—for guidance in the form of a briefing, and are looking for evidence and visualisations that they can use in their campaign comms.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#is-airbnb-out-of-control-in-london",
    "href": "assessments/content.html#is-airbnb-out-of-control-in-london",
    "title": "Content",
    "section": "1. Is Airbnb out of control in London?",
    "text": "1. Is Airbnb out of control in London?\n\n( points; Answer due Week 7 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#how-many-professional-landlords-are-there",
    "href": "assessments/content.html#how-many-professional-landlords-are-there",
    "title": "Content",
    "section": "2. How many professional landlords are there?",
    "text": "2. How many professional landlords are there?\n\n( points; Answer due Week 8 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#how-many-properties-will-be-impacted-by-the-oppositions-proposal",
    "href": "assessments/content.html#how-many-properties-will-be-impacted-by-the-oppositions-proposal",
    "title": "Content",
    "section": "3. How many properties will be impacted by the opposition’s proposal?",
    "text": "3. How many properties will be impacted by the opposition’s proposal?\n\n( points; Answer due Week 9 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#what-are-the-pros-and-cons-to-the-oppositions-proposal",
    "href": "assessments/content.html#what-are-the-pros-and-cons-to-the-oppositions-proposal",
    "title": "Content",
    "section": "4. What are the pros and cons to the opposition’s proposal?",
    "text": "4. What are the pros and cons to the opposition’s proposal?\n\n( points; Answer due Week 7 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#can-we-somehow-spin-this-scandal-into-a-positive-story-about-social-mobility",
    "href": "assessments/content.html#can-we-somehow-spin-this-scandal-into-a-positive-story-about-social-mobility",
    "title": "Content",
    "section": "5. Can we somehow spin this scandal into a positive story about social mobility?",
    "text": "5. Can we somehow spin this scandal into a positive story about social mobility?\n\n( points; Answer due Week 7 )",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#style",
    "href": "assessments/content.html#style",
    "title": "Content",
    "section": "Style",
    "text": "Style\nThis is not an essay, and students who submit answers using a traditional essay style will see their overall mark impacted as a result. You must preserve the question/response format, and your responses should be readily-grasped by an intelligent, but non-technical audience. This doesn’t mean that you don’t need citations, but you should not employ an academic writing style. See the models provided below for insights into how to write for a less technical audience.\nThere will also be opportunities to discuss the submission during the second half of term.\n\n\n\n\n\n\nWrite for Your Audience\n\n\n\nWhat makes writing a good briefing hard—and not just about writing good code—is finding the right balance of technical detail and high-level explanation: you can’t just say ‘here are the five types of accommodation we found…’, but you also can’t say ‘we tested clustering solutions in the range 3–50 and found the optimal result at k=12…’ You should have a look at the examples.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#word-counts-figures",
    "href": "assessments/content.html#word-counts-figures",
    "title": "Content",
    "section": "Word Counts & Figures",
    "text": "Word Counts & Figures\n\nWord Counts\nEach figure or table counts for 150 words, and so students should give careful consideration to the trade-offs involved: more figures may serve to illustrate your points but leave you with much less space to synthesise and present and argument.\nThe overall word limit for this assessment is 2,500 words.\n\n\nFigures & Tables\nUnless you are presenting (and citing) a figure from another source as part of your framing, all figures and tables used must be generated by Python code cells included in the markdown file. You may not modify or create figures in another application since this undermines the reproducibility of the analysis.\n\n\nA/B Figures\nA figure with A/B/C elements will count as one figure, but only where the parts parts are conceptually related (e.g. before/after; non-spatial/spatial distribution; type 1 and type 2; etc.). The output from PySAL’s LISA analysis library, for instance, is pre-formatted as 3 figures. Seaborn’s jointplot will only be considered to be one plot even though it is technically three because the distribution plots in the margin are related to the scatter plot that is the focus of the plot.\nIn principle, a briefing with 16 figures would have no space for any text or interpretation; this choice is deliberate because its purpose is to focus your attention on which charts and tables best-communicate your findings. In practice, using A/B/C figure layouts then you are looking at up to 48 separate figures before hitting the limit, though you would at this point be producing an infographic and not a briefing.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#assumptions",
    "href": "assessments/content.html#assumptions",
    "title": "Content",
    "section": "Assumptions",
    "text": "Assumptions\nIt is highly likely that you will need to make substantial use of assumptions in developing your briefing for the Mayor. These should be plausible and, where possible, documented. For example:\n\nBased on  (2021), we assume that the average visitor to London walks at a rate of 3m/s (180m/minute). We further assume that a 10-minute walk is reasonable, giving us a limit of 1,800m…\n\nOr:\n\nWe assume that tourists spend approximately £X/day in their local area (see, e.g., , 2016), implying that each property generates a maximum of £Y/year in local spending (assuming continuous occupation). In practice, we adopt the approach of XXX (2019) to estimate occupancy from reviews to generate a more realistic impact of…\n\nSo you can see that neither of these requires more than a couple of citations to allow you to estimate some reasonable threshold for a metric of interest. This will be a lot simpler than trying to look up global tourist spending information or TfL travel stats. Yes, it’s a rough-and-ready estimate, but it also doesn’t pull the number out of thin air.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#referencing",
    "href": "assessments/content.html#referencing",
    "title": "Content",
    "section": "Referencing",
    "text": "Referencing\nYou will need to make use of BibTeX and Markdown referencing in Quarto. ‘Hard-coded’ references will not be considered.\n\n\n\n\n\n\nReferencing in Quarto\n\n\n\nAlthough you can create BibTeX entries by hand, you will probaly want to make use of BibDesk (Mac) or JabRef (Mac/Windows). Zotero shuould also work to edit the BibTeX file.\nIn Google Scholar, if you want to add a reference to your BibTeX file there’s an option in the Cite functionality to copy a BibTeX entry to the clipboard and then pasted this into BibDesk or JabRef.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#models",
    "href": "assessments/content.html#models",
    "title": "Content",
    "section": "Models",
    "text": "Models\nAlthough the following examples are all much longer than permitted under the assessment format, they are exemplary in their communication of the data and key findings in a manner that is clear, straightforward, and well-illustrated using maps, charts, and tables. So while they wouldn’t work for a busy Mayor per se, they can still help you to think about how best to present information for non-expert audiences:\n\nTravers et al. (2016), Housing and Inequality in London, Centre for London; URL.\nBivens, J. (2019), The economic costs and benefits of Airbnb, Economic Policy Institute; URL.\nWachsmuth et al. (2018), The High Cost of Short-Term Rentals in New York City, Urban Politics and Governance research group, McGill University; URL.\n\n\n\n\n\n\n\nLonger Questions\n\n\n\nNotice how these ‘models’ differ from a traditional essay format. So instead of Introduction, Literature, etc. you will see the evidence is developed in parallel with the background material. This format provides for more flexibility in style and presentation, though you will note that they all refer to a mix of academic and grey literature as well!",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "assessments/content.html#partial-bibliography",
    "href": "assessments/content.html#partial-bibliography",
    "title": "Content",
    "section": "Partial Bibliography",
    "text": "Partial Bibliography\nYou will also want to expand on the partial bibliography shown in the Templates section. This is by means complete and you will likely find other relevant work ‘out there’, but this gives you a good starting point.",
    "crumbs": [
      "Elements",
      "Group Work",
      "Content"
    ]
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html",
    "href": "extra/Live-02-Foundations_1.html",
    "title": "Context",
    "section": "",
    "text": "&lt;h1 style=\"width:450px\"&gt;Live Coding 2: Foundations (Part 1)&lt;/h1&gt;\n&lt;h2 style=\"width:450px\"&gt;Getting to grips with the 'Basics'&lt;/h2&gt;\nWe’re going to start out with the live coding and practical sessions potentially looking quite different, but that’s because I don’t think you all need to spend 60-90 minutes reviewing lists and variables. Instead, the angle in Weeks 1-3 is about getting you organised, starting to explore the data in a way that’s more immediately flexible than using code, and familiarising you with the data we’ll be using across the rest of term.\nIn the live coding sessions we will be using Crime data from 2022 for Chicago. In the practical sessions we’ll be using Inside Airbnb data for London."
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-1.-creating-and-cloning-a-repo",
    "href": "extra/Live-02-Foundations_1.html#task-1.-creating-and-cloning-a-repo",
    "title": "Context",
    "section": "Task 1. Creating and Cloning a Repo",
    "text": "Task 1. Creating and Cloning a Repo\n\nSuggest fsds as private repo\nClone to local machine\nOpen and look around."
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-2.-adding-files-commit",
    "href": "extra/Live-02-Foundations_1.html#task-2.-adding-files-commit",
    "title": "Context",
    "section": "Task 2. Adding Files & Commit",
    "text": "Task 2. Adding Files & Commit\n\ngit status\nAdd a notebook file downloaded from i2p\n\ngit add\ngit commit\n\nNow let’s look on GitHub.\n\nNow let’s update the README\n\nTry to make a change locally and push it\n\nMake a change to README\nTry to push\ngit diff\n\nMerge and resolve.\nPush again"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-3.-more-on-docker",
    "href": "extra/Live-02-Foundations_1.html#task-3.-more-on-docker",
    "title": "Context",
    "section": "Task 3. More on Docker",
    "text": "Task 3. More on Docker\nCan expand this after asking students if they have questions about how Docker works. It is available directly on the jreades.github.io/sds_env/docker/#understanding-docker page\n\nUnderstanding Docker Section\n\nStarting up\nLogging in\nTurning on extensions\nLooking at what the extensions do\n\nEspecially the toc view\nVariable inspector\nGitPlus (untested)\n\nCreating a notebook\nUsing the Terminal"
  },
  {
    "objectID": "extra/Live-02-Foundations_1.html#task-4.-investigating-a-file",
    "href": "extra/Live-02-Foundations_1.html#task-4.-investigating-a-file",
    "title": "Context",
    "section": "Task 4. Investigating a File",
    "text": "Task 4. Investigating a File\n\nDo all of this through JupyterLab/Docker\n\n\nTask 4.1. Download the File\n\nThe full file\nThe sample file\n\nIdeally download the files to ~/Downloads/\n\n\nTask 4.2. Check Names of Files\nls ~/Downloads/*.gz\n\nExplain the ~.\nExplain the *.\nExplain the .gz.\n\n\n\nTask 4.3. How big are the files?\ndu -sh *.gz\n\nAsk them to explain the *.gz.\nWhat is du (Anyone want to guess? You can Google the answer right now!)\nHow do we figure out what du does?\n\nMany command line tools have a --help option, so you can run du --help to see the options.\nA full Unix/Linux/macOS installation also has a ‘manual’ option: man du…\n\nOk, how the hell do I navigate this?\n\narrow keys to move up and down one line at a time\nspace bar to move down one page at a time\nq to quit\n/ and then a term to search\n\n\n\n\n\n\nTask 4.4. Moving the file around\nMove the file to the cloned repo.\nmv ~/Downloads/&lt;fn&gt;.gz ~/Documents/&lt;repo&gt;/\ncd ~/Documents/&lt;repo&gt;/\nmkdir data\nmv &lt;fn&gt;.gz data/\n\nHow would we make sense of mkdir?\nWhat does -p option do?\nHow did you find it?\n\n\n\nTask 4.5. How many rows are in the data set?\n\nShow them what open . does on a Mac.\nDecompress a copy of the file (just double-click as compressed file will remain)\nOpen the copy in Excel.\nWhat are we looking at?\n\nDiscuss\nNotice any issues with the data?\n\nDate is actually datetime, and some is in 24-hour, others in 12-hour format\nWhat data types can we see? Think about what you’re learning in QM? (We’ve got floats [both ratio–meaningful zero–and interval], integers [discrete ids that are not case numbers], booleans, categorical…)\n\n\n\nNow back to the file… - How do we count things in a file? - wc - How do we count lines in a file? - Look it up! man wc\nwc -l *-Crime.csv\n\n\nTask 4.6. What are the names of the columns?\nObviously we can look in Excel, but what commands could you use to look at the first line of the file?\n\nWe’ve seen at least two ways to do this in my talk, but can you remember them?\n\nless 2022-Crime-data.csv\nand\nhead -n 1 2022-Crime-data.csv\nWith less, talk through the commands for stepping forwards, backwards, one page down, and quitting. Notice that these are the same as for the manpage help we used above with du.\n\nHow would you get the last 5 lines?\n\ntail -n 5 2022-Crime-data.csv\n\nHow would you get the 50th to 60th lines?\n\nhead -n 60 2022-Crime-data.csv | tail -n 10\n\n\nTask 4.7. How many ‘Narcotics’ charges are there?\nLead them through the fact that case matters:\n\nWhy doesn’t this work?\n\ngrep \"Narcotics\" 2022-Crime-data.csv\n\nHow could we get matches for Narcotics?\n\nOption #1: use “NARCOTICS”\nOption #2: use -i or --ignore-case\nWe get the latter by looking at man grep\n\n\ngrep -i \"Narcotics\" 2022-Crime-data.csv\nThere should be 7 rows.\n\nCan you figure out how many Narcotics charges there are in the file?\n\nYou’ve seen how to use the ‘pipe’ to pass output from head to tail.\nYou’ve seen how to find the rows that match using grep\nYou’ve seen how to count the number of lines\nYou need to put these pieces all together.\n\n\ngrep -i \"Narcotics\" 2022-Crime-data.csv | wc -l\nThere should be 7 rows.\n\n\nTask 4.8. How many charges mention values over $500?\nDemonstrate value of escape commands. Let’s have a look at the output from:\ngrep \"$500\" 2022-Crime-data.csv | wc -l\nNow let’s try:\ngrep '$500' 2022-Crime-data.csv | wc -l\nAnd finally:\ngrep \"\\$500\" 2022-Crime-data.csv | wc -l\n\n\nTask 4.9. More Complex Command Chaining\nOther fun things we can do…\n\nHow do we know what types of primary crimes there are in the data?\n\nawk -F',' '{print $4}' 2022-Crime-data.csv | sort | uniq\n\nOr how about what types of descriptions there are of Narcotics charges?\n\ngrep -i 'Narcotics' 2022-Crime-data.csv | awk -F',' '{print $5}' | sort | uniq\n\nOr how may there are of Possession?\n\ngrep -i 'Narcotics' 2022-Crime-data.csv | grep 'POSS' | wc -l"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html",
    "href": "extra/Live-03-Foundations_2_Intro.html",
    "title": "Getting Started…",
    "section": "",
    "text": "SEATS QR Code:\n\n\n\n\n\n\n\n\nGroup Sign-up\n\n\n\n\n\n\n\n\n\n\n\nwww.menti.com/al8qfmocpxkg\n\n\n\n\n\n\nDon’t forget to start Lecture Capture!"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#check-in",
    "href": "extra/Live-03-Foundations_2_Intro.html#check-in",
    "title": "Getting Started…",
    "section": "",
    "text": "SEATS QR Code:"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#organise-your-group",
    "href": "extra/Live-03-Foundations_2_Intro.html#organise-your-group",
    "title": "Getting Started…",
    "section": "",
    "text": "Group Sign-up"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#comment-on-progress",
    "href": "extra/Live-03-Foundations_2_Intro.html#comment-on-progress",
    "title": "Getting Started…",
    "section": "",
    "text": "www.menti.com/al8qfmocpxkg"
  },
  {
    "objectID": "extra/Live-03-Foundations_2_Intro.html#on-with-the-show",
    "href": "extra/Live-03-Foundations_2_Intro.html#on-with-the-show",
    "title": "Getting Started…",
    "section": "",
    "text": "Don’t forget to start Lecture Capture!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#who-is-this-guy",
    "href": "lectures/1.1-Getting_Started.html#who-is-this-guy",
    "title": "Getting Started",
    "section": "Who is this Guy?",
    "text": "Who is this Guy?\n\nA few fun facts about your module lead…\n\n\nJon:\n\nHas a first degree in literature.\nHas never studied computer science and has only ever taken night classes in programming.\nWorked for a dot.com start-up in New York before transferring to London.\nHas taught undergraduate geography at King’s, from which this module evolved, and has taught this module for five years.\nWhen not working on teaching, being head of department, and squeezing in some reseach, Jon can be found looking after his two young daughters, mountain biking with Andy or hiking in Scotland."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#useful-information",
    "href": "lectures/1.1-Getting_Started.html#useful-information",
    "title": "Getting Started",
    "section": "Useful Information",
    "text": "Useful Information\nFoundations is distributed across two web sites:\n\nThe micro-site: jreades.github.io/fsds/ – lectures, practicals, readings, and information about the assessments. This will remain accessible to you after graduation.\nMoodle: moodle…?id=54436 – recorded sessions, booking drop-in hours, group messaging, ‘answer sheets’, and submission of assessments, as well as other formal components. This is tied to your enrolment at UCL.\n\nAnd don’t forget about this quick introduction to Python: jreades.github.io/code-camp/!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#where-does-fsds-fit",
    "href": "lectures/1.1-Getting_Started.html#where-does-fsds-fit",
    "title": "Getting Started",
    "section": "Where Does FSDS Fit?",
    "text": "Where Does FSDS Fit?\n\n\nGeographic Information Systems (GIS)\n\nFoundations of spatial analysis\nWorking with geo-data\n\nQuantitative Methods (QM)\n\nFoundations of statistical analysis\nWorking with data\n\n\nFoundations of Spatial Data Science (FSDS)\n\nFoundations of applied spatial and statistical analysis\nIntegrating and applying concepts from GIS & QM to a problem\nDeveloping programming and practical analysis skills\nSeeing the ‘data science’ pipeline from end to end"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#what-are-we-trying-to-do",
    "href": "lectures/1.1-Getting_Started.html#what-are-we-trying-to-do",
    "title": "Getting Started",
    "section": "What Are We Trying to Do?",
    "text": "What Are We Trying to Do?\nThis class hopes to achieve four things:\n\nTo teach you the basics of how to code.\nTo teach you the basics of how to think through code.\nTo teach you how to engage with data critically.\nTo help you integrate concepts taught across Term 1 and prepare you to apply them in Term 2.\n\nThese skills are intended to be transferrable to post-degree employment or research."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#the-challenges",
    "href": "lectures/1.1-Getting_Started.html#the-challenges",
    "title": "Getting Started",
    "section": "The Challenges",
    "text": "The Challenges\n\nTo learn a bit of programming and to connect it to the bigger picture.\nTo be ok with learning to walk before you run.\nTo learn not to rely (too much) on ChatGPT.\nTo communicate your thoughts through code and text.\n\n\nThis is a new one for us too. We don’t want to pretend that ChatGPT doesn’t exist. It’s how you will do your work. Unquestionably. But it is also a trap. This year we’re hoping to show you that."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#the-rewards",
    "href": "lectures/1.1-Getting_Started.html#the-rewards",
    "title": "Getting Started",
    "section": "The Rewards",
    "text": "The Rewards\n\nSkills that are highly transferrable and highly sought-after professionally.\nProblem-solving and practical skills that are valued by the private and public sectors.\nA whole new way of seeing the world and interacting with it.\nLots of support along the way… if you remember to ask for it!\n\nSee this thread on moving from academia to data science."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#narrative-arc",
    "href": "lectures/1.1-Getting_Started.html#narrative-arc",
    "title": "Getting Started",
    "section": "Narrative ‘Arc’",
    "text": "Narrative ‘Arc’\n\nPart 1: Foundations: Weeks 1–5 to cover the ‘basics’ and set out a data science workflow.\nPart 2: Data: Weeks 6–10 look at the same data through three lenses.\nPart 3: Bonus: Weeks 11–12 additional content if you want it.\n\n\n1-5 means tackling the ‘basics’ of Python, foundational concepts in programming, and practicing with the ‘tools of the trade’ for programmers.\n6-10 means different types of data (numeric, spatial and textual) with a view to understanding how such data can be cleaned, processed, and aggregated for use in a subsequent analysis. It is commonly held that 80% of ‘data science’ involves data cleaning, so this is a critical phase in developing an understanding of data. We also look at selection and visualisation.\n11-12 is about classification, dimensionality reduction, and clustering. These concepts will have been encountered in other modules, so the intention is that the student will see how these fit into the ‘bigger picture’ of applied spatial analysis."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#week-to-week",
    "href": "lectures/1.1-Getting_Started.html#week-to-week",
    "title": "Getting Started",
    "section": "Week-to-Week",
    "text": "Week-to-Week\nThe specific activities for each week can be found on the microsite. These include:\n\nPreparation: readings, pre-recorded lectures, quizzes/feedback.\nIn-Person: discussing readings and lectures; responding to assessment requirements; discussing issues arising from the previous week’s practical, and some ‘live coding’.\nPracticals: working through a weekly ‘programming notebook’ with support from your PGTAs.\n\n\n\n\n\n\n\nBring Your Computer\n\n\n\nPlease remember to bring your own computer to the practical sessions! The tools we use are not installed on cluster systems."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#assessments",
    "href": "lectures/1.1-Getting_Started.html#assessments",
    "title": "Getting Started",
    "section": "Assessments",
    "text": "Assessments\n\nTimed, Open Book Exam (30% of module grade): A quiz requiring a mix of numeric and textual answers to short data analysis questions for which you must write the code.\nReproducible Analysis (25% of module grade): A small-group submission of a ‘tangled’ document that demonstrates reproducibility through an exploratory analysis of the assigned data set.\nStructured Report (35% of module grade): A structured, small-group submission which responds to set questions and develops an exploratory analysis of the assigned data set.\nSelf- and Peer-Evaluation (10% of module grade): A short individual reflection combined with numerical scoring by peers on their contribution to the group’s outcomes.\n\n\nAssessment logic:\n\nTeach and test the most challenging aspects of data science ‘work’ without mastery of Python.\nDiscover transferrability of skills and tools across projects, disciplines, and industries.\nBuild on content from QM (e.g. setting quantitative research questions) and GIS (e.g. spatial statistics).\nDevelop experience with non-academic research formats and writing."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#formats-due-dates",
    "href": "lectures/1.1-Getting_Started.html#formats-due-dates",
    "title": "Getting Started",
    "section": "Formats & Due Dates",
    "text": "Formats & Due Dates\n\nTimed, Open Book Exam: is a Moodule quiz due Friday, 21 November 2025 (after Reading Week) and it will focus on content from the first five weeks of class.\nReproducible Analysis: is a Quarto document due Tuesday, 16 December 2025 and written in small groups to demonstrate reproducibility through an exploratory analysis of the assigned data set.\nStructured Report: is a PDF (output from Quarto) due Tuesday, 16 December 2025 written in small groups which responds to set questions and develops an exploratory analysis of the assigned data set.\nSelf- and Peer-Evaluation: is a Moodle ‘IPAC’ assessment due Thursday, 18 December 2025 combining short individual reflection combined with numerical scoring by peers on their contribution to the group’s outcomes.\n\n\nWe will talk more about these over the course of the term."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#actual-feedback",
    "href": "lectures/1.1-Getting_Started.html#actual-feedback",
    "title": "Getting Started",
    "section": "Actual Feedback…",
    "text": "Actual Feedback…\n\nI was really struggling with the concepts of lists, dictionaries and iterations (I basically could not do any of Practical 3 without panicking) and I was telling  that it felt like Workshop 3 was all in a foreign language - I was so lost. \n But both yesterday and today, I have been over all the content, recordings and even code camp again and I’ve just had a penny drop moment, I could cry woohooo!!!!!! \nI really appreciate all the effort you’ve put into recording the concepts ahead of lectures and the way you’ve structured the module, although it is very fast-moving you have provided all the resources for us to do well."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#more-feedback",
    "href": "lectures/1.1-Getting_Started.html#more-feedback",
    "title": "Getting Started",
    "section": "More Feedback",
    "text": "More Feedback\n\nI just wanted to update you on my progress. Since flipping the content round following your advice, I have been feeling much much better. I followed what you were doing in the workshop and also have completed the practical in about half the time than I usually do. Thanks so much for responding and for your effort with this module."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#lots-of-help-out-there",
    "href": "lectures/1.1-Getting_Started.html#lots-of-help-out-there",
    "title": "Getting Started",
    "section": "Lots of Help ‘Out There’",
    "text": "Lots of Help ‘Out There’\nWhen you need an answer right now:\n\nGoogle\nStack Overflow\nSlack\nChatGPT / Copilot\n\nWhen you want to learn more:\n\nMedium\nPocket\n\n\nGoogle will become more useful as you learn more and this is definitely one class in which “I Googled it” is a good answer.\nAs of early September 2020, Stack Overflow contains over 1.5 million Python questions alone! Chances are someone else has had your question before."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#when-to-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#when-to-ask-for-help",
    "title": "Getting Started",
    "section": "When to Ask for Help",
    "text": "When to Ask for Help\n\nWhen you get warning messages from your computer’s Operating System.\nWhen you cannot get the coding environment to run at all.\nWhen even simple commands return line after line of error code.\nWhen you have no clue what is going on or why.\nWhen you have been wrestling with a coding question for more than 20 minutes (but see: How to Ask for Help!)"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#before-you-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#before-you-ask-for-help",
    "title": "Getting Started",
    "section": "Before You Ask for Help",
    "text": "Before You Ask for Help\nFrom the Computer Science Wiki:\n\nDraw a picture of the problem\nExplain the problem out loud to a friend, teddy bear or whatever (really!)\nForget about a computer; how would you solve this with a pencil and paper?\n\nTo which we would add:\n\nUse print(variable) statements liberally in your code!\n\n\nWe’ll cover this last bit as we get more used to coding!\nIn order to learn you do need to struggle, but only up to a point! So we don’t think that giving you the answer to a coding question as soon as you get stuck is a good way for you to learn. At the same time, I remain sad to this day that one of the most insightful students I’ve ever taught in a lecture context dropped out of our module because they were having trouble with their computer and thought it was their fault nothing was working right. By we had realised what was going on it was too late: they were so far behind that they didn’t feel able to catch up. We’d rather that you asked and we said “Close, but try it again” than you didn’t ask and checked out thinking that you couldn’t ‘do’ programming."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#how-to-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#how-to-ask-for-help",
    "title": "Getting Started",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nIn addition to what we have provided, we like the “How to ask programming questions” page provided by ProPublica:\n\nDo some research first.\nBe specific.\nRepeat.\nDocument and share.\n\nIf you find yourself wanting to ask a question on Stack Exchange then they also have a guide, and there are plenty of checklists.\n\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#where-to-ask-for-help",
    "href": "lectures/1.1-Getting_Started.html#where-to-ask-for-help",
    "title": "Getting Started",
    "section": "Where to Ask for Help",
    "text": "Where to Ask for Help\nThere is no shame in asking for help. None. We are here to support your learning and we have chosen a range of tools to support that:\n\nSlack: use the #fsds channel for help with coding, practical, and related course questions.\nDrop-in Hours: use Booking Form\nOut-of-Hours: use email to raise personal circumstances and related issues for focussed support.\nEmergencies: contact Bartlett.Postgraduate (CASA)1 for support as-needed and/or to preserve privacy.\n\n\nWe’ll talk about Slack more later, but we think that this is the best way to get help when you need it. Slack enables us to support you as a community of learners across computer / tablet / phone.\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time, as well as how to read and how to think."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#learn-from-your-mistakes",
    "href": "lectures/1.1-Getting_Started.html#learn-from-your-mistakes",
    "title": "Getting Started",
    "section": "Learn from Your Mistakes",
    "text": "Learn from Your Mistakes"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#one-more-thing",
    "href": "lectures/1.1-Getting_Started.html#one-more-thing",
    "title": "Getting Started",
    "section": "One More Thing…",
    "text": "One More Thing…\nYou will get things wrong. We will get things wrong.\nWe will assume that you are trying your best. Please assume the same about us!\nIt’s going to be messy, but we’re really excited about it!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#and-finally",
    "href": "lectures/1.1-Getting_Started.html#and-finally",
    "title": "Getting Started",
    "section": "And Finally…",
    "text": "And Finally…\n\n\n\n\n\n\nAuto-Updates\n\n\n\nDo not allow your computer to auto-update during term. Inevitably, major upgrades will break developer tools. Do this by choice only when you have time. MacOS Sonoma is out 26 September, do not install it!\n\n\n\nMany students allowed their computer to update to Big Sur last year and it broke their entire computing environment. Some did this shortly before a submission was due. Do not do this!"
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#additional-resources",
    "href": "lectures/1.1-Getting_Started.html#additional-resources",
    "title": "Getting Started",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSee the GIS&T Body of Knowledge (BoK) for quick overviews of concepts, techniques, and tools."
  },
  {
    "objectID": "lectures/1.1-Getting_Started.html#footnotes",
    "href": "lectures/1.1-Getting_Started.html#footnotes",
    "title": "Getting Started",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nbartlett.pg-casa@ucl.ac.uk↩︎"
  },
  {
    "objectID": "lectures/1.3-Principles.html",
    "href": "lectures/1.3-Principles.html",
    "title": "Principles into Practices",
    "section": "",
    "text": "Foundations was designed around these principles:\n\nSoftware and data should be free (as far as practicable).\nSoftware and data should be open (as far as practicable).\nSoftware should run on all platforms (you get the idea).\nSoftware and data should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.3-Principles.html#principles",
    "href": "lectures/1.3-Principles.html#principles",
    "title": "Principles into Practices",
    "section": "",
    "text": "Foundations was designed around these principles:\n\nSoftware and data should be free (as far as practicable).\nSoftware and data should be open (as far as practicable).\nSoftware should run on all platforms (you get the idea).\nSoftware and data should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.3-Principles.html#how-is-open-source-helpful",
    "href": "lectures/1.3-Principles.html#how-is-open-source-helpful",
    "title": "Principles into Practices",
    "section": "How is Open Source Helpful?",
    "text": "How is Open Source Helpful?\n\n\nIf you can think of it, someone has probably built it.\nGiven enough eyeballs all bugs are shallow.1\n‘Free as in speech’ (always).\n‘Free as in beer’ (often).2\n\n\n. . .\nAll of the tools used this week are open source and free.\n\n\nBy this I mean that, if you use Microsoft to do everything then you are dependent on their tools, even when they aren’t appropriate. There are many small, open source applications that do really useful but narrow things.\nWhen you have many people involved and they are working in an open environment then issues are often spotted spotted and solved more quickly. While this claim is open to some debate, there’s certainly no evidence that closed source code is any better than open source.\nA more interesting claim has to do with the potential for interested people to get involved in making code better – for instance, for some languages there may be very few translators available, leading private companies to simply ignore localisation; whereas for open source you can contribute the translation yourself! Or if you really think something isn’t working the way it should then you can offer up a solution that will make it better and, if the people running the project don’t agree, you can still do it and launch your own, competing project.\nFinally, many (though by no means all) open source projects are also free as free food, beer, or rides. If you have been taught to use ESRI’s ArcGIS or Microsoft Excel this seems crazy: how can you have free competition to those? Well, why not give QGIS and LibreOffice a try!"
  },
  {
    "objectID": "lectures/1.3-Principles.html#fair-play",
    "href": "lectures/1.3-Principles.html#fair-play",
    "title": "Principles into Practices",
    "section": "FAIR Play",
    "text": "FAIR Play\nWilkinson et al. (2016) set out the following principles:\n\n\nFindable: data and metadata should be easy to find for computers and humans.\nAccessible: it should be clear how the data found can be accessed.\nInteroperable: data should work for range of analyses, storage, and processing needs.3\nReusable: metadata and data should be well-described so they can be used/combined easily.\n\n\n. . .\nThere are strict and loose versions of these principles. Many governments truggle with the ‘R’ part because of licensing restrictions."
  },
  {
    "objectID": "lectures/1.3-Principles.html#how-is-open-data-helpful",
    "href": "lectures/1.3-Principles.html#how-is-open-data-helpful",
    "title": "Principles into Practices",
    "section": "How is Open Data Helpful?",
    "text": "How is Open Data Helpful?\nAccording to Open Data Institute (n.d.) (amongst others):\n\n\nPromotes transparency and accountability in government and services.\nPromotes efficiency and service delivery.\nPromotes innovation and economic growth.\nEmpowers citizens.\nReduces costs.\n\n\n. . .\nOpen data is valuable!\n\n\nThis can be scary for many, especially those in government; however, making data open can help to debunk conspiracies and demonstrate that government is not corrupt! It can also help government to spot where there are problems.\nWe get increased efficiency and better service delivery if governments can look across departments or states to compare performance or enable others to turn up opportunities for improvement.\nCompanies can innovate off of government data: Land Registry and EPC data in the UK, for instance, give companies certainty that they won’t have to pay for this data and can build commercial products using it. The biggest gain here is in making government mapping data available.\nThere’s a strong assumption here that citizens are data literate, which I think doesn’t always hold; however, crime mapping, bus route mapping, and so on can help citizens to advocate for resources and support.\nGovernments can also save by not having to constantly respond to FOI requests or rebut other access requests from citizens and companies."
  },
  {
    "objectID": "lectures/1.3-Principles.html#how-is-open-code-helpful",
    "href": "lectures/1.3-Principles.html#how-is-open-code-helpful",
    "title": "Principles into Practices",
    "section": "How is Open Code Helpful?",
    "text": "How is Open Code Helpful?\n\n\nAllow others to build on your work (reuse, collaboration).\nAllow others to learn from your work (speed, bug detection).\nA way to attract contributors to your project (visibility, collaboration).\nA mechanism for perpetuating a potlatch ecosystem (community building, recognition, visibility).4\n\n\n. . .\nThere are many ways to share code, including contributing questions and answers to public fora like Stack Overflow,"
  },
  {
    "objectID": "lectures/1.3-Principles.html#footnotes",
    "href": "lectures/1.3-Principles.html#footnotes",
    "title": "Principles into Practices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso known as Linus’ law via Eric Raymond’s The Cathedral and the Bazaar.↩︎\nOr chai, thalis, etc.↩︎\nShape files fail this test.↩︎\nWhere ‘knowledge is power’, not ‘power is right’.↩︎"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html",
    "href": "lectures/10.1-Visualising_Data.html",
    "title": "Visualising Data",
    "section": "",
    "text": "Building on Week 7, here are some deeper links between models and visualistions:\n\nWhat are we trying to model?\nWhat are we trying to penalise?\nWhat isn’t fitting into the model?\n\n\nInspired by this: https://towardsdatascience.com/when-averages-lie-moving-beyond-single-point-predictions-23201e8c04c8/\nLink it to the following kinds of issues:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#start-with-chart-part-2",
    "href": "lectures/10.1-Visualising_Data.html#start-with-chart-part-2",
    "title": "Visualising Data",
    "section": "",
    "text": "Building on Week 7, here are some deeper links between models and visualistions:\n\nWhat are we trying to model?\nWhat are we trying to penalise?\nWhat isn’t fitting into the model?\n\n\nInspired by this: https://towardsdatascience.com/when-averages-lie-moving-beyond-single-point-predictions-23201e8c04c8/\nLink it to the following kinds of issues:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#what-are-we-modelling",
    "href": "lectures/10.1-Visualising_Data.html#what-are-we-modelling",
    "title": "Visualising Data",
    "section": "What are We Modelling?",
    "text": "What are We Modelling?\n\nIssues of skew and leverage. It’s not (so much) about normality per se with OLS."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#what-are-we-penalising",
    "href": "lectures/10.1-Visualising_Data.html#what-are-we-penalising",
    "title": "Visualising Data",
    "section": "What are We Penalising?",
    "text": "What are We Penalising?"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#what-isnt-fitting",
    "href": "lectures/10.1-Visualising_Data.html#what-isnt-fitting",
    "title": "Visualising Data",
    "section": "What isn’t Fitting?",
    "text": "What isn’t Fitting?"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#choices-choices",
    "href": "lectures/10.1-Visualising_Data.html#choices-choices",
    "title": "Visualising Data",
    "section": "Choices, Choices…",
    "text": "Choices, Choices…\n\nmatplotlib: the ‘big beast’ of visualisation in Python. Similar to MATLAB. Highly customisable. Very complex.\nseaborn: a layer that sits over top of matplotlib and makes it easier to produce good-quality graphics.\nbokeh: web-based visualisation tool that can integrate with Jupyter or output to static HTML files.\nplotly: another web-based visualisation tool that can integrate with Jupyter.\n\nMore emerging all the time: Vega/Altair, HoloViews, etc.\n\n\nIf you’re really wedded to ggplot, plotnine is a clone of ggplot’s interface (Grammer of Graphics) in Python. A brief overview of visualisation libraries could be helpful."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#seaborn",
    "href": "lectures/10.1-Visualising_Data.html#seaborn",
    "title": "Visualising Data",
    "section": "Seaborn",
    "text": "Seaborn\nDesigned to provide ggplot-like quality output using matplotlib:\n\nImprove on default colourmaps and colour defaults.\nIntegration with pandas data frames (Note: not geopandas!).\nOffers more plot types out of the box.\nStill offers access to matplotlib’s back-end."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#plot-types",
    "href": "lectures/10.1-Visualising_Data.html#plot-types",
    "title": "Visualising Data",
    "section": "Plot Types",
    "text": "Plot Types\n\n\n\nPartial Overview of Seaborn Plots\n\n\n\n\nFor the fuller overview see Overview of seaborn plotting functions and the full API reference."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#in-practice",
    "href": "lectures/10.1-Visualising_Data.html#in-practice",
    "title": "Visualising Data",
    "section": "In Practice",
    "text": "In Practice\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nfmri = sns.load_dataset(\"fmri\")\nsns.lineplot(x=\"timepoint\", y=\"signal\",\n             hue=\"region\", style=\"event\",\n             data=fmri)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#in-practice-2",
    "href": "lectures/10.1-Visualising_Data.html#in-practice-2",
    "title": "Visualising Data",
    "section": "In Practice 2",
    "text": "In Practice 2\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\ndf = sns.load_dataset(\"penguins\")\n\nax = sns.swarmplot(data=df, x=\"body_mass_g\", y=\"sex\", hue=\"species\")\nax.set(ylabel=\"\")"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#configuring-seaborn",
    "href": "lectures/10.1-Visualising_Data.html#configuring-seaborn",
    "title": "Visualising Data",
    "section": "Configuring Seaborn",
    "text": "Configuring Seaborn\nSeaborn ‘themes’ act as shortcuts for setting multiple matplotlib parameters:\n\n\n\nSeaborn Command\nAccomplishes\n\n\n\n\nset_theme(...)\nSet multiple theme parameters in one step.\n\n\naxes_style(...)\nReturn a parameter dict for the aesthetic style of the plots.\n\n\nset_style(...)\nSet the aesthetic style of the plots.\n\n\nplotting_context(...)\nReturn a parameter dict to scale elements of the figure.\n\n\nset_context(...)\nSet the plotting context parameters.\n\n\n\nYou can also access:\n\nPalettes: colormaps can be generated using sns.color_palette(...) and set using sns.set_palette(...).\nAxes Styles: includes darkgrid, whitegrid, dark, white, ticks."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#anatomy-of-a-figure",
    "href": "lectures/10.1-Visualising_Data.html#anatomy-of-a-figure",
    "title": "Visualising Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\nSource."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#writing-a-figure",
    "href": "lectures/10.1-Visualising_Data.html#writing-a-figure",
    "title": "Visualising Data",
    "section": "Writing a Figure",
    "text": "Writing a Figure\nThere are multiple ways to access/write elements of a plot:\n\nFigure: high-level features (e.g. title, padding, etc.). Can be accessed via plt.gcf() (get current figure) or upon creation (e.g. f, ax = plt.subplots(1,1) or f = plt.figure()).\nAxes: axis-level features (e.g. labels, tics, spines, limits, etc.). Can be accessed via plt.gca() (get current axes) or upon creation (e.g. f, ax = plt.subplots(1,1) or ax = f.add_subplot(1,1,1)).\n\nAnnotations, artists, and other features are typically written into the axes using the coordinate space of the figure (e.g. decimal degrees for lat/long, metres for BNG, etc.)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#adding-a-3rd-dimension",
    "href": "lectures/10.1-Visualising_Data.html#adding-a-3rd-dimension",
    "title": "Visualising Data",
    "section": "Adding a 3rd Dimension",
    "text": "Adding a 3rd Dimension\nThis ‘feature’ is less well-developed but does work:\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax  = plt.axes(projection='3d')\n# OR\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\n# THEN\nax.contour3D(X, Y, Z, ...)\nax.plot_surface(x, y, z, ...)\nax.plot3D(xline, yline, zline, ...)\nax.scatter3D(x, y, z, ...)\n# ax.plot_surface and ax.plot_wire also give you 3D renderings\nYou can then set the elevation and azimuth using: ax.view_init(&lt;elevation&gt;, &lt;azimuth&gt;)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#saving-outputs",
    "href": "lectures/10.1-Visualising_Data.html#saving-outputs",
    "title": "Visualising Data",
    "section": "Saving Outputs",
    "text": "Saving Outputs\nStraightforward via save figure function, but lots of options!\nplt.savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n    orientation='portrait', papertype=None, format=None,\n    transparent=False, bbox_inches=None, pad_inches=0.1,\n    frameon=None, metadata=None)\nThe format can be largely determined by the file extension in the fname (file name) and the supported formats depends on what you’ve installed! You can find out what’s available to you using: plt.gcf().canvas.get_supported_filetypes()."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#jupyter",
    "href": "lectures/10.1-Visualising_Data.html#jupyter",
    "title": "Visualising Data",
    "section": "Jupyter",
    "text": "Jupyter\nBy default, Jupyter’s output is static matplotlib, but we can extend this in three ways:\n\nMake the static plot zoomable and pannable using %matplotlib widget (declare this at the top of your notebook).\nMake the plot more directly interactive using ipywidgets (import interact and related libs as needed).\nUse a browser-based visualisation tool such as bokeh, plotly, altair/vega, holoviews, or even d3 (format may be very, very different from what you are ‘used to’ in Python)."
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#widgets",
    "href": "lectures/10.1-Visualising_Data.html#widgets",
    "title": "Visualising Data",
    "section": "Widgets",
    "text": "Widgets\n%matplotlib widget\nrs = gpd.sjoin(gdf, hackney, how='left', op='within')\nrs.NAME.fillna('None', inplace=True)\nax = hackney.plot(edgecolor='k', facecolor='none')\nrs.plot(ax=ax, column='NAME', legend=True)"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#interact",
    "href": "lectures/10.1-Visualising_Data.html#interact",
    "title": "Visualising Data",
    "section": "Interact()",
    "text": "Interact()\nTaking an example from Dani’s work:\nfrom ipywidgets import interact\n# Alternatives: interactive, fixed, interact_manual\ninteract(\n    &lt;function&gt;, # Function to make interactive\n    &lt;param0&gt;,   # e.g. Data to use\n    &lt;param1&gt;,   # e.g. Range start/end/step\n    &lt;param2&gt;    # e.g. Fixed value\n);"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#bokeh",
    "href": "lectures/10.1-Visualising_Data.html#bokeh",
    "title": "Visualising Data",
    "section": "Bokeh",
    "text": "Bokeh"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#automation",
    "href": "lectures/10.1-Visualising_Data.html#automation",
    "title": "Visualising Data",
    "section": "Automation",
    "text": "Automation\nPlots built on top of matploblib can, to some extent, be automated using functions. For example, to draw circles and place text:\ndef circle(ax, x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, \n                    linewidth=1, edgecolor='black', \n                    facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, \n                                  foreground='w')])\n    ax.add_artist(circle)\n\ndef text(ax, x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n         ha='center', va='top', weight='bold', color='blue')"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nLabel directly1:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-1",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-1",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nRepeat measurement units2 3:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-2",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-2",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nPut the axes where they’re needed4:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-3",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-3",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nEmphasise and explain with annotation5:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-4",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-4",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nLead the eye with font sizes, styles, and colors6 7:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-5",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-5",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nSee what I said on the previous slide about ‘too much going on’8:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-6",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-6",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nFor the sake of all that is holy, please don’t center-align text9:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-7",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-7",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nMake reading easy and interpretation intuitive10 11:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-8",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-8",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nUse outlines to allow overlaps12 13:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-9",
    "href": "lectures/10.1-Visualising_Data.html#dont-underestimate-text-9",
    "title": "Visualising Data",
    "section": "Don’t Underestimate Text!",
    "text": "Don’t Underestimate Text!\nGet to the point in a way that works for the reader14 15:"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#additional-resources",
    "href": "lectures/10.1-Visualising_Data.html#additional-resources",
    "title": "Visualising Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nMatplotlib Cheatsheets\nIntroduction to PyPlot (includes lots of parameter information)\nVisualisation with Seaborn\nSeaborn Tutorial\nElite Data Science Seaborn Tutorial\nDatacamp Seaborn Tutorial\nThree-Dimensional Plotting in Matplotlib\nAn easy introduction to 3D plotting with Matplotlib\n\n\n\nDataWrapper Blog\nUsing text effectively in data viz\nChoosing fonts for charts and tables\nBokeh Gallery\nBokeh User Guide\nProgramming Historian: Visualizing Data with Bokeh and Pandas\nReal Python: Data Viz with Bokeh\nData Viz with Bokeh (Pt. 1)\nUsing Interact\nText in Data Visualizations"
  },
  {
    "objectID": "lectures/10.1-Visualising_Data.html#footnotes",
    "href": "lectures/10.1-Visualising_Data.html#footnotes",
    "title": "Visualising Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShamelssly taken from Datawrapper↩︎\nShamelssly taken from Datawrapper↩︎\nInterestingly, this flies in the face of the ‘chart junk minimisation’ of Tufte and Graves-Morris (1983)↩︎\nShamelssly taken from Datawrapper↩︎\nShamelssly taken from Datawrapper↩︎\nShamelssly taken from Datawrapper↩︎\nJust not too many of them in too many different styles!↩︎\nShamelssly taken from Datawrapper↩︎\nShamelssly taken from Datawrapper↩︎\nShamelssly taken from Datawrapper↩︎\nYour readers don’t need to legal names for each country, nor do they like akward hypens.↩︎\nShamelssly taken from Datawrapper↩︎\nNotice how this supports the focus and interpretation.↩︎\nShamelssly taken from Datawrapper↩︎\nRephrase for legibility and intelligibility.↩︎"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html",
    "href": "lectures/11.1-Data_Space.html",
    "title": "The Data Space",
    "section": "",
    "text": "Source.\n\n\n\n\nSource.\n\n\n\nQuestion: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)\n\n\n\n\nSource.\n\nWhat does this suggest about the DGP?"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#the-data-generating-process",
    "href": "lectures/11.1-Data_Space.html#the-data-generating-process",
    "title": "The Data Space",
    "section": "",
    "text": "Source."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#the-data-generating-process-1",
    "href": "lectures/11.1-Data_Space.html#the-data-generating-process-1",
    "title": "The Data Space",
    "section": "",
    "text": "Source."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#cashier-income-as-dgp",
    "href": "lectures/11.1-Data_Space.html#cashier-income-as-dgp",
    "title": "The Data Space",
    "section": "",
    "text": "Question: Retail cashier annual salaries have a Normal distribution with a mean equal to $25,000 and a standard deviation equal to $2,000. What is the probability that a randomly selected retail cashier earns more than $27,000?\nAnswer: 15.87%\nResult: All models are wrong, but some are useful (George Box)"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#house-prices-as-dgp",
    "href": "lectures/11.1-Data_Space.html#house-prices-as-dgp",
    "title": "The Data Space",
    "section": "",
    "text": "Source.\n\nWhat does this suggest about the DGP?"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "href": "lectures/11.1-Data_Space.html#is-bill-gates-as-rich-as-he-is-tall",
    "title": "The Data Space",
    "section": "Is Bill Gates as Rich as He is Tall?",
    "text": "Is Bill Gates as Rich as He is Tall?\nInstinctively, we know that Bill Gates’ wealth is much further from ‘normal’ than is his height. But how?\n\nHow can we compare income and height if they share no common units?\nHow can we compare the biodiversity of sites in the tropics with those of sub-Arctic areas given that there are different numbers of species to begin with?\n\nWe need:\n\nWays to make different dimensions comparable, and\nWays to remove unit effects from distance measures."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-1d",
    "href": "lectures/11.1-Data_Space.html#distance-in-1d",
    "title": "The Data Space",
    "section": "Distance in 1D",
    "text": "Distance in 1D\n\\[\nd(i,j) = |(i_{1}-j_{1})|\n\\]"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-2d",
    "href": "lectures/11.1-Data_Space.html#distance-in-2d",
    "title": "The Data Space",
    "section": "Distance in 2D",
    "text": "Distance in 2D\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}}\n\\]"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#distance-in-3d-or-more",
    "href": "lectures/11.1-Data_Space.html#distance-in-3d-or-more",
    "title": "The Data Space",
    "section": "Distance in 3D… or More",
    "text": "Distance in 3D… or More\nWe can keep adding dimensions…\n\\[\nd(i,j) = \\sqrt{(i_{1}-j_{1})^{2}+(i_{2}-j_{2})^{2}+(i_{3}-j_{3})^{2}}\n\\]\nYou continue adding dimensions indefinitely, but from here on out you are dealing with hyperspaces!"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#thinking-in-data-space",
    "href": "lectures/11.1-Data_Space.html#thinking-in-data-space",
    "title": "The Data Space",
    "section": "Thinking in Data Space",
    "text": "Thinking in Data Space\nWe can write the coordinates of an observation with 3 attributes (e.g. height, weight, income) as:\n\\[\nx_{i} = { {x_{i1}, x_{i2}, x_{i3} } }\n\\]\nSomething with 8 attributes (e.g. height, weight, income, age, year of birth, …) ‘occupies’ an 8-dimensional space…"
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#two-propositions",
    "href": "lectures/11.1-Data_Space.html#two-propositions",
    "title": "The Data Space",
    "section": "Two Propositions",
    "text": "Two Propositions\n\n\nThat geographical space is no different from any other dimension in a data set.\nThat geographical space is still special when it comes to thinking about relationships."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#implication",
    "href": "lectures/11.1-Data_Space.html#implication",
    "title": "The Data Space",
    "section": "Implication",
    "text": "Implication\nIf you can shift from thinking in columns of data, to thinking of a data space then you’ll have a much easier time dealing with dimensionality reduction and clustering."
  },
  {
    "objectID": "lectures/11.1-Data_Space.html#additional-resources",
    "href": "lectures/11.1-Data_Space.html#additional-resources",
    "title": "The Data Space",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nAre Statisticians Cold-Blooded Bosses?\nBeyond 3D: Thinking in Higher Dimensions\nVisualizing beyond 3 Dimensions\nThe things you’ll find in higher dimensions (also useful for dimensionality reduction)\nWhat’s a Tensor? (heavy on the Physics relevance, but a lot of useful terminology and abstraction)"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html",
    "href": "lectures/11.3-Dimensionality.html",
    "title": "Dimensionality",
    "section": "",
    "text": "More dimensions means more information.\nMore dimensions makes for easier seperation.\nMore dimensions inflates distance.\nMore dimensions increases the risk of overfitting.\n\n\nOr as Analytics India Magazine puts it:\n\nHigh-dimensional spaces have geometrical properties that are counter-intuitive and far from the properties observed in two- or three-dimensional spaces.\nData analysis tools are often designed with intuitive properties and low-dimensional spaces in mind."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#curse-and-blessing",
    "href": "lectures/11.3-Dimensionality.html#curse-and-blessing",
    "title": "Dimensionality",
    "section": "",
    "text": "More dimensions means more information.\nMore dimensions makes for easier seperation.\nMore dimensions inflates distance.\nMore dimensions increases the risk of overfitting.\n\n\nOr as Analytics India Magazine puts it:\n\nHigh-dimensional spaces have geometrical properties that are counter-intuitive and far from the properties observed in two- or three-dimensional spaces.\nData analysis tools are often designed with intuitive properties and low-dimensional spaces in mind."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#pca",
    "href": "lectures/11.3-Dimensionality.html#pca",
    "title": "Dimensionality",
    "section": "PCA",
    "text": "PCA\nWorkhorse dimensionality reduction method: simple, fast, and effective. Can be thought of as freely rotating axes to align with directions of maximum variance. I like this summary:\n\nPCA (Principal Components Analysis) gives us our ‘ideal’ set of features. It creates a set of principal components that are rank ordered by variance (the first component has higher variance than the second, the second has higher variance than the third, and so on), uncorrelated (all components are orthogonal), and low in number (we can throw away the lower ranked components as they usually contain little signal).\n\nBut I particularly liked this exposition in Towards Data Science."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice",
    "href": "lectures/11.3-Dimensionality.html#in-practice",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(data)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\npca.transform(data)\nSee also: Kernel PCA for non-linear problems."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#rtfm",
    "href": "lectures/11.3-Dimensionality.html#rtfm",
    "title": "Dimensionality",
    "section": "RT(F)M",
    "text": "RT(F)M\nWhy was I banging on about transformations? Well, what does this assume about the data?\n\nLinear dimensionality reduction using Singular Value Decomposition projects data into to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\n\nI found a nice explanation of PCA using dinner conversation over several bottles of wine as an example on Stats.StackExhcange.com. There are many good illustrations of this process on stats.stackexchange.com."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#other-considerations",
    "href": "lectures/11.3-Dimensionality.html#other-considerations",
    "title": "Dimensionality",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nPCA is a form of unsupervised learning that does not take output labels into account. Other approaches (such as Linear Discriminant Analysis [note: not Latent Dirichlet Allocation]) consider the output as part of the transformation. PCA is also deterministic.\n\nSee this discussion."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#t-sne",
    "href": "lectures/11.3-Dimensionality.html#t-sne",
    "title": "Dimensionality",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbour Embedding is best understood as a visualisation technique, not an analytical one. This is because it is probabilistic and not deterministic."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice-1",
    "href": "lectures/11.3-Dimensionality.html#in-practice-1",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nfrom sklearn.manifold import TSNE\nembedded = TSNE(n_components=d).fit_transform(x)\nThe choice of perplexity and n_iter matter, and so does the metric. In practice you will need to experiment with these.\n\nt-SNE is also much harder computationally than PCA and it may be preferrable on very high-D data sets to apply PCA first and then t-SNE to the reduced data set! The output could then be fed to a clustering algorithm to make predictions about where new observations belong, but do not confuse that with meaning."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#umap",
    "href": "lectures/11.3-Dimensionality.html#umap",
    "title": "Dimensionality",
    "section": "UMAP",
    "text": "UMAP\nNon-linear dimensionality reduction that tries to preserve both local and global structure. Puts it between PCA and t-SNE.\n\n\n\nNote that the library is called umap-learn.\n\nsee examples on umap-learn.readthedocs.io"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#in-practice-2",
    "href": "lectures/11.3-Dimensionality.html#in-practice-2",
    "title": "Dimensionality",
    "section": "In Practice…",
    "text": "In Practice…\nimport umap\ntransformer = umap.UMAP(n_components=d)\nembedded = transformer.fit_transform(x)\nThe choice of n_neighbors, min_dist, and metric matter. In practice you may need to experiment with these."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#gotcha",
    "href": "lectures/11.3-Dimensionality.html#gotcha",
    "title": "Dimensionality",
    "section": "Gotcha!",
    "text": "Gotcha!\nt-SNE (less so UMAP) requires very careful handling:\n\nHyperparameters matter a lot\nCluster size means nothing\nCluster distances mean nothing\nClusters may mean nothing (low neighbour count/perplexity)\nOutputs are stochastic (not deterministic)\n\nBoth likely require repeated testing and experimentation."
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#other-approaches",
    "href": "lectures/11.3-Dimensionality.html#other-approaches",
    "title": "Dimensionality",
    "section": "Other Approaches",
    "text": "Other Approaches\n\nFeature selection, including forwards/backwards (sklearn.feature_selection here)\nDecomposition (sklearn.decomposition here, especiall SVD)\nOther types of manifold learning (sklearn.manifold here)\nRandom projection (sklearn.random_projection here)\nSupport Vector Machines (sklearn.svm here)\nEnsemble Methods (such as Random Forests: sklearn.ensemble.ExtraTreesClassifier and sklearn.ensemble.ExtraTreesRegressor here and here)"
  },
  {
    "objectID": "lectures/11.3-Dimensionality.html#additional-resources",
    "href": "lectures/11.3-Dimensionality.html#additional-resources",
    "title": "Dimensionality",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nRethinking ‘distance’ in New York City Medium URL\nFive Boroughs for the 21\\(^{st}\\) Century Medium URL\nCurse of Dimensionality\nThe Curse of Dimensionality\nUnderstanding Curse of Dimensionality\nCurse of Dimensionality – A ‘Curse’ to Machine Learning\nImportance of Feature Scaling\nUnderstanding PCA\nIntroduction to t-SNE in Python\n\n\n\nHow to Use t-SNE Effectively\nHow to tune the Hyperparameters of t-SNE\nUnderstanding UMAP (Compares to t-SNE)\nHow UMAP Works\n3 New Techniques for Data-Dimensionality Reduction in ML\nUMAP for Dimensionality Reduction (Video)\nA Bluffer’s Guide to Dimensionality Reduction (Video)"
  },
  {
    "objectID": "lectures/12.2-Classification.html",
    "href": "lectures/12.2-Classification.html",
    "title": "Classification",
    "section": "",
    "text": "Group observations by ‘class’.\nTypically based on 1-D distribution.\nClasses are assigned by user choice.\n\n\n\n\n\n\nLabel observations by ‘class’.\nTypically based on model outputs.\nLabels are assigned by user feedback.\n\n\n\n\n\nIn this session we are primarily concerned with the first column — classification as a modelling process is better considered a data science/modelling problem that is beyond the scope of this module."
  },
  {
    "objectID": "lectures/12.2-Classification.html#spot-the-difference",
    "href": "lectures/12.2-Classification.html#spot-the-difference",
    "title": "Classification",
    "section": "",
    "text": "Group observations by ‘class’.\nTypically based on 1-D distribution.\nClasses are assigned by user choice.\n\n\n\n\n\n\nLabel observations by ‘class’.\nTypically based on model outputs.\nLabels are assigned by user feedback.\n\n\n\n\n\nIn this session we are primarily concerned with the first column — classification as a modelling process is better considered a data science/modelling problem that is beyond the scope of this module."
  },
  {
    "objectID": "lectures/12.2-Classification.html#map-classification-choices",
    "href": "lectures/12.2-Classification.html#map-classification-choices",
    "title": "Classification",
    "section": "Map Classification Choices",
    "text": "Map Classification Choices\n\nAssign classes manually.\nSplit range evenly.\nSplit data evenly\nSplit data according to distribution\nSplit data according to their similarity to each other.\n\n\n\nAccording to some logic/theory/regulatory or policy fact or objective.\nEqual intervals for cases without heavy skew\nQuantiles or HeadTailBreaks for cases with heavy skew\nSD for cases with normal distribution; BoxPlot for others.\nNatural breaks/FIsher Jenks for cases where distribution is discontinuous"
  },
  {
    "objectID": "lectures/12.2-Classification.html#mapclassify",
    "href": "lectures/12.2-Classification.html#mapclassify",
    "title": "Classification",
    "section": "Mapclassify",
    "text": "Mapclassify\nMapclassify (part of PySAL) provides a wide range of classifiers:\n\n\n\nNo Parameters\nk Parameter\n\n\n\n\nBoxPlot\nUserDefined\n\n\nStdMean\nPercentiles\n\n\nMaxP\nQuantiles\n\n\nHeadTailBreaks\nNatural Breaks\n\n\nEqualInterval\nMaximum Breaks\n\n\n\nJenksCaspall/Sampled/Forced\n\n\n\nFisherJenks/Sampled\n\n\n\nk will a user-specified number of classes or binning criterion."
  },
  {
    "objectID": "lectures/12.2-Classification.html#raw",
    "href": "lectures/12.2-Classification.html#raw",
    "title": "Classification",
    "section": "Raw",
    "text": "Raw"
  },
  {
    "objectID": "lectures/12.2-Classification.html#user-defined",
    "href": "lectures/12.2-Classification.html#user-defined",
    "title": "Classification",
    "section": "User Defined",
    "text": "User Defined\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, 125000.00]\n0\n\n\n( 125000.00, 250000.00]\n4\n\n\n( 250000.00, 925000.00]\n865\n\n\n( 925000.00, 1500000.00]\n85\n\n\n(1500000.00, 4500000.00]\n29"
  },
  {
    "objectID": "lectures/12.2-Classification.html#box-plot",
    "href": "lectures/12.2-Classification.html#box-plot",
    "title": "Classification",
    "section": "Box Plot",
    "text": "Box Plot\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -31429.25]\n0\n\n\n( -31429.25, 391267.00]\n246\n\n\n( 391267.00, 495010.00]\n246\n\n\n( 495010.00, 673064.50]\n245\n\n\n( 673064.50, 1095760.75]\n175\n\n\n(1095760.75, 4416659.00]\n70"
  },
  {
    "objectID": "lectures/12.2-Classification.html#standard-deviations",
    "href": "lectures/12.2-Classification.html#standard-deviations",
    "title": "Classification",
    "section": "Standard Deviations",
    "text": "Standard Deviations\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n( -inf, -171366.63]\n0\n\n\n(-171366.63, 216174.43]\n0\n\n\n( 216174.43, 991256.55]\n892\n\n\n( 991256.55, 1378797.61]\n53\n\n\n(1378797.61, 4416659.00]\n38"
  },
  {
    "objectID": "lectures/12.2-Classification.html#max-p",
    "href": "lectures/12.2-Classification.html#max-p",
    "title": "Classification",
    "section": "Max P",
    "text": "Max P\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346594.00]\n142\n\n\n( 346594.00, 461577.00]\n279\n\n\n( 461577.00, 529197.00]\n140\n\n\n( 529197.00, 530662.00]\n3\n\n\n( 530662.00, 613465.00]\n115\n\n\n( 613465.00, 842387.00]\n167\n\n\n( 842387.00, 4416659.00]\n137"
  },
  {
    "objectID": "lectures/12.2-Classification.html#head-tail-breaks",
    "href": "lectures/12.2-Classification.html#head-tail-breaks",
    "title": "Classification",
    "section": "Head Tail Breaks",
    "text": "Head Tail Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 603715.49]\n670\n\n\n( 603715.49, 976290.79]\n218\n\n\n( 976290.79, 1508985.73]\n66\n\n\n(1508985.73, 2257581.55]\n16\n\n\n(2257581.55, 2826007.08]\n9\n\n\n(2826007.08, 3553496.25]\n3\n\n\n(3553496.25, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#equal-interval",
    "href": "lectures/12.2-Classification.html#equal-interval",
    "title": "Classification",
    "section": "Equal Interval",
    "text": "Equal Interval\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 825125.00]\n842\n\n\n( 825125.00, 1423714.00]\n108\n\n\n(1423714.00, 2022303.00]\n17\n\n\n(2022303.00, 2620892.00]\n10\n\n\n(2620892.00, 3219481.00]\n4\n\n\n(3219481.00, 3818070.00]\n1\n\n\n(3818070.00, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#quantiles",
    "href": "lectures/12.2-Classification.html#quantiles",
    "title": "Classification",
    "section": "Quantiles",
    "text": "Quantiles\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 346009.00]\n140\n\n\n( 346009.00, 405677.86]\n140\n\n\n( 405677.86, 461959.29]\n140\n\n\n( 461959.29, 529612.86]\n141\n\n\n( 529612.86, 639488.86]\n140\n\n\n( 639488.86, 827691.43]\n140\n\n\n( 827691.43, 4416659.00]\n141"
  },
  {
    "objectID": "lectures/12.2-Classification.html#natural-breaks",
    "href": "lectures/12.2-Classification.html#natural-breaks",
    "title": "Classification",
    "section": "Natural Breaks",
    "text": "Natural Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 433543.00]\n356\n\n\n( 433543.00, 605879.00]\n316\n\n\n( 605879.00, 842387.00]\n174\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/12.2-Classification.html#maximum-breaks",
    "href": "lectures/12.2-Classification.html#maximum-breaks",
    "title": "Classification",
    "section": "Maximum Breaks",
    "text": "Maximum Breaks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 1688895.00]\n961\n\n\n(1688895.00, 1926265.50]\n4\n\n\n(1926265.50, 2278155.50]\n5\n\n\n(2278155.50, 2929865.50]\n9\n\n\n(2929865.50, 3349991.00]\n2\n\n\n(3349991.00, 3959682.50]\n1\n\n\n(3959682.50, 4416659.00]\n1"
  },
  {
    "objectID": "lectures/12.2-Classification.html#fisher-jenks",
    "href": "lectures/12.2-Classification.html#fisher-jenks",
    "title": "Classification",
    "section": "Fisher Jenks",
    "text": "Fisher Jenks\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 435961.00]\n363\n\n\n( 435961.00, 607480.00]\n310\n\n\n( 607480.00, 842387.00]\n173\n\n\n( 842387.00, 1179615.00]\n80\n\n\n(1179615.00, 1866335.00]\n39\n\n\n(1866335.00, 2762387.00]\n14\n\n\n(2762387.00, 4416659.00]\n4"
  },
  {
    "objectID": "lectures/12.2-Classification.html#jenks-caspall",
    "href": "lectures/12.2-Classification.html#jenks-caspall",
    "title": "Classification",
    "section": "Jenks Caspall",
    "text": "Jenks Caspall\n\n\n\n\n\n\n\nInterval\nCount\n\n\n\n\n[ 226536.00, 365741.00]\n188\n\n\n( 365741.00, 441979.00]\n187\n\n\n( 441979.00, 520791.00]\n167\n\n\n( 520791.00, 638474.00]\n160\n\n\n( 638474.00, 890055.00]\n156\n\n\n( 890055.00, 1626454.00]\n103\n\n\n(1626454.00, 4416659.00]\n22"
  },
  {
    "objectID": "lectures/12.2-Classification.html#summary",
    "href": "lectures/12.2-Classification.html#summary",
    "title": "Classification",
    "section": "Summary",
    "text": "Summary\n\nThe choice of classification scheme should be data- and distribution-led. This is simply a demonstration of how different schemes can shape your understanding of the data."
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips",
    "href": "lectures/12.2-Classification.html#code-useful-tips",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the classes:\nkl = 7\ncls = [mapclassify.BoxPlot, ...,  mapclassify.JenksCaspall]\nSetting up the loop:\nfor cl in cls:\n    try: \n        m = cl(ppd.Value, k=kl)\n    except TypeError:\n        m = cl(ppd.Value)\n    \n    f = plt.figure()\n    gs = f.add_gridspec(nrows=2, ncols=1, height_ratios=[1,4])\n\n    ax1 = f.add_subplot(gs[0,0])\n    ...\n\n    ax2 = f.add_subplot(gs[1,0])\n    ..."
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips-1",
    "href": "lectures/12.2-Classification.html#code-useful-tips-1",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nSetting up the distribution:\n    ax1 = f.add_subplot(gs[0,0])\n    sns.kdeplot(ppd.Value, ax=ax1, color='r')\n    ax1.ticklabel_format(style='plain', axis='x') \n\n    y = ax1.get_ylim()[1]\n    for b in m.bins:\n        ax1.vlines(b, 0, y, linestyles='dotted')"
  },
  {
    "objectID": "lectures/12.2-Classification.html#code-useful-tips-2",
    "href": "lectures/12.2-Classification.html#code-useful-tips-2",
    "title": "Classification",
    "section": "Code (Useful Tips)",
    "text": "Code (Useful Tips)\nAdjusting the legend text:\ndef replace_legend_items(legend, mapping):\n    for txt in legend.texts:\n        for k,v in mapping.items():\n            if txt.get_text() == str(k):\n                txt.set_text(v)\nSetting up the map:\n    ax2 = f.add_subplot(gs[1,0])\n    ppd.assign(cl=m.yb).plot(column='cl', k=len(m.bins), categorical=True, legend=True, ax=ax2)\n    \n    mapping = dict([(i,s) for i,s in enumerate(m.get_legend_classes())])\n    ax2.set_axis_off()\n    replace_legend_items(ax2.get_legend(), mapping)"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html",
    "href": "lectures/12.4-Clustering_and_Geography.html",
    "title": "Clustering & Geography",
    "section": "",
    "text": "We now have to consider two more types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#space-adds-complexity",
    "href": "lectures/12.4-Clustering_and_Geography.html#space-adds-complexity",
    "title": "Clustering & Geography",
    "section": "",
    "text": "We now have to consider two more types of clustering:\n\nWith respect to polygons: regions are built from adjacent zones that are more similar to one another than to other adjacent zones.\nWith respect to points: points are distributed in a way that indicates ‘clumping’ at particular scales.\n\n\nType 1 is probably what you were thinking of in terms of clustering.\nType 2 is point pattern analysis and should be considered a substantially different area of research and type of analysis."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#trade-offs-again",
    "href": "lectures/12.4-Clustering_and_Geography.html#trade-offs-again",
    "title": "Clustering & Geography",
    "section": "Trade-offs (Again)…",
    "text": "Trade-offs (Again)…\nConsider:\n\nClustering algorithms are inherently spatial.\nClustering algorithms do not take space geography into account.\n\nDoes this matter?\n\nAll clustering algorithms are about inter-observation and intra-cluster distances so they have some conceptualisation of ‘space’.\nSpatially-aware clustering algorithms exist but are generally much more computationally-intensive than ‘regular ones’."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#different-approaches",
    "href": "lectures/12.4-Clustering_and_Geography.html#different-approaches",
    "title": "Clustering & Geography",
    "section": "Different Approaches",
    "text": "Different Approaches\n\n\n\nAlgorithm\nPros\nCons\nGeographically Aware?\n\n\n\n\nk-Means\nFast. Deterministic.\nEvery observation to a cluster.\nN.\n\n\nDBSCAN\nAllows for clusters and outliers.\nSlower. Choice of \\(\\epsilon\\) critical. Can end up with all outliers.\nN, but implicit in \\(\\epsilon\\).\n\n\nOPTICS\nFewer parameters than DBSCAN.\nEven slower.\nN, but implicit in \\(\\epsilon\\).\n\n\nHierarchical/ HDBSCAN\nCan cut at any number of clusters.\nNo ‘ideal’ solution.\nY, with connectivity parameter.\n\n\nADBSCAN\nScales. Confidence levels.\nMay need large data set to be useful. Choice of \\(\\epsilon\\) critical.\nY.\n\n\nMax-p\nCoherent regions returned.\nVery slow if model poorly specified.\nY."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "href": "lectures/12.4-Clustering_and_Geography.html#setting-the-relevant-distance",
    "title": "Clustering & Geography",
    "section": "Setting the Relevant Distance",
    "text": "Setting the Relevant Distance\nMany clustering algorithms rely on a distance specification (usually \\(\\epsilon\\)). So to set this threshold:\n\nIn high-dimensional spaces this threshold will need to be large.\nIn high-dimensional spaces the scale will be meaningless (i.e. not have a real-world meaning, only an abstract one).\nIn 2- or 3-dimensional (geographical) space this threshold could be meaningful (i.e. a value in metres could work)."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "href": "lectures/12.4-Clustering_and_Geography.html#choosing-a-distance-metric",
    "title": "Clustering & Geography",
    "section": "Choosing a Distance Metric",
    "text": "Choosing a Distance Metric\n\n\n\n\n\n\n\n\nn Dimensions\nHow to Set\nExamples\n\n\n\n\n2 or 3\nTheory/Empirical Data\nWalking speed; Commute distance\n\n\n2 or 3\nK/L Measures\nPlot with Simulation for CIs to identify significant ‘knees’.\n\n\n3\nMarked Point Pattern?\n\n\n\n&gt; 3\nkNN\nCalculate average kNN distance based on some expectation of connectivity.\n\n\n\n\nRemember: inter-observation distance increases with dimensionality!"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian",
    "title": "Clustering & Geography",
    "section": "Experian",
    "text": "Experian\nSpecialist in consumer segmentation and geodemographics (bit.ly/2jMRhAW).\n\nMarket cap: £14.3 billion.\nMosaic: “synthesises of 850 million pieces of information… to create a segmentation that allocates 49 million individuals and 26 million households into one of 15 Groups and 66 detailed Types.””\nMore than 450 variables used.\n\nMost retail companies will have their own segmentation scheme. Competitors: CACI, Nielsen, etc."
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian-groups",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian-groups",
    "title": "Clustering & Geography",
    "section": "Experian Groups",
    "text": "Experian Groups"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#experian-mapping",
    "href": "lectures/12.4-Clustering_and_Geography.html#experian-mapping",
    "title": "Clustering & Geography",
    "section": "Experian Mapping",
    "text": "Experian Mapping"
  },
  {
    "objectID": "lectures/12.4-Clustering_and_Geography.html#output-area-classification",
    "href": "lectures/12.4-Clustering_and_Geography.html#output-area-classification",
    "title": "Clustering & Geography",
    "section": "Output Area Classification",
    "text": "Output Area Classification\nOAC set up as ‘open source’ alternative to Mosaic:\n\nWell documented (UCL Geography a major contributor)\nDoesn’t require a license or payment\nCan be tweaked/extended/reweighted by users as needed"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html",
    "href": "lectures/2.2-Python_the_Basics-2.html",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "The simplest condition only considers one outcome:\nif &lt;condition is true&gt;:\n    #...do something...\nBut you’ll often needs something a little more sophisticated:\nif &lt;condition is true&gt;:\n    #...do something...\nelif &lt;some other condition is true&gt;:\n    #...do something else...\nelse:\n    #...if no conditions are true...\n\n#...code continues...\n\nBut no matter how complex, conditions always ultimately evaluate to True or False.\n\n\n\n\nif x &lt; y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x &lt; y:\n  print(\"x is less than y\")\nelif x &gt; y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")\n\n\n\nThe most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic.\n\n\n\n\nif hours &gt;= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?\n\n\n\nOutput from the Python interpreter:\n&gt;&gt;&gt; if hours &gt;= 0:\n... print(\"Hours were worked.\")\n  File \"&lt;stdin&gt;\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n&gt;&gt;&gt; else\n  File \"&lt;stdin&gt;\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt;     print \"No hours were worked.\")\n  File \"&lt;stdin&gt;\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent\n\n\n\nIt’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours &gt; 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#conditions-consequences",
    "href": "lectures/2.2-Python_the_Basics-2.html#conditions-consequences",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "The simplest condition only considers one outcome:\nif &lt;condition is true&gt;:\n    #...do something...\nBut you’ll often needs something a little more sophisticated:\nif &lt;condition is true&gt;:\n    #...do something...\nelif &lt;some other condition is true&gt;:\n    #...do something else...\nelse:\n    #...if no conditions are true...\n\n#...code continues...\n\nBut no matter how complex, conditions always ultimately evaluate to True or False."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#for-example",
    "href": "lectures/2.2-Python_the_Basics-2.html#for-example",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "if x &lt; y:\n  print(\"x is less than y\")\nelse:\n  print(\"x is not less than y\"\nOr:\nif x &lt; y:\n  print(\"x is less than y\")\nelif x &gt; y:\n  print(\"x is greater than y\")\nelse:\n  print(\"x equals y\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#conditional-syntax",
    "href": "lectures/2.2-Python_the_Basics-2.html#conditional-syntax",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "The most common sources of syntax errors in conditions are:\n\nIncorrect indenting;\nMissing colons on conditional code;\nUnbalanced parentheses;\nIncorrect logic."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-input",
    "href": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-input",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "if hours &gt;= 0:\nprint(\"Hours were worked.\")\nelse\n    print \"No hours were worked.\")\nAll four errors can be found here, can you spot them?"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-output",
    "href": "lectures/2.2-Python_the_Basics-2.html#all-of-them-together-output",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "Output from the Python interpreter:\n&gt;&gt;&gt; if hours &gt;= 0:\n... print(\"Hours were worked.\")\n  File \"&lt;stdin&gt;\", line 2\n    print(\"Hours were worked.\")\n    ^\nIndentationError: expected an indented block\n&gt;&gt;&gt; else\n  File \"&lt;stdin&gt;\", line 1\n    else\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt;     print \"No hours were worked.\")\n  File \"&lt;stdin&gt;\", line 1\n    print \"No hours were worked.\")\n    ^\nIndentationError: unexpected indent"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#thats-better",
    "href": "lectures/2.2-Python_the_Basics-2.html#thats-better",
    "title": "Python: the Basics(Part 2)",
    "section": "",
    "text": "It’s relatively straightforward to figure out the syntax errors, but the logical error is much less obvious. Over time, you become far more likely to make logical errors than syntactical ones.\nif hours &gt; 0:\n    print(\"Hours were worked.\")\nelse:\n    print(\"No hours were worked.\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#make-your-life-easy-well-easier",
    "href": "lectures/2.2-Python_the_Basics-2.html#make-your-life-easy-well-easier",
    "title": "Python: the Basics(Part 2)",
    "section": "Make Your Life Easy (Well, Easier)",
    "text": "Make Your Life Easy (Well, Easier)\nAlways comment your code:\n\nSo that you know what is going on.\nSo that you know why it is going on.\nSo that others can read your code.\nTo help you plan your code\n\n\nYou are reminding your future self what your code was for and helping to give it structure (explaining==thinking!)."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#different-comment-styles",
    "href": "lectures/2.2-Python_the_Basics-2.html#different-comment-styles",
    "title": "Python: the Basics(Part 2)",
    "section": "Different Comment Styles",
    "text": "Different Comment Styles\n# This is a short comment\nprint(\"Foo\")\nprint(\"Bar\") # Also a short comment\n\n# ------- New Section --------\n# You can have comments span multiple\n# lines just by adding more '#' at the \n# start of the line.\n\n# You can keep code from running\n# print(\"Baz\")"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#comments-follow-indentation",
    "href": "lectures/2.2-Python_the_Basics-2.html#comments-follow-indentation",
    "title": "Python: the Basics(Part 2)",
    "section": "Comments Follow Indentation",
    "text": "Comments Follow Indentation\n# Function for processing occupational data\n# from the 2001 and 2011 Censuses.\ndef occ_data(df):\n  #  Columns of interest\n  cols = ['Managerial','Professional','Technical']\n    \n  # Integrate results into single dataset -- \n  # right now we don't replicate Jordan's approach of\n  # grouping them into 'knowledge worker' and 'other'. \n  for i in df.iterrows():\n    # For each column...\n    for j in cols:\n      # Do something\n      ..."
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#easier-multi-line-comments",
    "href": "lectures/2.2-Python_the_Basics-2.html#easier-multi-line-comments",
    "title": "Python: the Basics(Part 2)",
    "section": "Easier Multi-Line Comments",
    "text": "Easier Multi-Line Comments\nThe below are not real comments, but they can help when you have a really long comment that you want to make. They are also used to help explain what a function does (called a docstring).\n\"\"\"\nSo I was thinking that what we need here is \na way to handle the case where the data is\nincomplete or contains an observation that we\nweren't expecting (e.g. \"N/A\" instead of \"0\").\n\"\"\""
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#tips",
    "href": "lectures/2.2-Python_the_Basics-2.html#tips",
    "title": "Python: the Basics(Part 2)",
    "section": "Tips",
    "text": "Tips\nSome useful tips for commenting your code:\n\nInclude general information at the top of your programming file.\nAssume the person reading the code is a coder themselves.\nGood commenting is sparse in the sense that it is used judiciously, and concise without being gnomic.\nUse comments to track the logic of your code (especially in conditionals and loops)"
  },
  {
    "objectID": "lectures/2.2-Python_the_Basics-2.html#additional-resources",
    "href": "lectures/2.2-Python_the_Basics-2.html#additional-resources",
    "title": "Python: the Basics(Part 2)",
    "section": "Additional Resources",
    "text": "Additional Resources\nHere are some links to videos on LinkedIn Learning that might help, and YouTube will undoubtedly have lots more options and styles of learning:\n\nConditional structures\nIf Statements\nIf-Else Statements\nIf-Elif\nWhitespace and comments\nUsing print()\nConditional syntax\nConditional operators\nConditional assignment"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#iteration",
    "href": "lectures/2.4-Iteration.html#iteration",
    "title": "Iteration",
    "section": "it·er·a·tion",
    "text": "it·er·a·tion\n/itə’rāSHən/\nNoun\nThe repetition of a process or utterance.\n\nrepetition of a mathematical or computational procedure applied to the result of a previous application, typically as a means of obtaining successively closer approximations to the solution of a problem.\na new version of a piece of computer hardware or software. plural noun: iterations\n\n\nMany programmers also call these loops."
  },
  {
    "objectID": "lectures/2.4-Iteration.html#two-types-of-iteration",
    "href": "lectures/2.4-Iteration.html#two-types-of-iteration",
    "title": "Iteration",
    "section": "Two Types of Iteration",
    "text": "Two Types of Iteration\n\n\n\n‘For’ loops\n\nUsed with finite lists of definite length\nFor each item in this list do something…\n\n\n\n\n‘While’ loops:\n\nUsed with unknown or non-finite lists\nWhile a condition is still True, do something to the list…"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#making-the-difference-memorable",
    "href": "lectures/2.4-Iteration.html#making-the-difference-memorable",
    "title": "Iteration",
    "section": "Making the Difference Memorable",
    "text": "Making the Difference Memorable"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#for-loops-1",
    "href": "lectures/2.4-Iteration.html#for-loops-1",
    "title": "Iteration",
    "section": "For Loops",
    "text": "For Loops\nThis ‘simple’ loop allows us to print out every element of the list in turn:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\nNotice the format:\nfor x in list:\n  # ...do something using the current value of x...\n\nNotice that this is the same in that we saw with if 'Batty' in geographers!"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#while-loops-1",
    "href": "lectures/2.4-Iteration.html#while-loops-1",
    "title": "Iteration",
    "section": "While Loops",
    "text": "While Loops\nThis loop does the same thing, but differently:\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )\n  g += 1\nNotice the format:\nwhile &lt;some condition is true&gt;:\n  # ...do something..."
  },
  {
    "objectID": "lectures/2.4-Iteration.html#nesting-loops",
    "href": "lectures/2.4-Iteration.html#nesting-loops",
    "title": "Iteration",
    "section": "Nesting Loops",
    "text": "Nesting Loops\nWe can use one loop ‘inside’ another loop! What do you think this might print?\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  for h in g:\n    print(h)\nHuh??? Let’s puzzle this out…"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#debugging",
    "href": "lectures/2.4-Iteration.html#debugging",
    "title": "Iteration",
    "section": "Debugging",
    "text": "Debugging\nWhen you see something completely new, it’s often good to:\n\nAdd print(...) statements to see how the values of a variable are changing.\nComment out the parts you don’t understand so that you can focus on the parts you do\nThen iteratively add complexity!"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-1-the-outer-loop",
    "href": "lectures/2.4-Iteration.html#step-1-the-outer-loop",
    "title": "Iteration",
    "section": "Step 1: The ‘Outer Loop’",
    "text": "Step 1: The ‘Outer Loop’\nSo I would start off with:\ngeographers = ['Rose','Massey','Jefferson']\nfor g in geographers:\n  print(g)\n#   for h in g:\n#   print(h)\nThis prints:\n'Rose'\n'Massey'\n'Jefferson'"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-1-summing-up",
    "href": "lectures/2.4-Iteration.html#step-1-summing-up",
    "title": "Iteration",
    "section": "Step 1: Summing Up",
    "text": "Step 1: Summing Up\nOK, so now we know:\n\nThat g is the name of a geographer.\nThe ‘outer’ loop sets g to the name of a new geographer on each iteration.\nSo if g is set to 'Rose' what does for h in g: do?"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-2-the-inner-loop",
    "href": "lectures/2.4-Iteration.html#step-2-the-inner-loop",
    "title": "Iteration",
    "section": "Step 2: The ‘Inner’ Loop",
    "text": "Step 2: The ‘Inner’ Loop\nWe know change it like this:\nfor g in geographers:\n  print(g)\n  for h in g:\n    print(h)\n  break # &lt;-- Notice this!\nThis prints:\nRose\nR\no\ns\ne"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#step-2-summing-up",
    "href": "lectures/2.4-Iteration.html#step-2-summing-up",
    "title": "Iteration",
    "section": "Step 2: Summing Up",
    "text": "Step 2: Summing Up\nAnd now we know that:\n\nh is looping over the string, meaning that a string can be treated as a list!\nAnd break is a really useful way to control a loop while you’re testing your code!"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#recap",
    "href": "lectures/2.4-Iteration.html#recap",
    "title": "Iteration",
    "section": "Recap",
    "text": "Recap\n\nfor iterates once over a collection items (e.g. a list).\nwhile keeps going until a condition is False."
  },
  {
    "objectID": "lectures/2.4-Iteration.html#test-yourself",
    "href": "lectures/2.4-Iteration.html#test-yourself",
    "title": "Iteration",
    "section": "Test Yourself",
    "text": "Test Yourself\nWhat will this code print? I’d suggest that you don’t run it!\ngeographers = ['Rose','Massey','Jefferson']\ng = 0\nwhile g &lt; len(geographers):\n  print( geographers[g] )"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#test-yourself-tricksy-version",
    "href": "lectures/2.4-Iteration.html#test-yourself-tricksy-version",
    "title": "Iteration",
    "section": "Test Yourself (Tricksy Version)",
    "text": "Test Yourself (Tricksy Version)\nHere’s a really tricky one! The following two blocks of code produce the same output, how are they different?\ngeographers = ['Rose','Massey','Jefferson']\ngeographers.reverse()\nfor g in geographers:\n  print(g)\nAnd:\ngeographers = ['Rose','Massey','Jefferson']\ng = len(geographers)-1\nwhile g &gt;= 0:\n  print( geographers[g] )\n  g -= 1"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#one-more-thing",
    "href": "lectures/2.4-Iteration.html#one-more-thing",
    "title": "Iteration",
    "section": "One More Thing…",
    "text": "One More Thing…\nLet’s go back to the Lists examples for a second:\nfemale_geographers = ['Rose','Valentine','Massey','Jefferson']\nmale_geographers = ['Von Humboldt','Harvey','Hägerstrand']\nall_geographers = []\nall_geographers.append(female_geographers)\nall_geographers.append(male_geographers)\nHave a think about how this code works:\nfor ag in all_geographers:\n  for g in ag:\n    print(g)"
  },
  {
    "objectID": "lectures/2.4-Iteration.html#additional-resources",
    "href": "lectures/2.4-Iteration.html#additional-resources",
    "title": "Iteration",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWhat is Iteration?\nLoops\nFor Loop\nWhile Loop\n\nWe don’t cover the concept of recursion, but it’s quite a powerful idea and links nicely with Iteration:\n\nWhat is a recursive function?\nDefine recursive functions"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html",
    "href": "lectures/2.6-What_We_Do.html",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "",
    "text": "“I used to think that writing is the most trivial part of the job of a data scientist; because the analyses and numbers should speak for themselves right? Wrong!” ~ Xie (2024b)1"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#the-pipeline",
    "href": "lectures/2.6-What_We_Do.html#the-pipeline",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nCode\nAnalyse\nUnderstand\nCommunicate\nReport\n\n\nHere are five things that we might imagine any data scientists, spatial or otherwise, does.\nDo we think they’re in the right order?\nI can tell you right now that this sequence of steps is how to get a mark of between 45 and 63 in your dissertation."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#the-pipeline-1",
    "href": "lectures/2.6-What_We_Do.html#the-pipeline-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nUnderstand\nCode\nReport\nAnalyse\nCommunicate\n\n\nHere’s a slightly better order, here’s why:\n\nYou can’t answer a question – from your boss, from an academic, from your friends even – if you don’t understand it. So before you run off and start writing some code, the first thing you need to do is understand the problem you’re trying to solve. That problem should not be technical, it should be practical.\nOnce you understand the problem you can start trying to code a solution.\nThe code will allow you to produce reports. These reports might be to do with data quality, they might be diagnostics from a Random Forest Machine Learning algorithm. At each stage in the development of your results you should be generating reports that help you to better-understand your problem and work out if your code is working.\nOnce the reports have given you confidence in your findings now you can actually write the analysis. Your analysis might lead you to realise that you need to go back and write more code and produce more reports, but that’s normal.\nFinally, you need to work out how to communicate your analysis. If you understand the problem then you’ll find this process rewarding. If you don’t then you’ll find it frustrating and want to brush it off quickly.\n\nSo the fact these are in a list is still rather misleading because at each point you get feedback effects, and it’s also a loop."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-understand",
    "href": "lectures/2.6-What_We_Do.html#how-to-understand",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-understand-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-understand-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?\n\n\nWhy are we doing this?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-report",
    "href": "lectures/2.6-What_We_Do.html#how-to-report",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-report-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-report-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number.\n\n\nThese are the takeaways."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-analyse",
    "href": "lectures/2.6-What_We_Do.html#how-to-analyse",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods we can use."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-analyse-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-analyse-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods I can use.\n\n\nThis is the method that matches the need."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-communicate",
    "href": "lectures/2.6-What_We_Do.html#how-to-communicate",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#how-to-communicate-1",
    "href": "lectures/2.6-What_We_Do.html#how-to-communicate-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?\n\n\nWho needs to know?"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#tell-me-a-story",
    "href": "lectures/2.6-What_We_Do.html#tell-me-a-story",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#tell-me-a-story-1",
    "href": "lectures/2.6-What_We_Do.html#tell-me-a-story-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story.\n\n\nFocus on the ‘so what’."
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#tell-me-a-story-2",
    "href": "lectures/2.6-What_We_Do.html#tell-me-a-story-2",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story.\n\n\nFocus on the ‘so what’.\n\n\nDon’t bury the lede.\n\n\nAll data science is, ultimately, a story. A story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways.\n\n\n\n\nTell the Right Story\n\n\n(That’s why we’re torturing you.)\n\n\n\nSo what’s fine for consultancy, might not be for academia or newspaper reporting. And vice versa.\nAcross the last five years what we’ve noticed"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#additional-resources",
    "href": "lectures/2.6-What_We_Do.html#additional-resources",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nVersion Control with Git\nSetting up and managing your GitHub user account\nPersonal Access Tokens on Git\nGit Cheat Sheet"
  },
  {
    "objectID": "lectures/2.6-What_We_Do.html#footnotes",
    "href": "lectures/2.6-What_We_Do.html#footnotes",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis session draws liberally on her experience.↩︎"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html",
    "href": "lectures/2.8-On_Coding.html",
    "title": "On Coding",
    "section": "",
    "text": "According to the BBC & Google, programming is about:"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#what-about-computational-thinking",
    "href": "lectures/2.8-On_Coding.html#what-about-computational-thinking",
    "title": "On Coding",
    "section": "What About Computational Thinking?",
    "text": "What About Computational Thinking?\n\nComputational thinking is not thinking like a Computer Scientist. It is about recognising how to code can help us to understand, and manipulate, the world."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#key-features",
    "href": "lectures/2.8-On_Coding.html#key-features",
    "title": "On Coding",
    "section": "Key Features",
    "text": "Key Features\nAspects of computational thinking include:\n\nRecognising how one problem connects to other problems.\nRecognising when and how to make things simpler and faster\nRecognising how different ways of tackling a problem gives you power to tackle new problems.\n\nSee this keynote by Lorena Barba (2014); esp. from 52:00 onwards.\n\nYou already do a lot of this when you generalise from your readings to your ideas/understanding!"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#why-are-these-virtues",
    "href": "lectures/2.8-On_Coding.html#why-are-these-virtues",
    "title": "On Coding",
    "section": "Why Are These Virtues?",
    "text": "Why Are These Virtues?\nAccording to Larry Wall the three virtues of the programmer are:\n\nLaziness\nImpatience\nHubris\n\nThese are not to be taken literally (see Larry Wall’s “Three Virtues of a Programmer” are Utter Bull💩).\n\nAutomate the boring stuff, focus on the interesting bits! And it’s not about quantity of code, it’s about quality.\nUse code to save time, but don’t just jump head-first into problems.\nWhen something isn’t working well you want to make it work better/faster/more efficiently…"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#four-quotes-to-remember",
    "href": "lectures/2.8-On_Coding.html#four-quotes-to-remember",
    "title": "On Coding",
    "section": "Four Quotes to Remember",
    "text": "Four Quotes to Remember\n\nComputers make very fast, very accurate mistakes.\nA computer program does what you tell it to do, not what you want it to do.\nOnly half of programming is coding. The other 90% is debugging.\nWeeks of coding can save you hours of planning."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#and-one-more",
    "href": "lectures/2.8-On_Coding.html#and-one-more",
    "title": "On Coding",
    "section": "And One More…",
    "text": "And One More…"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#following-a-recipe-is-easy-right",
    "href": "lectures/2.8-On_Coding.html#following-a-recipe-is-easy-right",
    "title": "On Coding",
    "section": "Following a Recipe is Easy, Right?",
    "text": "Following a Recipe is Easy, Right?\n\nSource"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#calculating-the-mean",
    "href": "lectures/2.8-On_Coding.html#calculating-the-mean",
    "title": "On Coding",
    "section": "Calculating the Mean",
    "text": "Calculating the Mean\nGiven these numbers, what’s the average?\n1, 4, 7, 6, 4, 2"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#as-a-recipe",
    "href": "lectures/2.8-On_Coding.html#as-a-recipe",
    "title": "On Coding",
    "section": "As a Recipe",
    "text": "As a Recipe\n\nTake a list of numbers\nStart a count of numbers in the list at 0\nStart a sum of numbers in the list at 0\nTake a number from the list:\n\nAdd 1 to the count\nAdd the value of the number to the sum\n\nRepeat step #4 until no numbers are left in the list.\nDivide the sum by the count\nReport this number back to the user"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#as-python",
    "href": "lectures/2.8-On_Coding.html#as-python",
    "title": "On Coding",
    "section": "As Python",
    "text": "As Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#why-we-still-havent-solved-it",
    "href": "lectures/2.8-On_Coding.html#why-we-still-havent-solved-it",
    "title": "On Coding",
    "section": "Why We Still Haven’t ‘Solved It’",
    "text": "Why We Still Haven’t ‘Solved It’\n\nSource\n\nComputers are good at:\n\nDoing exactly what they are told.\nDoing it over and over indefinitely.\n\nComputers are not good at:\n\nSolving problems that you can’t define.\nCreating new knowledge."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#languages",
    "href": "lectures/2.8-On_Coding.html#languages",
    "title": "On Coding",
    "section": "Languages",
    "text": "Languages\nComputer languages come with all of the ‘baggage’ of human languages; they have:\n\nA vocabulary (reserved words)\nA grammar (syntax)\nRules about the kinds of things you can say (grammar)\nStyles and idiosyncrasies all their own (history)\n\nIn this module we will use the Python programming language. We could also teach this same content in R."
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#python",
    "href": "lectures/2.8-On_Coding.html#python",
    "title": "On Coding",
    "section": "Python",
    "text": "Python\nnumbers = [1, 4, 7, 6, 4, 2]\ntotal   = 0\ncount   = 0\nfor num in numbers:\n  total = total + num \n  count += 1 # An alternative to count = count + 1\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#r",
    "href": "lectures/2.8-On_Coding.html#r",
    "title": "On Coding",
    "section": "R",
    "text": "R\nnumbers = c(1, 4, 7, 6, 4, 2)\ntotal   = 0\ncount   = 0\nfor (num in numbers) {\n  total = total + num\n  count = count + 1\n}\nmean = total / count\nprint(mean)"
  },
  {
    "objectID": "lectures/2.8-On_Coding.html#finally-style",
    "href": "lectures/2.8-On_Coding.html#finally-style",
    "title": "On Coding",
    "section": "Finally: Style",
    "text": "Finally: Style\nAlthough all programmers develop their own style (see: writing in any language), Python encourages coders to use a consistent style so that others can pick up your code and make sense of what’s going on (see: comments!).\nTwo useful resources:\n\nThe Hitchhiker’s Guide to Python\nA summary of Python code style conventions"
  },
  {
    "objectID": "lectures/3.2-LOLs.html",
    "href": "lectures/3.2-LOLs.html",
    "title": "LoLs",
    "section": "",
    "text": "my_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nHow about if we rewrote it like this?\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nWhat does this look like?\n\n\n\nWe can ‘unpack’ my_list in stages in order to make sense of it:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nfor i in my_list:\n  print(i)\nWhat do you think this will print?\n\n\n\nLet’s make it a little more obvious:\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [7, 8, 9]\n\nmy_list = [a, b, c]\n\nfor i in my_list:\n  print(i) # Prints a, b, c in turn...\n\n\n\nWe could then try this:\nfor i in my_list:\n  print(f\" &gt;&gt; {i}\")\n  for j in i: # Remember that i is a list!\n    print(j)\nThis produces:\n &gt;&gt; [1, 2, 3]\n1\n2\n3\n &gt;&gt; [4, 5, 6]\n4\n...\n\n\n\nSome observations:\n\nWe can access i in my_list using either for i in my_list (every element in turn) or my_list[i] (one element only).\nWe can access j in list i using for j in i (every element in turn) or i[j] (one element only).\n\nDoes that mean we can also do this:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\ni,j = 0,1\nprint(my_list[i][j])\n\n\n\nSo if we write:\ni,j = 0,1\nprint(my_list[i][j])\nThen:\n\nmy_list[i] returns [1,2,3] (because i==0 and the first list is [1,2,3]), and\nmy_list[i][j] returns 2 (because j==1 and the [1,2,3][1]==2).\n\nSimilarly, my_list[2] grabs the third list ([7,8,9]) and then my_list[2][2] tells Python to get the third item in that third list (i.e. 9).\n\nHow you print the number 5 from this list-of-lists?\n\n\n\n\nIf I rewrite the list this way perhaps it looks a little more useful?\nmy_cities = [\n  ['London', 51.5072, 0.1275, +0], \n  ['New York', 40.7127, 74.0059, -5], \n  ['Tokyo', 35.6833, 139.6833, +8]\n]\nNow we have something that is starting to look like data!"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-sense-of-this",
    "href": "lectures/3.2-LOLs.html#making-sense-of-this",
    "title": "LoLs",
    "section": "",
    "text": "We can ‘unpack’ my_list in stages in order to make sense of it:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\nfor i in my_list:\n  print(i)\nWhat do you think this will print?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "href": "lectures/3.2-LOLs.html#debugging-our-thinking",
    "title": "LoLs",
    "section": "",
    "text": "Let’s make it a little more obvious:\na = [1, 2, 3]\nb = [4, 5, 6]\nc = [7, 8, 9]\n\nmy_list = [a, b, c]\n\nfor i in my_list:\n  print(i) # Prints a, b, c in turn..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#the-next-step",
    "href": "lectures/3.2-LOLs.html#the-next-step",
    "title": "LoLs",
    "section": "",
    "text": "We could then try this:\nfor i in my_list:\n  print(f\" &gt;&gt; {i}\")\n  for j in i: # Remember that i is a list!\n    print(j)\nThis produces:\n &gt;&gt; [1, 2, 3]\n1\n2\n3\n &gt;&gt; [4, 5, 6]\n4\n..."
  },
  {
    "objectID": "lectures/3.2-LOLs.html#putting-it-together",
    "href": "lectures/3.2-LOLs.html#putting-it-together",
    "title": "LoLs",
    "section": "",
    "text": "Some observations:\n\nWe can access i in my_list using either for i in my_list (every element in turn) or my_list[i] (one element only).\nWe can access j in list i using for j in i (every element in turn) or i[j] (one element only).\n\nDoes that mean we can also do this:\nmy_list = [\n  [1, 2, 3], \n  [4, 5, 6], \n  [7, 8, 9]\n]\n\ni,j = 0,1\nprint(my_list[i][j])"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "href": "lectures/3.2-LOLs.html#lets-talk-it-out",
    "title": "LoLs",
    "section": "",
    "text": "So if we write:\ni,j = 0,1\nprint(my_list[i][j])\nThen:\n\nmy_list[i] returns [1,2,3] (because i==0 and the first list is [1,2,3]), and\nmy_list[i][j] returns 2 (because j==1 and the [1,2,3][1]==2).\n\nSimilarly, my_list[2] grabs the third list ([7,8,9]) and then my_list[2][2] tells Python to get the third item in that third list (i.e. 9).\n\nHow you print the number 5 from this list-of-lists?"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#making-this-useful",
    "href": "lectures/3.2-LOLs.html#making-this-useful",
    "title": "LoLs",
    "section": "",
    "text": "If I rewrite the list this way perhaps it looks a little more useful?\nmy_cities = [\n  ['London', 51.5072, 0.1275, +0], \n  ['New York', 40.7127, 74.0059, -5], \n  ['Tokyo', 35.6833, 139.6833, +8]\n]\nNow we have something that is starting to look like data!"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "href": "lectures/3.2-LOLs.html#down-the-rabbit-hole",
    "title": "LoLs",
    "section": "Down the Rabbit Hole",
    "text": "Down the Rabbit Hole"
  },
  {
    "objectID": "lectures/3.2-LOLs.html#lols-of-lols",
    "href": "lectures/3.2-LOLs.html#lols-of-lols",
    "title": "LoLs",
    "section": "LOLs of LOLs",
    "text": "LOLs of LOLs\nThis is also a legitimate list in Python.\nmy_cities = [\n  ['London', [51.5072, 0.1275], +0], \n  ['New York', [40.7127, 74.0059], -5], \n  ['Tokyo', [35.6833, 139.6833], +8]\n]\nprint(my_cities[0][0])\n&gt; London\nprint(my_cities[0][1][0])\n&gt; 51.5072\n\nWhy might it be a better choice of data structure than the earlier version?"
  },
  {
    "objectID": "lectures/3.4-Git.html",
    "href": "lectures/3.4-Git.html",
    "title": "Getting to Grips with Git",
    "section": "",
    "text": "You will need two things to get started (you should have already done this if following the class in sequence):\n\ngit installed on your computer: git-scm.com/downloads\nA GitHub account: github.com\n\n\n\n\n\nVersion control has been around longer than the Internet. Broadly, it was designed to achieve three things:\n\nA record of any and all edits made to a file containing code.\nA means of allowing developers to share edits with one another.\nA way of reconciling conflicts when two (or more) developers edited the same code."
  },
  {
    "objectID": "lectures/3.4-Git.html#how-it-works",
    "href": "lectures/3.4-Git.html#how-it-works",
    "title": "Getting to Grips with Git",
    "section": "How It Works",
    "text": "How It Works\nThe natural way normal people think about managing versions of a document is to save a copy with a new name that somehow shows which version is most recent.\nThe natural way developers used to think about managing versions of a document is to have a master copy somewhere. Everyone asks the server for the master copy, makes some changes, and then checks those changes back in.\nThis is not how Git works.\n\nThe way normal people approach this problem assumes that, usually, only one or two people are making changes. But how do you coordinate with 20 other people to find out who has the most recent copy then collect all 21 people’s changes?\nThe way developers used to approach this problem assumes that someone is in final charge. That a company or organisation runs a server which will decide whose changes are allowed, and whose are not."
  },
  {
    "objectID": "lectures/3.4-Git.html#how-git-works",
    "href": "lectures/3.4-Git.html#how-git-works",
    "title": "Getting to Grips with Git",
    "section": "How Git Works",
    "text": "How Git Works\nGit is distributed, meaning that every computer where git is installed has its own master copy.\nSo every computer has a full history of any git project (aka. repository or ‘repo’). Indeed, you don’t have to synchronise your repo with any other computer or server at all! 1\n\nIn order to make this useful, you need ways to synchronise changes between computers that all think they’re right."
  },
  {
    "objectID": "lectures/3.4-Git.html#github",
    "href": "lectures/3.4-Git.html#github",
    "title": "Getting to Grips with Git",
    "section": "GitHub",
    "text": "GitHub\nGitHub is nothing special to Git, just another Git server with which to negotiate changes. Do not think of GitHub as the ‘master’ copy. There isn’t one.\nThere are, however, upstream and remote repositories.\n\nAn ‘upstream’ repository is where there’s a ‘gatekeeper’: e.g. the people who run PySAL have a repo that is considered the ‘gatekeeper’ for PySAL.\nA remote repository is any repository with which your copy synchronises. So the remote repository can be ‘upstream’ or it can just be another computer you run, or you GitHub account."
  },
  {
    "objectID": "lectures/3.4-Git.html#a-dropbox-analogy",
    "href": "lectures/3.4-Git.html#a-dropbox-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Dropbox Analogy",
    "text": "A Dropbox Analogy\n\nThink of JupyterLab as being like Word or Excel: an application that allows you to read/write/edit notebook files.\nThink of GitHub as being like Dropbox: a place somewhere in the cloud that files on your home machine can be backed up.\n\nBut Dropbox doesn’t have the .gitignore file and fine-grained control over what is synchronised when!\n\nGitHub offers a lot of ‘value added’ features (like simple text editing) on top of the basic service of ‘storing files’."
  },
  {
    "objectID": "lectures/3.4-Git.html#getting-started",
    "href": "lectures/3.4-Git.html#getting-started",
    "title": "Getting to Grips with Git",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\nTerm\nMeans\n\n\n\n\nRepository (Repo)\nA project or achive stored in Git.\n\n\ninit\nTo create a new repo on your computer.\n\n\nclone\nTo make a full copy of a repo somewhere else.\n\n\n\nThis creates a local repo that is unsynchronised with anything else:\nmkdir test\ncd test\ngit init\nWhereas this creates a local clone that is fully synchronised with GitHub:\ncd .. # To move out of 'test'\ngit clone https://github.com/jreades/fsds.git"
  },
  {
    "objectID": "lectures/3.4-Git.html#working-on-a-file",
    "href": "lectures/3.4-Git.html#working-on-a-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a File",
    "text": "Working on a File\n\n\n\nTerm\nMeans\n\n\n\n\nadd\nAdd a file to a repo.\n\n\nmv\nMove/Rename a file in a repo.\n\n\nrm\nRemove a file from a repo.\n\n\n\nFor example:\ncd test # Back into the new Repo\ntouch README.md # Create empty file called README.md\ngit add README.md # Add it to the repository\ngit mv README.md fileA.md # Rename it (move it)\ngit rm fileA.md # Remove it... which is an Error!\nThis produces:\nerror: the following file has changes staged in the index:\n    fileA.md\n(use --cached to keep the file, or -f to force removal)\n\nThis is telling you that you can force remove (git rm -f fileA.md) if you really want, but you’d probably be better off commiting the changes that have been ‘staged’… more on this in a second!\nAlso: no one else knows about these changes yet!"
  },
  {
    "objectID": "lectures/3.4-Git.html#looking-at-the-history",
    "href": "lectures/3.4-Git.html#looking-at-the-history",
    "title": "Getting to Grips with Git",
    "section": "Looking at the History",
    "text": "Looking at the History\n\n\n\nTerm\nMeans\n\n\n\n\ndiff\nShow changes between commits.\n\n\nstatus\nShow status of files in repo.\n\n\nlog\nShow history of commits.\n\n\n\nFor example:\ncd ../test/ # In case you weren't already there\ngit status  # What's the status\nThis produces:\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   fileA.md\n\nSo again, git is giving us hints as to the options: ‘changes to be committed’ vs. ‘unstage’ the changes. We can also see what files are to be committed (i.e. have changed)."
  },
  {
    "objectID": "lectures/3.4-Git.html#working-on-a-project-or-file",
    "href": "lectures/3.4-Git.html#working-on-a-project-or-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a Project or File",
    "text": "Working on a Project or File\n\n\n\nTerm\nMeans\n\n\n\n\ncommit\nTo record changes to the repo.\n\n\nbranch\nCreate or delete branches.\n\n\ncheckout\nJump to a different branch.\n\n\n\nFor example:\ngit commit -m \"Added and then renamed the README.\"\ngit status\nYou should see:\n[master (root-commit) e7a0b25] Added and then renamed the README Markdown file.\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 fileA.md\n# ... and then this:\nOn branch master\nnothing to commit, working tree clean\nMake a note of the number after ‘root-commit’!"
  },
  {
    "objectID": "lectures/3.4-Git.html#recovery",
    "href": "lectures/3.4-Git.html#recovery",
    "title": "Getting to Grips with Git",
    "section": "Recovery",
    "text": "Recovery\ngit rm fileA.md\ngit status\ngit commit -m \"Removed file.\"\nls \ngit checkout &lt;number you wrote down earlier&gt;\nls \n\nSo every operation on a file is recorded in the repository: adding, renaming, deleting, and so on. And we can roll back any change at any time. For plain-text files (such as Markdown, Python and R scripts) these changes are recorded at the level of each line of code: so you can jump around through your entire history of a project and trace exactly when and what changes you (or anyone else) made."
  },
  {
    "objectID": "lectures/3.4-Git.html#a-rock-climbing-analogy",
    "href": "lectures/3.4-Git.html#a-rock-climbing-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Rock-Climbing Analogy",
    "text": "A Rock-Climbing Analogy"
  },
  {
    "objectID": "lectures/3.4-Git.html#collaborating-on-a-project",
    "href": "lectures/3.4-Git.html#collaborating-on-a-project",
    "title": "Getting to Grips with Git",
    "section": "Collaborating on a Project",
    "text": "Collaborating on a Project\n\n\n\nTerm\nMeans\n\n\n\n\npull\nTo request changes on a repo from another computer.\n\n\npush\nTo send changes on a repo to another computer.\n\n\n\nFor example:\ngit push"
  },
  {
    "objectID": "lectures/3.4-Git.html#a-note-on-workflow",
    "href": "lectures/3.4-Git.html#a-note-on-workflow",
    "title": "Getting to Grips with Git",
    "section": "A Note on Workflow",
    "text": "A Note on Workflow\nSo your workflow should be:\n\nSave edits to Jupyter notebook.\nRun git add &lt;filename.ipynb&gt; to record changes to the notebook (Note: replace &lt;filename.ipynb&gt; completely with the notebook filename).\nRun git commit -m \"Adding notes based on lecture\" (or whatever message is appropriate: -m means ‘message’).\nThen run git push to push the changes to GitHub.\n\nIf any of those commands indicate that there are no changes being recorded/pushed then it might be that you’re not editing the file that you think you are (this happens to me!).\nOn the GitHub web site you may need to force reload the view of the repository: Shift + the Reload button usually does it in most browsers. You may also need to wait 5 to 10 seconds for the changes to become ‘visible’ before reloading. It’s not quite instantaeous."
  },
  {
    "objectID": "lectures/3.4-Git.html#this-is-not-easy",
    "href": "lectures/3.4-Git.html#this-is-not-easy",
    "title": "Getting to Grips with Git",
    "section": "This is not easy",
    "text": "This is not easy\n\n\n\n\n\nSource"
  },
  {
    "objectID": "lectures/3.4-Git.html#so-here-are-some-cheat-sheets",
    "href": "lectures/3.4-Git.html#so-here-are-some-cheat-sheets",
    "title": "Getting to Grips with Git",
    "section": "So here are some cheat sheets",
    "text": "So here are some cheat sheets"
  },
  {
    "objectID": "lectures/3.4-Git.html#additional-resources",
    "href": "lectures/3.4-Git.html#additional-resources",
    "title": "Getting to Grips with Git",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nUnderstanding Git (Part 1) – Explain it Like I’m Five\nTrying Git\nVisualising Git\nGit Novice\nConfusing Git Terminology\nGit Cheat Sheet: Commands and Best Practices\nAndy’s R-focussed Tutorial\n\n\nI now have everything in Git repos: articles, research, presentations, modules… the uses are basically endless once you start using Markdown heavily (even if you don’t do much coding)."
  },
  {
    "objectID": "lectures/3.4-Git.html#footnotes",
    "href": "lectures/3.4-Git.html#footnotes",
    "title": "Getting to Grips with Git",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’d suggest that this is leaving the benefit of free backups on the table for no good reason!↩︎"
  },
  {
    "objectID": "lectures/4.2-Decorators.html",
    "href": "lectures/4.2-Decorators.html",
    "title": "Decorators",
    "section": "",
    "text": "hello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#a-basic-function",
    "href": "lectures/4.2-Decorators.html#a-basic-function",
    "title": "Decorators",
    "section": "",
    "text": "hello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#modifying-the-function",
    "href": "lectures/4.2-Decorators.html#modifying-the-function",
    "title": "Decorators",
    "section": "Modifying the Function",
    "text": "Modifying the Function\nLet’s get meta:\n\n\n+ Calling hello...\n  + Defining hello!\n+ hello returned Hello World!\n\n\n'Hello World!'"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#decorating-the-function",
    "href": "lectures/4.2-Decorators.html#decorating-the-function",
    "title": "Decorators",
    "section": "Decorating the Function",
    "text": "Decorating the Function\n\n\nHello World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#wtf",
    "href": "lectures/4.2-Decorators.html#wtf",
    "title": "Decorators",
    "section": "WTF?",
    "text": "WTF?\nHow did that happen?"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#the-decorator-function",
    "href": "lectures/4.2-Decorators.html#the-decorator-function",
    "title": "Decorators",
    "section": "The Decorator Function",
    "text": "The Decorator Function\nLet’s unpack the wrapper:"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#reusing-a-decorator",
    "href": "lectures/4.2-Decorators.html#reusing-a-decorator",
    "title": "Decorators",
    "section": "Reusing a Decorator",
    "text": "Reusing a Decorator\nEverything’s ‘better’ now:\n\n\nGoodbye World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#but",
    "href": "lectures/4.2-Decorators.html#but",
    "title": "Decorators",
    "section": "But…",
    "text": "But…\nBut this:\nWill trigger this:\n      2 def wrapper():\n----&gt; 3   return func().title() + \"!\"\n\nAttributeError: 'int' object has no attribute 'title'"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#chaining-decorators",
    "href": "lectures/4.2-Decorators.html#chaining-decorators",
    "title": "Decorators",
    "section": "Chaining Decorators1",
    "text": "Chaining Decorators1\n\n\n['Hello', 'World!']"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#using-functiontools",
    "href": "lectures/4.2-Decorators.html#using-functiontools",
    "title": "Decorators",
    "section": "Using Func(tion)Tools",
    "text": "Using Func(tion)Tools\nAnd there are decorators for decorators…\n\n\nHello World!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#unpicking-functools",
    "href": "lectures/4.2-Decorators.html#unpicking-functools",
    "title": "Decorators",
    "section": "Unpicking Functools",
    "text": "Unpicking Functools"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#making-use-of-metadata",
    "href": "lectures/4.2-Decorators.html#making-use-of-metadata",
    "title": "Decorators",
    "section": "Making Use of Metadata",
    "text": "Making Use of Metadata\nCompare:\n\n\n\n\nwrapper\nWraps a function to format it.\n\n\n\n\n\nhello\nPrints hello world"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#some-applications-1",
    "href": "lectures/4.2-Decorators.html#some-applications-1",
    "title": "Decorators",
    "section": "Some Applications 12",
    "text": "Some Applications 12\n\n\n+ Executing 'add' with args: (2, 5)\n  + Result is: 7\n\n\n7"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#some-applications-2",
    "href": "lectures/4.2-Decorators.html#some-applications-2",
    "title": "Decorators",
    "section": "Some Applications 23",
    "text": "Some Applications 23\nThis would output:\nInside the main function.\nExiting the program. Cleanup tasks can be performed here."
  },
  {
    "objectID": "lectures/4.2-Decorators.html#benefits-of-decorators",
    "href": "lectures/4.2-Decorators.html#benefits-of-decorators",
    "title": "Decorators",
    "section": "Benefits of Decorators4",
    "text": "Benefits of Decorators4\n\nCode readability: each function is more narrowly focussed on the ‘thing’ it’s supposed to do, without extraneous validation, logging, or authentication ‘cruft’.\nCode reuse: keep your code ‘DRY’ (Don’t Repeat Yourself) by applying the same code across multiple functions (e.g. log the arguments this function received)\nModification without alteration: extending the behaviour of something else (e.g. you can’t/don’t want to modify someone else’s code, but need some additional step to be performend)\nLogging made simple: add/remove debugging and logging functionality quickly, easily, and consistently.\nDevelopment: Python web frameworks (Django, Flask) use decorators to handle requests.\nError handling: manage error-handling ‘centrally’ by placing try and except around every function you want to manage."
  },
  {
    "objectID": "lectures/4.2-Decorators.html#theres-more",
    "href": "lectures/4.2-Decorators.html#theres-more",
    "title": "Decorators",
    "section": "There’s More…",
    "text": "There’s More…\nWe’ve barely scratched the surface, decorators can:\n\nTake arguments (which might alter the behavour of the wrapped function).\nHelp to make classes and methods more useful (see the Methods and Classes lectures).\nManage common tasks like authorisation and permissions.\n\nThere’s lots, lost more, but using decorators effectively will seriously impress anyone interviewing you for a job while also helping you to understand a lot more about good programming!"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#additional-resources",
    "href": "lectures/4.2-Decorators.html#additional-resources",
    "title": "Decorators",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nSpecial /ht to @wraps Explained in 30 seconds\nPython Decorators Explained\nPython Tips: Decorators\nPython Decorators: How to Use it and Why?\n5 Key Benefits of Using Python Decorators for Optimized COding Practices\n\n\n\nPython’s Most Powerful Decorator\nChaining Decorators\nDecorators and Generators in Python\nAvailable Standard Decorators in Python\nAwesome Python Decorators"
  },
  {
    "objectID": "lectures/4.2-Decorators.html#footnotes",
    "href": "lectures/4.2-Decorators.html#footnotes",
    "title": "Decorators",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDatacamp↩︎\nKDNuggets: 8 Built-in Python Decorators↩︎\nKDNuggets: 8 Built-in Python Decorators↩︎\n5 Key Benefits of Using Python Decorators↩︎"
  },
  {
    "objectID": "lectures/4.4-Assessments.html",
    "href": "lectures/4.4-Assessments.html",
    "title": "Assessments",
    "section": "",
    "text": "I am working to allocate everyone to a group of at least four so:\n\nIf you are in a group of less than four you may be partnered with students in another practical session to make a ‘full strength’ group.\nIn this case, you will be reallocated to a mutually convenient practical session so that you are not penalised by being unable to work together.\nI would encourage you to start sitting and working together, and for one of you to set up and share a private GitHub repo for Assessment #2 (more on this in a second).\n\nAnd remember: talk about how you want your group to work! See: Group Working talk.1"
  },
  {
    "objectID": "lectures/4.4-Assessments.html#organise-your-group",
    "href": "lectures/4.4-Assessments.html#organise-your-group",
    "title": "Assessments",
    "section": "",
    "text": "I am working to allocate everyone to a group of at least four so:\n\nIf you are in a group of less than four you may be partnered with students in another practical session to make a ‘full strength’ group.\nIn this case, you will be reallocated to a mutually convenient practical session so that you are not penalised by being unable to work together.\nI would encourage you to start sitting and working together, and for one of you to set up and share a private GitHub repo for Assessment #2 (more on this in a second).\n\nAnd remember: talk about how you want your group to work! See: Group Working talk.1"
  },
  {
    "objectID": "lectures/4.4-Assessments.html#assessment-1",
    "href": "lectures/4.4-Assessments.html#assessment-1",
    "title": "Assessments",
    "section": "Assessment #1",
    "text": "Assessment #1\n\n1. What is it?\nTimed, Open Book Exam\n\n\n2. What does ‘timed’ mean?\nOnce you start the assessment you will have a fixed amount of time in which to complete it; 1h 20m for most students, but students with relevant SORAs have 2 hours (please email me).\n\n\nOk, so when is it?\nFriday, 21 November 2025\n\n\nWhat is the window?\n11am-11pm or 1pm-3pm2\n\n\n\nHow do I complete it?\nYou will access your questions through Moodle. You can write the code that you need to answer the question in JupyterLab or any other Python programming environment.\n\n\nWhat does ‘open book’ mean?\nYou may look at any resource you like, you may not ask another human being for assistance. You may use ChatGPT or another LLM but must report when you do.\n\n\nWhat will the exam be on?\nAny concept up to and including Week 5 is ‘fair game’. The exam will consist of no more than 10 questions.3\n\n\n\nHow is it graded?\nWe will manually grade it in Moodle. There are roughly 960 answers so we may make some mistakes and you can flag these where it is clear this has happened.\n\n\nWhy should I report ChatGDP?\nThe use of ChatGPT has no impact on your grade, we are primarily curious about how useful people are finding it and whether LLMs affect your understanding."
  },
  {
    "objectID": "lectures/4.4-Assessments.html#assessment-2",
    "href": "lectures/4.4-Assessments.html#assessment-2",
    "title": "Assessments",
    "section": "Assessment #2",
    "text": "Assessment #2\nThe reproducible analysis must be a runnable QMD (Quarto Markdown Document) file that addresses the set questions provided in class. The QMD file will be assessed on two components:\n\nIts content (60% of this assessment): do the answers written by the group engage through a mix of literature, critical thinking, and data analysis with the set questions?\nIts reproducibility (40% of this assessment): do the analyses employed, and outputs created by the group run fully and without errors on a different computer, and do they show evidence of thought in relation to the quality of coding and outputs?\n\nA template has been provided. You can see both PDF and HTML output, but please only submit the PDF!\n\n\nPart 1: Set Questions (60%)\nStarting in Week 7, groups will be randomly selected to present their answer to one of that week’s question in class.\n\nFormative feedback will be provided in class on the quality of the response.\nIf the selected group has not prepared an answer then no formative feedback will be provided for that question.\n\nThe format is not academic: while referencing is still expected, the style for all questions should be written for a non-specialist audience. I am updating the rubric to clarify this.\n\n\n\nPart 2: Reproducibility (40%)\nYour QMD document will be evaluated separately from its content for:\n\nReproducibility (20%) — Can the analysis be run on another system? Two considerations: 1) without changes to the code; and 2) with useful guidance/documentation being offered in terms of what to do if ‘things go wrong’.\nQuality (20%) — Is the code/analysis intelligible and well-presented such that another user doesn’t just ‘run the code’ but actually understands and has confidence in the how and why of what you’ve done? This is about making maximal use of the tools at their disposal such that the code is efficient, even elegant, and the outputs are clear and legible."
  },
  {
    "objectID": "lectures/4.4-Assessments.html#assessment-3",
    "href": "lectures/4.4-Assessments.html#assessment-3",
    "title": "Assessments",
    "section": "Assessment #3",
    "text": "Assessment #3\nWe know that people contribute differently to groups or, sometimes, not at all. This self- and peer-assessment seeks to quantify that contribution while also prompting you to reflect on what you contributed to your group."
  },
  {
    "objectID": "lectures/4.4-Assessments.html#footnotes",
    "href": "lectures/4.4-Assessments.html#footnotes",
    "title": "Assessments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: In my experience overly narrow specialisation does not work.↩︎\nThis is the ‘window’ for doing the assessment. You can start any time after the window opens but must finish before the window closes.↩︎\nSo, roughly, that means about 12 minutes/question.↩︎"
  },
  {
    "objectID": "lectures/5.1-Methods.html",
    "href": "lectures/5.1-Methods.html",
    "title": "Methods",
    "section": "",
    "text": "We know that a function looks like this:\n&lt;function name&gt;( &lt;input&gt; )\nAnd we know that a function in a package looks like this:\n&lt;package name&gt;.&lt;function name&gt;( &lt;input&gt; )\nSo is list a package?\nmy_list.append( &lt;value&gt; )"
  },
  {
    "objectID": "lectures/5.1-Methods.html#todays-question",
    "href": "lectures/5.1-Methods.html#todays-question",
    "title": "Methods",
    "section": "",
    "text": "We know that a function looks like this:\n&lt;function name&gt;( &lt;input&gt; )\nAnd we know that a function in a package looks like this:\n&lt;package name&gt;.&lt;function name&gt;( &lt;input&gt; )\nSo is list a package?\nmy_list.append( &lt;value&gt; )"
  },
  {
    "objectID": "lectures/5.1-Methods.html#well",
    "href": "lectures/5.1-Methods.html#well",
    "title": "Methods",
    "section": "Well…",
    "text": "Well…\nmy_list.append( &lt;value&gt; ) is a function.\nmy_list.append( &lt;value&gt; ) is a special type of function called a method."
  },
  {
    "objectID": "lectures/5.1-Methods.html#whats-a-method-then",
    "href": "lectures/5.1-Methods.html#whats-a-method-then",
    "title": "Methods",
    "section": "What’s a Method Then?",
    "text": "What’s a Method Then?\nPackages group useful constants and functions together in one place.\nMethods group constants and functions together in one place with data.\nSo my_list.append(...) is called a list method:\n\nIt only knows how to append things to lists.\nIt is only available as a function when you have an insantiated list (e.g. [] or [1,'dog',3.5]).\nIt is bound to variables (aka. objects) of class list."
  },
  {
    "objectID": "lectures/5.1-Methods.html#proof",
    "href": "lectures/5.1-Methods.html#proof",
    "title": "Methods",
    "section": "Proof!",
    "text": "Proof!\nmy_list = [] # \nhelp(my_list)\nThis will give you:\nHelp on list object:\n\nclass list(object)\n |  list(iterable=(), /)\n |  Built-in mutable sequence.\n |  If no argument is given, the constructor creates a new empty list.\n |  Methods defined here:\n | ...\n |  append(self, object, /)\n |      Append object to the end of the list.\n |\n |  clear(self, /)\n |      Remove all items from list.\n |\n |  copy(self, /)\n |      Return a shallow copy of the list.\n | ...\n\nIt’s not obvious here, but you can also create lists by writing list()."
  },
  {
    "objectID": "lectures/5.1-Methods.html#its-all-methods",
    "href": "lectures/5.1-Methods.html#its-all-methods",
    "title": "Methods",
    "section": "It’s all Methods",
    "text": "It’s all Methods\nmsg = 'Hello World'\ndir(msg)\n['__add__', '__class__', ..., 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', ... 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\nAnd then we can inquire about these methods:\nhelp(msg.capitalize)\nHelp on built-in function capitalize:\n\ncapitalize() method of builtins.str instance\n    Return a capitalized version of the string.\n\n    More specifically, make the first character have upper case and the rest lower\n    case."
  },
  {
    "objectID": "lectures/5.1-Methods.html#final-fundamental-concepts",
    "href": "lectures/5.1-Methods.html#final-fundamental-concepts",
    "title": "Methods",
    "section": "Final Fundamental Concepts",
    "text": "Final Fundamental Concepts\nFrom here on out, nearly all of what you learn will be new applications, not new concepts and terminology.\n\n\n\n\n\n\n\n\nTerm\nMeans\nExample\n\n\n\n\nClass\nThe template for a ‘thing’.\nRecipe for a pizza.\n\n\nObject\nThe instantiated ‘thing’.\nA pizza I can eat!\n\n\nMethod\nFunctions defined for the class and available to the object.\nThings I can do with a pizza (eat, cook, make).\n\n\nConstructor\nThe special method that builds new objects of that class.\nHow to start a new pizza!\n\n\nSelf\nA reference to the current object.\nThe pizza in front of me!"
  },
  {
    "objectID": "lectures/5.3-Design.html",
    "href": "lectures/5.3-Design.html",
    "title": "Object-Oriented Design",
    "section": "",
    "text": "Encapsulation\nInheritance\nPolymorphism\n\n\nEncapsulation: the bundling of the data and the methods that work on that data in one unit; data is often accessed via methods and not directly. I don’t need to care how something works, only that it works.\nInheritance: classes can inherit functionality from ‘parents’ and then extend this in useful ways. So geopandas extends pandas, and movingpandas extends geopandas!\nPolymorphism: a ‘child’ class can ideally be used anywhere its parent class can be used because it only extends (i.e. adds) functionality and does not break parent behaviour. A class that implements functionality from multiple parents can be used anywhere those parents would be used.\n\n\n\n\n\n\n\nMost people would call this a class hierarchy or diagram.\n\nThere is no natural order here: where do e-bikes, unicycles, and rickshaws go?\n\nIndeed, we could map out vehicles based on the number of axles, their source of power, their driver positioning, etc., etc. The point here is that the designer must make choices that are influenced by (and will influence) the design of the application for which this class hierarchy is buing developed. An automobile manufacturer might make different choices from a government trying to implement a tax policy.\n\n\n\n\n\nFunctionally, a class and a package are indistinguishable, but a class produces objects that use methods and instance or class variables, whereas a package is a group of functions and constants that may, or may not, include classes.\n\nUgh, now try to keep this straight in your head.\n\n\n\n\nYou’ve been using Classes and Methods since you started.\nYou can ‘package up’ useful code into functions, and useful functions into packages.\nTogether, packages and classes will turbo-charge your programming skills.\nYou can stand on the shoulders of giants!\n\n\n\n\n\nWhat is object-oriented programming?\nPython object-oriented programming\nObject-oriented programming refresher\nUnderstanding inheritance\nAbstract base classes\nPython - Object-Oriented"
  },
  {
    "objectID": "lectures/5.3-Design.html#tree-of-vehicles",
    "href": "lectures/5.3-Design.html#tree-of-vehicles",
    "title": "Object-Oriented Design",
    "section": "",
    "text": "Most people would call this a class hierarchy or diagram.\n\nThere is no natural order here: where do e-bikes, unicycles, and rickshaws go?\n\nIndeed, we could map out vehicles based on the number of axles, their source of power, their driver positioning, etc., etc. The point here is that the designer must make choices that are influenced by (and will influence) the design of the application for which this class hierarchy is buing developed. An automobile manufacturer might make different choices from a government trying to implement a tax policy."
  },
  {
    "objectID": "lectures/5.3-Design.html#classes-vs-packages",
    "href": "lectures/5.3-Design.html#classes-vs-packages",
    "title": "Object-Oriented Design",
    "section": "",
    "text": "Functionally, a class and a package are indistinguishable, but a class produces objects that use methods and instance or class variables, whereas a package is a group of functions and constants that may, or may not, include classes.\n\nUgh, now try to keep this straight in your head."
  },
  {
    "objectID": "lectures/5.3-Design.html#key-takeaways",
    "href": "lectures/5.3-Design.html#key-takeaways",
    "title": "Object-Oriented Design",
    "section": "",
    "text": "You’ve been using Classes and Methods since you started.\nYou can ‘package up’ useful code into functions, and useful functions into packages.\nTogether, packages and classes will turbo-charge your programming skills.\nYou can stand on the shoulders of giants!"
  },
  {
    "objectID": "lectures/5.3-Design.html#additional-resources",
    "href": "lectures/5.3-Design.html#additional-resources",
    "title": "Object-Oriented Design",
    "section": "",
    "text": "What is object-oriented programming?\nPython object-oriented programming\nObject-oriented programming refresher\nUnderstanding inheritance\nAbstract base classes\nPython - Object-Oriented"
  },
  {
    "objectID": "lectures/6.1-Logic.html",
    "href": "lectures/6.1-Logic.html",
    "title": "Logic",
    "section": "",
    "text": "Consider the following three things I’ve said to my toddler:\n\nDo you want to go to the park and have ice cream?\nDo you want cereal or toast for breakfast?\nDo not touch that again or else!\n\nUnlike my toddler, the computer does listen, but my toddler has more common sense!\n\n\n\n\n\n\n\n(x,y) = True,False\nif x:\n  print(\"x\")\n\nif x and y:\n  print(\"x and y\")\n\nif x or y:\n  print(\"x or y\")\n\nif x and not y:\n  print(\"x and not y\")\n\nif not(x and y):\n  print(\"not x and y\")\n\nif x is the simplest: if True then print \"x\". There is no logic, we just do it or we don’t depending on whether x is True.\nif x and y is the ‘park and ice cream question’: there’s no ice cream without going to the park. So both things have to be True for this to work out happily and print(\"x and y\").\nif x or y is the ‘cereal or toast for breakfast’ question: my daughter is having breakfast either way, so if she answers ‘Yes’ (True) to either then it doesn’t matter that the other one is False, she’s still had breakfast, or we’re still printing “x or y”. And note, if she was really hungry and said yes to both then that would also be breakfast. So if they are both True that’s also fine.\nif x and not y: is the negation of one term. So ‘if you behave and don’t touch that again then you will get a treat’ is what we’re looking for here. But you can also negate the entire thing: not(x and y) is also valid logic. So this turns a False on if x and y into a True.\n\n\n\n\nRemember that operators like &lt;= and == also produce True/False answers:\nx = y = 5\nz = 3\nif x==y:\n  print(\"x==y\")\n\nif x==y and x==z:\n  print(\"x==y and x==z\")\n\nif x==y or x==z:\n  print(\"x==y or x==z\")\n\nif x==y and not x==z:\n  print(\"x==y and not x==z\")\n\n\n\nThere is a second set of logical operators that apply in very specific circumstances. These are called ‘bitwise’ operators and apply to data specified in bits.\n\n\n\nRegular Operator\nBitwise Equivalent\n\n\n\n\nand\n&\n\n\nor\n|\n\n\nnot\n~\n\n\n\nLet’s see (briefly) how these work…\n\n\n\nx,y = 38,3\nprint(f\"{x:b}\") # `:b` means byte-format\nprint(f\"{y:b}\")\nThis gives us that x is '100110' and y is '11', so now:\nprint(f\"{x & y:b}\")  # 10\nprint(f\"{x | y:b}\")  # 100111\nprint(f\"{x & ~y:b}\") # 100100\n\n\n\n\n\n\nOperator\n1\n2\n3\n4\n5\n6\n\n\n\n\nx\n1\n0\n0\n1\n1\n0\n\n\ny\n0\n0\n0\n0\n1\n1\n\n\nx & y\n0\n0\n0\n0\n1\n0\n\n\nx | y\n1\n0\n0\n1\n1\n1\n\n\n~y\n1\n1\n1\n1\n0\n0\n\n\nx & ~y\n1\n0\n0\n1\n0\n0\n\n\n\nBitwise operations are very, very fast and so are a good way to, say, find things in large data sets. You’ve been warned.\n\nThey are how pandas and numpy manage indexes and queries against data frames.\n\n\n\n\nBeware of using logic with things that are not what they appear:\n\nNone is Python’s way of saying that something has no value at all (not 0 or \"\"… but None). It is a class.\nNaN (Not a Number) is a special numeric data type provided by the numpy package to deal with things like -ve and +ve infinity and similar ‘issues’.\n\nnp.nan should be used whenever you are dealing with data (e.g. see Pandas!).\n\n\n\nimport numpy as np\nprint(type(np.nan))     # float\nprint(type(None))       # NoneType\nCritically:\nprint(\"\"==None)         # False\nprint(None==None)       # True\nprint(np.nan==None)     # False\nprint(np.nan==np.nan)   # False!\nprint(np.nan is np.nan) # True\nprint(np.isnan(np.nan)) # True"
  },
  {
    "objectID": "lectures/6.1-Logic.html#do-you-want",
    "href": "lectures/6.1-Logic.html#do-you-want",
    "title": "Logic",
    "section": "",
    "text": "Consider the following three things I’ve said to my toddler:\n\nDo you want to go to the park and have ice cream?\nDo you want cereal or toast for breakfast?\nDo not touch that again or else!\n\nUnlike my toddler, the computer does listen, but my toddler has more common sense!"
  },
  {
    "objectID": "lectures/6.1-Logic.html#in-code-now",
    "href": "lectures/6.1-Logic.html#in-code-now",
    "title": "Logic",
    "section": "",
    "text": "(x,y) = True,False\nif x:\n  print(\"x\")\n\nif x and y:\n  print(\"x and y\")\n\nif x or y:\n  print(\"x or y\")\n\nif x and not y:\n  print(\"x and not y\")\n\nif not(x and y):\n  print(\"not x and y\")\n\nif x is the simplest: if True then print \"x\". There is no logic, we just do it or we don’t depending on whether x is True.\nif x and y is the ‘park and ice cream question’: there’s no ice cream without going to the park. So both things have to be True for this to work out happily and print(\"x and y\").\nif x or y is the ‘cereal or toast for breakfast’ question: my daughter is having breakfast either way, so if she answers ‘Yes’ (True) to either then it doesn’t matter that the other one is False, she’s still had breakfast, or we’re still printing “x or y”. And note, if she was really hungry and said yes to both then that would also be breakfast. So if they are both True that’s also fine.\nif x and not y: is the negation of one term. So ‘if you behave and don’t touch that again then you will get a treat’ is what we’re looking for here. But you can also negate the entire thing: not(x and y) is also valid logic. So this turns a False on if x and y into a True."
  },
  {
    "objectID": "lectures/6.1-Logic.html#combining-logic-with-operators",
    "href": "lectures/6.1-Logic.html#combining-logic-with-operators",
    "title": "Logic",
    "section": "",
    "text": "Remember that operators like &lt;= and == also produce True/False answers:\nx = y = 5\nz = 3\nif x==y:\n  print(\"x==y\")\n\nif x==y and x==z:\n  print(\"x==y and x==z\")\n\nif x==y or x==z:\n  print(\"x==y or x==z\")\n\nif x==y and not x==z:\n  print(\"x==y and not x==z\")"
  },
  {
    "objectID": "lectures/6.1-Logic.html#a-special-case",
    "href": "lectures/6.1-Logic.html#a-special-case",
    "title": "Logic",
    "section": "",
    "text": "There is a second set of logical operators that apply in very specific circumstances. These are called ‘bitwise’ operators and apply to data specified in bits.\n\n\n\nRegular Operator\nBitwise Equivalent\n\n\n\n\nand\n&\n\n\nor\n|\n\n\nnot\n~\n\n\n\nLet’s see (briefly) how these work…"
  },
  {
    "objectID": "lectures/6.1-Logic.html#working-with-bits",
    "href": "lectures/6.1-Logic.html#working-with-bits",
    "title": "Logic",
    "section": "",
    "text": "x,y = 38,3\nprint(f\"{x:b}\") # `:b` means byte-format\nprint(f\"{y:b}\")\nThis gives us that x is '100110' and y is '11', so now:\nprint(f\"{x & y:b}\")  # 10\nprint(f\"{x | y:b}\")  # 100111\nprint(f\"{x & ~y:b}\") # 100100"
  },
  {
    "objectID": "lectures/6.1-Logic.html#perhaps-easier-to-see-this-way",
    "href": "lectures/6.1-Logic.html#perhaps-easier-to-see-this-way",
    "title": "Logic",
    "section": "",
    "text": "Operator\n1\n2\n3\n4\n5\n6\n\n\n\n\nx\n1\n0\n0\n1\n1\n0\n\n\ny\n0\n0\n0\n0\n1\n1\n\n\nx & y\n0\n0\n0\n0\n1\n0\n\n\nx | y\n1\n0\n0\n1\n1\n1\n\n\n~y\n1\n1\n1\n1\n0\n0\n\n\nx & ~y\n1\n0\n0\n1\n0\n0\n\n\n\nBitwise operations are very, very fast and so are a good way to, say, find things in large data sets. You’ve been warned.\n\nThey are how pandas and numpy manage indexes and queries against data frames."
  },
  {
    "objectID": "lectures/6.1-Logic.html#nulls-none-vs.-nan",
    "href": "lectures/6.1-Logic.html#nulls-none-vs.-nan",
    "title": "Logic",
    "section": "",
    "text": "Beware of using logic with things that are not what they appear:\n\nNone is Python’s way of saying that something has no value at all (not 0 or \"\"… but None). It is a class.\nNaN (Not a Number) is a special numeric data type provided by the numpy package to deal with things like -ve and +ve infinity and similar ‘issues’.\n\nnp.nan should be used whenever you are dealing with data (e.g. see Pandas!)."
  },
  {
    "objectID": "lectures/6.1-Logic.html#none-vs.-nan",
    "href": "lectures/6.1-Logic.html#none-vs.-nan",
    "title": "Logic",
    "section": "",
    "text": "import numpy as np\nprint(type(np.nan))     # float\nprint(type(None))       # NoneType\nCritically:\nprint(\"\"==None)         # False\nprint(None==None)       # True\nprint(np.nan==None)     # False\nprint(np.nan==np.nan)   # False!\nprint(np.nan is np.nan) # True\nprint(np.isnan(np.nan)) # True"
  },
  {
    "objectID": "lectures/6.1-Logic.html#in-not-in",
    "href": "lectures/6.1-Logic.html#in-not-in",
    "title": "Logic",
    "section": "In / Not In",
    "text": "In / Not In\nWe’ve touched on these before:\ng = ['Harvey','Rose','Batty','Jefferson']\n\nif 'Batty' in g:\n  print(\"In the group!\")\n\nif 'Marx' not in g:\n  print(\"Not in the group!\")\nThe set data type also supports in, and not in together with all of the set maths (union, intersect, etc.).\n\nThis is a good place for a recap though."
  },
  {
    "objectID": "lectures/6.1-Logic.html#sets",
    "href": "lectures/6.1-Logic.html#sets",
    "title": "Logic",
    "section": "Sets",
    "text": "Sets\nMembership maths:\ns1 = {'cherry','orange','banana','tomato'} # Or s1(...)\ns2 = {'potato','celery','carrot','tomato'} # Or s2(...)\nprint('potato' in s1)      # False\nprint(s1.difference(s2))   # {'banana', ...}\nprint(s1.intersection(s2)) # {'tomato'}\nprint(s1.union(s2))        # {'orange', ...}\n\nThese more advanced functions are only for sets, not lists or dicts."
  },
  {
    "objectID": "lectures/6.1-Logic.html#additional-resources",
    "href": "lectures/6.1-Logic.html#additional-resources",
    "title": "Logic",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nLogical operators: And, or, not\nComparison operators\nBitwise operators\nComparison operators\n\n\n\nBoolean operators\nOperator precedence\nNaN and None in Python\nHandling Missing Data"
  },
  {
    "objectID": "lectures/6.3-Files.html",
    "href": "lectures/6.3-Files.html",
    "title": "Data Formats",
    "section": "",
    "text": "In order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data set have row or column names? (a.k.a. headers & metadata)\nIs the metadata in a separate file or embedded in the file?"
  },
  {
    "objectID": "lectures/6.3-Files.html#from-files-to-data",
    "href": "lectures/6.3-Files.html#from-files-to-data",
    "title": "Data Formats",
    "section": "",
    "text": "In order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data set have row or column names? (a.k.a. headers & metadata)\nIs the metadata in a separate file or embedded in the file?"
  },
  {
    "objectID": "lectures/6.3-Files.html#structure-of-a-tabular-data-file",
    "href": "lectures/6.3-Files.html#structure-of-a-tabular-data-file",
    "title": "Data Formats",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names make it a lot easier to find and refer to data (e.g. the ‘East of England row’ or the ‘Total column’) but they are not data and don’t belong in the data set itself.\nUsually, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ’carriage return (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nUsually, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,) which gives rise to CSV (Comma-Separate Values), but it could be tabs (\\t) or anything else too (; or | or EOF).\n\nHow would we choose a good field separator?\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/6.3-Files.html#most-common-formats",
    "href": "lectures/6.3-Files.html#most-common-formats",
    "title": "Data Formats",
    "section": "Most Common Formats",
    "text": "Most Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/6.3-Files.html#mapping-data-types",
    "href": "lectures/6.3-Files.html#mapping-data-types",
    "title": "Data Formats",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often see the term ‘mapping’ used in connection to data that is not spatial, what do they mean? A map is the term used in some programming languages for a dict! So it’s about key : value pairs again.\nHere’s a mapping\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/6.3-Files.html#testing-a-mapping",
    "href": "lectures/6.3-Files.html#testing-a-mapping",
    "title": "Data Formats",
    "section": "Testing a Mapping",
    "text": "Testing a Mapping\nWorking out an appropriate mapping (representation of the data) is hugely time-consuming.\n\nIt’s commonly held that 80% of data science is data cleaning.\n\nThe Unix utilities (grep, awk, tail, head) can be very useful for quickly exploring the data in order to develop a basic understanding of the data and to catch obvious errors.\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/6.3-Files.html#label-these",
    "href": "lectures/6.3-Files.html#label-these",
    "title": "Data Formats",
    "section": "Label These",
    "text": "Label These\n\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/6.3-Files.html#things-that-can-go-wrong",
    "href": "lectures/6.3-Files.html#things-that-can-go-wrong",
    "title": "Data Formats",
    "section": "Things That Can Go Wrong…",
    "text": "Things That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/6.3-Files.html#arrow-and-parquet",
    "href": "lectures/6.3-Files.html#arrow-and-parquet",
    "title": "Data Formats",
    "section": "Arrow and Parquet",
    "text": "Arrow and Parquet\n\nArrow is an in-memory columnar format for data. Data is stored in a structured way in RAM making it blazingly fast for operations.\nParquet is a highly-compressed columnar file format for data. Data is stored in a structured way on your hard drive.\nFeather is a raw storage format for Arrow.\n\nTL;DR: for most applications Parquet will give nice, small files on disk and the benefits of columnar file storage; for computationally intensive applications where disk space and interoperability with other systems isn’t an issue then Feather might work."
  },
  {
    "objectID": "lectures/6.3-Files.html#what-about-the-duck",
    "href": "lectures/6.3-Files.html#what-about-the-duck",
    "title": "Data Formats",
    "section": "What About the Duck?",
    "text": "What About the Duck?\n\n\n\n\n\nServerless SQL queries against Parquet files\nQueries returned as Pandas data frames\nSelect and filter before loading\nFast conversion between CSV and Parquet via Arrow"
  },
  {
    "objectID": "lectures/6.3-Files.html#for-later-reference",
    "href": "lectures/6.3-Files.html#for-later-reference",
    "title": "Data Formats",
    "section": "For (Later) Reference",
    "text": "For (Later) Reference\n# Notice the engine and dtype_backend options\ndf = pandas.read_csv(fname, engine='pyarrow', \n                dtype_backend='pyarrow')\n\n# And for parquet files\ndf = pandas.read_parquet(fname, columns=[...])\n\n# And for DuckDB we can actually joing two\n# files before they even get to Python!\nq = f'''\n  SELECT * FROM \n    read_parquet('epc-ppd-2022-*.parquet') as ppd, \n    read_parquet('epc-ldd-2022-*.parquet') as ldd,\n  WHERE ppd.uid=ldd.uid\n'''\ndf = duckdb.query(q).df()\nP.S. There’s also a command-line tool for DuckDB so you don’t even need Python."
  },
  {
    "objectID": "lectures/6.3-Files.html#additional-resources",
    "href": "lectures/6.3-Files.html#additional-resources",
    "title": "Data Formats",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\n\nUnderstanding Directories and Subdirectories\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\n\n\n\nText vs. binary mode\nText files\npetl\npandas 2.0 and the Arrow revolution (Part 1)\nWhat parquet files are my preferred API for bulk open data\nDuckDB Documentation"
  },
  {
    "objectID": "lectures/7.0-Feedback.html",
    "href": "lectures/7.0-Feedback.html",
    "title": "Responding to Feedback",
    "section": "",
    "text": "We are not perfect and we are always looking to improve.\n\n\nCASA does not get to pick the rooms in which these classes are taught! And we barely have any control at all over timings1.\n\n\n\nYou might see your comments here. We are not trying to embarrass anyone. We are trying to improve the course.\n\n\n\nI paraphrase:\n\nActivities, reading, and exam relate and the lectures have some fun examples that are easy to follow.\nThere is some reading, followed by questions which makes students engage with the homework.\nPracticals are interactive, with clear tutorials explaining the topic step by step.\nA lot of materials for further learning and organized so that I could practice a lot by myself.\nReally strong PGTAs.\n\n\n\n\nI paraphrase:\n\nToo much ethics in FSDS so I didn’t have time to practice my coding skills.\nI think it’s not necessary to spend too much time discussing reading materials at class. Instead, we can ask some questions on Slack after class.\nLectures are focused on summarizing the videos and the readings. I would get much more out of them if they were more discussion-based, and we could discuss the readings on a deeper level.\nThe discussion in the lecture is just a very low-level recap of the readings… For those of us that have read and understood the readings, we don’t learn much from the questions being discussed. We could be spending that time on technical elements instead.\n\n\n\n\nI paraphrase:\n\nThe technical content is very slow and we don’t write a lot of code in the practicals, so I feel like I’m not learning and remembering it very well.\nI’ve tried my best to review and catch up but I don’t know why I still struggle a lot figuring the whole picture. Suddenly the script went from simple into complex really quick.\nI’m quite confused about the content of practicals 1-5. Maybe combining the real data analysis with detailed data learning (objects, function) is better to understand.\nToo much emphasis on Git, Podman/Docker, etc.\nThere was a lot of information about different concepts (Git, Jupyter, Podman/Docker, Python) and it was very overwhelming.\n\n\n\n\nStudent feedback is often bimodal and motivated by frustration:\n\nSome students want more technical content. Some find the technical content we have too much.\nSome students want more discussion. Some find the discussion we have too much.\n\n\n\n\n\nIf you think the live session is just a recap of the readings, why aren’t you chiming in with your thoughts?\nIf you think that the discussion should ‘go deeper’, why aren’t you asking deeper questions?\nIf you think that Git and Podman/Docker aren’t part of doing data science, why aren’t you pushing us to explain it better?\nIf you are struggling with the practical examples, why aren’t you asking for more examples?"
  },
  {
    "objectID": "lectures/7.0-Feedback.html#important-qualification-1",
    "href": "lectures/7.0-Feedback.html#important-qualification-1",
    "title": "Responding to Feedback",
    "section": "",
    "text": "CASA does not get to pick the rooms in which these classes are taught! And we barely have any control at all over timings1."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#important-qualification-2",
    "href": "lectures/7.0-Feedback.html#important-qualification-2",
    "title": "Responding to Feedback",
    "section": "",
    "text": "You might see your comments here. We are not trying to embarrass anyone. We are trying to improve the course."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#the-good",
    "href": "lectures/7.0-Feedback.html#the-good",
    "title": "Responding to Feedback",
    "section": "",
    "text": "I paraphrase:\n\nActivities, reading, and exam relate and the lectures have some fun examples that are easy to follow.\nThere is some reading, followed by questions which makes students engage with the homework.\nPracticals are interactive, with clear tutorials explaining the topic step by step.\nA lot of materials for further learning and organized so that I could practice a lot by myself.\nReally strong PGTAs."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-1",
    "href": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-1",
    "title": "Responding to Feedback",
    "section": "",
    "text": "I paraphrase:\n\nToo much ethics in FSDS so I didn’t have time to practice my coding skills.\nI think it’s not necessary to spend too much time discussing reading materials at class. Instead, we can ask some questions on Slack after class.\nLectures are focused on summarizing the videos and the readings. I would get much more out of them if they were more discussion-based, and we could discuss the readings on a deeper level.\nThe discussion in the lecture is just a very low-level recap of the readings… For those of us that have read and understood the readings, we don’t learn much from the questions being discussed. We could be spending that time on technical elements instead."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-2",
    "href": "lectures/7.0-Feedback.html#the-bad-and-the-ugly-2",
    "title": "Responding to Feedback",
    "section": "",
    "text": "I paraphrase:\n\nThe technical content is very slow and we don’t write a lot of code in the practicals, so I feel like I’m not learning and remembering it very well.\nI’ve tried my best to review and catch up but I don’t know why I still struggle a lot figuring the whole picture. Suddenly the script went from simple into complex really quick.\nI’m quite confused about the content of practicals 1-5. Maybe combining the real data analysis with detailed data learning (objects, function) is better to understand.\nToo much emphasis on Git, Podman/Docker, etc.\nThere was a lot of information about different concepts (Git, Jupyter, Podman/Docker, Python) and it was very overwhelming."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#common-threads",
    "href": "lectures/7.0-Feedback.html#common-threads",
    "title": "Responding to Feedback",
    "section": "",
    "text": "Student feedback is often bimodal and motivated by frustration:\n\nSome students want more technical content. Some find the technical content we have too much.\nSome students want more discussion. Some find the discussion we have too much."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#my-challenge-to-you",
    "href": "lectures/7.0-Feedback.html#my-challenge-to-you",
    "title": "Responding to Feedback",
    "section": "",
    "text": "If you think the live session is just a recap of the readings, why aren’t you chiming in with your thoughts?\nIf you think that the discussion should ‘go deeper’, why aren’t you asking deeper questions?\nIf you think that Git and Podman/Docker aren’t part of doing data science, why aren’t you pushing us to explain it better?\nIf you are struggling with the practical examples, why aren’t you asking for more examples?"
  },
  {
    "objectID": "lectures/7.0-Feedback.html#my-current-takeaways",
    "href": "lectures/7.0-Feedback.html#my-current-takeaways",
    "title": "Responding to Feedback",
    "section": "My Current Takeaways",
    "text": "My Current Takeaways\nPending additional feedback from you, I am looking at:\n\nResequencing lectures in weeks 1–3 to try to introduce fewer tools at once.\nHaving a chat with Andy to ensure we align Git content and develop understanding across both modules.\nRevisiting weeks 1–5 to see where practicals can be anchored in examples and possibly even introduce a very small version of the InsideAirbnb data set (instead of the city pseudo-data).\nDeveloping additional materials for those who want to go deeper.\nReviewing signposting in lectures and practicals."
  },
  {
    "objectID": "lectures/7.0-Feedback.html#footnotes",
    "href": "lectures/7.0-Feedback.html#footnotes",
    "title": "Responding to Feedback",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFun fact: we have been offered practicals before lectures on occasion.↩︎"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html",
    "href": "lectures/7.2-Geopandas.html",
    "title": "Geopandas",
    "section": "",
    "text": "‘Geospatial Pandas’ provides a set of tools for working with geo-data in a pandas-like way. See: GeoPandas.org and Read the Docs.\n\n\n\nThe Python Spatial Analysis Library provides an array of tools for performing spatial analysis. Now uses geo-pandas as default data structure. See: PySAL.org and Read the Docs.\n\n\n\nA newer library that builds on Geopandas to offer movement analysis tools."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reading-writing",
    "href": "lectures/7.2-Geopandas.html#reading-writing",
    "title": "Geopandas",
    "section": "Reading & Writing",
    "text": "Reading & Writing\nSupported file formats:\n\n\n\nType\nExtension\nNotes\n\n\n\n\nShape\n.shp (etc.)\nMaximum compatibility\n\n\nGeoPackage\n.gpkg\nGood default choice\n\n\nGeoJSON\n.geojson\nFor web mapping\n\n\nZip\n.zip\nFor use with Shapefiles\n\n\nWKT\n.txt\nPlain-text & SQL\n\n\nGeoParquet\n.geoparquet\nGood for large data sets & SQL\n\n\n\nAdditionally, it is possible to read only subsets of the data using row, column, geometry, and bbox filters."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reading-remote-files",
    "href": "lectures/7.2-Geopandas.html#reading-remote-files",
    "title": "Geopandas",
    "section": "Reading (Remote Files)",
    "text": "Reading (Remote Files)\nAgain, depending on file size you may want to save these locally, but…\nimport geopandas as gpd\ngpkg_src = 'https://bit.ly/2K4JcsB'\nworld = gpd.read_file(gpkg_src)\n# The ';' suppresses matplotlib output\nworld.plot(facecolor='white', edgecolor='darkblue');"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#writing-local-files",
    "href": "lectures/7.2-Geopandas.html#writing-local-files",
    "title": "Geopandas",
    "section": "Writing (Local Files)",
    "text": "Writing (Local Files)\nWrite any OGR-supported vector drivers.\nworld.to_file('world.gpkg', driver='GPKG')\nworld.to_file('world.shp', driver='ESRI Shapefile')\nworld.to_file('world.geojson', driver='GeoJSON')\n\nIf you forget to specify the driver it writes shapefiles by default. This is mainly an issue if you try to write a GeoPackage or GeoJSON file but then end up writing a shapefile to a directory called world.gpkg!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#data-structures",
    "href": "lectures/7.2-Geopandas.html#data-structures",
    "title": "Geopandas",
    "section": "Data Structures",
    "text": "Data Structures\nGeoPandas does all this by adding just two new classes:\n\nGeoDataFrame\nGeoSeries\n\nIn principle, a GeoSeries can contain multiple geo-data types, but in practice you’ll want to be one of the following shapely classes:\n\nPoints / Multi-Points\nLines / Multi-Lines\nPolygons / Multi-Polygons"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#consider",
    "href": "lectures/7.2-Geopandas.html#consider",
    "title": "Geopandas",
    "section": "Consider",
    "text": "Consider\nRecall that we can ask if a particular object is an instance of any given class:\nimport pandas as pd\nprint(isinstance(world, str))\nprint(isinstance(world, pd.DataFrame))\nprint(isinstance(world, gpd.GeoDataFrame))\nPrints: False, True, True.\nimport pandas as pd\nprint(isinstance(world.geometry, str))\nprint(isinstance(world.geometry, pd.Series))\nprint(isinstance(world.geometry, gpd.GeoSeries))\nAlso prints: False, True, True.\n\nSo converting from Pandas to GeoPandas works well because GeoPandas knows all about Pandas.\nYou can use a GeoDataFrame anywhere you’d use a DataFrame with no loss of functionality! Same for a GeoSeries, though in this case a GeoSeries cannot perform the same statistical operations."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#projections",
    "href": "lectures/7.2-Geopandas.html#projections",
    "title": "Geopandas",
    "section": "Projections",
    "text": "Projections\nDepending on your data source, you may or may not have projection information attached to your GeoDataFrame:\nprint(world.crs)\noutputs epsg:4326, but:\nworld.crs\noutputs:\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#finding-projections",
    "href": "lectures/7.2-Geopandas.html#finding-projections",
    "title": "Geopandas",
    "section": "Finding Projections",
    "text": "Finding Projections\nYou’ll have already covered this in GIS, but you can find nearly any EPSG you might need at epsg.io. By far the most commonly-used here are:\n\nEPSG:4326 for the World Geodetic System 84 used in GPS.\nEPSG:27700 for OSGB 1936/British National Grid used in the UK.\n\nNote: recall that large territories (such as Canada, China and Russia) may well have multiple projections at the state of provincial level."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#reprojection",
    "href": "lectures/7.2-Geopandas.html#reprojection",
    "title": "Geopandas",
    "section": "Reprojection",
    "text": "Reprojection\nFor data sets without projection information (i.e. often anything loaded from a shapefile) you must gdf.set_crs(&lt;spec&gt;). For all others you should gdf.to_crs(&lt;spec&gt;).\nworld2 = world.to_crs('ESRI:54030')\nworld2.plot();\n\n\nUnlike a shapefile, you can have more than one geometry column, each with a different projection. However, only one will be plotted (the one named geometry or specified via set_geometry())."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#the-spatial-index",
    "href": "lectures/7.2-Geopandas.html#the-spatial-index",
    "title": "Geopandas",
    "section": "The Spatial Index",
    "text": "The Spatial Index\nWe can use GeoSeries’ spatial index directly to perform simple spatial queries:\nimport matplotlib.pyplot as plt\nwslice = world.cx[-50:50, -20:20] # cx = coordinate index\nax = wslice.plot()\nplt.axvline(-50, linestyle='--', color='red')\nplt.axvline(50, linestyle='--', color='red')\nplt.axhline(-20, linestyle='--', color='red')\nplt.axhline(20, linestyle='--', color='red');"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#attributes",
    "href": "lectures/7.2-Geopandas.html#attributes",
    "title": "Geopandas",
    "section": "Attributes",
    "text": "Attributes\nA GeoSeries has attributes like any other Series, but also includes some spatially-specifc ones:\n\narea — if a polygon\nbounds — for each feature\ntotal_bounds — for each GeoSeries\ngeom_type — if you don’t already know\nis_valid — if you need to test"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#methods",
    "href": "lectures/7.2-Geopandas.html#methods",
    "title": "Geopandas",
    "section": "Methods",
    "text": "Methods\nAdditional GeoSeries methods icnlude:\n\ndistance() — returns Series measuring distances to some other feature (called as: &lt;GeoSeries&gt;.distance(&lt;feature&gt;))\ncentroid — returns GeoSeries of strict centroids (called as: &lt;GeoSeries&gt;.centroid)\nrepresentative_point() — returns GeoSeries of points within features\nto_crs() and plot(), which you’ve already seen.\n\n\nNote that centroid is not called with parentheses. Technically it’s more like an attribute than a method."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#relationship-tests",
    "href": "lectures/7.2-Geopandas.html#relationship-tests",
    "title": "Geopandas",
    "section": "Relationship Tests",
    "text": "Relationship Tests\nSimple geographical tests:\n\ngeom_almost_equals() — tries to deal with rounding issues when comparing two features.\ncontains() — is shape contained within some other features.\nintersects() — does shape intersect some other features."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#converting-non-spatial-data-1",
    "href": "lectures/7.2-Geopandas.html#converting-non-spatial-data-1",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 1",
    "text": "Converting Non-Spatial Data 1\nLat/Long and Northing/Easting benefit from a helper function gpd.points_from_xy():\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url)\n\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.plot()\n\nYou can also use list comprehensions ([x for x in list]) and zip to combine two lists but then need to specify the CRS as a separate step!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#csv-to-points-in-3-lines",
    "href": "lectures/7.2-Geopandas.html#csv-to-points-in-3-lines",
    "title": "Geopandas",
    "section": "CSV to Points in 3 Lines!",
    "text": "CSV to Points in 3 Lines!\n\n\nNotice that the default plot from a GeoDataFrame is… a map!"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#converting-non-spatial-data-2",
    "href": "lectures/7.2-Geopandas.html#converting-non-spatial-data-2",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 2",
    "text": "Converting Non-Spatial Data 2\nOther feature types need to be in some kind of regular format such as Well-Known Text (WKT), GeoJSON, or something readable as a Shapely geometry.\nfrom shapely import wkt\n\n# Notice coordinate pairs and last point is same as first one\nbbox = 'POLYGON((5000000.0 2500000.0, 5000000.0 -2500000.0, -5000000.0 -2500000.0, -5000000.0 2500000.0, 5000000.0 2500000.0))'\n\n# Create GeoPandas from dict just like Pandas\nbgdf = gpd.GeoDataFrame({'id':[0], 'coordinates':bbox})\n\n# Turn it into a geometry\nbgdf['geometry'] = bgdf.coordinates.apply(wkt.loads)\nbgdf = bgdf.set_crs('ESRI:54030')\nbgdf.plot() # Not very interesting but...\n\nThese are more rarely used for our purposes but knowing that they exist is useful."
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#from-text-to-bounding-box",
    "href": "lectures/7.2-Geopandas.html#from-text-to-bounding-box",
    "title": "Geopandas",
    "section": "From Text to Bounding Box",
    "text": "From Text to Bounding Box\nscale = int(float('1e7'))\nf,ax=plt.subplots(figsize=(8,4))\nworld2.plot(ax=ax)\nbgdf.plot(ax=ax, color='none', edgecolor='r')\nax.set_xlim([-0.75*scale, +0.75*scale])\nax.set_ylim([-3*scale/10, +3*scale/10])"
  },
  {
    "objectID": "lectures/7.2-Geopandas.html#additional-resources",
    "href": "lectures/7.2-Geopandas.html#additional-resources",
    "title": "Geopandas",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nI Hate Coordinate Systems\nGeoPandas on ReadTheDocs\nDani’s GDS Course\nDani’s Web Mapping Course"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-is-it-1",
    "href": "lectures/7.4-Signal_and_Noise.html#what-is-it-1",
    "title": "Signal & Noise",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-is-it-2",
    "href": "lectures/7.4-Signal_and_Noise.html#what-is-it-2",
    "title": "Signal & Noise",
    "section": "What is it?",
    "text": "What is it?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#start-with-a-chart",
    "href": "lectures/7.4-Signal_and_Noise.html#start-with-a-chart",
    "title": "Signal & Noise",
    "section": "Start with a Chart",
    "text": "Start with a Chart\nThe problem of relying on statistics alone was amply illustrated by Anscombe’s Quartet (1973)…\n\nWe are not very good at looking at spreadsheets.\nWe are very good at spotting patterns visually.\n\nSometimes, we are too good; that’s where the stats comes in. Think of it as the ‘tiger in the jungle’ problem.."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#anscombes-quartet",
    "href": "lectures/7.4-Signal_and_Noise.html#anscombes-quartet",
    "title": "Signal & Noise",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\n\nX1\nY1\nX2\nY2\nX3\nY3\nX4\nY4\n\n\n\n\n10.0\n8.04\n10.0\n9.14\n10.0\n7.46\n10.0\n6.58\n\n\n8.0\n6.95\n8.0\n8.14\n8.0\n6.77\n8.0\n5.76\n\n\n13.0\n7.58\n13.0\n8.74\n13.0\n12.74\n13.0\n7.71\n\n\n9.0\n8.81\n9.0\n8.77\n9.0\n7.11\n9.0\n8.84\n\n\n11.0\n8.33\n11.0\n9.26\n11.0\n7.81\n11.0\n8.47\n\n\n14.0\n9.96\n14.0\n8.10\n14.0\n8.84\n14.0\n7.04\n\n\n6.0\n7.24\n6.0\n6.13\n6.0\n6.08\n6.0\n5.25\n\n\n4.0\n4.26\n4.0\n3.10\n4.0\n5.39\n4.0\n12.5\n\n\n12.0\n10.84\n12.0\n9.13\n12.0\n8.15\n12.0\n5.56\n\n\n7.0\n4.82\n7.0\n7.26\n7.0\n6.42\n7.0\n7.91\n\n\n5.0\n5.68\n5.0\n4.74\n5.0\n5.73\n5.0\n6.89"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#summary-statistics-for-the-quartet",
    "href": "lectures/7.4-Signal_and_Noise.html#summary-statistics-for-the-quartet",
    "title": "Signal & Noise",
    "section": "Summary Statistics for the Quartet",
    "text": "Summary Statistics for the Quartet\n\n\n\nProperty\nValue\n\n\n\n\nMean of x\n9.0\n\n\nVariance of x\n11.0\n\n\nMean of y\n7.5\n\n\nVariance of y\n4.12\n\n\nCorrelation between x and y\n0.816\n\n\nLinear Model\ny = 3 + 0.5x"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#but-what-do-they-look-like",
    "href": "lectures/7.4-Signal_and_Noise.html#but-what-do-they-look-like",
    "title": "Signal & Noise",
    "section": "But What do They Look Like?",
    "text": "But What do They Look Like?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#think-it-through",
    "href": "lectures/7.4-Signal_and_Noise.html#think-it-through",
    "title": "Signal & Noise",
    "section": "Think it Through",
    "text": "Think it Through\nYou can make a lot of progress in your research without any advanced statistics!\n\nA ‘picture’ isn’t just worth 1,000 words, it could be a whole dissertation!\nThe right chart makes your case eloquently and succinctly.\n\nAlways ask yourself:\n\nWhat am I trying to say?\nHow can I say it most effectively?\nIs there anything I’m overlooking in the data?\n\nA good chart is a good way to start!"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#what-makes-a-good-plot",
    "href": "lectures/7.4-Signal_and_Noise.html#what-makes-a-good-plot",
    "title": "Signal & Noise",
    "section": "What Makes a Good Plot?",
    "text": "What Makes a Good Plot?\nA good chart or table:\n\nServes a purpose — it is clear how it advances the argument in a way that could not be done in the text alone.\nContains only what is relevant — zeroes in on what the reader needs and is not needlessly cluttered.\nUses precision that is meaningful — doesn’t clutter the chart with needless numbers.\n\n\nFar too many charts or tables could be easily written up in a single sentence.\nFar too many charts or tables contain redundancy, clutter, and ‘flair’.\nDon’t report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#for-example",
    "href": "lectures/7.4-Signal_and_Noise.html#for-example",
    "title": "Signal & Noise",
    "section": "For Example…",
    "text": "For Example…\nHow much precision is necessary in measuring degrees at the equator?\n\n\n\nDecimal Places\nDegrees\nDistance\n\n\n\n\n0\n1\n111km\n\n\n1\n0.1\n11.1km\n\n\n2\n0.01\n1.11km\n\n\n3\n0.001\n111m\n\n\n4\n0.0001\n11.1m\n\n\n5\n0.00001\n1.11m\n\n\n6\n0.000001\n11.1cm\n\n\n7\n0.0000001\n1.11cm\n\n\n8\n0.00000001\n1.11mm"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final",
    "href": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final",
    "title": "Signal & Noise",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final-1",
    "href": "lectures/7.4-Signal_and_Noise.html#goals-by-world-cup-final-1",
    "title": "Signal & Noise",
    "section": "Goals by World Cup Final",
    "text": "Goals by World Cup Final"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#average-goals-by-world-cup-final",
    "href": "lectures/7.4-Signal_and_Noise.html#average-goals-by-world-cup-final",
    "title": "Signal & Noise",
    "section": "Average Goals by World Cup Final",
    "text": "Average Goals by World Cup Final\n\n\nIn 1982 the number of teams went from 16 to 24, and in 1998 it went from 24 to 32!"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality",
    "href": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality",
    "title": "Signal & Noise",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality-1",
    "href": "lectures/7.4-Signal_and_Noise.html#how-far-from-equality-1",
    "title": "Signal & Noise",
    "section": "How far from Equality?",
    "text": "How far from Equality?"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#the-purpose-of-a-chart",
    "href": "lectures/7.4-Signal_and_Noise.html#the-purpose-of-a-chart",
    "title": "Signal & Noise",
    "section": "The Purpose of a Chart",
    "text": "The Purpose of a Chart\nThe purpose of a graph is to show that there are relationships within the data set that are not trivial/expected.\nChoose the chart to highlight relationships, or the lack thereof:\n\nThink of a chart or table as part of your ‘argument’ – if you can’t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably don’t need it.\nIdentify & prioritise the relationships in the data.\nChoose a chart type/chart symbology that gives emphasis to the most important relationships."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#real-numbers",
    "href": "lectures/7.4-Signal_and_Noise.html#real-numbers",
    "title": "Signal & Noise",
    "section": "Real Numbers",
    "text": "Real Numbers\nConsider the difference in emphasis between:\n\n11316149\n11,316,149\n11.3 million\n11 x 10\\(^{6}\\)\n22%\n22.2559%\n\nAlways keep in mind the purpose of the number."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#theres-still-a-role-for-tables",
    "href": "lectures/7.4-Signal_and_Noise.html#theres-still-a-role-for-tables",
    "title": "Signal & Noise",
    "section": "There’s Still a Role for Tables",
    "text": "There’s Still a Role for Tables\nWhy a table is sometimes better than a chart:\n\nYou need to present data values with greater detail\nYou need to enable readers to draw comparisons between data values\nYou need to present the same data in multiple ways (e.g. raw number and percentage)\nYou want to show many dimensions for a small number of observations\n\n\ne.g. percentage of people falling into each ethnic or income category for a small number of wards or boroughs."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-failing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-failing-grade",
    "title": "Signal & Noise",
    "section": "Undergraduate Tables (Failing Grade)",
    "text": "Undergraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-passing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#undergraduate-tables-passing-grade",
    "title": "Signal & Noise",
    "section": "Undergraduate Tables (Passing Grade)",
    "text": "Undergraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-failing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-failing-grade",
    "title": "Signal & Noise",
    "section": "Postgraduate Tables (Failing Grade)",
    "text": "Postgraduate Tables (Failing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-passing-grade",
    "href": "lectures/7.4-Signal_and_Noise.html#postgraduate-tables-passing-grade",
    "title": "Signal & Noise",
    "section": "Postgraduate Tables (Passing Grade)",
    "text": "Postgraduate Tables (Passing Grade)"
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#design-for-tables",
    "href": "lectures/7.4-Signal_and_Noise.html#design-for-tables",
    "title": "Signal & Noise",
    "section": "Design for Tables",
    "text": "Design for Tables\nPrinciples:\n\nReduce the number of lines to a minimum (and you should almost never need vertical lines).\nUse ‘white-space’ to create visual space between groups of unrelated (or less related) elements.\nRemove redundancy (if you find yourself typing ‘millions’ or ‘GBP’ or ‘Male’ repeatedly then you’ve got redundancy).\nEnsure that meta-data is clearly separate from, but attached to, the graph (i.e. source, title, etc.)."
  },
  {
    "objectID": "lectures/7.4-Signal_and_Noise.html#additional-resources",
    "href": "lectures/7.4-Signal_and_Noise.html#additional-resources",
    "title": "Signal & Noise",
    "section": "Additional Resources",
    "text": "Additional Resources\nThere’s so much more to find, but:\n\n\n\n26 Ways to Enhance your Tables in Tableau\nTables Aren’t Boring, You Are\n\n\n\nThe Visual Display of Quantitative Information (the Tufte and Graves-Morris 1983 classic)\nWhy Tufte’s Wrong…\nTufte is Dead; Long Live Tufte"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html",
    "href": "lectures/8.1-Notebooks_as_Documents.html",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "In conjunction with Pandoc and/or LaTeX they become platforms for publication.\n\n\nIt’s not just about mixing code and comment, we also want:\n\nTo separate content from presentation\nTo define mappings between presentation styles\nTo produce the best-quality output for the format chosen\n\n\n\nExamples of this include CSS for web sites, LaTeX templates, and Markdown styles.\nMVC approach to software design.\n\n\n\n\nTool for converting documents between formats, including:\n\nPlain Text/YAML\nMarkdown\nLaTeX/PDF\nHTML/Reveal.js\nJupyter Notebook\nXML/ODT/DOCX\nEPUB/DocBook\nBibTeX/JSON\n\n\n\n\n\n\n\n\n\n\n\n\nIntended for type-setting of scientific documents, but has been used for slides, posters, CVs, etc. It is not a word processor, it’s more like a compiler.\nThis format is based on Edward Tufte’s VSQD and can be found on GitHub.\n\n\n\n\nYou write LaTeX in any text editor, but specialist apps like Texpad or Overleaf make it easier.\n\\documentclass[11pt,article,oneside]{memoir}\n\\newcommand{\\bl}{\\textsc{bl}~\\/}\n\\usepackage{tabularx}\n\n\\begin{document}\n\\maketitle \n\nThis report provides an overview of activities ...\n\n\\section{Applications}\nA primary objective was the submission...\n\nUCL has an institutional license for Overleaf.\nThis document is then compiled (or ‘typeset’) with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.\nCompilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references.\n\n\n\n\nProvides bilbiographic support for LaTeX but widely used by other utilities as is also plain-text.\n@article{Lavin:2019,\n        Author = {Lavin, Matthew J.},\n        Doi = {10.46430/phen0082},\n        Journal = {The Programming Historian},\n        Number = {8},\n        Title = {Analyzing Documents with TF-IDF},\n        Year = {2019},\n        Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}\n\n@incollection{Kitchin:2016,\n        Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},\n        Booktitle = {Smart Urbanism},\n        Chapter = {2},\n        Editor = {Marvin, Luque-Ayala, McFarlane},\n        Title = {Smart Cities and the Politics of Urban Data},\n        Year = {2016}}\n\n\n\nTo reference a document we then need to tell LaTeX or Pandoc where to look:\n\\bibliographystyle{apacite} # LaTeX\n\\bibliography{Spatial_Data_Chapter.bib} # LaTeX\nWith citations following formats like:\n\\citep[p.22]{Reades2018} # LaTeX\nOr:\n[@dignazio:2020, chap. 4] # Markdown\n\n\n\nJavaScript-based presentation framework. Can use Markdown to generate portable interactive slides including references/bibliographies.\nHow this presentation was created.\nCompare:\n\nMarkdown\nHTML\nReveal\n\n\n\n\n\n\npandoc Syllabus.md \\\n  -H ./bib/head.tex \\\n  -H ./bib/chapter.tex \\\n  -H ./bib/refs.tex \\\n  -V documentclass=memoir \\\n  --pdf-engine=xelatex \\\n  --filter=pandoc-citeproc \\\n  --metadata-file=metadata.yml \\\n  --highlight-style=pygments \\\n  -o Syllabus.pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplicity (while writing)\nFlexibility of Form\nVersion Control\nHigh-Quality Outputs\nEnforced Structure\n\n\n\n\n\n\nComplexity (when formatting)\nCollaboration (can be harder)\n\n\n\n\n\nIf your document lacks structure (headings, sub-headings, etc.) then it’s impossible to get a good-looking document.\nConversely, because LaTeX and Markdown force you to add structure they ensure that things like ToCs, ToFs, and Bibliographies will generally work ‘as advertised’."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "href": "lectures/8.1-Notebooks_as_Documents.html#recall-tangled-workflows",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "It’s not just about mixing code and comment, we also want:\n\nTo separate content from presentation\nTo define mappings between presentation styles\nTo produce the best-quality output for the format chosen\n\n\n\nExamples of this include CSS for web sites, LaTeX templates, and Markdown styles.\nMVC approach to software design."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#pandoc",
    "href": "lectures/8.1-Notebooks_as_Documents.html#pandoc",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "Tool for converting documents between formats, including:\n\nPlain Text/YAML\nMarkdown\nLaTeX/PDF\nHTML/Reveal.js\nJupyter Notebook\nXML/ODT/DOCX\nEPUB/DocBook\nBibTeX/JSON"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#latex",
    "href": "lectures/8.1-Notebooks_as_Documents.html#latex",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "Intended for type-setting of scientific documents, but has been used for slides, posters, CVs, etc. It is not a word processor, it’s more like a compiler.\nThis format is based on Edward Tufte’s VSQD and can be found on GitHub."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#latex-in-practice",
    "href": "lectures/8.1-Notebooks_as_Documents.html#latex-in-practice",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "You write LaTeX in any text editor, but specialist apps like Texpad or Overleaf make it easier.\n\\documentclass[11pt,article,oneside]{memoir}\n\\newcommand{\\bl}{\\textsc{bl}~\\/}\n\\usepackage{tabularx}\n\n\\begin{document}\n\\maketitle \n\nThis report provides an overview of activities ...\n\n\\section{Applications}\nA primary objective was the submission...\n\nUCL has an institutional license for Overleaf.\nThis document is then compiled (or ‘typeset’) with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.\nCompilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#bibtex",
    "href": "lectures/8.1-Notebooks_as_Documents.html#bibtex",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "Provides bilbiographic support for LaTeX but widely used by other utilities as is also plain-text.\n@article{Lavin:2019,\n        Author = {Lavin, Matthew J.},\n        Doi = {10.46430/phen0082},\n        Journal = {The Programming Historian},\n        Number = {8},\n        Title = {Analyzing Documents with TF-IDF},\n        Year = {2019},\n        Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}\n\n@incollection{Kitchin:2016,\n        Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},\n        Booktitle = {Smart Urbanism},\n        Chapter = {2},\n        Editor = {Marvin, Luque-Ayala, McFarlane},\n        Title = {Smart Cities and the Politics of Urban Data},\n        Year = {2016}}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "href": "lectures/8.1-Notebooks_as_Documents.html#bibtex-in-practice",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "To reference a document we then need to tell LaTeX or Pandoc where to look:\n\\bibliographystyle{apacite} # LaTeX\n\\bibliography{Spatial_Data_Chapter.bib} # LaTeX\nWith citations following formats like:\n\\citep[p.22]{Reades2018} # LaTeX\nOr:\n[@dignazio:2020, chap. 4] # Markdown"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#reveal.js",
    "href": "lectures/8.1-Notebooks_as_Documents.html#reveal.js",
    "title": "Notebooks to Documents",
    "section": "",
    "text": "JavaScript-based presentation framework. Can use Markdown to generate portable interactive slides including references/bibliographies.\nHow this presentation was created.\nCompare:\n\nMarkdown\nHTML\nReveal\n\n\n\n\n\n\npandoc Syllabus.md \\\n  -H ./bib/head.tex \\\n  -H ./bib/chapter.tex \\\n  -H ./bib/refs.tex \\\n  -V documentclass=memoir \\\n  --pdf-engine=xelatex \\\n  --filter=pandoc-citeproc \\\n  --metadata-file=metadata.yml \\\n  --highlight-style=pygments \\\n  -o Syllabus.pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplicity (while writing)\nFlexibility of Form\nVersion Control\nHigh-Quality Outputs\nEnforced Structure\n\n\n\n\n\n\nComplexity (when formatting)\nCollaboration (can be harder)\n\n\n\n\n\nIf your document lacks structure (headings, sub-headings, etc.) then it’s impossible to get a good-looking document.\nConversely, because LaTeX and Markdown force you to add structure they ensure that things like ToCs, ToFs, and Bibliographies will generally work ‘as advertised’."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#headings",
    "href": "lectures/8.1-Notebooks_as_Documents.html#headings",
    "title": "Notebooks to Documents",
    "section": "Headings",
    "text": "Headings\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n# Heading Level 1\n\\section{Heading Level 1}\n\n\n## Heading Level 2\n\\subsection{Heading Level 2}\n\n\n### Heading Level 3\n\\subsubsection{Heading Level 3}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#inline-elements",
    "href": "lectures/8.1-Notebooks_as_Documents.html#inline-elements",
    "title": "Notebooks to Documents",
    "section": "Inline Elements",
    "text": "Inline Elements\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\n1. Numbered item 1\n\\begin{enumerate} \\n \\item ... \\end{enumerate}\n\n\n- Bulleted list item 1\n\\begin{itemize} \\n \\item ... \\n \\end{itemize}\n\n\n_italics_ or *italics*\n\\emph{italics} or \\textit{italics}\n\n\n**bold**\n\\textbf{bold}\n\n\n&gt; blockquote\n\\begin{quote} \\n blockquote \\end{quote}\n\n\nSome `code` is here\nSome \\texttt{code} is here\n\n\n[Link Text](URL)\n\\href{Link Text}{URL}\n\n\n![Alt Text](Image URL)\n\\begin{figure}\\n \\includegraphics[opts]{...}\\n \\end{figure}"
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#mathematics",
    "href": "lectures/8.1-Notebooks_as_Documents.html#mathematics",
    "title": "Notebooks to Documents",
    "section": "Mathematics",
    "text": "Mathematics\n\n\n\n\n\n\n\nMarkdown\nLaTeX\n\n\n\n\nSame, but either 1 or 2 $’s\n$x=5$\n\n\nSame, but either 1 or 2 $’s\n$\\pi$\n\n\nSame, but either 1 or 2 $’s\n$e = mc^{2}$\n\n\n\nWe can show all this directly in the Notebook! \\(\\pi\\); \\(e = mc^{2}\\); \\(\\int_{0}^{\\inf} x^2 \\,dx\\); \\(\\sum_{n=1}^{\\infty} 2^{-n} = 1\\)\n\n\nOverleaf has good documentation for most (basic) applications."
  },
  {
    "objectID": "lectures/8.1-Notebooks_as_Documents.html#additional-resources",
    "href": "lectures/8.1-Notebooks_as_Documents.html#additional-resources",
    "title": "Notebooks to Documents",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nJupyter Tips and Tricks\nPandoc Demos\nBeginner’s Guide to Jupyter Notebooks\n7 Essential Tips to Writing With Jupyter Notebooks\nVersion Control with Jupyter\nSustainable Publishing using Pandoc and Markdown\nMaking Pretty PDFs with Quarto\nHOw to use Quarto for Parameterised Reporting"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html",
    "href": "lectures/8.3-Cleaning_Text.html",
    "title": "Cleaning Text",
    "section": "",
    "text": "Another great example of something where what makes sense to a human is not always the fastest way to get things done.\nCode is embarassingly parallel if the problem so clearly lacks interconnections with other problems that the code can be run in parallel without any effort.\nVectorisation is the process (usually done ‘under the hood’ for you rather than explicitly by you) by which code that, by default, runs linearly is modified to run in parallel.\n\n\n\nIf you need to apply the same operation to lots of data why do it sequentially?\n\nYour computer has many cores and can run many threads in parallel.\nThe computer divides the work across the threads it sees fit.\nThe computer reassemble the answer at the end from the threads.\n\nIf you have 4 cores then parallelisation cuts analysis time by 75%.\n\n\n\n\nMany libraries/packages implement weak forms of vectorisation or parallelisation, but some libraries do more.\nYou must request it because it requires hardware or other support and it is highly optimsed.\nMultiple separate machines acting as one.\nMultiple GPUs acting as one.\n\n\nConceptually, these get challenging if you can’t clearly separate/parallelise tasks.\n\n\n\n\nNumpy is fully vectorised and will almost always out-perform Pandas apply, but both are massive improvements on for loops:\n\nExecute row-wise and column-wise operations.\nApply any arbitrary function to individual elements or whole axes (i.e. row or col).\nCan make use of lambda functions too for ‘one off’ operations (ad-ohoc functions).\n\n\n\n\nFunctional equivalent of list comprehensions: 1-line, anonymous functions.\nFor example:\nx = lambda a : a + 10\nprint(x(5)) # 15\nOr:\nfull_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\nprint(full_name('guido', 'van rossum')) # 'Guido Van Rossum'\nThese are very useful with pandas.\n\n\n\nimport time\nimport numpy as np\ndef func(a,b):\n  c = 0\n  for i in range(len(a)): c += a[i]*b[i]\n  return c\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nt1 = time.time()\nprint(func(a,b))\nt2 = time.time()\nprint(np.dot(a,b))\nt3 = time.time()\n\nprint(f\"For loop took {(t2-t1)*1000:.0f} milliseconds\")\nprint(f\"Numpy took {(t3-t2)*1000:.0f} milliseconds\")\nGenerally, I get numpy taking 86ms, while the for loop takes 331ms!\n\n\n/ht to The Last Byte for inspiration."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "href": "lectures/8.3-Cleaning_Text.html#when-a-loop-is-not-best",
    "title": "Cleaning Text",
    "section": "",
    "text": "If you need to apply the same operation to lots of data why do it sequentially?\n\nYour computer has many cores and can run many threads in parallel.\nThe computer divides the work across the threads it sees fit.\nThe computer reassemble the answer at the end from the threads.\n\nIf you have 4 cores then parallelisation cuts analysis time by 75%."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "href": "lectures/8.3-Cleaning_Text.html#so-do-more-with-each-clock-cycle",
    "title": "Cleaning Text",
    "section": "",
    "text": "Many libraries/packages implement weak forms of vectorisation or parallelisation, but some libraries do more.\nYou must request it because it requires hardware or other support and it is highly optimsed.\nMultiple separate machines acting as one.\nMultiple GPUs acting as one.\n\n\nConceptually, these get challenging if you can’t clearly separate/parallelise tasks."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "href": "lectures/8.3-Cleaning_Text.html#pandas.apply-vs.-numpy",
    "title": "Cleaning Text",
    "section": "",
    "text": "Numpy is fully vectorised and will almost always out-perform Pandas apply, but both are massive improvements on for loops:\n\nExecute row-wise and column-wise operations.\nApply any arbitrary function to individual elements or whole axes (i.e. row or col).\nCan make use of lambda functions too for ‘one off’ operations (ad-ohoc functions)."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#lambda-functions",
    "href": "lectures/8.3-Cleaning_Text.html#lambda-functions",
    "title": "Cleaning Text",
    "section": "",
    "text": "Functional equivalent of list comprehensions: 1-line, anonymous functions.\nFor example:\nx = lambda a : a + 10\nprint(x(5)) # 15\nOr:\nfull_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\nprint(full_name('guido', 'van rossum')) # 'Guido Van Rossum'\nThese are very useful with pandas."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#lets-compare",
    "href": "lectures/8.3-Cleaning_Text.html#lets-compare",
    "title": "Cleaning Text",
    "section": "",
    "text": "import time\nimport numpy as np\ndef func(a,b):\n  c = 0\n  for i in range(len(a)): c += a[i]*b[i]\n  return c\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\nt1 = time.time()\nprint(func(a,b))\nt2 = time.time()\nprint(np.dot(a,b))\nt3 = time.time()\n\nprint(f\"For loop took {(t2-t1)*1000:.0f} milliseconds\")\nprint(f\"Numpy took {(t3-t2)*1000:.0f} milliseconds\")\nGenerally, I get numpy taking 86ms, while the for loop takes 331ms!\n\n\n/ht to The Last Byte for inspiration."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#beautiful-soup-selenium",
    "href": "lectures/8.3-Cleaning_Text.html#beautiful-soup-selenium",
    "title": "Cleaning Text",
    "section": "Beautiful Soup & Selenium",
    "text": "Beautiful Soup & Selenium\nTwo stages to acquiring web-based documents:\n\nAccessing the document: urllib can deal with many issues (even authentication), but not with dynamic web pages (which are increasingly common); for that, you need Selenium (library + driver).\nProcessing the document: simple data can be extracted from web pages with RegularExpressions, but not with complex (esp. dynamic) content; for that, you need BeautifulSoup4.\n\nThese interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#regular-expressions-breaks",
    "href": "lectures/8.3-Cleaning_Text.html#regular-expressions-breaks",
    "title": "Cleaning Text",
    "section": "Regular Expressions / Breaks",
    "text": "Regular Expressions / Breaks\nNeed to look at how the data is organised:\n\nFor very large corpora, you might want one document at a time (batch).\nFor very large files, you might want one line at a time (streaming).\nFor large files in large corpora, you might want more than one ‘machine’.\n\n\nSee the OpenVirus Project."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#starting-points",
    "href": "lectures/8.3-Cleaning_Text.html#starting-points",
    "title": "Cleaning Text",
    "section": "Starting Points",
    "text": "Starting Points\nThese strategies can be used singly or all-together:\n\nStopwords\nCase\nAccent-stripping\nPunctuation\nNumbers\n\nBut these are just a starting point!\n\nWhat’s the semantic difference between 1,000,000 and 999,999?"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#distributional-pruning",
    "href": "lectures/8.3-Cleaning_Text.html#distributional-pruning",
    "title": "Cleaning Text",
    "section": "Distributional Pruning",
    "text": "Distributional Pruning\nWe can prune from both ends of the distribution:\n\nOverly rare words: what does a word used in one document help us to understand about a corpus?\nOverly common ones: what does a word used in every document help us to understand about a corpus?\n\n\nAgain, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#different-approaches",
    "href": "lectures/8.3-Cleaning_Text.html#different-approaches",
    "title": "Cleaning Text",
    "section": "Different Approaches",
    "text": "Different Approaches\nHumans use a lot of words/concepts1:\n\nStemming: rules-based truncation to a stem (can be augmented by language awareness).\nLemmatisation: usually dictionary-based ‘deduplication’ to a lemma (can be augmented by POS-tagging)."
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#different-outcomes",
    "href": "lectures/8.3-Cleaning_Text.html#different-outcomes",
    "title": "Cleaning Text",
    "section": "Different Outcomes",
    "text": "Different Outcomes\n\n\n\nSource\nPorter\nSnowball\nLemmatisation\n\n\n\n\nmonkeys\nmonkey\nmonkey\nmonkey\n\n\ncities\nciti\nciti\ncity\n\n\ncomplexity\ncomplex\ncomplex\ncomplexity\n\n\nReades\nread\nread\nReades\n\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nfor w in ['monkeys','cities','complexity','Reades']:\n    print(f\"Porter: {PorterStemmer().stem(w)}\")\n    print(f\"Snowball: {SnowballStemmer('english').stem(w)}\")\n    print(f\"Lemmatisation: {wnl.lemmatize(w)}\")"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#additional-resources",
    "href": "lectures/8.3-Cleaning_Text.html#additional-resources",
    "title": "Cleaning Text",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nVectorisation in Python\nLambda Functions\nReal Python Lambda Functions\nStemming words with NLTK\nStemming and Lemmatisation in Python\nKD Nuggets: A Practitioner’s Guide to NLP\nKD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics\nRoadmap to Natural Language Processing (NLP)"
  },
  {
    "objectID": "lectures/8.3-Cleaning_Text.html#footnotes",
    "href": "lectures/8.3-Cleaning_Text.html#footnotes",
    "title": "Cleaning Text",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA recent digitsation effort by Harvard and Google estimated 1,022,000 unique word-forms in English alone (Source).↩︎"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "href": "lectures/9.1-Linking_Data.html#general-join-syntax",
    "title": "Linking Data",
    "section": "General Join Syntax",
    "text": "General Join Syntax\nA join refers to the merging of two (or more) data tables using one (or more) matching columns:\npd.merge(df1, df2, on='SensorID')\n\nNote that if you want to use the index column which isn’t, technically, a column then you need to use left_index=True and right_index=True — where left is the first data set in the join.\nNote that the default behaviour is an inner join (i.e. defaults to how='inner')"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join",
    "href": "lectures/9.1-Linking_Data.html#inner-join",
    "title": "Linking Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\n\nData Set 1\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n1 ⇒\nLHR\nBAA\n\n\n2 ✘\nLGA\nGIP\n\n\n3 ⇒\nSTA\nMAG\n\n\n4 ⇒\nLUT\nLuton LA\n\n\n5 ✘\nSEN\nStobart\n\n\n\n\n\n\nData Set 2\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n1 ⇐\nTemperature\n5ºC\n\n\n1 ⇐\nHumidity\n15%\n\n\n3 ⇐\nTemperature\n7ºC\n\n\n4 ⇐\nTemperature\n7ºC\n\n\n6 ✘\nHumidity\n18%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#inner-join-result",
    "href": "lectures/9.1-Linking_Data.html#inner-join-result",
    "title": "Linking Data",
    "section": "Inner Join Result",
    "text": "Inner Join Result\nOn an Inner Join all non-matching rows are dropped:\npd.merge(df1, df2, \n         how = 'inner',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\nHumidity\n15%\n\n\n3\nSTA\nMAG\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#but-what-if",
    "href": "lectures/9.1-Linking_Data.html#but-what-if",
    "title": "Linking Data",
    "section": "But What If…",
    "text": "But What If…\nIf Data Set 2 had a SensorKey instead of a SensorID then:\npd.merge(df1, df2, \n         how      = 'inner',\n         left_on  = 'SensorID',\n         right_on = 'SensorKey')\n \nWe will get an ‘extra’ field:\n\n\n\nSensorID\nPlace\nOwner\nSensorKey\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\n1\nTemperature\n5ºC\n\n\n1\nLHR\nBAA\n1\nHumidity\n15%\n\n\n3\nSTA\nMAG\n3\nTemperature\n7ºC\n\n\n4\nLUT\nLuton LA\n4\nTemperature\n7ºC"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#outer-join",
    "href": "lectures/9.1-Linking_Data.html#outer-join",
    "title": "Linking Data",
    "section": "Outer Join",
    "text": "Outer Join\nOn an Outer Join all rows are retained, including ones with no match:\npd.merge(df1, df2,\n         how = 'outer',\n         on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNaN\n\n\n5\nSEN\nStobart\nNaN\nNaN\n\n\n6\nNaN\nNaN\nHum.\n20%"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#left-join",
    "href": "lectures/9.1-Linking_Data.html#left-join",
    "title": "Linking Data",
    "section": "Left Join",
    "text": "Left Join\nOn a Left Join all rows on the left table are retained, including ones with no match, but unmatched right rows are dropped:\npd.merge(df1, df2, \n        how = 'left',\n        on  = 'SensorID')\n \n\n\n\nSensorID\nPlace\nOwner\nParameter\nValue\n\n\n\n\n1\nLHR\nBAA\nTemp.\n5℃\n\n\n1\nLHR\nBAA\nHum.\n15%\n\n\n2\nLGA\nGIP\nNaN\nNaN\n\n\n3\nSTA\nMAG\nTemp.\n7℃\n\n\n4\nLUT\nLuton Borough\nNaN\nNULL\n\n\n5\nSEN\nStobart\nNaN\nNaN"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append-concat",
    "href": "lectures/9.1-Linking_Data.html#append-concat",
    "title": "Linking Data",
    "section": "Append & Concat",
    "text": "Append & Concat\nPandas has two additional join-like functions:\n\nAppend: can be used to add a dict, Series, or DataFrame to the ‘bottom’ of an existing df. It’s not advisable to extend a df one row at a time (do bulk concatenations instead).\nConcat: can be used to concatenate two dfs together along either axis (rows or columns) “while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.”"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#concat",
    "href": "lectures/9.1-Linking_Data.html#concat",
    "title": "Linking Data",
    "section": "Concat",
    "text": "Concat\ndf3 = pd.DataFrame.from_dict({\n    'SensorID': [2,3,8,9,10],\n    'Place': ['STA','LUT','BHX','MAN','INV'],\n    'Owner': ['BAA','Luton LA','???','???','???']\n})\npd.concat([df1, df3], ignore_index=True)\nOutputs:\n\n\n\n\nSensorID\nPlace\nOwner\n\n\n\n\n0\n1\nLHR\nBAA\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n5\nSEN\nStobart\n\n\n5\n2\nSTA\nBAA\n\n\n6\n3\nLUT\nGIP\n\n\n7\n8\nBHX\n???\n\n\n8\n9\nMAN\n???\n\n\n9\n10\nINV\n???"
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#append",
    "href": "lectures/9.1-Linking_Data.html#append",
    "title": "Linking Data",
    "section": "Append",
    "text": "Append\nto_append = [\n    {'SensorID': 0, 'Parameter': 5,  'Humidity', 'Value': 0.45},\n    {'SensorID': 1, 'Parameter': 5,  'Humidity', 'Value': 0.31},\n    {'SensorID': 2, 'Parameter': 4, 'Temperature', 'Value': 2},\n    {'SensorID': 3, 'Parameter': 3, 'Temperature', 'Value': 3}]\ndf2.append(to_append)\nOutputs:\n\n\n\nSensorID\nParameter\nValue\n\n\n\n\n\n0\n1\nTemperature\n5.00\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n4\n6\nHumidity\n0.18\n\n\n0\n5\nHumidity\n0.45\n\n\n1\n5\nHumidity\n0.31\n\n\n2\n4\nTemperature\n2.00\n\n\n3\n3\nTemperature\n3.00\n\n\n\n\nNote that a Dictionary-of-Lists would also work for an append and that appending a column that doesn’t exist (for vertical appends) will cause the column to be created while appending a row that doesn’t exist (for horizontal appends) with cause the row to be created."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nAs usual, Stack Overflow to the rescue:\n\nA very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n\n\njoin() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().\n\nHint: axis=0 refers to the row index & axis=1 to the column index."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "href": "lectures/9.1-Linking_Data.html#merge-vs.-append-vs.-concat-vs.-join-1",
    "title": "Linking Data",
    "section": "Merge vs. Append vs. Concat vs. Join?",
    "text": "Merge vs. Append vs. Concat vs. Join?\nThese achieve the same thing, but they are not always equivalent:\npd.merge(df1, df2, left_index=True, right_index=True)\npd.concat([df1, df2], axis=1)\ndf1.join(df2)\nGenerally:\n\nConcat expects the number of columns in all data frames to match (if concatenating vertically) and the number of rows in all data frames to match (if concatenating horizontally). It does not deal well with linking.\nAppend assumes that either the columns or the rows will match.\nJoin is basically a functionality-restricted merge."
  },
  {
    "objectID": "lectures/9.1-Linking_Data.html#additional-resources",
    "href": "lectures/9.1-Linking_Data.html#additional-resources",
    "title": "Linking Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPandas Guide to Merges\nDatabase-style joining/merging\nPandas Concat\nPandas Append"
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#useful-but-limited",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#useful-but-limited",
    "title": "Pandas v Ducks",
    "section": "Useful, But Limited?",
    "text": "Useful, But Limited?\n\n\n\nMethod\nAchieves\n\n\n\n\ncount()\nTotal number of items\n\n\nfirst(), last()\nFirst and last item\n\n\nmean(), median()\nMean and median\n\n\nmin(), max()\nMinimum and maximum\n\n\nstd(), var()\nStandard deviation and variance\n\n\nmad()\nMean absolute deviation\n\n\nprod()\nProduct of all items\n\n\nsum()\nSum of all items\n\n\n\n\nHere are a bunch of pandas functions that have to do with aggregating data in some way. Some of these you’ll have seen before, some you may not. However, up to this point if you wanted to to know the median price of each type of Airbnb listing, or the sum of each type of vehicle sold, you’d have had to select out one listing type or vehicle type, call median or sum, and then remember the result. Let’s change that."
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#grouping-operations",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#grouping-operations",
    "title": "Pandas v Ducks",
    "section": "Grouping Operations",
    "text": "Grouping Operations\nIn Pandas these follow a split / apply / combine approach:\n\n\nNote that, for simplicity, I’ve abbreviate the Local Authority names since this is just a simplified example: TH (Tower Hamlets), HAK (Hackney), W (Westminster)."
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#in-practice",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#in-practice",
    "title": "Pandas v Ducks",
    "section": "In Practice",
    "text": "In Practice\ngrouped_df = df.groupby(&lt;fields&gt;).&lt;function&gt;\nFor instance, if we had a Local Authority (LA) field:\ngrouped_df = df.groupby('LA').sum()\nUsing apply the function could be anything:\ndef norm_by_data(x): # x is a column from the grouped df\n    x['d1'] /= x['d2'].sum() \n    return x\n\ndf.groupby('LA').apply(norm_by_data)"
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#grouping-by-arbitrary-mappings",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#grouping-by-arbitrary-mappings",
    "title": "Pandas v Ducks",
    "section": "Grouping by Arbitrary Mappings",
    "text": "Grouping by Arbitrary Mappings\nmapping = {'HAK':'Inner', 'TH':'Outer', 'W':'Inner'}\ndf.set_index('LA', inplace=True)\ndf.groupby(mapping).sum()"
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#pivot-tables",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#pivot-tables",
    "title": "Pandas v Ducks",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nA ‘special case’ of Group By features:\n\nCommonly-used in business to summarise data for reporting.\nGrouping (summarisation) happens along both axes (Group By operates only on one).\npandas.cut(&lt;series&gt;, &lt;bins&gt;) can be a useful feature here since it chops a continuous feature into bins suitable for grouping."
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#in-practice-1",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#in-practice-1",
    "title": "Pandas v Ducks",
    "section": "In Practice",
    "text": "In Practice\nage = pd.cut(titanic['age'], [0, 18, 80])\ntitanic.pivot_table('survived', ['sex', age], 'class')"
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#counts",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#counts",
    "title": "Pandas v Ducks",
    "section": "Counts",
    "text": "Counts"
  },
  {
    "objectID": "lectures/9.3-Pandas_vs_Ducks.html#pivots-groups",
    "href": "lectures/9.3-Pandas_vs_Ducks.html#pivots-groups",
    "title": "Pandas v Ducks",
    "section": "Pivots & Groups",
    "text": "Pivots & Groups"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html",
    "href": "practicals/Practical-03-Foundations_2.html",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "",
    "text": "In this notebook we are exploring basic (in the sense of fundamental) data structures so that you understand both how to manage more complex types of data and are prepared for what we will encounter when we start using pandas to perform data analysis. To achieve that, you will need to be ‘fluent’ in “nested” lists and dictionaries; we will focus primarily on lists-of-lists and dictionaries-of-lists, but note that file formats like JSON can be understood as dictionaries-of-dictionaries-of-lists-of-… so this is just a taster of real-world data structures."
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#list-refresher",
    "href": "practicals/Practical-03-Foundations_2.html#list-refresher",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "List Refresher",
    "text": "List Refresher\n\n\n\n\n\n\nDifficulty: Low.\n\n\n\n\n\n\n\n\n\nTo complete these tasks, all of the methods that you need are listed above, so this is about testing yourself on your understanding both of how to read the help and how to index elements in a list.\nThe next line creates a list of city names (each element is a string):\n\ncities = [\"Bristol\", \"London\", \"Manchester\", \"Edinburgh\", \"Belfast\", \"York\"]\n\n\nList Arithmetic\nReplace the ?? so that it prints Belfast.\n\nQuestion\n\nprint(cities[?? + 2])\n\n\n\n\nNegative List Arithmetic\nUse a negative index to print Belfast:\n\nQuestion\n\nprint(cities[??])\n\n\n\n\nFinding a Position in a List\nReplace the ?? so that it prints the index for Manchester in the list.\n\nQuestion\n\nprint(\"The position of Manchester in the list is: \" + str( ?? ))"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#looking-across-lists",
    "href": "practicals/Practical-03-Foundations_2.html#looking-across-lists",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Looking Across Lists",
    "text": "Looking Across Lists\n\n\n\n\n\n\nConnections\n\n\n\nThis section draws on the LOLs lecture and you will also find Code Camp’s Loops session useful here.\n\n\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNotice that the list of temperatures below is the same length as the list of cities, that’s because these are (roughly) the average temperatures for each city.\n\ncities = [\"Bristol\", \"London\", \"Manchester\", \"Edinburgh\", \"Belfast\", \"York\"]\ntemperatures = [15.6, 16.5, 13.4, 14.0, 15.2, 14.8]\n\n\nLateral Thinking\nGiven what you know about cities and temperatures, how do you print:\n\n“The average temperature in Manchester is 13.4 degrees”\n\nBut you have to do this without doing any of the following:\n\nUsing a list index directly (i.e. cities[2] and temperatures[2]) or\nHard-coding the name of the city?\n\nTo put it another way, neither of these solutions is the answer:\nprint(\"The average temperature in Manchester is \" + str(temperatures[2]) + \" degrees.\")\n# ...OR...\ncity=2\nprint(\"The average temperature in \" + cities[city] + \" is \" + str(temperatures[city]) + \" degrees.\")\n\n\n\n\n\n\nTip\n\n\n\nYou will need to combine some of the ideas above and also think about the fact that the list index is that we need is the same in both lists… Also, remember that you’ll need to wrap a str(...) around your temperature to make it into a string.\n\n\n\nQuestion\n\ncity=\"Manchester\" # Use this to get the solution...\n\n# This way is perfectly fine\nprint(\"The average temperature in \" + ?? + \" is \" + str(??))\n\n# This way is more Python 3 and a bit easier to read\nprint(f\"The average temperature in {??} is {??}\")\n\n\n\n\nDouble-Checking Your Solution\nYou’ll know that you got the ‘right’ answer to the question above if you can copy+paste your code and change only one thing in order to print out: “The average temperature in Belfast is 15.2 degrees”\n\nQuestion\n\ncity=\"Belfast\"\nprint(??)\n\n\n\n\nLoops\nNow use a for loop over the cities to print out the average temperature in each city:\n\nQuestion\n\nfor c in cities:\n    ??\n\nThe output should be:\n\nThe average temperature in Bristol is 15.6\nThe average temperature in London is 16.5\nThe average temperature in Manchester is 13.4\nThe average temperature in Edinburgh is 14.0\nThe average temperature in Belfast is 15.2\nThe average temperature in York is 14.8"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#creating-an-atlas",
    "href": "practicals/Practical-03-Foundations_2.html#creating-an-atlas",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Creating an Atlas",
    "text": "Creating an Atlas\nThe code below creates an Atlas using a dictionary. The dictionary key is a city name, and the value is the latitude, longitude, and main airport code.\n\ncities = {\n    'San Francisco': [37.77, -122.43, 'SFO'],\n    'London': [51.51, -0.08, 'LDN'],\n    'Paris': [48.86,2.29, 'PAR'],\n    'Beijing': [39.92,116.40 ,'BEI'],\n}\n\n\nAdding to a Dict\nAdd a record to the dictionary for Chennai (data here)\n\nQuestion\n\n??\n\n\n\n\nAccessing a Dict\nIn one line of code, print out the airport code for Chennai (MAA):\n\nQuestion\n\n??"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#dealing-with-errors",
    "href": "practicals/Practical-03-Foundations_2.html#dealing-with-errors",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Dealing With Errors",
    "text": "Dealing With Errors\nCheck you understand the difference between the following two blocks of code by running them.\n\ntry:\n    print(cities['Berlin'])\nexcept KeyError as e:\n    print(\"Error found\")\n    print(e)\n\nError found\n'Berlin'\n\n\n\ntry:\n    print(cities.get('Berlin','Not Found'))\nexcept KeyError as e:\n    print(\"Error found\")\n    print(e)\n\nNot Found\n\n\nNotice that trying to access a non-existent element of a dict triggers a KeyError, while asking the dict to get the same element does not, it simply returns None. Can you think why, depending on the situtation, either of these might be the ‘correct’ answer?"
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#thinking-data",
    "href": "practicals/Practical-03-Foundations_2.html#thinking-data",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Thinking Data",
    "text": "Thinking Data\nThis section makes use of both the Dictionaries lecture and the DOLs to Data lecture.\n\n\n\n\n\n\nTip\n\n\n\nIn this section you’ll need to look up (i.e. Google) and make use of a few new functions that apply to dictionaries: &lt;dictionary&gt;.items(), &lt;dictionary&gt;.keys(). Remember: if in doubt, add print(...) statements to see what is going on!\n\n\n\nIterating over a Dict\nAdapting the code below, print out the city name and airport code for every city in our Atlas.\n\nQuestion\n\nfor c in cities.keys():\n    print(??)\n\nThe output should look something like this:\n\nSan Francisco -&gt; SFO\nLondon -&gt; LDN\nParis -&gt; PAR\nBeijing -&gt; BEI\nChennai -&gt; MAA\n\n\n\n\nMore Complex Dicts\nHow would your code need to change to produce the same output from this data structure:\n\ncities = {\n    'San Francisco': {\n        'lat': 37.77, \n        'lon': -122.43,\n        'airport': 'SFO'},\n    'London': {\n        'lat': 51.51, \n        'lon': -0.08, \n        'airport': 'LDN'},\n    'Paris': {\n        'lat': 48.86,\n        'lon': 2.29, \n        'airport': 'PAR'},\n    'Beijing': {\n        'lat': 39.92,\n        'lon': 116.40,\n        'airport': 'BEI'},\n    'Chennai': { \n        'lat': 13.08, \n        'lon': 80.28, \n        'airport': 'MAA'}\n}\n\n\nQuestion\n\nfor c in cities.keys():\n    print(??)\n\n\n\n\nMore Dictionary Action!\nAnd how would it need to change to print out the name and latitude of every city?\n\nQuestion\n\nfor c in cities.keys():\n    print(??)\n\nThe output should be something like this:\n\nSan Francisco is at latitude 37.77\nLondon is at latitude 51.51\nParis is at latitude 48.86\nBeijing is at latitude 39.92\nChennai is at latitude 13.08\n\n\n\n\nAnd Another Way to Use a Dict\nNow produce the same output using this new data structure:\n\ncities_alt = [\n    {'name':     'San Francisco',\n     'position': [37.77, -122.43],\n     'airport':  'SFO'},\n    {'name':     'London',\n     'position': [51.51, -0.08],\n     'airport':  'LDN'},\n    {'name':     'Paris',\n     'position': [48.86, 2.29],\n     'airport':  'PAR'},\n    {'name':     'Beijing',\n     'position': [39.92, 116.40],\n     'airport':  'BEI'},\n    {'name':     'Chennai', \n     'position': [13.08, 80.28],\n      'airport': 'MAA'}\n]\n\n\nQuestion\n\nfor c in cities_alt:\n    print(??)\n\nThe output should be something like this:\n\nSan Francisco is at latitude 37.77\nLondon is at latitude 51.51\nParis is at latitude 48.86\nBeijing is at latitude 39.92\nChennai is at latitude 13.08\n\n\n\n\nThink Data!\nWhat are some of the main differences that you can think of between cities and cities_alt as data? There is no right answer.\n\n\n\n\n\n\nTip\n\n\n\nI just want you to think about these as data! If you were trying to use cities and cities_alt as data what differences would you find when accessing one or more ‘records’?\n\n\n\nPoint 1 here.\nPoint 2 here.\nPoint 3 here."
  },
  {
    "objectID": "practicals/Practical-03-Foundations_2.html#add-to-gitgithub",
    "href": "practicals/Practical-03-Foundations_2.html#add-to-gitgithub",
    "title": "Practical 3: Foundations (Part 2)",
    "section": "Add to Git/GitHub",
    "text": "Add to Git/GitHub\nNow follow the same process that you used last week to ensure that your edited notebook is updated in Git and then synchronised with GitHub."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html",
    "href": "practicals/Practical-07-Spatial_Data.html",
    "title": "Practical 7: Spatial Data",
    "section": "",
    "text": "Last week we did some initial processing on the Inside Airbnb listings data, focussing on its numeric properties. This week we are going to focus on the spatial properties of the data set."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#caching-remote-data",
    "href": "practicals/Practical-07-Spatial_Data.html#caching-remote-data",
    "title": "Practical 7: Spatial Data",
    "section": "Caching Remote Data",
    "text": "Caching Remote Data\nWe don’t want to continually download data over the Internet: 1) because it’s not nice to whoever is hosting the data; and 2) because it requires you be online in order to run your code. So if we can ‘cache’ the data locally so that it’s only downloaded once this makes life much, much easier.\nI’ve used the Numpy-style comments here, but the Google-style also look good in this context and all styles of answer are acceptable so long as they work. See overview of commenting styles on DataCamp.\n\n\n\n\n\n\nTip\n\n\n\nUse this as an opportunity to improve your ability to read code and to learn through documentation.\n\n\n\n\n\n\n\n\nDifficulty level: Moderate\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nimport os\nfrom requests import get\nfrom urllib.parse import urlparse\n\ndef cache_data(src:str, dest:str) -&gt; str:\n    \"\"\"\n    \n    ??\n    \n    \n    \"\"\"    \n    url = urlparse(src) # We assume that this is some kind of valid URL \n    fn  = os.path.split(url.path)[??] # Extract the filename\n    dfn = os.path.join(dest,fn) # Destination filename\n    \n    if not os.path.isfile(dfn) or os.path.getsize(dfn) &lt; 250:\n        \n        print(f\"{dfn} not found, downloading!\")\n\n        path = os.path.split(dest)\n        \n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n            \n        with open(dfn, \"wb\") as file:\n            response = get(src)\n            file.write(??.content)\n            \n        print(\"\\tDone downloading...\")\n\n    else:\n        print(f\"Found {dfn} locally!\")\n\n    return dfn\n\nhelp(cache_data) # &lt;- This should show the docstring you've written"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#downloading-the-data",
    "href": "practicals/Practical-07-Spatial_Data.html#downloading-the-data",
    "title": "Practical 7: Spatial Data",
    "section": "Downloading the Data",
    "text": "Downloading the Data\n\n\n\n\n\n\nDifficulty level: Low, if your function works!\n\n\n\n\n\n\n\n\n\n\nGeopackages\nUse the function above to download and cache the GeoPackage files found on GitHub for Boroughs, Water, and Greenspace, then pass the output of these to GeoPandas. If you have been having trouble downloading files from GitHub, then use the understanding of the function developed above to download the file manually and place it where this function expects to find it!\n\nQuestion\n\nddir  = os.path.join('data','geo') # destination directory\nspath = 'https://github.com/jreades/i2p/blob/master/data/src/' # source path\n\nboros = gpd.read_file( ??(spath+'Boroughs.gpkg?raw=true', ddir) )\nwater = gpd.read_file( ??(spath+'Water.gpkg?raw=true', ddir) )\ngreen = gpd.read_file( ??(spath+'Greenspace.gpkg?raw=true', ddir) )\n\nprint('Done.')\n\n\n\n\nParquet\nLet’s re-use our cache_data function to download and save the full Inside Airbnb data set. Again, if you have trouble with downloading via code, use your understanding of the function to work out where to save your own copy of this file so that the function works as expected.\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\n\nymd  = '20240614'\ncity = 'London'\nhost = 'https://orca.casa.ucl.ac.uk'\nurl  = f'{host}/~jreades/data/{ymd}-{city}-listings.parquet'\n\n\nQuestion\n\n# your code here\ndf = pd.read_parquet( ??(??, os.path.join('data','raw')) )\nprint(f\"Data frame is {df.shape[0]:,} x {df.shape[1]}\")\n\nYou should see that the file was ‘not found’ so ‘downloading’ happened and then the size of the data frame was printed out."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#creating-a-geodataframe",
    "href": "practicals/Practical-07-Spatial_Data.html#creating-a-geodataframe",
    "title": "Practical 7: Spatial Data",
    "section": "Creating a GeoDataFrame",
    "text": "Creating a GeoDataFrame\n\n\n\n\n\n\nDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nRight, we’re finally there! We need to convert our coordinates into some kind of geo-data. GeoPandas offers two ways to do this: the original way using zip and a new utility method called points_from_xy. Here’s the old way:\nfrom shapely.geometry import Point\ngdf = gpd.GeoDataFrame(df, \n                geometry=[Point(x,y) for x, y in zip(df.Longitude,df.Latitude)])\nNote, however, that this did not automatically set a projection, unlike the new approach with the ‘helper function’:\n\ngdf = gpd.GeoDataFrame(df,\n      geometry=gpd.points_from_xy(df.longitude, df.latitude, crs='epsg:4326'))\n\n\nprint(type(gdf))\nprint(type(gdf.geometry))\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\n&lt;class 'geopandas.geoseries.GeoSeries'&gt;"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#saving-geo-data-locally",
    "href": "practicals/Practical-07-Spatial_Data.html#saving-geo-data-locally",
    "title": "Practical 7: Spatial Data",
    "section": "Saving Geo-Data Locally",
    "text": "Saving Geo-Data Locally\n\nThink Storage Formats\nWe want to save the InsideAirbnb GeoDataFrame to the ‘geo’ directory, but first let’s see what file formats are supported.\n\n\n\n\n\n\n\n\nFormat\nAdvantages\nDisadvantages\n\n\n\n\nShapefile\nWide support\nEasy to break via loss of individual file element and not very efficient format\n\n\nGeoJSON\nHuman-readable\nHighly inefficient storage format\n\n\nGeoPackage\nLightweight spatial database with benefits of indexing\nLightweight spatial database with limits on data types supported\n\n\nParquet\nHighly compressed columnar database\nLimited GIS support (except QGIS)\n\n\n\nGeoPackages are really well-supported by QGIS: you can even embed multiple layers with different style information so that your final analysis is easy to distribute; however, they also carry quite a bit of overhead that makes them inefficient for distributing smaller data sets, while also not supporting the full spectrum of Pythonic data structures such as categorical data or lists.\nGeoParquet is an extension of the Parquet format. For working with large data sets this has revolutionised my workflow: getting excited about a columnar database might seem a bit… nerdy… but it’s been transformative for many data scientists. First, because the data set is columnar you only read in the data that you need, so reading Parquet file is blindingly fast. Second, you can stream data from a Parquet file over the Internet, so that means you even gain these advantages reading remote files. Third, you have full support for Python data types, including (up to a point) objects. And, fourth, you can treat multiple Parquet files with the same layout as a single data set or mutiple Parquet files with different layouts as tables in a single database!\n\n\nSpecifying a Driver\nSince there are many formats in which to save geo-data, rather than have multiple to_format_x methods, GeoPandas has one for local files (to_file). If you are reading/writing a filename than ends in a valid extension (e.g. .shp, .gpkg, or .geojson) then GeoPandas will ‘do the right thing’. Where you may run into trouble is if you are reading/writing a URL (e.g. https://github.com/jreades/fsds/blob/master/data/src/Boroughs.gpkg?raw=true). With a URL ending in ?raw=true there’s no extension that GeoPandas can see so you will need to specify a driver. If in doubt, specify the driver.\n\n\n\n\n\n\nClarification\n\n\n\nIn this practical we are reading geodata from GitHub and I’m saying that we need to specify the driver. So why didn’t we need to do with the cache_data function earlier as well? Well, this was a side-benefit of using the standard URL library: it automatically stripped off the query string (?raw=true) when I asked it for the file name, so we saved the file locally as a GeoPackage with .gpkg extension, which means that GeoPandas could read it without any problems.\n\n\nSo the following two bits of code are equivalent:\n\n# This *may* not always do what we want, but should be fine for local files\nboros.to_file('test.gpkg') \n\n\n# This is safer if working across computers/the Internet\nboros.to_file('test.gpkg', driver='GPKG')\n\nSo, starting with fn = ‘20240614-listings.gpkg’\nNotice the difference:\n\nprint(f\"Using '{fn}' as basis for saving data...\")\ntry:\n   gdf.to_file(os.path.join('data','geo',fn), driver='GPKG')\nexcept TypeError as e:\n    print(\"Caught type error...\")\n    print(str(e)[:1000] + \" ...\")\n\nUsing '20240614-listings.gpkg' as basis for saving data...\n\n\nIf you try to save as a GeoPackage file then the code above typically throws a TypeError because of the presence of Categorical data.\nBut the below, in which we specify as a ‘geoparquet’ because of the coordinate data, does not:\n\nfn = fn.replace('.gpkg','.geoparquet')\ngdf.to_parquet(os.path.join('data','geo',fn))\nprint(\"Saved.\")\n\nSaved."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#spatial-indexing",
    "href": "practicals/Practical-07-Spatial_Data.html#spatial-indexing",
    "title": "Practical 7: Spatial Data",
    "section": "Spatial Indexing",
    "text": "Spatial Indexing\nIt’s also worth comparing the output of a point with the output of a polygon or multi-polygon because you may well come across data in formats (e.g. WKT) resembling both of these in real data sets and they can be read as well. Notice too that we can use loc and iloc accessor methods to pull individual points and polygons out of a GeoDataFrame!\n\nprint(gdf.geometry.iloc[1]) # Print out the object's contents\ngdf.geometry.iloc[1] # The object knows how to print itself as a point\n\nPOINT (-0.21707 51.49993)\n\n\n\n\n\n\n\n\n\n\n# Object to string then print out first 399 characters\nprint(str(boros.geometry.iloc[1])[:399] + \"...\") \n# So this is a multi-polygon boundary\nboros.geometry.iloc[1] \n\nMULTIPOLYGON (((535009.2 159504.7, 535005.5 159502, 535002.9 159500.1, 535000.1 159503.2, 535000 159503.2, 534747.8 159517.8, 534588 159522.1, 534512.3 159780.5, 534474.8 159760.8, 534398.8 159726.7, 534391.9 159723.3, 534378.9 159712.4, 534345.1 159702.8, 534314.3 159691.8, 534292.7 159683.9, 534253.1 159666.6, 534229.4 159657.9, 534207.5 159654.6, 534160.5 159651.5, 534159.7 159637.1, 534109.5 ...\n\n\n\n\n\n\n\n\n\nSo each element of this Series has text indicating the type of shape the geometry applies to (e.g. POLYGON) followed by a bunch of numbers. These numbers are truncated here just to make things a little more legible."
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#work-out-the-colour-scheme",
    "href": "practicals/Practical-07-Spatial_Data.html#work-out-the-colour-scheme",
    "title": "Practical 7: Spatial Data",
    "section": "Work Out the Colour Scheme",
    "text": "Work Out the Colour Scheme\n\n\n\n\n\n\nDifficulty level: Moderate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nR and Python take very different approaches to plotting. Do not think of Python’s output as being ‘maps’ in the GIS sense, they are composed of ‘patches’ of color on abstract ‘axes’ that can use any arbitrary coordinate space. So colours are ‘really’ triplet (or quadruplet if you have alpha-blending transparency) values in the range 0.0-1.0. Annotations are then added in similarly abstract fashion.\n\n\nI’d suggest the following colour scheme as a way to test out different ways of specifying colour (though anything you like is fine so long as you manipulate the colours):\n\nThe boroughs can have red edges and white fill with a thick edge.\nThe water should have no edges and XKCD Lightblue fill.\nThe greenspace should have edges and faces specified using different ‘alpha blending’ (i.e. transparency) levels.\n\n\nBoroughs\nBy way of a hint, matplotlib uses edgecolor and facecolor for controlling ‘patches’ (which is what polygons are considered), but the thicker-than-default line-width is specified differently (you’ll need to look this up). So the intention is:\n\nThick red borough borders, and\nWhite fill colour.\n\nJust to drive home how different this is from R, you can find the answer to question 1 on the page for bar plots.\n\nQuestion\n\nboros.plot(??)\n\nYour plot should look similar to this:\n\n\n\n\n\n\n\n\n\n\n\n\nWater\nThe process is the same as above, but I’d like you to work out how to specify: 1. No color for an edge, and 2. An XKCD color for the face.\n\nQuestion\n\nwater.plot(??)\n\nYour plot should look similar to this:\n\n\n\n\n\n\n\n\n\n\n\n\nGreenspace\nThe process is also the same as above, but I’d like you to work out how to specify colours and transparency using RGBA (red-green-blue-alpha transparency) tuples. So we’re looking for: 1. No edge color. 2. A partially transparent green specified as a ‘tuple’ (4 numbers in parentheses in the range 0.0-1.0).\n\nQuestion\n\ngreen.plot(??)\n\nYour plot should look similar to this:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#combining-layers",
    "href": "practicals/Practical-07-Spatial_Data.html#combining-layers",
    "title": "Practical 7: Spatial Data",
    "section": "Combining Layers",
    "text": "Combining Layers\n\n\n\n\n\n\nWarning\n\n\n\nR and Python take very different approaches to plotting. Do not think of Python’s output as being ‘maps’ in the GIS sense, they are composed of ‘patches’ of color on abstract ‘axes’ that can use any arbitrary coordinate space. So colours are ‘really’ numerical triplets (or quadruplets if you have transparency as well) in the range 0.0-1.0. Annotations are then added in similarly abstract fashion.\n\n\nNow that we’ve got our layers looking roughly how we want them, it’s time to combine them. This is also reliant on matplotlib and basically involves plotting items to shared axes which is done by passing in ax=&lt;axis object&gt; to each plot(...). By convention, if you only have a single figure (e.g. a single map) then you create an axis object and name it ax so you will see a lot of ax=ax code in graphing libraries, but =ax is just saying ‘assign to the axis object that I created’.\nSince the axes are how you control what is shown, see if you can find out by Googling how to set the x- and y-limits on the map so that it shows only London and trims out the much larger area of water that is outside of the Greater London Authority. As a rough guideline, this has the Easting range 501,000 to 563,000, and the Northing range 155,000 to 202,000.\nYou can set these limits before or after you start adding layers to the ‘map’, but it’s probably easier conceptually to add them after with the idea of ‘zooming in’ on the features of interest. It’s also easier to debug since you can start by seeing if you can plot the elements at all, and then add the limits to zoom.\nSo the steps are:\n\nWrite the code to plot every image on the same set of axes (I’ve given you something to get started).\nGoogle how to set the limits of the map and then use the ranges I’ve offered above.\nWork out how to change the width of the edges for the boroughs layer.\nSave it somewhere local so that you could, say, load it into a Markdown file!\n\n\n\n\n\n\n\nTip\n\n\n\nThis is a first pass at a map, over the next few weeks we’ll see how to add things like axis labels and titles to make it more ‘map-like’. We don’t have quite the built-in functionality of ggplot alas, but Python is advancing very quickly in this area. There is even an implementation of ggplot in Python, but it’s functionality is more limited. In fact, there’s more than one…\n\n\n\n\n\n\n\n\nDifficulty level: Hard\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n# Creates a new figure with specified number of\n# subplots (we'll see more of this later) and \n# and the specified size (in inches by default).\nfig, ax = plt.subplots(1,1, figsize=(8,6))\n\n# Plot all three GeoPackages to the same axes\nwater.plot(??, ax=ax)\ngreen.??\nboros.??\n\n# Set the x and y limits\n\n\n# Save the image (dpi is 'dots per inch')\nos.??('img', exist_ok=True)\nplt.savefig(os.path.join('img','My_First_Map.png'), dpi=150)\n\nYou may wish for a different look, but here’s one version of the output:"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#choropleth-plots",
    "href": "practicals/Practical-07-Spatial_Data.html#choropleth-plots",
    "title": "Practical 7: Spatial Data",
    "section": "Choropleth Plots",
    "text": "Choropleth Plots\n\n\n\n\n\n\nDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nNow that we’ve converted the InsideAirbnb data to a GeoDataFrame, we can plot it, reproject it, etc.\nSee if you can work out how to plot the points coloured by their price using the appropriate BNG projection.\n\nQuestion\n\ngdf.to_crs(??).plot(column=??, cmap=??, alpha=0.25, markersize=1, figsize=(8,6));\n\nUsing the Viridis colourmap I get the following:\n\n\n\n\n\n\n\n\n\n\n\nWork out the Data Range\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nAs we saw above with the point-plot, in its original form the pricing data will not reveal much of interest because of the range of the data. However, as you will have seen in QM already (and as we explore in greater detail in Weeks 7/8), using transformations we can manipulate the data to increase its tractability for analysis.\nLet’s start by getting a feel for the full data set in terms of the range of prices that it contains:\n\nQuestion\n\nprint(f\"The range of price is ${??:,.2f} to ${??:,.2f}\")\nprint(f\"The mean and median of the price are ${??:,.2f} and ${??:,.2f}\")\n\nNotice the neat little comma-separated thousands in there? That’s fairly easy to do in English, but to use a thousands separator common to another language you would need to do something a little more tricky.\n\n\n\nInheritance!\nWe already know that GeoPandas inherits functionality from Pandas, but let’s formalise this…\nFirst, let’s check what class of object gdf is using the isinstance function:\n\n# Is gdf a GeoDataFrame object?\nif isinstance(gdf, gpd.GeoDataFrame): \n    print(\"\\tI'm a geopandas data frame!\")\n\n# Is gdf *also* a DataFrame object?\nif isinstance(gdf, pd.DataFrame): \n    print(\"\\tI'm a pandas data frame!\")\n\n    I'm a geopandas data frame!\n    I'm a pandas data frame!\n\n\n\n\nBenefiting from Inheritance\nThat result means that we can also investigate the data using, for instance, a pandas histogram:\n\nQuestion\n\n# Oooooh, let's use a *pandas* method here\ngdf.price.plot.??(bins=??, figsize=(8,3)); \n\nNotice how we’ve used our GeoDataFrame as if it’s a plain old DataFrame here? That’s the miracle of Object-Oriented Design: we can do anything we would with a regular Pandas df as we do with a GeoPandas gdf because GeoPandas inherits all the methods of its parent super-class.\nWe can see that there’s very little data above (at a guess) about $2,000, but at this scale it’s hard to tell. We’ve already seen that you can use axes limits to adjust the display of a map, but the same technique applies to plain old plots because they’re fundamentally the same thing.\nTry adjusting the axis so that the x-range is 0..2500:\nax = gdf.price.plot.??(bins=??, figsize=(8,3));\nax.??\nYou can do the same thing with a boxplot:\n\n\nQuestion\n\nax = gdf.price.plot.??(vert=False, figsize=(8,3))\nax.??\n\nMore complex formatting is also possible if you really know your pandas and your matplotlib:\n\ngdf.price.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#truncate-and-transform",
    "href": "practicals/Practical-07-Spatial_Data.html#truncate-and-transform",
    "title": "Practical 7: Spatial Data",
    "section": "Truncate and Transform",
    "text": "Truncate and Transform\n\n\n\n\n\n\nDifficulty level: Hard\n\n\n\n\n\n\n\n\n\n\nWorking it Out\nAnyway, drawing on everything we’ve seen over the past couple of weeks (and in this practical) I’d like you to:\n\nTry to take the natural-log of the price (hint: use numpy) and assign to a new Series called lnprice.\nWork out what the error means.\nWork out how to fix the error and then repeate step 1.\nWork out how many rows were affected.\nReport on the new min/max values.\nWork out if other outliers need to be removed (use code from above).\nRemove outliers and then continue with your work…\n\n# Use this as a 'scratch space' to work out what's needed below...\n\nQuestion\n\nprint(f\"gdf has {gdf.shape[0]:,.0f} rows.\")\n\n# ---------- Do the processing -------------\n# You may need more than one of these 'drops'\n# to get the data the way you want...\ngdf.drop(gdf[??].index, axis=0, inplace=True)\ngdf['lnprice'] = np.log(gdf.price)\n\n# ---------- Check effects -----------\nprint(f\"gdf now has {gdf.shape[0]:,.0f} rows.\")\nprint(f\"The range of price is {gdf.price.min():,.2f} to {gdf.price.max():,.2f}\")\nprint(f\"The range of ln(price) is {gdf.lnprice.min():,.4f} to {gdf.lnprice.max():,.4f}\")\n\ngdf.price.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);\nplt.title(\"Price (Outliers not shown)\")\nplt.show()\n\ngdf.lnprice.plot(kind='box', vert=False, \n             color=dict(boxes='r', whiskers='r', medians='r', caps='r'),\n             boxprops=dict(linestyle='-', linewidth=1.5),\n             flierprops=dict(linestyle='-', linewidth=1.5),\n             medianprops=dict(linestyle='-', linewidth=1.5),\n             whiskerprops=dict(linestyle='-', linewidth=1.5),\n             capprops=dict(linestyle='-', linewidth=1.5),\n             showfliers=False, grid=False, rot=0);\nplt.title(\"Ln(Price) (Outliers not shown)\")\nplt.show()"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#plot-options",
    "href": "practicals/Practical-07-Spatial_Data.html#plot-options",
    "title": "Practical 7: Spatial Data",
    "section": "Plot Options",
    "text": "Plot Options\nNow plot the ln(price) as a chloropleth using:\n\nA figure size of 9 x 6\nA marker size of 0.25\nThe Viridis colourmap\nA legend\nA legend label of ‘Natural Log of Price per Night ($)’\n\nI’d suggest referring to the documentation.\n\nQuestion\n\nax = gdf.plot(figsize=??, marker='*', markersize=0.25, \n         column=??, cmap=??, \n         legend=??, legend_kwds=??);\nax.set_title(\"Plot of Natural Log of Nightly Price for Airbnb Listings (Outliers Removed)\");\n\nYou should get something like:\n\n\n\n\n\nNatural Log of Price per Night ($) with Outliers REmoved"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#zooming-inout",
    "href": "practicals/Practical-07-Spatial_Data.html#zooming-inout",
    "title": "Practical 7: Spatial Data",
    "section": "Zooming In/Out",
    "text": "Zooming In/Out\n\n\n\n\n\n\nDifficulty Level: Low\n\n\n\n\n\n\n\n\n\nThat’s a little hard to see, let’s try zooming in on Central London! Very roughly, let’s call that an Easting range of 525,000 to 535,000 and a Northing range of 178,000 to 185,000.\n\n\n\n\n\n\nPlotting\n\n\n\nWe show one way to do this below (f,ax = plt.subplots(...)) because it gives you greater control, but gdf.plot() can return an axis object (ax = gdf.plot(...)) that gives you the same kind of access… but with a bit more ‘faff’.\n\n\n\nQuestion\n\n# Note this new f,ax syntax and that we then\n# pass ax=ax to gdf.plot -- this has to do with\n# where and how things are plotted.\nf,ax = plt.subplots(1,1,figsize=(8,6))\ngdf.plot(ax=ax, marker='*', markersize=0.25, \n         column='lnprice', cmap='viridis', \n         legend=True, legend_kwds={'label':'Natural Log of Price per Night ($)'});\nax.set_title(\"Ln(Price/Night) for Airbnb Listings (Central London Detail)\")\nax.??\nax.??\nf\n\nYour result should look something like this:\n\n\n\n\n\nNatural Log of Price per Night ($)\n\n\n\n\nThat’s a little better, but ideally we’d do more thinking about outliers…"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#changing-the-classification-scheme",
    "href": "practicals/Practical-07-Spatial_Data.html#changing-the-classification-scheme",
    "title": "Practical 7: Spatial Data",
    "section": "Changing the Classification Scheme",
    "text": "Changing the Classification Scheme\n\n\n\n\n\n\nDifficulty Level: Moderate (mainly computation time)\n\n\n\n\n\n\n\n\n\nLet’s give this one last try using the quantiles classification scheme for Central London!\n\nQuestion\n\nf,ax = plt.subplots(1,1,figsize=(8,6))\nax = gdf.plot(marker='*', markersize=0.25, \n         column='lnprice', cmap='viridis', ??, k=5, \n         legend=True, ax=ax); \n         # Note that the legend *label* had to go -- \n         # there are other ways to add it.\n\nax.set_xlim([525000,535000])\nax.set_ylim([178000,185000])\n\nYour answer should look like:\n\n\n\n\n\nNatural Log of Price per Night ($) in Central London"
  },
  {
    "objectID": "practicals/Practical-07-Spatial_Data.html#credits",
    "href": "practicals/Practical-07-Spatial_Data.html#credits",
    "title": "Practical 7: Spatial Data",
    "section": "Credits!",
    "text": "Credits!\n\nContributors:\nThe following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk), James Millington (james.millington@kcl.ac.uk)\n\n\nLicense\nThese teaching materials are licensed under a mix of The MIT License and the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license.\n\n\nAcknowledgements:\nSupported by the Royal Geographical Society (with the Institute of British Geographers) with a Ray Y Gildea Jr Award.\n\n\nPotential Dependencies:\nThis notebook may depend on the following libraries: geopandas, pandas, matplotlib, seaborn"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html",
    "href": "practicals/Practical-10-Presenting_Data.html",
    "title": "Practical 10: Presenting Data",
    "section": "",
    "text": "Important\n\n\n\nThis practical focusses on the final topic we want to cover in Foundations: visualisation! You will have seen quite a bit of this across the preceding three to four weeks, but it was done in an ad-hoc way, here we try to systematise things a bit."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#anatomy-of-a-figure",
    "href": "practicals/Practical-10-Presenting_Data.html#anatomy-of-a-figure",
    "title": "Practical 10: Presenting Data",
    "section": "Anatomy of a Figure",
    "text": "Anatomy of a Figure\n\n\n\n\n\n\nTip\n\n\n\nYou might want to bookmark the ‘Anatomy of a Figure’ image so that you can easily find and refer to it in the future. This structure is why matplotlib is so much nastier than ggplot, but it does also give you greater control over the output if you really dig into the guts of things.\n\n\nOne of the reasons that Matplotlib is so much more complex than ggplot is that it can actually do a lot more than ggplot, including image manipulation, axis translation, and even 3D. You can get a sense of this by looking at the tutorials since the Users guide can be a bit overwhelming.\nNevertheless, the core components of all matplotlib figures can be seen here:"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#finding-fonts",
    "href": "practicals/Practical-10-Presenting_Data.html#finding-fonts",
    "title": "Practical 10: Presenting Data",
    "section": "Finding Fonts",
    "text": "Finding Fonts\nI find matplotlib’s use of fonts to be profoundly weird. If you use conda and install directly on to the computer then you might have access to all of your computer’s fonts (though there are different types of fonts as well, not all of which will show up), but for most users it will be those that were installed into Docker.\n\nListing Fonts\n\n\n\n\n\n\nWarning\n\n\n\nDepending on how you are running the notebook, you may not be able to see all of the fonts available on your system. If the notebook is running in a Docker container it only has access to the fonts that are installed in the container. In other words: don’t panic if you don’t see the font(s) you want..\n\n\n\nfrom matplotlib import font_manager\nfrom IPython.core.display import HTML\n\nflist = font_manager.findSystemFonts()\nnames = []\nfor fname in flist:\n    try:\n        names.append(font_manager.FontProperties(fname=fname).get_name())\n    except RuntimeError:\n        pass # Think the issue is emoji-support/colour fonts\n\nprint(f\"Found {len(set(names))} fonts.\")\n\ndef make_html(fontname):\n    return \"&lt;p&gt;{font}: &lt;span style='font-family:{font}; font-size: 18px;'&gt;{font}&lt;/p&gt;\".format(font=fontname)\n\ncode = \"\\n\".join([make_html(font) for font in sorted(set(names))])\n\nHTML(\"&lt;div style='column-count: 2;'&gt;{}&lt;/div&gt;\".format(code))\n\nFound 392 fonts.\n\n\n.Aqua Kana: .Aqua Kana\n.Keyboard: .Keyboard\n.LastResort: .LastResort\n.New York: .New York\n.SF Arabic: .SF Arabic\n.SF Arabic Rounded: .SF Arabic Rounded\n.SF Armenian: .SF Armenian\n.SF Armenian Rounded: .SF Armenian Rounded\n.SF Camera: .SF Camera\n.SF Compact: .SF Compact\n.SF Compact Rounded: .SF Compact Rounded\n.SF Georgian: .SF Georgian\n.SF Georgian Rounded: .SF Georgian Rounded\n.SF Hebrew: .SF Hebrew\n.SF Hebrew Rounded: .SF Hebrew Rounded\n.SF NS Mono: .SF NS Mono\n.SF NS Rounded: .SF NS Rounded\n.SF Soft Numeric: .SF Soft Numeric\n.ThonburiUI: .ThonburiUI\nAcademy Engraved LET: Academy Engraved LET\nAl Bayan: Al Bayan\nAl Nile: Al Nile\nAl Tarikh: Al Tarikh\nAmerican Typewriter: American Typewriter\nAmethyst: Amethyst\nAndale Mono: Andale Mono\nAntonio: Antonio\nApple Braille: Apple Braille\nApple Chancery: Apple Chancery\nApple SD Gothic Neo: Apple SD Gothic Neo\nApple Symbols: Apple Symbols\nAppleGothic: AppleGothic\nAppleMyungjo: AppleMyungjo\nArial: Arial\nArial Black: Arial Black\nArial Hebrew: Arial Hebrew\nArial Narrow: Arial Narrow\nArial Rounded MT Bold: Arial Rounded MT Bold\nArial Unicode MS: Arial Unicode MS\nAthelas: Athelas\nAvenir: Avenir\nAvenir Next: Avenir Next\nAvenir Next Condensed: Avenir Next Condensed\nAyuthaya: Ayuthaya\nBaghdad: Baghdad\nBangla MN: Bangla MN\nBangla Sangam MN: Bangla Sangam MN\nBarlow: Barlow\nBaskerville: Baskerville\nBeirut: Beirut\nBergamo Caps: Bergamo Caps\nBergamo Pro: Bergamo Pro\nBergamo Std: Bergamo Std\nBig Caslon: Big Caslon\nBig Shoulders Display: Big Shoulders Display\nBodoni 72: Bodoni 72\nBodoni 72 Oldstyle: Bodoni 72 Oldstyle\nBodoni 72 Smallcaps: Bodoni 72 Smallcaps\nBodoni Ornaments: Bodoni Ornaments\nBradley Hand: Bradley Hand\nBrush Script MT: Brush Script MT\nChalkboard: Chalkboard\nChalkboard SE: Chalkboard SE\nChalkduster: Chalkduster\nCharter: Charter\nCochin: Cochin\nComic Sans MS: Comic Sans MS\nCopperplate: Copperplate\nCorsiva Hebrew: Corsiva Hebrew\nCourier: Courier\nCourier New: Courier New\nDIN Alternate: DIN Alternate\nDIN Condensed: DIN Condensed\nDamascus: Damascus\nDecoType Naskh: DecoType Naskh\nDevanagari MT: Devanagari MT\nDevanagari Sangam MN: Devanagari Sangam MN\nDidot: Didot\nDiwan Kufi: Diwan Kufi\nDiwan Thuluth: Diwan Thuluth\nEB Garamond: EB Garamond\nESRI Default Marker: ESRI Default Marker\nESRI Public1: ESRI Public1\nESRI Transportation & Civic: ESRI Transportation & Civic\nESRI US MUTCD 1: ESRI US MUTCD 1\nESRI US MUTCD 2: ESRI US MUTCD 2\nESRI US MUTCD 3: ESRI US MUTCD 3\nEuphemia UCAS: Euphemia UCAS\nFarah: Farah\nFarisi: Farisi\nFira Code: Fira Code\nFira Mono: Fira Mono\nFira Sans: Fira Sans\nFira Sans Condensed: Fira Sans Condensed\nFira Sans Extra Condensed: Fira Sans Extra Condensed\nFutura: Futura\nGallagher: Gallagher\nGalvji: Galvji\nGasolyn Free: Gasolyn Free\nGeeza Pro: Geeza Pro\nGeneva: Geneva\nGeorgia: Georgia\nGill Sans: Gill Sans\nGujarati MT: Gujarati MT\nGujarati Sangam MN: Gujarati Sangam MN\nGurmukhi MN: Gurmukhi MN\nGurmukhi MT: Gurmukhi MT\nGurmukhi Sangam MN: Gurmukhi Sangam MN\nHeiti TC: Heiti TC\nHelvetica: Helvetica\nHelvetica Neue: Helvetica Neue\nHerculanum: Herculanum\nHiragino Maru Gothic Pro: Hiragino Maru Gothic Pro\nHiragino Mincho ProN: Hiragino Mincho ProN\nHiragino Sans: Hiragino Sans\nHiragino Sans GB: Hiragino Sans GB\nHoefler Text: Hoefler Text\nITF Devanagari: ITF Devanagari\nImpact: Impact\nInaiMathi: InaiMathi\nInconsolata: Inconsolata\nInconsolata Condensed: Inconsolata Condensed\nInconsolata Expanded: Inconsolata Expanded\nInconsolata ExtraCondensed: Inconsolata ExtraCondensed\nInconsolata ExtraExpanded: Inconsolata ExtraExpanded\nInconsolata SemiCondensed: Inconsolata SemiCondensed\nInconsolata SemiExpanded: Inconsolata SemiExpanded\nInconsolata UltraCondensed: Inconsolata UltraCondensed\nInconsolata UltraExpanded: Inconsolata UltraExpanded\nInput Mono: Input Mono\nInput Mono Compressed: Input Mono Compressed\nInput Mono Condensed: Input Mono Condensed\nInput Mono Narrow: Input Mono Narrow\nInput Sans: Input Sans\nInput Sans Compressed: Input Sans Compressed\nInput Sans Condensed: Input Sans Condensed\nInput Sans Narrow: Input Sans Narrow\nInput Serif: Input Serif\nInput Serif Compressed: Input Serif Compressed\nInput Serif Condensed: Input Serif Condensed\nInput Serif Narrow: Input Serif Narrow\nInter Tight: Inter Tight\nIowan Old Style: Iowan Old Style\nJetBrains Mono: JetBrains Mono\nJohnston ITC Std: Johnston ITC Std\nKailasa: Kailasa\nKannada MN: Kannada MN\nKannada Sangam MN: Kannada Sangam MN\nKefa: Kefa\nKhmer MN: Khmer MN\nKhmer Sangam MN: Khmer Sangam MN\nKhula: Khula\nKings Caslon Display: Kings Caslon Display\nKings Caslon Text: Kings Caslon Text\nKingsBureauGrot: KingsBureauGrot\nKlavika: Klavika\nKohinoor Bangla: Kohinoor Bangla\nKohinoor Devanagari: Kohinoor Devanagari\nKohinoor Gujarati: Kohinoor Gujarati\nKohinoor Telugu: Kohinoor Telugu\nKokonor: Kokonor\nKrungthep: Krungthep\nKubos: Kubos\nKufiStandardGK: KufiStandardGK\nLao MN: Lao MN\nLao Sangam MN: Lao Sangam MN\nLiberation Mono: Liberation Mono\nLiberation Sans: Liberation Sans\nLiberation Sans Narrow: Liberation Sans Narrow\nLiberation Serif: Liberation Serif\nLobster: Lobster\nLucida Grande: Lucida Grande\nLuminari: Luminari\nMalayalam MN: Malayalam MN\nMalayalam Sangam MN: Malayalam Sangam MN\nMarion: Marion\nMarker Felt: Marker Felt\nMenlo: Menlo\nMicro 5: Micro 5\nMicrosoft Sans Serif: Microsoft Sans Serif\nMiguta: Miguta\nMishafi: Mishafi\nMishafi Gold: Mishafi Gold\nMonaco: Monaco\nMshtakan: Mshtakan\nMukta Mahee: Mukta Mahee\nMuna: Muna\nMyanmar MN: Myanmar MN\nMyanmar Sangam MN: Myanmar Sangam MN\nNadeem: Nadeem\nNew Peninim MT: New Peninim MT\nNoble Company: Noble Company\nNorse: Norse\nNoteworthy: Noteworthy\nNoto Nastaliq Urdu: Noto Nastaliq Urdu\nNoto Sans Adlam: Noto Sans Adlam\nNoto Sans Armenian: Noto Sans Armenian\nNoto Sans Avestan: Noto Sans Avestan\nNoto Sans Bamum: Noto Sans Bamum\nNoto Sans Bassa Vah: Noto Sans Bassa Vah\nNoto Sans Batak: Noto Sans Batak\nNoto Sans Bhaiksuki: Noto Sans Bhaiksuki\nNoto Sans Brahmi: Noto Sans Brahmi\nNoto Sans Buginese: Noto Sans Buginese\nNoto Sans Buhid: Noto Sans Buhid\nNoto Sans Canadian Aboriginal: Noto Sans Canadian Aboriginal\nNoto Sans Carian: Noto Sans Carian\nNoto Sans Caucasian Albanian: Noto Sans Caucasian Albanian\nNoto Sans Chakma: Noto Sans Chakma\nNoto Sans Cham: Noto Sans Cham\nNoto Sans Coptic: Noto Sans Coptic\nNoto Sans Cuneiform: Noto Sans Cuneiform\nNoto Sans Cypriot: Noto Sans Cypriot\nNoto Sans Duployan: Noto Sans Duployan\nNoto Sans Egyptian Hieroglyphs: Noto Sans Egyptian Hieroglyphs\nNoto Sans Elbasan: Noto Sans Elbasan\nNoto Sans Glagolitic: Noto Sans Glagolitic\nNoto Sans Gothic: Noto Sans Gothic\nNoto Sans Gunjala Gondi: Noto Sans Gunjala Gondi\nNoto Sans Hanifi Rohingya: Noto Sans Hanifi Rohingya\nNoto Sans Hanunoo: Noto Sans Hanunoo\nNoto Sans Hatran: Noto Sans Hatran\nNoto Sans Imperial Aramaic: Noto Sans Imperial Aramaic\nNoto Sans Inscriptional Pahlavi: Noto Sans Inscriptional Pahlavi\nNoto Sans Inscriptional Parthian: Noto Sans Inscriptional Parthian\nNoto Sans Javanese: Noto Sans Javanese\nNoto Sans Kaithi: Noto Sans Kaithi\nNoto Sans Kannada: Noto Sans Kannada\nNoto Sans Kayah Li: Noto Sans Kayah Li\nNoto Sans Kharoshthi: Noto Sans Kharoshthi\nNoto Sans Khojki: Noto Sans Khojki\nNoto Sans Khudawadi: Noto Sans Khudawadi\nNoto Sans Lepcha: Noto Sans Lepcha\nNoto Sans Limbu: Noto Sans Limbu\nNoto Sans Linear A: Noto Sans Linear A\nNoto Sans Linear B: Noto Sans Linear B\nNoto Sans Lisu: Noto Sans Lisu\nNoto Sans Lycian: Noto Sans Lycian\nNoto Sans Lydian: Noto Sans Lydian\nNoto Sans Mahajani: Noto Sans Mahajani\nNoto Sans Mandaic: Noto Sans Mandaic\nNoto Sans Manichaean: Noto Sans Manichaean\nNoto Sans Marchen: Noto Sans Marchen\nNoto Sans Masaram Gondi: Noto Sans Masaram Gondi\nNoto Sans Meetei Mayek: Noto Sans Meetei Mayek\nNoto Sans Mende Kikakui: Noto Sans Mende Kikakui\nNoto Sans Meroitic: Noto Sans Meroitic\nNoto Sans Miao: Noto Sans Miao\nNoto Sans Modi: Noto Sans Modi\nNoto Sans Mongolian: Noto Sans Mongolian\nNoto Sans Mro: Noto Sans Mro\nNoto Sans Multani: Noto Sans Multani\nNoto Sans Myanmar: Noto Sans Myanmar\nNoto Sans NKo: Noto Sans NKo\nNoto Sans Nabataean: Noto Sans Nabataean\nNoto Sans New Tai Lue: Noto Sans New Tai Lue\nNoto Sans Newa: Noto Sans Newa\nNoto Sans Ol Chiki: Noto Sans Ol Chiki\nNoto Sans Old Hungarian: Noto Sans Old Hungarian\nNoto Sans Old Italic: Noto Sans Old Italic\nNoto Sans Old North Arabian: Noto Sans Old North Arabian\nNoto Sans Old Permic: Noto Sans Old Permic\nNoto Sans Old Persian: Noto Sans Old Persian\nNoto Sans Old South Arabian: Noto Sans Old South Arabian\nNoto Sans Old Turkic: Noto Sans Old Turkic\nNoto Sans Oriya: Noto Sans Oriya\nNoto Sans Osage: Noto Sans Osage\nNoto Sans Osmanya: Noto Sans Osmanya\nNoto Sans Pahawh Hmong: Noto Sans Pahawh Hmong\nNoto Sans Palmyrene: Noto Sans Palmyrene\nNoto Sans Pau Cin Hau: Noto Sans Pau Cin Hau\nNoto Sans PhagsPa: Noto Sans PhagsPa\nNoto Sans Phoenician: Noto Sans Phoenician\nNoto Sans Psalter Pahlavi: Noto Sans Psalter Pahlavi\nNoto Sans Rejang: Noto Sans Rejang\nNoto Sans Samaritan: Noto Sans Samaritan\nNoto Sans Saurashtra: Noto Sans Saurashtra\nNoto Sans Sharada: Noto Sans Sharada\nNoto Sans Siddham: Noto Sans Siddham\nNoto Sans Sora Sompeng: Noto Sans Sora Sompeng\nNoto Sans Sundanese: Noto Sans Sundanese\nNoto Sans Syloti Nagri: Noto Sans Syloti Nagri\nNoto Sans Syriac: Noto Sans Syriac\nNoto Sans Tagalog: Noto Sans Tagalog\nNoto Sans Tagbanwa: Noto Sans Tagbanwa\nNoto Sans Tai Le: Noto Sans Tai Le\nNoto Sans Tai Tham: Noto Sans Tai Tham\nNoto Sans Tai Viet: Noto Sans Tai Viet\nNoto Sans Takri: Noto Sans Takri\nNoto Sans Thaana: Noto Sans Thaana\nNoto Sans Tifinagh: Noto Sans Tifinagh\nNoto Sans Tirhuta: Noto Sans Tirhuta\nNoto Sans Ugaritic: Noto Sans Ugaritic\nNoto Sans Vai: Noto Sans Vai\nNoto Sans Wancho: Noto Sans Wancho\nNoto Sans Warang Citi: Noto Sans Warang Citi\nNoto Sans Yi: Noto Sans Yi\nNoto Serif Ahom: Noto Serif Ahom\nNoto Serif Balinese: Noto Serif Balinese\nNoto Serif Hmong Nyiakeng: Noto Serif Hmong Nyiakeng\nNoto Serif Myanmar: Noto Serif Myanmar\nNoto Serif Yezidi: Noto Serif Yezidi\nOS OpenMap Local: OS OpenMap Local\nOlympicons: Olympicons\nOlympicons 2: Olympicons 2\nOptima: Optima\nOriya MN: Oriya MN\nOriya Sangam MN: Oriya Sangam MN\nPT Mono: PT Mono\nPT Sans: PT Sans\nPT Serif: PT Serif\nPT Serif Caption: PT Serif Caption\nPalatino: Palatino\nPapyrus: Papyrus\nParty LET: Party LET\nPhosphate: Phosphate\nPingFang HK: PingFang HK\nPlantagenet Cherokee: Plantagenet Cherokee\nRODE Noto Sans CJK SC B: RODE Noto Sans CJK SC B\nRODE Noto Sans CJK SC R: RODE Noto Sans CJK SC R\nRODE Noto Sans Hindi B: RODE Noto Sans Hindi B\nRODE Noto Sans Hindi R: RODE Noto Sans Hindi R\nRaanana: Raanana\nRoboto Flex: Roboto Flex\nRoboto Mono: Roboto Mono\nRoboto Serif: Roboto Serif\nRoboto Slab: Roboto Slab\nRockwell: Rockwell\nRolves Free: Rolves Free\nSTIX Two Math: STIX Two Math\nSTIX Two Text: STIX Two Text\nSTIXGeneral: STIXGeneral\nSTIXIntegralsD: STIXIntegralsD\nSTIXIntegralsSm: STIXIntegralsSm\nSTIXIntegralsUp: STIXIntegralsUp\nSTIXIntegralsUpD: STIXIntegralsUpD\nSTIXIntegralsUpSm: STIXIntegralsUpSm\nSTIXNonUnicode: STIXNonUnicode\nSTIXSizeFiveSym: STIXSizeFiveSym\nSTIXSizeFourSym: STIXSizeFourSym\nSTIXSizeOneSym: STIXSizeOneSym\nSTIXSizeThreeSym: STIXSizeThreeSym\nSTIXSizeTwoSym: STIXSizeTwoSym\nSTIXVariants: STIXVariants\nSana: Sana\nSathu: Sathu\nSatisfy: Satisfy\nSavoye LET: Savoye LET\nSedan: Sedan\nSedan SC: Sedan SC\nSeravek: Seravek\nShree Devanagari 714: Shree Devanagari 714\nSignPainter: SignPainter\nSilkscreen: Silkscreen\nSilom: Silom\nSinhala MN: Sinhala MN\nSinhala Sangam MN: Sinhala Sangam MN\nSkia: Skia\nSnell Roundhand: Snell Roundhand\nSongti SC: Songti SC\nSource Code Pro: Source Code Pro\nSource Sans 3: Source Sans 3\nSource Serif 4: Source Serif 4\nSource Serif Pro: Source Serif Pro\nSourceCodePro+Powerline+Awesome Regular: SourceCodePro+Powerline+Awesome Regular\nSpectral: Spectral\nStar Jedi: Star Jedi\nStar Jedi Hollow: Star Jedi Hollow\nSukhumvit Set: Sukhumvit Set\nSuperclarendon: Superclarendon\nSymbol: Symbol\nSystem Font: System Font\nTahoma: Tahoma\nTamil MN: Tamil MN\nTamil Sangam MN: Tamil Sangam MN\nTelugu MN: Telugu MN\nTelugu Sangam MN: Telugu Sangam MN\nThonburi: Thonburi\nTimes: Times\nTimes New Roman: Times New Roman\nTrattatello: Trattatello\nTrebuchet MS: Trebuchet MS\nUnica One: Unica One\nVerdana: Verdana\nWaseem: Waseem\nWebdings: Webdings\nWingdings: Wingdings\nWingdings 2: Wingdings 2\nWingdings 3: Wingdings 3\nZapf Dingbats: Zapf Dingbats\nZapfino: Zapfino\nbridamount: bridamount\n\n\n\n\nUsing Fontconfig\nfontconfig is the base Linux utility for managing fonts. We can list font using fc-list and then a set of ‘switches’ determining the kind of information we want back. Since fontconfig doesn’t exist on OSX or Windows, you’ll need to do some more investigating and poking around to get these details on a conda install (I’ll show an option further down)…\nHere we ask fontconfig to format the output so that we only get the first part of the family name, and then we pipe (recall | sends output from one utility to another!) the output of that to sort, which sorts the output, and uniq which removes duplicates (which there will be because there are bold, italic, small-caps, etc. versions of each font). To make better sense of this you can always try playing around with all three steps in the output below!\n\nfonts = ! fc-list --format=\"%{family[0]}\\n\" | sort | uniq\nprint(fonts[:5])\n\n['.Al Bayan PUA', '.Al Nile PUA', '.Al Tarikh PUA', '.Apple Color Emoji UI', '.Apple SD Gothic NeoI']\n\n\n\n\n\n\n\n\nCapturing output\n\n\n\nNotice that we’re able to capture the output of an external application (called via the Terminal) with fonts = ! .... This can be useful when something is easy to do on the command line but hard to do in Python.\n\n\nThe below option also pipes output from fonctconfig, but to the grep utility which checks each line for the character sequence Liberation. Now we’re asking fontconfig to include style details which will relate to both weight (regular, bold, extra bold, light, etc.) and italic, bold, small caps, etc.\n\nfonts = ! fc-list : family style | grep \"Liberation\"\nprint(sorted(fonts)[:5])\n\n['Liberation Mono:style=Bold', 'Liberation Mono:style=Bold Italic', 'Liberation Mono:style=Italic', 'Liberation Mono:style=Regular', 'Liberation Sans Narrow:style=Bold']\n\n\nYou can find more examples here, a more detailed set of instructions here, and even information about (for example) supported languages based on RFC 3066.\nHere are the languages supported by the Ubuntu Light font:\n\nlangs = ! fc-list \"Liberation Mono\" : lang\nprint(sorted(langs)[:5], \"...\")\n\n[':lang=aa|af|av|ay|be|bg|bi|br|bs|ca|ce|ch|co|cs|cy|da|de|el|en|eo|es|et|eu|fi|fj|fo|fr|fur|fy|gd|gl|gv|ho|hr|hu|ia|id|ie|ik|io|is|it|ki|kl|kum|la|lb|lez|lt|lv|mg|mh|mo|mt|nb|nds|nl|nn|no|nr|nso|ny|oc|om|os|pl|pt|rm|ro|ru|se|sel|sk|sl|sma|smj|smn|so|sq|sr|ss|st|sv|sw|tk|tl|tn|tr|ts|uk|uz|vo|vot|wa|wen|wo|xh|yap|zu|an|crh|csb|fil|hsb|ht|jv|kj|ku-tr|kwm|lg|li|ms|na|ng|pap-an|pap-aw|rn|rw|sc|sg|sn|su|za'] ...\n\n\nHere are the monospace fonts installed:\n\nmonos = ! fc-list :spacing=mono : family | sort | uniq\nprint(sorted(monos)[:5], \"...\")\n\n['.Apple Color Emoji UI', '.LastResort', '.SF NS Mono', '.Times LT MM', 'Adobe Garamond'] ...\n\n\n\n\nFontdicts\nNow that we know what’s available, the next step is to set up some useful defaults that we can re-use across multiple plots to ensure consistency of output. The format for specifying fonts on a per-figure basis is a dictionary, so where you see fontdict in the matplotlib documentation the following should work:\nHere’s the example:\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n\nff='Liberation Sans'\ntfont = {'fontname':ff}\nbfont = {'fontname':ff, 'weight':'bold', 'horizontalalignment':'left'}\nafont = {'fontname':ff}\n\nI am setting the ‘title font’ (tfont) and ‘body copy font’ (bfont) and ‘axis font’ (afont) here to use in the output below. You can pick another font and see what happens.\n\n\n2.3: Using Fonts\nAt this point we’re going to work towards a kind of ‘atlas’ that would make it easy to compare some features for different London boroughs. I basically implemented a the basic matplotlib version of QGIS’ Atlas functionality.\n\n# This will be whatever LA you processed last week\nLA = 'Waltham Forest'\n\n\nmsoa_gdf = gpd.read_parquet(os.path.join('data','geo',f'{LA}-MSOA_data.geoparquet'))\n\n\nmedian_gdf  = msoa_gdf[['MSOA11CD','median_price','geometry']]\nlisting_gdf = msoa_gdf[['MSOA11CD','listing_count','geometry']]\n\n\nimport matplotlib.pyplot as plt\n\n\n\nThe Defaults\nHere is a demonstration of some of the ways you can adjust features in a Python matplotlib plot. I’m not suggesting either of these is a good output, but that’s not the point! The idea is to see the various ways you can tweak a plot… And notice that we’ve not yet changed any fonts. And it shows.\n\n# Set up a 1 x 2 plot (you can also leave off the nrows= and ncols=)\nf,axes = plt.subplots(nrows=1, ncols=2, figsize=(8,6))\n# ax1 will be the first plot on the left, ax2 will be on the right;\n# a 2 (or more) *row* plot will return a list of lists... 1 list/row.\nax1 = axes[0]\nax2 = axes[1]\n\n# Left plot is the median price\nmedian_gdf.plot(column='median_price', ax=ax1, legend=True, cmap='viridis')\nax1.set_title(\"Median Price per MSOA\");\n# Turn off the frame, one side of the plat at a time\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n# Set the labels\nax1.set_xlabel(\"Easting\");\nax1.set_ylabel(\"Northing\");\n\n# Right plot is the number of listings; note\n# here the use of both zorder (which is the \n# 'stacking order' of elements on the plot, and\n# the legend_kwds (keywords) to change the \n# orientation of the plot to horizontal\nlisting_gdf.plot(column='listing_count', ax=ax2, legend=True, cmap='plasma', zorder=1, \n                 legend_kwds={\"orientation\": \"horizontal\"})\nax2.set_title(\"Count of Listings per MSOA\");\n# Set a background colour for the plot\nax2.set_facecolor((.4, .4, .4, .2))\n# Add grid lines and set their zorder to\n# below that of the data on the plot\nplt.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\nax2.set_axisbelow(True)\n\n# This is equivalent to the ax1.spines... \n# above, but if you use it here you lose\n# the background to the plot as well!\n#plt.gca().set(frame_on=False)\n\n# Remove the labels on the ticks of the \n# axes (meaning: remove the numbers on \n# x- and y-axes).\nax2.set_xticklabels([])\nax2.set_yticklabels([])\n\n# Set the labels\nax2.set_xlabel(\"Easting\");\nax2.set_ylabel(\"Northing\");\n\n\n\n\n\n\n\n\n\n\nImproving on Defaults\n\nf,axes = plt.subplots(1,2,figsize=(8,6))\n\n# Set up the plots\nmedian_gdf.plot(column='median_price', ax=axes[0], legend=True, cmap='viridis')\nlisting_gdf.plot(column='listing_count', ax=axes[1], legend=True, cmap='plasma')\nfor ax in axes:\n    ax.axis('off')\n    # Note that here, set_facebolor doesn't work,\n    # presumably because the axis is 'off'\n    ax.set_facecolor((.4, .4, .4, .2))\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expading the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\naxes[0].set_title('Median Price', size=20, **afont)\naxes[1].set_title('Count', size=20, **afont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.92, linespacing=1.4, va='top', size=12, \n            s=f\"Total listings: {listing_gdf['listing_count'].sum():,.0f}\\nMedian price: ${median_gdf['median_price'].median():,.2f}\", **bfont);"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#adding-picture-in-picture",
    "href": "practicals/Practical-10-Presenting_Data.html#adding-picture-in-picture",
    "title": "Practical 10: Presenting Data",
    "section": "Adding Picture-in-Picture",
    "text": "Adding Picture-in-Picture\nWe’re now going to emulate a bit of QGIS’ Atlas function by creating two subplots and then adding a third plot afterwards that shows where the borough is.\n\nf,axes = plt.subplots(1,3,gridspec_kw={'width_ratios':[1,4,4]}, figsize=(8,6))\n\n# Plot 0 is basically being used as a 'spacer' \n# as you'll see below\naxes[0].axis('off')\n\n# Plot 1 is the median price\nmedian_gdf.plot(column='median_price', ax=axes[1], legend=True, cmap='viridis')\naxes[1].set_title('Median Price', size=20, **afont)\n\n# Plot 2 is the count of listings\nlisting_gdf.plot(column='listing_count', ax=axes[2], legend=True, cmap='plasma')\naxes[2].set_title('Count', size=20, **afont)\n\n# For plots 1 and 2... if you were doing this a lot it could be a function!\nfor ax in axes[1:]:\n    ax.set_facecolor((.9, .9, .9, .5))\n    ax.grid(visible=True, which='major', axis='both', color='w', linestyle='-', linewidth=2, zorder=0)\n    ax.set_axisbelow(True)\n    ax.spines['top'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.tick_params(axis='both', which='both', length=0)\n\n# Add a *third* chart that we use as a kind of 'PiP'\n# to show which borough we're talking about. The \n# add_axes call is here taking information about the\n# positioning and size of the additional figure.\n# Disable ax2.axis('off') if you want to see the\n# figure in full.\nax3 = f.add_axes([0.015, 0.7, 0.2, 0.2])\nspath = 'https://github.com/jreades/fsds/blob/master/data/src/' # source path\nddir  = os.path.join('data','geo') # destination directory\nboros = gpd.read_file( cache_data(spath+'Boroughs.gpkg?raw=true', ddir) )\nboros.plot(facecolor='lightgrey', edgecolor='w', linewidth=1, ax=ax3)\nboros[boros.NAME==LA].plot(facecolor='r', edgecolor='none', hatch='///', ax=ax3)\nax3.axis('off')\n\n# Add the 'super-title', but notice that it is not \n# longer either centered (x=0.025) or centre-aligned\n# (horizonal alignment=left). We also see **tfont, which\n# is a way of expanding the 'tfont' dictionary into a \n# set of parameters to a function call. We do the same\n# for the titles on each figure, but passing a different\n# fontdict.\nf.suptitle(LA, x=0.025, ha='left', size=24, **tfont)\n\n# And add a short piece of text below the borough\nplt.figtext(x=0.025, y=0.65, s=f\"Total listings: {listing_gdf['listing_count'].sum():,.0f}\", size=12, **bfont);\n\n+ data/geo/Boroughs.gpkg found locally!"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#bonus-achievement-unlocked",
    "href": "practicals/Practical-10-Presenting_Data.html#bonus-achievement-unlocked",
    "title": "Practical 10: Presenting Data",
    "section": "Bonus Achievement Unlocked!",
    "text": "Bonus Achievement Unlocked!\nIf you have the time and inclination, see if you can convert the above to an actual atlas output:\n\nYou’ll want to turn this plot into a function so as to be able to produce (and save) the map for every borough.\nYou’ll even need to parameterise the filename so that you save to different PNG files as well as going back to see how we generated the listing and pricing data frames for the Local Authority…\nAnd you’ll also need to make sure that you ensure a consistent colour bar (for all of London, because the median price and number of listings will vary rather a lot by LA)\nThen there’s the placement of the PiP for some boroughs with long names\nAnd finally, you might consider adding some more text to atlas–maybe pull some content from Wikipedia using Beautiful Soup (bs4)?"
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#for-a-chart",
    "href": "practicals/Practical-10-Presenting_Data.html#for-a-chart",
    "title": "Practical 10: Presenting Data",
    "section": "For a Chart",
    "text": "For a Chart\nGroup the listings by Borough and Room Type, and aggregate by median price, also producing a count variable for the number of listings of each type in each Borough.\n\nla_tots = gdf_la[gdf_la.NAME==LA].groupby(by='room_type', observed=False).agg(\n                            {'price':'median', 'listing_url':'count'}\n                        ).reset_index().rename(columns={'listing_url':'count'})\nla_tots\n\n\n\n\n\n\n\n\nroom_type\nprice\ncount\n\n\n\n\n0\nEntire home/apt\n117.0\n946\n\n\n1\nHotel room\nNaN\n0\n\n\n2\nPrivate room\n47.0\n687\n\n\n3\nShared room\n24.5\n6\n\n\n\n\n\n\n\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.palettes import Spectral4\nfrom bokeh.models import CustomJS, Dropdown\n\noutput_notebook()\n\nroom_types = la_tots.room_type.to_list()\nprices     = la_tots.price.to_list()\ncounts     = la_tots['count'].to_list()\n\n# Add hover tooltip\nsource = ColumnDataSource(data=dict(\n    rt=room_types,\n    count=counts,\n    price=prices,\n))\n\nTOOLTIPS = [\n    (\"Room Type\", \"@rt\"),\n    (\"Number of Listings\", \"@count{,}\"),\n    (\"Median Price\", \"$@price{,}/night\")\n]\n\np = figure(x_range=room_types, height=300, tooltips=TOOLTIPS,\n           title=f\"Median Price by Room Type in {LA}\",\n           toolbar_location=None, tools=\"\")\n\np.vbar(x='rt', top='count', width=0.9, source=source)\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nshow(p)\n\n    \n    \n        \n        Loading BokehJS ..."
  },
  {
    "objectID": "practicals/Practical-10-Presenting_Data.html#for-a-map",
    "href": "practicals/Practical-10-Presenting_Data.html#for-a-map",
    "title": "Practical 10: Presenting Data",
    "section": "For a Map",
    "text": "For a Map\nThis is not the prettiest code, but it should work…\n\nfrom bokeh.plotting import figure\n\nfrom bokeh.io import output_file, show, output_notebook, push_notebook, export_png\nfrom bokeh.models import ColumnDataSource, GeoJSONDataSource, LinearColorMapper, ColorBar, HoverTool\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import brewer\n\n#output_notebook()\n\n\nmsoadf = gpd.sjoin(\n            gdf_la[gdf_la.NAME==LA].reset_index(), \n            msoas[msoas.Borough==LA].drop(columns=['index_right']), predicate='within')\n\n\nmsoagrdf = msoadf.groupby('MSOA11NM').agg({'price':['median','count']}).reset_index()\nmsoagrdf.columns=['msoa11nm','median','count']\n\nI cobbled the mapping functions below together from two tutorials I found online (this one and this one). As you can see, this is a very different approach to mapping data, but it has clear benefits for exploratory purposes and produces fast, interactive maps… and I’ve not even added selection and filtering tools!\n\nimport json\n\ndef get_geodatasource(gdf):    \n    \"\"\"Get getjsondatasource from geopandas object\"\"\"\n    json_data = json.dumps(json.loads(gdf.to_json()))\n    return GeoJSONDataSource(geojson = json_data)\n\ndef bokeh_plot_map(gdf, column=None, title=''):\n    \"\"\"Plot bokeh map from GeoJSONDataSource \"\"\"\n\n    geosource = get_geodatasource(gdf)\n    palette = brewer['OrRd'][8]\n    palette = palette[::-1]\n    vals = gdf[column]\n    \n    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n    color_mapper = LinearColorMapper(palette=palette, low=vals.min(), high=vals.max())\n    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8, width=500, height=10,\n                         location=(0,0), orientation='horizontal')\n\n    tools = 'wheel_zoom,pan,reset,hover'\n    \n    p = figure(title = title, height=700, width=850, toolbar_location='right', tools=tools)\n    p.add_tile(\"CartoDB Positron\", retina=True)\n    p.xgrid.grid_line_color = None\n    p.ygrid.grid_line_color = None\n    \n    # Add patch renderer to figure\n    p.patches('xs','ys', source=geosource, fill_alpha=0.5, line_width=0.5, line_color='white',  \n              fill_color={'field' :column , 'transform': color_mapper})\n    \n    # Specify figure layout.\n    p.add_layout(color_bar, 'below')\n    \n    # Add hover\n    hover = p.select_one(HoverTool)\n    hover.point_policy = \"follow_mouse\"\n    hover.tooltips = [(\"Borough\", \"@Borough\"),\n                      (\"Neighbourhood\", \"@msoa11hclnm\"),\n                      (\"Count of Listings\", \"@count\"),\n                      (\"Median Price\", \"$@median\")]\n    \n    return p\n\nReproject to Web Mercator:\n\nmsoa_gdf = pd.merge(msoagrdf, msoas, left_on='msoa11nm', right_on='MSOA11NM', how='inner')\nmsoa_gdf = msoa_gdf.set_geometry('geometry').set_crs('epsg:27700')\n\n\nmsoageo = msoa_gdf.to_crs('epsg:3785')\nmsoageo.total_bounds\n\narray([-6.74542047e+03,  6.71906611e+06,  3.04361304e+03,  6.73637453e+06])\n\n\nAnd map it!\n\n# Need to drop the right geometry column\n# as Bokeh doesn't know how to handle two\n# and tries to 'serialise' the second geom.\np = bokeh_plot_map(msoageo.drop(columns=['geometry_right','index_right']), 'median', title=f'MSOA-Level Activity in {LA}')\n\nhandle = show(p, notebook_handle=True)\npush_notebook(handle=handle)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n🔗 Connections\n\n\n\nAnd that’s it. That’s all she wrote! You’ve now covered in &lt;10 weeks what many people might take 10 months to cover. So do not feel like either: 1) you know it all; or 2) you know nothing. You have learned a lot, but it’s probably just enough to see how much you don’t know. And that is the start of wisdom. Good luck, young Python-master!"
  },
  {
    "objectID": "readings/ArribasBel-2018.html",
    "href": "readings/ArribasBel-2018.html",
    "title": "Geography and computers: Past, present, and future",
    "section": "",
    "text": "1. What kind of reading is it?\n“Geography and Computers: Past, Present, and Future” by Dani Arribas-Bel and Jon Reades is an academic paper published in Geography Compass in 2018¹. The paper aims to make a conceptual and theoretical contribution by exploring the historical and contemporary relationship between geography and computational methods, and by proposing future directions for the integration of data science into geographical research².\n\n\n2. Who is the intended audience?\nThe intended audience for this paper includes academics, field experts, and advanced students in geography and related disciplines². The paper’s detailed historical review, theoretical discussions, and focus on recent developments in data science suggest it is meant for those who are already familiar with the basics of geographical study and are interested in the evolving role of computational methods in the field².\n\n\n3. How is the piece structured?\nThe paper is structured as follows:\n\nIntroduction: Sets the context for the discussion by highlighting the rise of big data and its impact on geography.\nHistorical Perspective: Reviews the history of computation in geography to provide context for contemporary debates.\nRecent Developments: Documents recent advancements in data science and their implications for geography.\nFuture Directions: Reflects on how Geographic Data Science might provide a foundation for further development.\nConclusion: Summarizes the main points and discusses the potential future of geography in the context of computational advancements¹².\n\nThis structure is designed to cater to an academic audience, providing a clear and logical flow of information that helps readers navigate through complex theoretical and methodological discussions².\n\n\n4. What are the key ideas, concepts, or theories discussed?\nThe key ideas and concepts discussed include:\n\nHistorical Relationship: The long-standing relationship between geography and computers.\nBig Data: The impact of big data on geographical research and the potential for a new computational paradigm.\nGeographic Data Science: The proposal for a new field that integrates geography and data science to enhance the understanding of spatial phenomena.\nMethodological Shifts: The evolution of geographical methods in response to technological changes in computer hardware and software¹².\n\nThese concepts are central to understanding the transformation of geography in the context of computational advancements and are discussed in detail to illustrate the significance of integrating data science into geographical research².\n\n\n5. What is the overall contribution?\nThe paper builds on the existing body of geographical knowledge by highlighting the historical and contemporary relationship between geography and computational methods. It responds to the gap in understanding how big data and data science can be integrated into geographical research. The key findings include the identification of significant methodological shifts and the proposal for the establishment of Geographic Data Science as a new field¹².\n\n\n6. What issues or gaps remain?\nSome potential issues or gaps include:\n\nAssumptions: The paper assumes that the integration of data science into geography is universally beneficial, which might not hold in all contexts.\nContextual Variability: The impact of computational methods might vary across different regions and sub-disciplines of geography.\nFuture Work: Areas for future work could include exploring the applicability of data science in diverse geographical contexts and integrating qualitative approaches to provide a more holistic understanding¹².\n\n\n\n7. How does this connect to the module/programme?\nThis reading connects to the module by providing a historical and theoretical foundation for understanding the evolution of geographical methods in the context of computational advancements. It prompts reflection on the methodological shifts in the discipline and encourages students to think critically about the applicability and limitations of data science in geography. This reading likely complements other modules focused on research methods, theoretical frameworks, and the history of geographical thought, raising further questions about the integration of different methodological approaches in geography¹².\nIf you have any more questions or need further details, feel free to ask!\n¹: Geography Compass ²: University of Liverpool Repository\nConversation with Copilot 24/09/2024\n\nGeography and computers: Past, present, and future. https://scholar.archive.org/work/j7nnki6bczg25fenio5u36bslq.\nGeography and Computers: Past, present, and future. https://core.ac.uk/download/pdf/161102322.pdf.\nGeography & Computers: Past, present, and future – Geography. https://blogs.kcl.ac.uk/geography/2018/09/09/geography-computers-past-present-and-future/.\nundefined. https://web.archive.org/web/20200322035948/https://kclpure.kcl.ac.uk/portal/files/105003893/Geography_Compass_Self_Archiving_Version.pdf.\n\n\n\n\n\n\nReferences\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403."
  },
  {
    "objectID": "readings/Franklin-2024.html",
    "href": "readings/Franklin-2024.html",
    "title": "Quantitative methods III: Strength in numbers?",
    "section": "",
    "text": "Here’s a detailed breakdown of “Quantitative Methods III: Strength in Numbers?” by Rachel Franklin, addressing your questions:\n\nWhat kind of reading is it?\n\nType of Document: This is an academic journal article published in the journal Progress in Human Geography. It is the third in a series discussing quantitative methods in geography.\nContribution Type: The paper makes a conceptual and critical contribution. It reflects on the growing influence of quantitative methods within human geography, examining their role, limitations, and the broader implications of relying heavily on numerical data. It does not present original empirical findings but critically assesses the field and suggests areas for reflection.\n\n\n\nWho is the intended audience?\n\nAudience:\n\nThe paper is written primarily for academics and experts in the field of human geography, particularly those interested in the methodological debates within the discipline. It engages with scholars and researchers who are familiar with the ongoing discourse on quantitative vs. qualitative approaches in geography.\n\nHow do we know?:\n\nThe language and concepts used in the article are highly specialized and assume a background in geography and an understanding of quantitative methods. It references theoretical discussions and trends within human geography, which suggests that it is targeting a well-informed, scholarly audience.\nThe article doesn’t aim to teach new skills or offer practical advice for policymakers, but rather to engage with methodological debates within academia.\n\n\n\n\nHow is the piece structured?\n\nStructure:\n\nIntroduction: Sets the context for the discussion and reviews recent trends in the use of quantitative methods in human geography.\nMain Sections:\n\nThe paper is divided into several sections, each addressing different aspects of quantitative methods. It covers topics such as:\n\nThe growing emphasis on big data and data analytics.\nThe balance between quantitative and qualitative methods in the discipline.\nThe critical perspective on the dominance of numerical methods.\n\n\nConclusion: Reflects on the potential risks of over-reliance on quantitative approaches and offers suggestions for the future.\n\nResponse to Audience and Reading Type:\n\nThe structure of the article is analytical and discursive, with each section building on the previous one to present a cohesive argument. This structure suits the intended academic audience, facilitating a reflective critique of quantitative methods and their role in human geography.\n\n\n\n\nWhat are the key ideas, concepts, or theories discussed?\n\nKey Ideas:\n\nBig Data and Quantification: The paper discusses the increasing role of big data and how the availability of vast amounts of information has led to a rise in the use of quantitative techniques in geography.\nCritique of Quantitative Dominance: Franklin critiques the dominance of quantitative methods, arguing that while numbers can provide strength, they can also mask important contextual and qualitative insights. She questions whether this reliance on data may overlook the nuances of human experiences and socio-spatial processes.\nBalancing Quantitative and Qualitative Approaches: The article emphasizes the need to balance quantitative methods with qualitative approaches to avoid a one-dimensional view of geographic phenomena.\n\nHow do we know?:\n\nThese ideas are explicitly discussed throughout the paper. Franklin references the growing use of computational tools and big data in geography and critiques the risks associated with relying too heavily on these methods. The argument for balance between qualitative and quantitative methods is a central theme.\n\n\n\n\nWhat is the overall contribution?\n\nMain Contribution:\n\nThe paper contributes to the ongoing methodological debate in human geography about the role of quantitative methods. It provides a critical reflection on the growing reliance on big data and advanced analytics in the field and calls for a more thoughtful integration of qualitative perspectives.\n\nWhat gap does it respond to?:\n\nIt addresses the gap between the rising dominance of quantitative methods and the need for a more nuanced understanding of social and spatial processes, which qualitative methods often provide. Franklin argues that the increased focus on big data may be narrowing the scope of geographic inquiry.\n\nKey Findings or Conclusions:\n\nThe main conclusion is that while quantitative methods offer valuable insights, human geography should be cautious about over-reliance on them. There is a risk of losing sight of important social and contextual factors that are not easily quantifiable.\n\n\n\n\nWhat issues or gaps remain?\n\nRemaining Issues:\n\nOver-reliance on Numbers: Franklin highlights the assumption that quantitative methods are inherently objective or superior. This assumption may not hold in contexts where human experiences and socio-spatial dynamics are complex and difficult to quantify.\nContexts Where It Applies: The reading is particularly relevant in fields where both qualitative and quantitative methods are used to understand human behavior and societal processes, such as urban studies, social geography, or policy research. In these contexts, a balanced approach is essential to capture both numerical trends and individual experiences.\n\nFuture Work Identified:\n\nFranklin suggests future work should explore how quantitative methods can be better integrated with qualitative insights. She calls for methodological pluralism, where both approaches are valued and used in tandem to provide a fuller understanding of geographical issues.\n\nWhy Is the Contribution Valid?\n\nThe contribution is valid because it highlights a critical concern in the discipline: the need to avoid methodological silos. By advocating for a balance between qualitative and quantitative methods, Franklin addresses a longstanding debate that is relevant to many fields beyond geography.\n\n\n\n\nConclusion\nRachel Franklin’s “Quantitative Methods III: Strength in Numbers?” provides a reflective and critical contribution to discussions about the role of quantitative methods in human geography. It responds to the increasing emphasis on big data and computational approaches, cautioning against over-reliance on these methods. The paper makes a compelling case for balancing quantitative and qualitative methods to ensure a more comprehensive understanding of geographical phenomena, leaving room for further exploration of how these approaches can be integrated more effectively in future research.\n\n\n\n\n\nReferences\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512."
  },
  {
    "objectID": "ref/Bibliography.html",
    "href": "ref/Bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Bibliography\n\n\nAbhinav. 2025. “Docker’s Gone — Here’s Why It’s Time to Move on | by Abhinav | Medium.” Online; Medium.\n\n\nAlsudais, Abdulkareem. 2021. “Incorrect Data in the Widely Used Inside Airbnb Dataset.” Decision Support Systems 141:113453. https://doi.org/10.1016/j.dss.2020.113453.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/10.1177/0263276419851846.\n\n\nAnderson, C. 2008. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete.” Wired. https://www.wired.com/2008/06/pb-theory/.\n\n\nArribas-Bel, Daniel. 2014. “Accidental, Open and Everywhere: Emerging Data Sources for the Understanding of Cities.” Applied Geography 49. Elsevier:45–53. https://doi.org/10.1016/j.apgeog.2013.09.012.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBivens, J. 2019. “The Economic Costs and Benefits of Airbnb.” Economic Policy Institute. The economic costs and benefits of Airbnb.\n\n\nBrockes, E. 2023. “Airbnb was wild, disruptive and cheap: we loved it. But it wasn’t a love strong enough to last.” The Guardian. https://www.theguardian.com/commentisfree/2023/mar/08/airbnb-wild-disruptive-cheap-lettings-agency.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nClark, J. 2023. “Bidding wars: inside the super-charged fight for rental properties.” The Guardian. https://www.theguardian.com/money/2023/apr/08/bidding-wars-inside-the-super-charged-fight-for-rental-properties.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020d. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020c. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020e. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020b. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020a. Data Feminism. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\nDark Matter Labs. 2019. “A Smart Commons: A New Model for INvesting in the Commons.” Medium. https://provocations.darkmatterlabs.org/a-smart-commons-528f4e53cec2.\n\n\nDavenport, T. H., and D. J. Patil. 2012. “Data Scientist: The Sexiest Job of the 21st Century.” Harvard Business Review. https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century.\n\n\nDelmelle, Elizabeth C, and Isabelle Nilsson. 2021. “The Language of Neighborhoods: A Predictive-Analytical Framework Based on Property Advertisement Text and Mortgage Lending Data.” Computers, Environment and Urban Systems 88. Elsevier:101658. https://doi.org/10.1016/j.compenvurbsys.2021.101658.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics‘.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching introductory GIS programming to geographers using an open source Python approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30. https://doi.org/10.1080/03098265.2015.1086981.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nFitzpatrick, B., and B. Collins-Sussman. n.d. “The Myth of the ’Genius Programmer’.” Google. https://www.youtube.com/watch?v=0SARbwvhupQ.\n\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGraham, P. 2022. “Putting Ideas into Words.” 2022. https://paulgraham.com/words.html.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHägerstrand, Torsten. 1967. “The Computer and the Geographer.” Transactions of the Institute of British Geographers. JSTOR, 1–19.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHarvey, D. 2008 [1972]. “Revolutionary and Counter-Revolutionary Theory in Geography and the Problem of Ghetto Formation.” In Geographic Thought, edited by G. Henderson and M. Waterstone, 1st Edition. Routledge. https://doi.org/10.4324/9780203893074.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nIqbal, N., and A. Chakrabortty. 2023. “Why are London’s inner-city schools disappearing?” Edited by A. Bransbury. The Guardian. 2023. https://www.theguardian.com/news/audio/2023/apr/26/why-are-london-schools-disappearing-podcast.\n\n\nJolly, J. 2023. “Owners of 100,000 properties held by foreign shell companies unknown despite new UK laws.” The Guardian. https://www.theguardian.com/business/2023/sep/03/owners-of-100000-properties-held-by-foreign-shell-companies-unknown-despite-new-uk-laws.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2). Oxford University Press:97–111.\n\n\n———. 1996. Selected Papers on Computer Science. Cambridge University Press.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLansley, Guy. 2016. “Cars and Socio-Economics: Understanding Neighbourhood Variations in Car Characteristics from Administrative Data.” Regional Studies, Regional Science 3 (1). Taylor & Francis:264–85. https://doi.org/10.1080/21681376.2016.1177466.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are statisticians cold-blooded bosses? a new perspective on the ’old’ concept of statistical population.” Teaching Statistics 35 (1). Wiley Online Library:66–71. https://doi.org/10.1111/j.1467-9639.2012.00524.x.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458.\n\n\nMattern, Shannon. 2015. “Mission control: A history of the urban dashboard.” Places Journal. https://doi.org/10.22269/150309.\n\n\n———. 2017. “A City Is Not a Computer.” Places Journal. https://doi.org/10.22269/170207.\n\n\nMcCullough, D. 2002. “Interview with NEH chairman Bruce Cole.” Humanities Magazine.\n\n\nMiller, Harvey J, and Michael F Goodchild. 2015. “Data-Driven Geography.” GeoJournal 80. Springer:449–61. https://doi.org/10.1007/s10708-014-9602-6.\n\n\nMinton, A. 2023. “New York is breaking free of Airbnb’s clutches. This is how the rest of the world can follow suit.” The Guardian. https://www.theguardian.com/commentisfree/2023/sep/27/new-york-airbnb-renters-cities-law-ban-properties.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nNeate, R. 2023. “‘This is where people with staggering wealth end up’: who will buy Britain’s most expensive house?” The Guardian. https://www.theguardian.com/money/2023/apr/08/britain-most-expensive-house-rutland-gate-mansion-london-super-rich-buyer.\n\n\nO’Sullivan, David, and Steven M Manson. 2015. “Do physicists have geography envy? And what can geographers learn from it?” Annals of the Association of American Geographers 105 (4). Taylor & Francis:704–22. https://doi.org/10.1080/00045608.2015.1039105.\n\n\nOpen Data Institute. n.d. “SOD0009 - Evidence on Statistics and Open Data.” https://committees.parliament.uk/writtenevidence/45220/pdf/.\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nReades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian. https://doi.org/10.46430/phen0111.\n\n\nReades, J., H. Yingjie, Emmanouil Tranos, and E. Delmelle. 2025. “The City as Text.” Nature Cities. https://doi.org/https://doi.org/10.1038/s44284-025-00314-x.\n\n\nRose, Gillian. 1997. “Situating Knowledges: Positionality, Reflexivities and Other Tactics.” Progress in Human Geography 21 (3):305–20. https://doi.org/10.1191/030913297673302122.\n\n\nScheider, Simon, Enkhbold Nyamsuren, Han Kruiger, and Haiqi Xu. 2020. “Why Geographic Data Science Is Not a Science.” Geography Compass 14 (11). Wiley Online Library:e12537.\n\n\nSchneider, A. 2013. “How to Become a Data Scientist Before You Graduate.” Berkeley Science Review. http://berkeleysciencereview.com/how-to-become-a-data-scientist-before-you-graduate/.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2P) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSmith, D. 2010. “Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London.” GLA Economics. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nStrauß, Stefan. 2015. “Datafication and the Seductive Power of Uncertainty–a Critical Exploration of Big Data Enthusiasm.” Information 6 (4). MDPI:836–47.\n\n\nSusnjara, S., and I. Smalley. 2025. “What Is Virtualization?” 2025. https://www.ibm.com/think/topics/virtualization.\n\n\nTravers, Tony, Sam Sims, and Nicolas Bosetti. 2016. “Housing and Inequality in London.” Centre for London. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nTufte, Edward R, and Peter R Graves-Morris. 1983. The Visual Display of Quantitative Information. Vol. 2. 9. Graphics press Cheshire, CT.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nVanderPlas, Jake. 2014. “Is Seattle Really Seeing an Uptick in Cycling?” http://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1):160018. https://doi.org/10.1038/sdata.2016.18.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608. https://doi.org/10.1177/0309132520924722.\n\n\nXiao, Ningchuan. 2016. GIS Algorithms: Theory and Applications for Geographic Information Science & Technology. Research Methods. SAGE. https://doi.org/https://dx.doi.org/10.4135/9781473921498.\n\n\nXie, Tessa. 2024a. “How to Better Communicate as a Data Scientist.” Towards Data Science. https://www.divingintodata.com/p/how-to-better-communicate-as-a-data-scientist-6fc5428d3143.\n\n\n———. 2024b. “One Mindset Shift That Will Make You a Better Data Scientist.” Diving Into Data. https://www.divingintodata.com/p/one-mindset-shift-that-will-make-you-a-better-data-scientist-a015f8000ad7.\n\n\n———. 2024c. “The Most Undervalued Skill for Data Scientists.” Towards Data Science. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321.\n\n\nZemanek, H. 1983. “Algorithmic Perfection.” Annals of the History of Computing. AMER FED INFORM PROCESSING SOC.\n\n\nZervas, Georgios, Davide Proserpio, and John W Byers. 2021. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” Marketing Letters 32. Springer:1–16.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500."
  }
]